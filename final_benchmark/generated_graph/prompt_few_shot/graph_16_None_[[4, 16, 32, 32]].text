
    Task: Translate the given CUDA code to its equivalent high-performance CPU C code.
    Context: You are provided with a CUDA code snippet that needs to be translated into CPU C code. The translation should preserve the same functionality as much as possible. Focus on translating the CUDA-specific parallel constructs into constructs supported by the CPU, such as using OpenMP for parallelism. The resulting CPU C code should be complete and ready to compile.

    Example 1:
    Input CUDA Code: extern "C" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {
  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);
}


    Input Tensor Shape: [[17, 12, 7, 9]]
    Output C Code: void default_function_kernel(float* compute, float* data) {
  #pragma omp parallel for
  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1428; ++i0_i1_fused_i2_fused) {
    for (int32_t i3_s = 0; i3_s < 9; ++i3_s) {
      compute[((i0_i1_fused_i2_fused * 9) + i3_s)] = atanf(data[((i0_i1_fused_i2_fused * 9) + i3_s)]);
    }
  }
}


    //|End-of-Code|

    Example 2:
    Input CUDA Code: extern "C" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {
  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);
}

extern "C" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {
  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));
}

extern "C" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {
  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);
}

extern "C" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {
  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);
}


    Input Tensor Shape: [[19, 17, 15]]
    Output C Code: void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {
  #pragma omp parallel for
  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4845; ++i0_i1_fused_i2_fused) {
    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);
  }
  #pragma omp parallel for
  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4845; ++i0_i1_fused_i2_fused_1) {
    compute_1[i0_i1_fused_i2_fused_1] = asinhf(cosf(ph_0[i0_i1_fused_i2_fused_1]));
  }
  #pragma omp parallel for
  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 4845; ++i0_i1_fused_i2_fused_2) {
    compute_2[i0_i1_fused_i2_fused_2] = sinf(ph_0[i0_i1_fused_i2_fused_2]);
  }
  #pragma omp parallel for
  for (int32_t i0 = 0; i0 < 19; ++i0) {
    for (int32_t i1 = 0; i1 < 17; ++i1) {
      for (int32_t i2 = 0; i2 < 15; ++i2) {
        compute_3[(((i0 * 255) + (i1 * 15)) + i2)] = atanf(ph_0[(((i0 * 255) + (i1 * 15)) + i2)]);
      }
    }
  }
}


    //|End-of-Code|

    Now translate the following CUDA code to its equivalent high-performance CPU C code:
    Input CUDA Code: extern "C" __global__ void __launch_bounds__(1024) default_function_kernel_5(float* __restrict__ T_softmax_maxelem, float* __restrict__ resize) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 625) {
    resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) / 625) * 2500) + (((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) % 2500))]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ ph, float* __restrict__ resize) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 625) {
    resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((((ph[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 625) * 1024) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (ph[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 625) * 1024) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (((ph[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 625) * 1024) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (ph[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 625) * 1024) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) * ((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))));
  }
}

extern "C" __global__ void __launch_bounds__(1024) default_function_kernel_6(float* __restrict__ resize) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 625) {
      int v_ = ((int)(floorf(((max(min(resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;
    resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) default_function_kernel_8(float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ resize) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 625) {
    T_softmax_norm[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] / T_softmax_maxelem[(((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) / 625) * 2500) + (((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) % 2500))]);
  }
}

extern "C" __global__ void __launch_bounds__(1024) default_function_kernel_3(float* __restrict__ resize) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 625) {
    resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((0.000000e+00f < resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]) ? resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] : (resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * 5.000000e-01f));
  }
}

extern "C" __global__ void __launch_bounds__(1024) default_function_kernel_4(float* __restrict__ T_softmax_maxelem, float* __restrict__ resize) {
  if (((((int)blockIdx.x) * 64) + (((int)threadIdx.x) >> 4)) < 625) {
    T_softmax_maxelem[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = -3.402823e+38f;
  }
  for (int k = 0; k < 16; ++k) {
    if (((((int)blockIdx.x) * 64) + (((int)threadIdx.x) >> 4)) < 625) {
      T_softmax_maxelem[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], resize[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 625) * 40000) + (k * 2500)) + (((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) % 2500))]);
    }
  }
}

extern "C" __global__ void __launch_bounds__(1024) default_function_kernel_7(float* __restrict__ T_softmax_maxelem, float* __restrict__ resize) {
  if (((((int)blockIdx.x) * 64) + (((int)threadIdx.x) >> 4)) < 625) {
    T_softmax_maxelem[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;
  }
  for (int k = 0; k < 16; ++k) {
    if (((((int)blockIdx.x) * 64) + (((int)threadIdx.x) >> 4)) < 625) {
      T_softmax_maxelem[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (T_softmax_maxelem[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + resize[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 625) * 40000) + (k * 2500)) + (((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) % 2500))]);
    }
  }
}

extern "C" __global__ void __launch_bounds__(1024) default_function_kernel_2(float* __restrict__ resize) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 625) {
    resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] * 7.692308e-02f);
  }
}

extern "C" __global__ void __launch_bounds__(1024) default_function_kernel_1(float* __restrict__ resize) {
  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 625) {
    resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] + 3.000000e+00f);
  }
}


    Input Tensor Shape: [[4, 16, 32, 32]]
    Print only a single C function implementation, ending with the comment '|End-of-Code|'.
    