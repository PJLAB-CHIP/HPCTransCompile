[
    {
        "op_name": "reorg",
        "c_code": "void default_function_kernel(float* A, float* T_reshape) {\n  float tensor[16777216];\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 64; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 128; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 128; ++ax3) {\n          tensor[((((ax0 * 1048576) + (ax1 * 16384)) + (ax2 * 128)) + ax3)] = A[((((((ax0 * 4194304) + ((ax1 & 15) * 65536)) + (ax2 * 512)) + ((ax1 >> 5) * 256)) + (ax3 * 2)) + ((ax1 & 31) >> 4))];\n        }\n      }\n    }\n  }\n  for (int32_t ax0_1 = 0; ax0_1 < 16; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 256; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 64; ++ax2_1) {\n        for (int32_t ax3_1 = 0; ax3_1 < 64; ++ax3_1) {\n          T_reshape[((((ax0_1 * 1048576) + (ax1_1 * 4096)) + (ax2_1 * 64)) + ax3_1)] = tensor[((((ax0_1 * 1048576) + (ax1_1 * 4096)) + (ax2_1 * 64)) + ax3_1)];\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ A, float* __restrict__ T_reshape) {\n  T_reshape[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = A[(((((((((int)blockIdx.x) >> 10) * 4194304) + ((((int)blockIdx.x) & 255) * 4096)) + ((((int)threadIdx.x) >> 7) * 512)) + (((((int)blockIdx.x) & 1023) >> 9) * 256)) + ((((int)threadIdx.x) & 127) * 2)) + ((((int)blockIdx.x) & 511) >> 8))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((16, 64, 128, 128), \"float32\"), T_reshape: T.Buffer((16, 256, 64, 64), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        tensor = T.allocate([16777216], \"float32\", \"global\")\n        tensor_1 = T.Buffer((16777216,), data=tensor)\n        for ax0, ax1, ax2, ax3 in T.grid(16, 64, 128, 128):\n            A_1 = T.Buffer((16777216,), data=A.data)\n            tensor_1[ax0 * 1048576 + ax1 * 16384 + ax2 * 128 + ax3] = A_1[ax0 * 4194304 + ax1 % 16 * 65536 + ax2 * 512 + ax1 // 32 * 256 + ax3 * 2 + ax1 % 32 // 16]\n        for ax0, ax1, ax2, ax3 in T.grid(16, 256, 64, 64):\n            cse_var_1: T.int32 = ax0 * 1048576 + ax1 * 4096 + ax2 * 64 + ax3\n            T_reshape_1 = T.Buffer((16777216,), data=T_reshape.data)\n            T_reshape_1[cse_var_1] = tensor_1[cse_var_1]",
        "op_args": "None",
        "input_shape": "[[16, 64, 128, 128]]",
        "output_shape": "[[16, 256, 64, 64]]",
        "input_name": [
            "A"
        ],
        "output_name": [
            "T_reshape"
        ],
        "deepseek_c": "void default_function_kernel(float* A, float* T_reshape) {\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 16384; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_reshape[ax0_ax1_fused_ax2_fused_ax3_fused] = A[((((((ax0_ax1_fused_ax2_fused_ax3_fused >> 10) * 4194304) + (((ax0_ax1_fused_ax2_fused_ax3_fused & 255) * 4096) + (((ax0_ax1_fused_ax2_fused_ax3_fused & 1023) >> 9) * 256))) + (((ax0_ax1_fused_ax2_fused_ax3_fused & 1023) & 255) >> 8)) + (((ax0_ax1_fused_ax2_fused_ax3_fused & 127) * 2)) + (((ax0_ax1_fused_ax2_fused_ax3_fused & 511) >> 8))))];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* A, float* T_reshape) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 16384; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 256; ++ax3) {\n      T_reshape[((ax0_ax1_fused_ax2_fused * 256) + ax3)] = A[((((((ax0_ax1_fused_ax2_fused >> 10) * 4194304) + ((ax0_ax1_fused_ax2_fused & 1023) * 4096)) + (((ax3 >> 7) * 512) + (((ax0_ax1_fused_ax2_fused & 511) >> 9) * 256))) + ((ax3 & 127) * 2)) + (ax0_ax1_fused_ax2_fused & 511))];\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* A, float* T_reshape) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1048576; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 256; ++ax3) {\n      T_reshape[((ax0_ax1_fused_ax2_fused * 256) + ax3)] = A[(((((ax0_ax1_fused_ax2_fused >> 11) * 4194304) + ((ax0_ax1_fused_ax2_fused & 2047) * 512)) + (ax3 * 2)) + ((ax0_ax1_fused_ax2_fused & 2047) >> 11))];\n    }\n  }\n}"
    },
    {
        "op_name": "scatter_nd",
        "c_code": "void default_function_kernel(float* data, int32_t* indices, float* scatter_nd_generic, float* updates) {\n  for (int32_t i = 0; i < 3120; ++i) {\n    scatter_nd_generic[i] = data[i];\n  }\n  for (int32_t j = 0; j < 3; ++j) {\n    #pragma omp parallel for\n    for (int32_t k = 0; k < 1560; ++k) {\n      scatter_nd_generic[((indices[j] * 1560) + k)] = (scatter_nd_generic[((indices[j] * 1560) + k)] + updates[((j * 1560) + k)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_1(float* __restrict__ atomic_add_return, int* __restrict__ indices, float* __restrict__ scatter_nd_cuda, float* __restrict__ updates) {\n  if (((((int)blockIdx.y) * 128) + (((int)threadIdx.x) >> 3)) < 195) {\n    atomic_add_return[0] = atomicAdd((&(scatter_nd_cuda[(((indices[((int)blockIdx.x)] * 1560) + (((int)blockIdx.y) * 1024)) + ((int)threadIdx.x))])), updates[(((((int)blockIdx.x) * 1560) + (((int)blockIdx.y) * 1024)) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ data, float* __restrict__ scatter_nd_cuda) {\n  if (((((int)blockIdx.x) * 64) + (((int)threadIdx.x) >> 4)) < 195) {\n    scatter_nd_cuda[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 1560), \"float32\"), indices: T.Buffer((1, 3), \"int32\"), updates: T.Buffer((3, 1560), \"float32\"), scatter_nd_generic: T.Buffer((2, 1560), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T.attr(0, \"extern_scope\", 0)\n        scatter_nd_generic_1 = T.Buffer((3120,), data=scatter_nd_generic.data)\n        for i in range(3120):\n            data_1 = T.Buffer((3120,), data=data.data)\n            scatter_nd_generic_1[i] = data_1[i]\n        for j in range(3):\n            for k in T.parallel(1560):\n                indices_1 = T.Buffer((3,), \"int32\", data=indices.data)\n                updates_1 = T.Buffer((4680,), data=updates.data)\n                scatter_nd_generic_1[indices_1[j] * 1560 + k] = scatter_nd_generic_1[indices_1[j] * 1560 + k] + updates_1[j * 1560 + k]",
        "op_args": "None",
        "input_shape": "[[2, 1560], [1, 3], [3, 1560]]",
        "output_shape": "[[2, 1560]]",
        "input_name": [
            "data",
            "indices",
            "updates"
        ],
        "output_name": [
            "scatter_nd.generic"
        ],
        "deepseek_c": "void default_function_kernel(float* data, float* scatter_nd_cuda, float* updates) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3120; ++i0_i1_fused_i2_fused) {\n    scatter_nd_cuda[i0_i1_fused_i2_fused] = data[i0_i1_fused_i2_fused];\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3120; ++i0_i1_fused_i2_fused_1) {\n    float atomic_add_return[1];\n    atomic_add_return[0] = 0.000000e+00f;\n    for (int32_t i0_i1_fused_i2_fused_inner = 0; i0_i1_fused_i2_fused_inner < 3; ++i0_i1_fused_i2_fused_inner) {\n      atomic_add_return[0] = (atomic_add_return[0] + scatter_nd_cuda[((((i0_i1_fused_i2_fused_inner * 1040) + (i0_i1_fused_i2_fused % 1040)) + (i0_i1_fused_i2_fused_inner * 1040)) + (i0_i1_fused_i2_fused % 1040)))]);\n    }\n    scatter_nd_cuda[i0_i1_fused_i2_fused_1] = atomic_add_return[0];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* data, int* indices, float* scatter_nd_cuda, float* updates) {\n  float atomic_add_return[1];\n  for (int32_t i1 = 0; i1 < 2; ++i1) {\n    for (int32_t i2 = 0; i2 < 1560; ++i2) {\n      scatter_nd_cuda[((i1 * 1560) + i2)] = data[((i1 * 1560) + i2)];\n    }\n  }\n  for (int32_t i1_1 = 0; i1_1 < 1; ++i1_1) {\n    for (int32_t i2_1 = 0; i2_1 < 3; ++i2_1) {\n      for (int32_t i3 = 0; i3 < 1560; ++i3) {\n        atomic_add_return[0] = 0.000000e+00f;\n        for (int32_t k = 0; k < 1; ++k) {\n          atomic_add_return[0] = (atomic_add_return[0] + updates[(((k * 3) + i2_1) * 1560) + i3]);\n        }\n        scatter_nd_cuda[(((i1_1 * 4680) + (i2_1 * 1560)) + i3)] = (scatter_nd_cuda[(((i1_1 * 4680) + (i2_1 * 1560)) + i3)] + atomic_add_return[0]);\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* data, float* scatter_nd_cuda, float* updates, int* indices) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3120; ++i0_i1_fused_i2_fused) {\n    scatter_nd_cuda[i0_i1_fused_i2_fused] = data[i0_i1_fused_i2_fused];\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 3120; ++i0_i1_fused) {\n    scatter_nd_cuda[i0_i1_fused] = (scatter_nd_cuda[i0_i1_fused] + updates[i0_i1_fused]);\n  }\n}"
    },
    {
        "op_name": "gather_nd",
        "c_code": "void default_function_kernel(float* A, float* T_gather_nd, float* indices) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 8; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 64; ++ax3) {\n      for (int32_t ax4_outer = 0; ax4_outer < 4; ++ax4_outer) {\n        for (int32_t ax4_inner = 0; ax4_inner < 16; ++ax4_inner) {\n          if (((ax4_outer * 8) + (ax4_inner >> 1)) < 25) {\n            T_gather_nd[((((ax0_ax1_fused_ax2_fused * 3200) + (ax3 * 50)) + (ax4_outer * 16)) + ax4_inner)] = A[(((((((int32_t)indices[ax0_ax1_fused_ax2_fused]) * 204800) + (((int32_t)indices[(ax0_ax1_fused_ax2_fused + 8)]) * 3200)) + (ax3 * 50)) + (ax4_outer * 16)) + ax4_inner)];\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ A, float* __restrict__ T_gather_nd, float* __restrict__ indices) {\n  T_gather_nd[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = A[(((((int)indices[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) / 25)]) * 204800) + (((int)indices[((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) / 25) + 8)]) * 3200)) + (((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) % 3200))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((32, 64, 64, 50), \"float32\"), indices: T.Buffer((2, 2, 2, 2), \"float32\"), T_gather_nd: T.Buffer((2, 2, 2, 64, 50), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(8):\n            for ax3, ax4_outer, ax4_inner in T.grid(64, 4, 16):\n                if T.likely(ax4_outer * 8 + ax4_inner // 2 < 25):\n                    cse_var_2: T.int32 = ax4_outer * 16\n                    cse_var_1: T.int32 = ax3 * 50\n                    T_gather_nd_1 = T.Buffer((25600,), data=T_gather_nd.data)\n                    A_1 = T.Buffer((6553600,), data=A.data)\n                    indices_1 = T.Buffer((16,), data=indices.data)\n                    T_gather_nd_1[ax0_ax1_fused_ax2_fused * 3200 + cse_var_1 + cse_var_2 + ax4_inner] = A_1[T.Cast(\"int32\", indices_1[ax0_ax1_fused_ax2_fused]) * 204800 + T.Cast(\"int32\", indices_1[ax0_ax1_fused_ax2_fused + 8]) * 3200 + cse_var_1 + cse_var_2 + ax4_inner]",
        "op_args": "None",
        "input_shape": "[[32, 64, 64, 50], [2, 2, 2, 2]]",
        "output_shape": "[[2, 2, 2, 64, 50]]",
        "input_name": [
            "A",
            "indices"
        ],
        "output_name": [
            "T_gather_nd"
        ],
        "deepseek_c": "void default_function_kernel(float* A, float* T_gather_nd, float* indices) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 204800; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_gather_nd[ax0_ax1_fused_ax2_fused_ax3_fused] = A[(((((ax0_ax1_fused_ax2_fused_ax3_fused / 3200) * 204800) + (((ax0_ax1_fused_ax2_fused_ax3_fused % 3200) / 8) * 3200)) + (((ax0_ax1_fused_ax2_fused_ax3_fused % 8) * 400) + (((ax0_ax1_fused_ax2_fused_ax3_fused % 3200) % 8) * 50))) + (((ax0_ax1_fused_ax2_fused_ax3_fused / 3200) * 50) + (((ax0_ax1_fused_ax2_fused_ax3_fused % 3200) / 8) * 50)))];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* A, float* T_gather_nd, float* indices) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 128; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 2; ++ax3) {\n        T_gather_nd[(((ax0_ax1_fused * 4) + (ax2 * 2)) + ax3)] = A[(((((int)indices[ax0_ax1_fused]) * 204800) + (((int)indices[((ax0_ax1_fused * 4) + (ax2 * 2))]) * 3200)) + (((ax0_ax1_fused % 2) * 1600) + (ax3 * 800)))];\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* A, float* T_gather_nd, float* indices) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 12288; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 50; ++ax3) {\n      T_gather_nd[((ax0_ax1_fused_ax2_fused * 50) + ax3)] = A[((((((int32_t)indices[((ax0_ax1_fused_ax2_fused / 100) * 2) + 16]) * 204800) + ((int32_t)indices[(((ax0_ax1_fused_ax2_fused / 100) * 2) + 17)] * 3200)) + ((ax0_ax1_fused_ax2_fused % 100) / 50 * 500)) + ((ax0_ax1_fused_ax2_fused % 50) * 2)) + ax3)];\n    }\n  }\n}"
    },
    {
        "op_name": "reshape",
        "c_code": "void default_function_kernel(float* A, float* T_reshape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 128; ++ax0) {\n    for (int32_t ax1_outer = 0; ax1_outer < 20; ++ax1_outer) {\n      for (int32_t ax1_inner = 0; ax1_inner < 16; ++ax1_inner) {\n        T_reshape[(((ax0 * 320) + (ax1_outer * 16)) + ax1_inner)] = A[(((ax0 * 320) + (ax1_outer * 16)) + ax1_inner)];\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ A, float* __restrict__ T_reshape) {\n  T_reshape[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = A[((((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) / 5) * 320) + (((((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) % 160) / 5) * 10)) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((2, 64, 32, 10), \"float32\"), T_reshape: T.Buffer((128, 320), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(128):\n            for ax1_outer, ax1_inner in T.grid(20, 16):\n                cse_var_1: T.int32 = ax0 * 320 + ax1_outer * 16 + ax1_inner\n                T_reshape_1 = T.Buffer((40960,), data=T_reshape.data)\n                A_1 = T.Buffer((40960,), data=A.data)\n                T_reshape_1[cse_var_1] = A_1[cse_var_1]",
        "op_args": "None",
        "input_shape": "[[2, 64, 32, 10]]",
        "output_shape": "[[128, 320]]",
        "input_name": [
            "A"
        ],
        "output_name": [
            "T_reshape"
        ],
        "deepseek_c": "void default_function_kernel(float* A, float* T_transpose) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 20480; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_transpose[(((((((int)blockIdx.x) / 20) * 20480) + ((((int)threadIdx.x) & 63) * 320)) + ((((int)blockIdx.x) % 20) * 16)) + (((int)threadIdx.x) >> 6))] = A[(((((((int)blockIdx.x) / 20) * 20480) + ((((int)threadIdx.x) & 63) * 320)) + ((((int)blockIdx.x) % 20) * 16)) + (((int)threadIdx.x) >> 6))];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* A, float* T_transpose) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1280; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n      T_transpose[((ax0_ax1_fused_ax2_fused * 16) + ax3)] = A[((((ax0_ax1_fused_ax2_fused / 32) * 20480) + ((ax3 * 320) + (ax0_ax1_fused_ax2_fused % 32))) + (ax0_ax1_fused_ax2_fused / 32))];\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* A, float* T_transpose) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1280; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 10; ++ax3) {\n      T_transpose[((ax0_ax1_fused_ax2_fused * 10) + ax3)] = A[((((ax0_ax1_fused_ax2_fused & 127) * 10) + ax3) + (((ax0_ax1_fused_ax2_fused & 192) >> 3) * 16))];\n    }\n  }\n}"
    },
    {
        "op_name": "transpose",
        "c_code": "void default_function_kernel(float* A, float* T_transpose) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 64; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n      for (int32_t ax3_outer = 0; ax3_outer < 4; ++ax3_outer) {\n        for (int32_t ax3_inner = 0; ax3_inner < 16; ++ax3_inner) {\n          T_transpose[((((ax0_ax1_fused * 640) + (ax2 * 64)) + (ax3_outer * 16)) + ax3_inner)] = A[((((((ax0_ax1_fused >> 5) * 20480) + (ax3_outer * 5120)) + (ax3_inner * 320)) + ((ax0_ax1_fused & 31) * 10)) + ax2)];\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ A, float* __restrict__ T_transpose) {\n  T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = A[(((((((int)blockIdx.x) / 20) * 20480) + ((((int)threadIdx.x) & 63) * 320)) + ((((int)blockIdx.x) % 20) * 16)) + (((int)threadIdx.x) >> 6))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((2, 64, 32, 10), \"float32\"), T_transpose: T.Buffer((2, 32, 10, 64), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(64):\n            for ax2, ax3_outer, ax3_inner in T.grid(10, 4, 16):\n                T_transpose_1 = T.Buffer((40960,), data=T_transpose.data)\n                A_1 = T.Buffer((40960,), data=A.data)\n                T_transpose_1[ax0_ax1_fused * 640 + ax2 * 64 + ax3_outer * 16 + ax3_inner] = A_1[ax0_ax1_fused // 32 * 20480 + ax3_outer * 5120 + ax3_inner * 320 + ax0_ax1_fused % 32 * 10 + ax2]",
        "op_args": "None",
        "input_shape": "[[2, 64, 32, 10]]",
        "output_shape": "[[2, 32, 10, 64]]",
        "input_name": [
            "A"
        ],
        "output_name": [
            "T_transpose"
        ]
    },
    {
        "op_name": "batch_to_space_nd",
        "c_code": "void default_function_kernel(float* T_strided_slice, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    float T_transpose[3040];\n    float T_reshape[80];\n    for (int32_t ax1 = 0; ax1 < 19; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        for (int32_t ax1_1 = 0; ax1_1 < 2; ++ax1_1) {\n          for (int32_t ax4 = 0; ax4 < 2; ++ax4) {\n            for (int32_t ax5 = 0; ax5 < 20; ++ax5) {\n              T_reshape[(((ax1_1 * 40) + (ax4 * 20)) + ax5)] = data[((((((ax2 * 4560) + (ax1_1 * 2280)) + (ax0 * 760)) + (ax1 * 40)) + (ax4 * 20)) + ax5)];\n            }\n          }\n        }\n        for (int32_t ax3 = 0; ax3 < 2; ++ax3) {\n          for (int32_t ax4_1 = 0; ax4_1 < 2; ++ax4_1) {\n            for (int32_t ax5_1 = 0; ax5_1 < 20; ++ax5_1) {\n              T_transpose[(((((ax1 * 160) + (ax2 * 80)) + (ax3 * 40)) + (ax4_1 * 20)) + ax5_1)] = T_reshape[(((ax4_1 * 40) + (ax3 * 20)) + ax5_1)];\n            }\n          }\n        }\n      }\n    }\n    for (int32_t ax1_2 = 0; ax1_2 < 38; ++ax1_2) {\n      for (int32_t ax2_1 = 0; ax2_1 < 4; ++ax2_1) {\n        for (int32_t ax3_1 = 0; ax3_1 < 20; ++ax3_1) {\n          T_strided_slice[((((ax0 * 3040) + (ax1_2 * 80)) + (ax2_1 * 20)) + ax3_1)] = T_transpose[(((ax1_2 * 80) + (ax2_1 * 20)) + ax3_1)];\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data) {\n  T_strided_slice[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = data[(((((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 3)) % 20) / 10) * 4560) + (((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 10) / 5) * 2280)) + ((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 3)) / 20) * 40)) + (((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 3)) % 10) / 5) * 20)) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 20))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 19, 2, 20), \"float32\"), T_strided_slice: T.Buffer((3, 38, 4, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(3):\n            T_transpose = T.allocate([3040], \"float32\", \"global\")\n            T_reshape = T.allocate([80], \"float32\", \"global\")\n            for ax1, ax2 in T.grid(19, 2):\n                T_reshape_1 = T.Buffer((80,), data=T_reshape)\n                for ax1_1, ax4, ax5 in T.grid(2, 2, 20):\n                    cse_var_1: T.int32 = ax4 * 20\n                    data_1 = T.Buffer((11400,), data=data.data)\n                    T_reshape_1[ax1_1 * 40 + cse_var_1 + ax5] = data_1[ax2 * 4560 + ax1_1 * 2280 + ax0 * 760 + ax1 * 40 + cse_var_1 + ax5]\n                for ax3, ax4, ax5 in T.grid(2, 2, 20):\n                    T_transpose_1 = T.Buffer((3040,), data=T_transpose)\n                    T_transpose_1[ax1 * 160 + ax2 * 80 + ax3 * 40 + ax4 * 20 + ax5] = T_reshape_1[ax4 * 40 + ax3 * 20 + ax5]\n            for ax1, ax2, ax3 in T.grid(38, 4, 20):\n                cse_var_3: T.int32 = ax1 * 80\n                cse_var_2: T.int32 = ax2 * 20\n                T_strided_slice_1 = T.Buffer((9120,), data=T_strided_slice.data)\n                T_transpose_1 = T.Buffer((3040,), data=T_transpose)\n                T_strided_slice_1[ax0 * 3040 + cse_var_3 + cse_var_2 + ax3] = T_transpose_1[cse_var_3 + cse_var_2 + ax3]",
        "op_args": [
            15,
            19,
            2,
            20
        ],
        "input_shape": "[[15, 19, 2, 20]]",
        "output_shape": "[[3, 38, 4, 20]]",
        "deepseek_c": "void default_function_kernel(float* T_strided_slice, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 570; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n      T_strided_slice[((ax0_ax1_fused_ax2_fused * 20) + ax3)] = data[((((((ax0_ax1_fused_ax2_fused / 10) * 4560) + (((ax0_ax1_fused_ax2_fused % 10) / 5) * 2280)) + (((ax0_ax1_fused_ax2_fused / 20) * 40)) + (((ax0_ax1_fused_ax2_fused % 10) / 5) * 20)) + (((ax0_ax1_fused_ax2_fused % 20) * 20)) + ax3))];\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* T_strided_slice, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 15; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n      T_strided_slice[((ax0_ax1_fused_ax2_fused * 20) + ax3)] = data[((((((ax0_ax1_fused_ax2_fused / 10) * 4560) + (((ax0_ax1_fused_ax2_fused % 10) / 5) * 2280)) + ((ax3 / 10) * 40)) + (((ax0_ax1_fused_ax2_fused % 10) / 5) * 20)) + (ax3 % 20))];\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_strided_slice, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 140; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n      T_strided_slice[((ax0_ax1_fused_ax2_fused * 20) + ax3)] = data[(((((((ax0_ax1_fused_ax2_fused % 10) / 5) * 4560) + (((ax0_ax1_fused_ax2_fused % 20) / 10) * 2280)) + ((ax0_ax1_fused_ax2_fused / 20) * 40)) + ((ax0_ax1_fused_ax2_fused % 5) * 20)) + ax3)];\n    }\n  }\n}"
    },
    {
        "op_name": "dilate",
        "c_code": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2464; ++i0_i1_fused_i2_fused_i3_fused) {\n    DilatedInput[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(44) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data) {\n  DilatedInput[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 11, 16, 7), \"float32\"), DilatedInput: T.Buffer((2, 11, 16, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(2464):\n            DilatedInput_1 = T.Buffer((2464,), data=DilatedInput.data)\n            data_1 = T.Buffer((2464,), data=data.data)\n            DilatedInput_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused]",
        "op_args": [
            2,
            11,
            16,
            7
        ],
        "input_shape": "[[2, 11, 16, 7]]",
        "output_shape": "[[2, 11, 16, 7]]",
        "deepseek_c": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2464; ++i0_i1_fused_i2_fused_i3_fused) {\n    DilatedInput[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2464; ++i0_i1_fused_i2_fused_i3_fused) {\n    DilatedInput[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}",
        "codellama_c": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1728; ++i0_i1_fused_i2_fused_i3_fused) {\n    DilatedInput[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}"
    },
    {
        "op_name": "flatten",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i = 0; i < 17; ++i) {\n    for (int32_t j = 0; j < 270; ++j) {\n      compute[((i * 270) + j)] = data[((i * 270) + j)];\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 3, 9, 10), \"float32\"), compute: T.Buffer((17, 270), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i in T.parallel(17):\n            for j in range(270):\n                cse_var_1: T.int32 = i * 270 + j\n                compute_1 = T.Buffer((4590,), data=compute.data)\n                data_1 = T.Buffer((4590,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1]",
        "op_args": [
            17,
            3,
            9,
            10
        ],
        "input_shape": "[[17, 3, 9, 10]]",
        "output_shape": "[[17, 270]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          compute[((((i0 * 270) + (i1 * 90)) + (i2 * 10)) + i3)] = data[((((i0 * 270) + (i1 * 90)) + (i2 * 10)) + i3)];\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 4680; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1780; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}"
    },
    {
        "op_name": "depth_to_space",
        "c_code": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 504; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 6; ++i3) {\n      depth_to_space[((i0_i1_fused_i2_fused * 6) + i3)] = data[((((((i0_i1_fused_i2_fused / 36) * 270) + (((i0_i1_fused_i2_fused % 36) % 2) * 108)) + ((i3 % 2) * 54)) + (((i0_i1_fused_i2_fused % 36) / 2) * 3)) + (i3 / 2))];\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) >> 2) * 270) + (((((((int)blockIdx.x) & 3) * 9) + (((int)threadIdx.x) / 6)) % 2) * 108)) + (((((int)threadIdx.x) % 6) % 2) * 54)) + (((((((int)blockIdx.x) & 3) * 9) + (((int)threadIdx.x) / 6)) / 2) * 3)) + ((((int)threadIdx.x) % 6) / 2))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 5, 18, 3), \"float32\"), depth_to_space: T.Buffer((14, 1, 36, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(504):\n            for i3 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused % 36\n                depth_to_space_1 = T.Buffer((3024,), data=depth_to_space.data)\n                data_1 = T.Buffer((3780,), data=data.data)\n                depth_to_space_1[i0_i1_fused_i2_fused * 6 + i3] = data_1[i0_i1_fused_i2_fused // 36 * 270 + T.truncmod(cse_var_1, 2) * 108 + T.truncmod(i3, 2) * 54 + T.Div(cse_var_1, 2) * 3 + T.Div(i3, 2)]",
        "op_args": [
            14,
            5,
            18,
            3
        ],
        "input_shape": "[[14, 5, 18, 3]]",
        "output_shape": "[[14, 1, 36, 6]]",
        "deepseek_c": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        for (int32_t i3 = 0; i3 < 3; ++i3) {\n          depth_to_space[((((i0 * 54) + (i1 * 18)) + (i2 * 3)) + i3)] = data[((((((i0 * 270) + ((i1 * 54) + ((i2 % 2) * 108))) + ((i3 % 2) * 54)) + (((i2 / 2) * 3) + (i3 / 2))) + (((((i0 >> 2) * 9) + (i1 * 9)) + (i2 / 2)) % 2) * 108))];\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1890; ++i0_i1_fused_i2_fused_i3_fused) {\n    depth_to_space[i0_i1_fused_i2_fused_i3_fused] = data[(((((((i0_i1_fused_i2_fused_i3_fused / 108) * 270) + ((((i0_i1_fused_i2_fused_i3_fused % 108) / 9) % 2) * 108)) + ((((i0_i1_fused_i2_fused_i3_fused % 9) % 2) * 54)) + ((((i0_i1_fused_i2_fused_i3_fused % 108) / 9) / 2) * 3)) + (((i0_i1_fused_i2_fused_i3_fused % 9) / 2))) + (((i0_i1_fused_i2_fused_i3_fused % 108) / 18) * 162)) + (((i0_i1_fused_i2_fused_i3_fused % 9) % 2) * 81))];\n  }\n}",
        "codellama_c": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1080; ++i0_i1_fused_i2_fused_i3_fused) {\n    depth_to_space[i0_i1_fused_i2_fused_i3_fused] = data[((((((i0_i1_fused_i2_fused_i3_fused % 270) / 108) * 270) + ((i0_i1_fused_i2_fused_i3_fused % 108) % 2) * 108)) + ((i0_i1_fused_i2_fused_i3_fused % 108) / 2) * 3) + ((i0_i1_fused_i2_fused_i3_fused % 270) / 270))];\n  }\n}"
    },
    {
        "op_name": "flip",
        "c_code": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 8; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 4; ++ax3) {\n        T_reverse_sequence[(((ax0_ax1_fused * 52) + (ax2 * 4)) + ax3)] = data[((((((ax0_ax1_fused & 1) * 52) + (ax2 * 4)) + ax3) + 312) - ((ax0_ax1_fused >> 1) * 104))];\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(13) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 13) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) & 7) * 13) + ((int)threadIdx.x)) + 312) - ((((int)blockIdx.x) >> 3) * 104))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 2, 13, 4), \"float32\"), T_reverse_sequence: T.Buffer((4, 2, 13, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3 in T.grid(13, 4):\n                cse_var_1: T.int32 = ax2 * 4\n                T_reverse_sequence_1 = T.Buffer((416,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((416,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused * 52 + cse_var_1 + ax3] = data_1[ax0_ax1_fused % 2 * 52 + cse_var_1 + ax3 + 312 - ax0_ax1_fused // 2 * 104]",
        "op_args": [
            4,
            2,
            13,
            4
        ],
        "input_shape": "[[4, 2, 13, 4]]",
        "output_shape": "[[4, 2, 13, 4]]",
        "deepseek_c": "#pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_outer_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner",
        "starcoder_c": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 104; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 4; ++ax3) {\n      T_reverse_sequence[((ax0_ax1_fused_ax2_fused * 4) + ax3)] = data[((((ax0_ax1_fused_ax2_fused % 13) * 4) + ax3) + 312) - ((ax0_ax1_fused_ax2_fused / 13) * 104));\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 112; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 4; ++ax3) {\n      T_reverse_sequence[((ax0_ax1_fused_ax2_fused * 4) + ax3)] = data[((((ax0_ax1_fused_ax2_fused % 28) * 4) + ax3) + 312)];\n    }\n  }\n}"
    },
    {
        "op_name": "mirror_pad",
        "c_code": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      MirrorPadInput[((i0 * 9) + i1)] = data[((((3 <= i0) ? (4 - i0) : ((i0 < 1) ? (0 - i0) : (i0 - 1))) * 6) + ((i1 == 8) ? (13 - i1) : ((i1 < 2) ? (1 - i1) : (i1 - 2))))];\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(11) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) < 45) {\n    MirrorPadInput[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))] = data[((((27 <= ((((int)blockIdx.x) * 11) + ((int)threadIdx.x))) ? (4 - (((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) / 9)) : ((((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) < 9) ? 0 : ((((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) / 9) - 1))) * 6) + (((((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 9) == 8) ? (13 - (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 9)) : (((((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 9) < 2) ? (1 - (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 9)) : ((((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 9) - 2))))];\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 6), \"float32\"), MirrorPadInput: T.Buffer((5, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(5):\n            for i1 in range(9):\n                MirrorPadInput_1 = T.Buffer((45,), data=MirrorPadInput.data)\n                data_1 = T.Buffer((12,), data=data.data)\n                MirrorPadInput_1[i0 * 9 + i1] = data_1[T.if_then_else(3 <= i0, 4 - i0, T.if_then_else(i0 < 1, 0 - i0, i0 - 1)) * 6 + T.if_then_else(i1 == 8, 13 - i1, T.if_then_else(i1 < 2, 1 - i1, i1 - 2))]",
        "op_args": [
            14,
            16,
            2,
            6
        ],
        "input_shape": "[[2, 6]]",
        "output_shape": "[[5, 9]]",
        "deepseek_c": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      MirrorPadInput[((i0 * 6) + i1)] = data[((((27 <= ((i0 * 6) + i1)) ? (4 - ((i1 / 9))) : ((((i0 * 6) + i1) < 9) ? 0 : (((i1 / 9) - 1) * 6) + (((((i0 * 2) + i1) % 9) == 8) ? (13 - ((i1 % 9))) : (((((i0 * 2) + i1) % 9) < 2) ? (1 - ((i1 % 9))) : (((i1 % 9) - 2)))))) * 6) + (((((i0 * 2) + i1) % 9) == 8) ? (13 - ((i1 % 9))) : (((((i0 * 2) + i1) % 9) < 2) ? (1 - ((i1 % 9))) : (((i1 % 9) - 2))))))];\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 45; ++i0_i1_fused) {\n    MirrorPadInput[i0_i1_fused] = data[((((27 <= i0_i1_fused)? (4 - (i0_i1_fused / 9)) : ((i0_i1_fused < 9)? 0 : ((i0_i1_fused / 9) - 1))) * 6) + (((i0_i1_fused % 9) == 8)? (13 - (i0_i1_fused % 9)) : (((i0_i1_fused % 9) < 2)? (1 - (i0_i1_fused % 9)) : ((i0_i1_fused % 9) - 2))))];\n  }\n}",
        "codellama_c": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 45; ++i0_i1_fused_i2_fused) {\n    MirrorPadInput[i0_i1_fused_i2_fused] = data[((((27 <= i0_i1_fused_i2_fused) ? (4 - (i0_i1_fused_i2_fused / 9)) : ((i0_i1_fused_i2_fused < 9) ? 0 : (i0_i1_fused_i2_fused / 9) - 1))) * 6) + ((((i0_i1_fused_i2_fused % 9) == 8)) ? (13 - (i0_i1_fused_i2_fused % 9)) : (((i0_i1_fused_i2_fused % 9) < 2)) ? (1 - (i0_i1_fused_i2_fused % 9)) : ((i0_i1_fused_i2_fused % 9) - 2)))];\n  }\n}"
    },
    {
        "op_name": "scale_shift_nchw",
        "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused < 1482; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused) {\n    for (int32_t b_inner = 0; b_inner < 5; ++b_inner) {\n      for (int32_t i_inner = 0; i_inner < 6; ++i_inner) {\n        ScaleShift[((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused / 741) * 22230) + (b_inner * 4446)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 741) / 19) * 114)) + (i_inner * 19)) + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 19))] = ((data[((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused / 741) * 22230) + (b_inner * 4446)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 741) / 19) * 114)) + (i_inner * 19)) + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 19))] * Scale[((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 741) / 57)]) + Shift[((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 741) / 57)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 11115) {\n    ScaleShift[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) % 2223) / 171)]) + Shift[((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) % 2223) / 171)]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 13, 18, 19), \"float32\"), Scale: T.Buffer((13,), \"float32\"), Shift: T.Buffer((13,), \"float32\"), ScaleShift: T.Buffer((10, 13, 18, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused in T.parallel(1482):\n            for b_inner, i_inner in T.grid(5, 6):\n                cse_var_3: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 741\n                cse_var_2: T.int32 = cse_var_3 // 57\n                cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused // 741 * 22230 + b_inner * 4446 + cse_var_3 // 19 * 114 + i_inner * 19 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 19\n                ScaleShift_1 = T.Buffer((44460,), data=ScaleShift.data)\n                data_1 = T.Buffer((44460,), data=data.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale[cse_var_2] + Shift[cse_var_2]",
        "op_args": [
            10,
            13,
            18,
            19
        ],
        "input_shape": "[[10, 13, 18, 19], [13], [13]]",
        "output_shape": "[[10, 13, 18, 19]]",
        "deepseek_c": "void default_function_kernel(float* Scale, float* Shift, float* data, float* ScaleShift) {\n  #pragma omp parallel for\n  for (int32_t b = 0; b < 10; ++b) {\n    for (int32_t i = 0; i < 13; ++i) {\n      for (int32_t j = 0; j < 18; ++j) {\n        for (int32_t k = 0; k < 19; ++k) {\n          ScaleShift[((((b * 4194) + (i * 312)) + (j * 17)) + k)] = ((data[((((b * 4194) + (i * 312)) + (j * 17)) + k)] * Scale[i]) + Shift[i]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "#include <stdio.h>\n\nvoid default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_inner_fused_j_inner_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_inner_fused_j_inner_inner_fused < 11115; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_inner_fused_j_inner_inner_fused) {\n    for (int32_t j_inner = 0; j_inner < 19; ++j_inner) {\n      ScaleShift[(((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_inner_fused_j_inner_inner_fused / 11115) * 323) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_inner_fused_j_inner_inner_fused % 11115)) * 19) + j_inner)] = ((data[(((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_inner_fused_j_inner_inner_fused / 11115) * 323) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_inner_fused_j_inner_inner_fused % 11115)) * 19) + j_inner)] * Scale[((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_",
        "codellama_c": "#pragma omp parallel for\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused < 130; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused) {\n    for (int32_t j_inner = 0; j_inner < 19; ++j_inner) {\n      ScaleShift[((((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused % 13) * 324) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused / 13) * 26)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused % 13) * 19)) + j_inner)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused / 13) * 26)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused % 13) * 19))) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused"
    },
    {
        "op_name": "scale_shift_nchwc",
        "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused < 196; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused) {\n    for (int32_t cb_outer_inner = 0; cb_outer_inner < 7; ++cb_outer_inner) {\n      for (int32_t i_inner = 0; i_inner < 16; ++i_inner) {\n        ScaleShift[((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused / 28) * 3136) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused & 1) * 1568)) + (i_inner * 98)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 28) >> 1) * 7)) + cb_outer_inner)] = ((data[((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused / 28) * 3136) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused & 1) * 1568)) + (i_inner * 98)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 28) >> 1) * 7)) + cb_outer_inner)] * Scale[(((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused & 1) * 7) + cb_outer_inner)]) + Shift[(((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused & 1) * 7) + cb_outer_inner)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[((((((((int)blockIdx.x) % 49) * 2) + (((int)threadIdx.x) >> 5)) / 49) * 7) + ((((int)blockIdx.x) + ((int)threadIdx.x)) % 7))]) + Shift[((((((((int)blockIdx.x) % 49) * 2) + (((int)threadIdx.x) >> 5)) / 49) * 7) + ((((int)blockIdx.x) + ((int)threadIdx.x)) % 7))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 2, 16, 14, 7), \"float32\"), Scale: T.Buffer((2, 7), \"float32\"), Shift: T.Buffer((2, 7), \"float32\"), ScaleShift: T.Buffer((7, 2, 16, 14, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused in T.parallel(196):\n            for cb_outer_inner, i_inner in T.grid(7, 16):\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 2\n                cse_var_2: T.int32 = cse_var_3 * 7 + cb_outer_inner\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused // 28 * 3136 + cse_var_3 * 1568 + i_inner * 98 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 28 // 2 * 7 + cb_outer_inner\n                ScaleShift_1 = T.Buffer((21952,), data=ScaleShift.data)\n                data_1 = T.Buffer((21952,), data=data.data)\n                Scale_1 = T.Buffer((14,), data=Scale.data)\n                Shift_1 = T.Buffer((14,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]",
        "op_args": [
            7,
            7,
            16,
            14
        ],
        "input_shape": "[[7, 2, 16, 14, 7], [2, 7], [2, 7]]",
        "output_shape": "[[7, 2, 16, 14, 7]]",
        "deepseek_c": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_outer_outer_fused = 0; b_outer_outer_outer_outer_outer_fused < 16; ++b_outer_outer_outer_outer_outer_fused) {\n    for (int32_t b_outer_outer_outer_outer_inner_fused = 0; b_outer_outer_outer_outer_inner_fused < 14; ++b_outer_outer_outer_outer_inner_fused) {\n      for (int32_t b_outer_outer_inner_fused = 0; b_outer_outer_inner_fused < 2; ++b_outer_outer_inner_fused) {\n        for (int32_t b_outer_fused = 0; b_outer_fused < 7; ++b_outer_fused) {\n          for (int32_t b_inner_fused = 0; b_inner_fused < 64; ++b_inner_fused) {\n            ScaleShift[((((((((b_outer_fused * 128) + (b_outer_outer_inner_fused * 64)) + (b_outer_outer_outer_outer_inner_fused * 32)) + (b_outer_outer_outer_outer_outer_fused * 2)) + b_outer_outer_inner_fused) * 64) + b_inner_fused) + (((((b_outer_outer_outer_outer_inner_fused * 2) + (((b_outer_outer_inner_fused * 2) + b_outer_outer_outer_outer_outer_fused) % 2)) / 2) * 4) + (b_inner_fused / 16)))] = ((data[((((((((b_outer_fused * 128) + (b_outer_outer_inner_fused * 64)) + (b_outer_outer_outer_outer_inner_fused * 32)) + (b_outer_outer_outer_outer_outer_fused * 2)) + b_outer_outer_inner_fused) * 64) + b_inner_fused) + (((((b_outer_outer_outer_outer_inner_fused * 2) + (((b_outer_outer_inner_fused * 2) + b_outer_outer_outer_outer_outer_fused) % 2)) / 2) * 4) + (b_inner_fused / 16)))] * Scale[((((((((b_outer_outer_outer_outer_inner_fused * 2) + (((b_outer_outer_inner_fused * 2) + b_outer_outer_outer_outer_outer_fused) % 2)) / 2) * 7) + (((((b_outer_outer_outer_outer_inner_fused * 2) + (((b_outer_outer_inner_fused * 2) + b_outer_outer_outer_outer_outer_fused) % 2)) / 2) * 4) + (b_inner_fused / 16))) * 7) + (((((b_outer_outer_outer_outer_inner_fused * 2) + (((b_outer_outer_inner_fused * 2) + b_outer_outer_outer_outer_outer_fused) % 2)) / 2) * 4) + (b_inner_fused / 16)))])) + Shift[((((((((b_outer_outer_outer_outer_inner_fused * 2) + (((b_outer_outer_inner_fused * 2) + b_outer_outer_outer_outer_outer_fused) % 2)) / 2) * 7) + (((((b_outer_outer_outer_outer_inner_fused * 2) + (((b_outer_outer_inner_fused * 2) + b_outer_outer_outer_outer_outer_fused) % 2)) / 2) * 4) + (b_inner_fused / 16))) * 7) + (((((b_outer_outer_outer_outer_inner_fused * 2) + (((b_outer_outer_inner_fused * 2) + b_outer_outer_outer_outer_outer_fused) % 2)) / 2) * 4) + (b_inner_fused / 16)))]);\n          }\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "#include <stdio.h>\n\nvoid default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cc_inner_fused_i_inner_fused_j_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cc_inner_fused_i_inner_fused_j_inner_fused < 1568; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cc_inner_fused_i_inner_fused_j_inner_fused) {\n    for (int32_t b_inner = 0; b_inner < 2; ++b_inner) {\n      for (int32_t cc_inner = 0; cc_inner < 2; ++cc_inner) {\n        ScaleShift[((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cc_inner_fused_i_inner_fused_j_inner_fused >> 10) * 112) + (cc_inner * 56)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cc_inner_fused_i_inner_fused_j_inner_fused & 511) >> 5) * 2)) + (b_inner * 2)) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cc_inner_fused_i_inner_fused_j_inner_fused & 31))] = ((data[((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cc_inner_fused_i_inner_fused_j_inner_fused >> 10) * 112) + (cc_inner * 56)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cc_inner_fused_i_inner_fused_j_inner_",
        "codellama_c": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused_i4_fused = 0; i0_i1_fused_i2_fused_i3_fused_i4_fused < 1248; ++i0_i1_fused_i2_fused_i3_fused_i4_fused) {\n    ScaleShift[i0_i1_fused_i2_fused_i3_fused_i4_fused] = ((data[i0_i1_fused_i2_fused_i3_fused_i4_fused] * Scale[((i0_i1_fused_i2_fused_i3_fused_i4_fused % 14) * 2) + (i0_i1_fused_i2_fused_i3_fused_i4_fused / 14)]) + Shift[((i0_i1_fused_i2_fused_i3_fused_i4_fused % 14) * 2) + (i0_i1_fused_i2_fused_i3_fused_i4_fused / 14)]);\n  }\n}"
    },
    {
        "op_name": "space_to_depth",
        "c_code": "void default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 648; ++i0_i1_fused_i2_fused_i3_fused) {\n    space_to_depth[i0_i1_fused_i2_fused_i3_fused] = data[((((((i0_i1_fused_i2_fused_i3_fused / 36) * 36) + ((((i0_i1_fused_i2_fused_i3_fused % 36) / 3) % 3) * 12)) + ((((i0_i1_fused_i2_fused_i3_fused % 36) / 3) / 6) * 6)) + ((i0_i1_fused_i2_fused_i3_fused % 3) * 2)) + ((((i0_i1_fused_i2_fused_i3_fused % 36) / 3) % 6) / 3))];\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = data[((((((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 2)) / 9) * 36) + ((((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 36) / 3) % 3) * 12)) + ((((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 36) / 3) / 6) * 6)) + ((((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 3) * 2)) + ((((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 36) / 3) % 6) / 3))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 3, 2, 6), \"float32\"), space_to_depth: T.Buffer((18, 12, 1, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(648):\n            cse_var_1: T.int32 = i0_i1_fused_i2_fused_i3_fused % 36 // 3\n            space_to_depth_1 = T.Buffer((648,), data=space_to_depth.data)\n            data_1 = T.Buffer((648,), data=data.data)\n            space_to_depth_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused // 36 * 36 + T.truncmod(cse_var_1, 3) * 12 + T.Div(cse_var_1, 6) * 6 + i0_i1_fused_i2_fused_i3_fused % 3 * 2 + T.Div(T.truncmod(cse_var_1, 6), 3)]",
        "op_args": [
            18,
            3,
            1,
            3
        ],
        "input_shape": "[[18, 3, 2, 6]]",
        "output_shape": "[[18, 12, 1, 3]]",
        "deepseek_c": "void default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        for (int32_t i3 = 0; i3 < 2; ++i3) {\n          space_to_depth[((((i0 * 36) + (i1 * 12)) + (i2 * 2)) + i3)] = data[((((((i0 / 9) * 36) + ((i1 % 3) * 12)) + ((i2 % 3) * 6)) + (i3 % 2)) + (((((i0 / 9) * 3) + (i1 / 3)) % 3) * 2))];\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 216; ++i0_i1_fused_i2_fused_i3_fused) {\n    space_to_depth[i0_i1_fused_i2_fused_i3_fused] = data[((((((i0_i1_fused_i2_fused_i3_fused / 36) * 36) + (((i0_i1_fused_i2_fused_i3_fused % 36) / 3) % 3) * 12)) + (((i0_i1_fused_i2_fused_i3_fused % 36) / 3) / 6) * 6) + (((i0_i1_fused_i2_fused_i3_fused % 36) / 3) % 6)) + (i0_i1_fused_i2_fused_i3_fused % 3))];\n  }\n}",
        "codellama_c": "void default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 126; ++i0_i1_fused_i2_fused_i3_fused) {\n    space_to_depth[i0_i1_fused_i2_fused_i3_fused] = data[((((i0_i1_fused_i2_fused_i3_fused / 36) * 36) + ((i0_i1_fused_i2_fused_i3_fused % 36) / 3) * 6)) + ((i0_i1_fused_i2_fused_i3_fused % 36) % 3) * 2)];\n  }\n}"
    },
    {
        "op_name": "strided_slice",
        "c_code": "void default_function_kernel(float* T_strided_slice, float* a) {\n  for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      T_strided_slice[((ax1 * 7) + ax2)] = a[(((ax1 * 15) + ax2) + 183)];\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  T_strided_slice[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = a[(((((int)blockIdx.x) * 15) + ((int)threadIdx.x)) + 183)];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((2, 10, 15), \"float32\"), T_strided_slice: T.Buffer((1, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax1, ax2 in T.grid(5, 7):\n            T_strided_slice_1 = T.Buffer((35,), data=T_strided_slice.data)\n            a_1 = T.Buffer((300,), data=a.data)\n            T_strided_slice_1[ax1 * 7 + ax2] = a_1[ax1 * 15 + ax2 + 183]",
        "op_args": [
            1,
            2,
            10,
            15
        ],
        "input_shape": "[[2, 10, 15]]",
        "output_shape": "[[1, 5, 7]]",
        "deepseek_c": "void default_function_kernel(float* T_strided_slice, float* a) {\n  for (int i0 = 0; i0 < 2; ++i0) {\n    for (int i1 = 0; i1 < 10; ++i1) {\n      for (int i2 = 0; i2 < 7; ++i2) {\n        T_strided_slice[((((i0 * 105) + (i1 * 7)) + i2) + 183)] = a[((((i0 * 105) + (i1 * 15)) + i2) + 183)];\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 20; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      T_strided_slice[((ax0_ax1_fused * 7) + ax2)] = a[(((((ax0_ax1_fused / 10) * 15) + (ax2 * 2)) + 183) + (ax0_ax1_fused % 10))];\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 300; ++ax0_ax1_fused_ax2_fused) {\n    T_strided_slice[ax0_ax1_fused_ax2_fused] = a[((ax0_ax1_fused_ax2_fused % 15) + 183)];\n  }\n}"
    },
    {
        "op_name": "unpack_NCHWc_to_nchw",
        "c_code": "void default_function_kernel(float* output_unpack, float* packed_out) {\n  #pragma omp parallel for\n  for (int32_t n_c_fused_h_fused = 0; n_c_fused_h_fused < 504; ++n_c_fused_h_fused) {\n    for (int32_t w = 0; w < 8; ++w) {\n      output_unpack[((n_c_fused_h_fused * 8) + w)] = packed_out[(((((n_c_fused_h_fused / 24) * 192) + ((n_c_fused_h_fused % 12) * 16)) + (w * 2)) + ((n_c_fused_h_fused % 24) / 12))];\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out) {\n  output_unpack[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = packed_out[(((((((int)blockIdx.x) / 3) * 192) + ((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) % 12) * 16)) + ((((int)threadIdx.x) & 7) * 2)) + ((((((int)blockIdx.x) % 3) * 2) + (((int)threadIdx.x) >> 5)) / 3))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(packed_out: T.Buffer((3, 7, 12, 8, 2), \"float32\"), output_unpack: T.Buffer((3, 14, 12, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for n_c_fused_h_fused in T.parallel(504):\n            for w in range(8):\n                output_unpack_1 = T.Buffer((4032,), data=output_unpack.data)\n                packed_out_1 = T.Buffer((4032,), data=packed_out.data)\n                output_unpack_1[n_c_fused_h_fused * 8 + w] = packed_out_1[n_c_fused_h_fused // 24 * 192 + n_c_fused_h_fused % 12 * 16 + w * 2 + n_c_fused_h_fused % 24 // 12]",
        "op_args": [
            3,
            7,
            12,
            8
        ],
        "input_shape": "[[3, 7, 12, 8, 2]]",
        "output_shape": "[[3, 14, 12, 8]]",
        "deepseek_c": "void default_function_kernel(float* output_unpack, float* packed_out) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused_i4_fused = 0; i0_i1_fused_i2_fused_i3_fused_i4_fused < 12096; ++i0_i1_fused_i2_fused_i3_fused_i4_fused) {\n    output_unpack[i0_i1_fused_i2_fused_i3_fused_i4_fused] = packed_out[((((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 192) * 192) + (((i0_i1_fused_i2_fused_i3_fused_i4_fused % 192) / 16) * 16)) + (((i0_i1_fused_i2_fused_i3_fused_i4_fused % 16) * 2) + (((i0_i1_fused_i2_fused_i3_fused_i4_fused % 3) * 2) + ((i0_i1_fused_i2_fused_i3_fused_i4_fused / 12) / 3))))];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* output_unpack, float* packed_out) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 252; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      for (int32_t ax4 = 0; ax4 < 2; ++ax4) {\n        output_unpack[(((ax0_ax1_fused_ax2_fused * 16) + (ax3 * 2)) + ax4)] = packed_out[((((((ax0_ax1_fused_ax2_fused / 12) * 192) + ((ax0_ax1_fused_ax2_fused % 12) * 16)) + (ax3 * 2)) + ax4) + (((ax0_ax1_fused_ax2_fused % 12) / 6) * 128))];\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* output_unpack, float* packed_out) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused_i4_fused = 0; i0_i1_fused_i2_fused_i3_fused_i4_fused < 252; ++i0_i1_fused_i2_fused_i3_fused_i4_fused) {\n    output_unpack[i0_i1_fused_i2_fused_i3_fused_i4_fused] = packed_out[((((i0_i1_fused_i2_fused_i3_fused_i4_fused % 192) / 16) * 192) + ((i0_i1_fused_i2_fused_i3_fused_i4_fused % 16) * 2)) + (i0_i1_fused_i2_fused_i3_fused_i4_fused / 192)];\n  }\n}"
    }
]