[
    {
        "op_name": "conv2d_opt",
        "c_code": "void default_function_kernel(float* data, float* kernel, float* output_unpack) {\n  float data_vec[4194304];\n  float kernel_vec[18432];\n  #pragma omp parallel for\n  for (int32_t bs_c_fused_h_fused = 0; bs_c_fused_h_fused < 4096; ++bs_c_fused_h_fused) {\n    for (int32_t w = 0; w < 256; ++w) {\n      for (int32_t vc = 0; vc < 4; ++vc) {\n        data_vec[(((bs_c_fused_h_fused * 1024) + (w * 4)) + vc)] = data[(((((bs_c_fused_h_fused >> 8) * 262144) + (vc * 65536)) + ((bs_c_fused_h_fused & 255) * 256)) + w)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t occ_k_h_fused = 0; occ_k_h_fused < 48; ++occ_k_h_fused) {\n    for (int32_t icc = 0; icc < 8; ++icc) {\n      for (int32_t k_w = 0; k_w < 3; ++k_w) {\n        for (int32_t icb = 0; icb < 4; ++icb) {\n          for (int32_t ocb = 0; ocb < 4; ++ocb) {\n            kernel_vec[(((((((occ_k_h_fused / 3) * 1152) + (icc * 144)) + ((occ_k_h_fused % 3) * 48)) + (k_w * 16)) + (icb * 4)) + ocb)] = kernel[(((((((occ_k_h_fused / 3) * 1152) + (ocb * 288)) + (icc * 36)) + (icb * 9)) + ((occ_k_h_fused % 3) * 3)) + k_w)];\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t n_c_outer_fused_h_fused = 0; n_c_outer_fused_h_fused < 8128; ++n_c_outer_fused_h_fused) {\n    float conv2d_NCHWc[1016];\n    float conv2d_NCHWc_global[8];\n    for (int32_t ow_outer = 0; ow_outer < 127; ++ow_outer) {\n      for (int32_t oc_block_c_init = 0; oc_block_c_init < 4; ++oc_block_c_init) {\n        conv2d_NCHWc_global[oc_block_c_init] = 0.000000e+00f;\n      }\n      for (int32_t oc_block_c_init_1 = 0; oc_block_c_init_1 < 4; ++oc_block_c_init_1) {\n        conv2d_NCHWc_global[(oc_block_c_init_1 + 4)] = 0.000000e+00f;\n      }\n      for (int32_t ic_outer = 0; ic_outer < 8; ++ic_outer) {\n        for (int32_t kh = 0; kh < 3; ++kh) {\n          for (int32_t kw = 0; kw < 3; ++kw) {\n            for (int32_t ic_inner = 0; ic_inner < 4; ++ic_inner) {\n              for (int32_t oc_block_c = 0; oc_block_c < 4; ++oc_block_c) {\n                conv2d_NCHWc_global[oc_block_c] = (conv2d_NCHWc_global[oc_block_c] + (data_vec[((((((((n_c_outer_fused_h_fused / 4064) * 2097152) + (ic_outer * 262144)) + (kh * 1024)) + ((n_c_outer_fused_h_fused % 254) * 1024)) + (ow_outer * 8)) + (kw * 4)) + ic_inner)] * kernel_vec[((((((((n_c_outer_fused_h_fused % 4064) / 254) * 1152) + (ic_outer * 144)) + (kh * 48)) + (kw * 16)) + (ic_inner * 4)) + oc_block_c)]));\n              }\n              for (int32_t oc_block_c_1 = 0; oc_block_c_1 < 4; ++oc_block_c_1) {\n                conv2d_NCHWc_global[(oc_block_c_1 + 4)] = (conv2d_NCHWc_global[(oc_block_c_1 + 4)] + (data_vec[(((((((((n_c_outer_fused_h_fused / 4064) * 2097152) + (ic_outer * 262144)) + (kh * 1024)) + ((n_c_outer_fused_h_fused % 254) * 1024)) + (ow_outer * 8)) + (kw * 4)) + ic_inner) + 4)] * kernel_vec[((((((((n_c_outer_fused_h_fused % 4064) / 254) * 1152) + (ic_outer * 144)) + (kh * 48)) + (kw * 16)) + (ic_inner * 4)) + oc_block_c_1)]));\n              }\n            }\n          }\n        }\n      }\n      for (int32_t ow_inner = 0; ow_inner < 2; ++ow_inner) {\n        for (int32_t oc_block = 0; oc_block < 4; ++oc_block) {\n          conv2d_NCHWc[(((ow_outer * 8) + (ow_inner * 4)) + oc_block)] = conv2d_NCHWc_global[((ow_inner * 4) + oc_block)];\n        }\n      }\n    }\n    for (int32_t w_outer = 0; w_outer < 127; ++w_outer) {\n      for (int32_t w_inner = 0; w_inner < 2; ++w_inner) {\n        for (int32_t c_inner = 0; c_inner < 4; ++c_inner) {\n          output_unpack[((((((n_c_outer_fused_h_fused / 254) * 258064) + (c_inner * 64516)) + ((n_c_outer_fused_h_fused % 254) * 254)) + (w_outer * 2)) + w_inner)] = conv2d_NCHWc[(((w_outer * 8) + (w_inner * 4)) + c_inner)];\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[8];\n  __shared__ float pad_temp_shared[32];\n  __shared__ float kernel_shared[576];\n  for (int yy_c_init = 0; yy_c_init < 2; ++yy_c_init) {\n    conv2d_nchw_local[yy_c_init] = 0.000000e+00f;\n    conv2d_nchw_local[(yy_c_init + 2)] = 0.000000e+00f;\n    conv2d_nchw_local[(yy_c_init + 4)] = 0.000000e+00f;\n    conv2d_nchw_local[(yy_c_init + 6)] = 0.000000e+00f;\n  }\n  for (int rc_outer = 0; rc_outer < 16; ++rc_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {\n      pad_temp_shared[(((((int)threadIdx.z) * 4) + (((int)threadIdx.x) * 2)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)] = data[(((((((((((int)blockIdx.z) >> 1) * 2097152) + (rc_outer * 131072)) + ((((int)threadIdx.z) >> 2) * 65536)) + (((int)blockIdx.y) * 512)) + ((((int)threadIdx.z) & 3) * 256)) + (((int)blockIdx.x) * 2)) + (((int)threadIdx.x) * 2)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)];\n    }\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 < 36; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1) {\n      kernel_shared[(((((int)threadIdx.z) * 72) + (((int)threadIdx.x) * 36)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1)] = kernel[(((((((((int)blockIdx.z) & 1) * 9216) + (((int)threadIdx.z) * 1152)) + (((int)threadIdx.x) * 576)) + ((ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 / 18) * 288)) + (rc_outer * 18)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 % 18))];\n    }\n    __syncthreads();\n    for (int rc_inner = 0; rc_inner < 2; ++rc_inner) {\n      for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n        for (int rx_inner = 0; rx_inner < 3; ++rx_inner) {\n          for (int yy_c = 0; yy_c < 2; ++yy_c) {\n            conv2d_nchw_local[yy_c] = (conv2d_nchw_local[yy_c] + (pad_temp_shared[(((((rc_inner * 16) + (yy_c * 4)) + (ry_inner * 4)) + ((int)threadIdx.x)) + rx_inner)] * kernel_shared[((((((int)threadIdx.z) * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner)]));\n            conv2d_nchw_local[(yy_c + 2)] = (conv2d_nchw_local[(yy_c + 2)] + (pad_temp_shared[(((((rc_inner * 16) + (yy_c * 4)) + (ry_inner * 4)) + ((int)threadIdx.x)) + rx_inner)] * kernel_shared[(((((((int)threadIdx.z) * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 144)]));\n            conv2d_nchw_local[(yy_c + 4)] = (conv2d_nchw_local[(yy_c + 4)] + (pad_temp_shared[(((((rc_inner * 16) + (yy_c * 4)) + (ry_inner * 4)) + ((int)threadIdx.x)) + rx_inner)] * kernel_shared[(((((((int)threadIdx.z) * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 288)]));\n            conv2d_nchw_local[(yy_c + 6)] = (conv2d_nchw_local[(yy_c + 6)] + (pad_temp_shared[(((((rc_inner * 16) + (yy_c * 4)) + (ry_inner * 4)) + ((int)threadIdx.x)) + rx_inner)] * kernel_shared[(((((((int)threadIdx.z) * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 432)]));\n          }\n        }\n      }\n    }\n  }\n  for (int yy_inner_inner_inner = 0; yy_inner_inner_inner < 2; ++yy_inner_inner_inner) {\n    conv2d_nchw[((((((((int)blockIdx.z) * 2064512) + (((int)threadIdx.z) * 64516)) + (((int)blockIdx.y) * 508)) + (yy_inner_inner_inner * 254)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.x))] = conv2d_nchw_local[yy_inner_inner_inner];\n    conv2d_nchw[(((((((((int)blockIdx.z) * 2064512) + (((int)threadIdx.z) * 64516)) + (((int)blockIdx.y) * 508)) + (yy_inner_inner_inner * 254)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.x)) + 516128)] = conv2d_nchw_local[(yy_inner_inner_inner + 2)];\n    conv2d_nchw[(((((((((int)blockIdx.z) * 2064512) + (((int)threadIdx.z) * 64516)) + (((int)blockIdx.y) * 508)) + (yy_inner_inner_inner * 254)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.x)) + 1032256)] = conv2d_nchw_local[(yy_inner_inner_inner + 4)];\n    conv2d_nchw[(((((((((int)blockIdx.z) * 2064512) + (((int)threadIdx.z) * 64516)) + (((int)blockIdx.y) * 508)) + (yy_inner_inner_inner * 254)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.x)) + 1548384)] = conv2d_nchw_local[(yy_inner_inner_inner + 6)];\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 32, 256, 256), \"float32\"), kernel: T.Buffer((64, 32, 3, 3), \"float32\"), output_unpack: T.Buffer((2, 64, 254, 254), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_vec = T.allocate([4194304], \"float32\", \"global\")\n        kernel_vec = T.allocate([18432], \"float32\", \"global\")\n        data_vec_1 = T.Buffer((4194304,), data=data_vec)\n        for bs_c_fused_h_fused in T.parallel(4096):\n            for w, vc in T.grid(256, 4):\n                data_1 = T.Buffer((4194304,), data=data.data)\n                data_vec_1[bs_c_fused_h_fused * 1024 + w * 4 + vc] = data_1[bs_c_fused_h_fused // 256 * 262144 + vc * 65536 + bs_c_fused_h_fused % 256 * 256 + w]\n        kernel_vec_1 = T.Buffer((18432,), data=kernel_vec)\n        for occ_k_h_fused in T.parallel(48):\n            for icc, k_w, icb, ocb in T.grid(8, 3, 4, 4):\n                cse_var_2: T.int32 = occ_k_h_fused % 3\n                cse_var_1: T.int32 = occ_k_h_fused // 3 * 1152\n                kernel_1 = T.Buffer((18432,), data=kernel.data)\n                kernel_vec_1[cse_var_1 + icc * 144 + cse_var_2 * 48 + k_w * 16 + icb * 4 + ocb] = kernel_1[cse_var_1 + ocb * 288 + icc * 36 + icb * 9 + cse_var_2 * 3 + k_w]\n        for n_c_outer_fused_h_fused in T.parallel(8128):\n            conv2d_NCHWc = T.allocate([1016], \"float32\", \"global\")\n            conv2d_NCHWc_global = T.allocate([8], \"float32\", \"global\")\n            conv2d_NCHWc_1 = T.Buffer((1016,), data=conv2d_NCHWc)\n            for ow_outer in range(127):\n                conv2d_NCHWc_global_1 = T.Buffer((8,), data=conv2d_NCHWc_global, align=32)\n                for oc_block_c_init in range(4):\n                    conv2d_NCHWc_global_1[oc_block_c_init] = T.float32(0)\n                for oc_block_c_init in range(4):\n                    conv2d_NCHWc_global_1[oc_block_c_init + 4] = T.float32(0)\n                for ic_outer, kh, kw, ic_inner in T.grid(8, 3, 3, 4):\n                    for oc_block_c in range(4):\n                        conv2d_NCHWc_global_1[oc_block_c] = conv2d_NCHWc_global_1[oc_block_c] + data_vec_1[n_c_outer_fused_h_fused // 4064 * 2097152 + ic_outer * 262144 + kh * 1024 + n_c_outer_fused_h_fused % 254 * 1024 + ow_outer * 8 + kw * 4 + ic_inner] * kernel_vec_1[n_c_outer_fused_h_fused % 4064 // 254 * 1152 + ic_outer * 144 + kh * 48 + kw * 16 + ic_inner * 4 + oc_block_c]\n                    for oc_block_c in range(4):\n                        cse_var_3: T.int32 = oc_block_c + 4\n                        conv2d_NCHWc_global_1[cse_var_3] = conv2d_NCHWc_global_1[cse_var_3] + data_vec_1[n_c_outer_fused_h_fused // 4064 * 2097152 + ic_outer * 262144 + kh * 1024 + n_c_outer_fused_h_fused % 254 * 1024 + ow_outer * 8 + kw * 4 + ic_inner + 4] * kernel_vec_1[n_c_outer_fused_h_fused % 4064 // 254 * 1152 + ic_outer * 144 + kh * 48 + kw * 16 + ic_inner * 4 + oc_block_c]\n                for ow_inner, oc_block in T.grid(2, 4):\n                    cse_var_4: T.int32 = ow_inner * 4\n                    conv2d_NCHWc_1[ow_outer * 8 + cse_var_4 + oc_block] = conv2d_NCHWc_global_1[cse_var_4 + oc_block]\n            for w_outer, w_inner, c_inner in T.grid(127, 2, 4):\n                output_unpack_1 = T.Buffer((8258048,), data=output_unpack.data)\n                output_unpack_1[n_c_outer_fused_h_fused // 254 * 258064 + c_inner * 64516 + n_c_outer_fused_h_fused % 254 * 254 + w_outer * 2 + w_inner] = conv2d_NCHWc_1[w_outer * 8 + w_inner * 4 + c_inner]",
        "op_args": "None",
        "input_shape": "[[2, 32, 256, 256], [64, 32, 3, 3]]",
        "output_shape": "[[2, 64, 254, 254]]",
        "input_name": [
            "data",
            "kernel"
        ],
        "output_name": [
            "output_unpack"
        ],
        "deepseek_c": "void default_function_kernel(float* conv2d_nchw, float* data, float* kernel) {\n  float conv2d_nchw_local[8];\n  for (int yy_c_init = 0; yy_c_init < 2; ++yy_c_init) {\n    conv2d_nchw_local[yy_c_init] = 0.000000e+00f;\n    conv2d_nchw_local[(yy_c_init + 2)] = 0.000000e+00f;\n    conv2d_nchw_local[(yy_c_init + 4)] = 0.000000e+00f;\n    conv2d_nchw_local[(yy_c_init + 6)] = 0.000000e+00f;\n  }\n  for (int rc_outer = 0; rc_outer < 16; ++rc_outer) {\n    for (int yy_c_init_1 = 0; yy_c_init_1 < 2; ++yy_c_init_1) {\n      for (int ry_outer = 0; ry_outer < 3; ++ry_outer) {\n        for (int rx_outer = 0; rx_outer < 3; ++rx_outer) {\n          for (int yy_c_inner = 0; yy_c_inner < 2; ++yy_c_inner) {\n            for (int rc_inner = 0; rc_inner < 2; ++rc_inner) {\n              conv2d_nchw_local[yy_c_inner] = (conv2d_nchw_local[yy_c_inner] + (data[(((((((((((int)blockIdx.z) >> 1) * 2097152) + (rc_outer * 131072)) + ((((int)threadIdx.z) >> 2) * 65536)) + (((int)blockIdx.y) * 512)) + ((((int)threadIdx.z) & 3) * 256)) + (((int)blockIdx.x) * 2)) + (((int)threadIdx.x) * 2)) + ((((rc_inner * 16) + (yy_c_inner * 4)) + (ry_outer * 4)) + rx_outer))] * kernel[((((((((((int)blockIdx.z) & 1) * 9216) + (((int)threadIdx.z) * 1152)) + (((int)threadIdx.x) * 576)) + ((rc_inner * 288) + (yy_c_init_1 * 144)) + (ry_outer * 48)) + (rx_outer * 16)) + (rc_inner * 9)) + (yy_c_init_1 * 3)) + rx_outer)]));\n              conv2d_nchw_local[(yy_c_inner + 2)] = (conv2d_nchw_local[(yy_c_inner + 2)] + (data[(((((((((((int)blockIdx.z) >> 1) * 2097152) + (rc_outer * 131072)) + ((((int)threadIdx.z) >> 2) * 65536)) + (((int)blockIdx.y) * 512)) + ((((int)threadIdx.z) & 3) * 256)) + (((int)blockIdx.x) * 2)) + (((int)threadIdx.x) * 2)) + ((((rc_inner * 16) + (yy_c_inner * 4)) + (ry_outer * 4)) + rx_outer))] * kernel[((((((((((int)blockIdx.z) & 1) * 9216) + (((int)threadIdx.z) * 1152)) + (((int)threadIdx.x) * 576)) + ((rc_inner * 288) + (yy_c_init_1 * 144)) + (ry_outer * 48)) + (rx_outer * 16)) + (rc_inner * 9)) + (yy_c_init_1 * 3)) + rx_outer) + 144)]));\n              conv2d_nchw_local[(yy_c_inner + 4)] = (conv2d_nchw_local[(yy_c_inner + 4)] + (data[(((((((((((int)blockIdx.z) >> 1) * 2097152) + (rc_outer * 131072)) + ((((int)threadIdx.z) >> 2) * 65536)) + (((int)blockIdx.y) * 512)) + ((((int)threadIdx.z) & 3) * 256)) + (((int)blockIdx.x) * 2)) + (((int)threadIdx.x) * 2)) + ((((rc_inner * 16) + (yy_c_inner * 4)) + (ry_outer * 4)) + rx_outer))] * kernel[((((((((((int)blockIdx.z) & 1) * 9216) + (((int)threadIdx.z) * 1152)) + (((int)threadIdx.x) * 576)) + ((rc_inner * 288) + (yy_c_init_1 * 144)) + (ry_outer * 48)) + (rx_outer * 16)) + (rc_inner * 9)) + (yy_c_init_1 * 3)) + rx_outer) + 288)]));\n              conv2d_nchw_local[(yy_c_inner + 6)] = (conv2d_nchw_local[(yy_c_inner + 6)] + (data[(((((((((((int)blockIdx.z) >> 1) * 2097152) + (rc_outer * 131072)) + ((((int)threadIdx.z) >> 2) * 65536)) + (((int)blockIdx.y) * 512)) + ((((int)threadIdx.z) & 3) * 256)) + (((int)blockIdx.x) * 2)) + (((int)threadIdx.x) * 2)) + ((((rc_inner * 16) + (yy_c_inner * 4)) + (ry_outer * 4)) + rx_outer))] * kernel[((((((((((int)blockIdx.z) & 1) * 9216) + (((int)threadIdx.z) * 1152)) + (((int)threadIdx.x) * 576)) + ((rc_inner * 288) + (yy_c_init_1 * 144)) + (ry_outer * 48)) + (rx_outer * 16)) + (rc_inner * 9)) + (yy_c_init_1 * 3)) + rx_outer) + 432)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  for (int yy_inner_inner_inner = 0; yy_inner_inner_inner < 2; ++yy_inner_inner_inner) {\n    conv2d_nchw[((((((((int)blockIdx.z) * 2064512) + (((int)threadIdx.z) * 64516)) + (((int)blockIdx.y) * 508)) + (yy_inner_inner_inner * 254)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.x))] = conv2d_nchw_local[yy_inner_inner_inner];\n    conv2d_nchw[(((((((((int)blockIdx.z) * 2064512) + (((int)threadIdx.z) * 64516)) + (((int)blockIdx.y) * 508)) + (yy_inner_inner_inner * 254)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.x)) + 516128)] = conv2d_nchw_local[(yy_inner_inner_inner + 2)];\n    conv2d_nchw[(((((((((int)blockIdx.z) * 2064512) + (((int)threadIdx.z) * 64516)) + (((int)blockIdx.y) * 508)) + (yy_inner_inner_inner * 254)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.x)) + 1032256)] = conv2d_nchw_local[(yy_inner_inner_inner + 4)];\n    conv2d_nchw[(((((((((int)blockIdx.z) * 2064512) + (((int)threadIdx.z) * 64516)) + (((int)blockIdx.y) * 508)) + (yy_inner_inner_inner * 254)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.x)) + 1548384)] = conv2d_nchw_local[(yy_inner_inner_inner + 6)];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* conv2d_nchw, float* data, float* kernel) {\n  #pragma omp parallel for\n  for (int32_t n_c_fused = 0; n_c_fused < 64; ++n_c_fused) {\n    float conv2d_nchw_local[16];\n    float pad_temp[2592];\n    float kernel_local[576];\n    for (int32_t n_c_fused_1 = 0; n_c_fused_1 < 16; ++n_c_fused_1) {\n      conv2d_nchw_local[n_c_fused_1] = 0.000000e+00f;\n    }\n    for (int32_t rc_outer_outer = 0; rc_outer_outer < 16; ++rc_outer_outer) {\n      for (int32_t rc_outer_inner = 0; rc_outer_inner < 2; ++rc_outer_inner) {\n        for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n          for (int32_t rx_outer = 0; rx_outer < 3; ++rx_outer) {\n            for (int32_t rc_inner = 0; rc_inner < 2; ++rc_inner) {\n              for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n                for (int32_t rx_inner = 0; rx_inner < 3; ++rx_inner) {\n                  for (int32_t rc_inner_inner = 0; rc_inner_inner < 2; ++rc_inner_inner) {\n                    conv2d_nchw_local[((rc_inner * 8) + (rc_inner_inner * 4))] = (conv2d_nchw_local[((rc_inner * 8) + (rc_inner_inner * 4))] + (data[((((((n_c_fused * 2097152) + (rc_outer_outer * 131072)) + (rc_inner * 65536)) + (ry_outer * 2048)) + (rc_outer_inner * 1024)) + (ry_inner * 32)) + (rc_inner_inner * 16))] * kernel[((((((n_c_fused_1 * 1152) + (rc_outer_outer * 18)) + (rc_inner * 9)) + (ry_outer * 3)) + (rc_outer_inner * 3)) + (ry_inner * 1)) + rx_inner]);\n                    conv2d_nchw_local[(((rc_inner * 8) + (rc_inner_inner * 4)) + 1)] = (conv2d_nchw_local[(((rc_inner * 8) + (rc_inner_inner * 4)) + 1)] + (data[((((((n_c_fused * 2097152) + (rc_outer_outer * 131072)) + (rc_inner * 65536)) + (ry_outer * 2048)) + (rc_outer_inner * 1024)) + (ry_inner * 32)) + (rc_inner_inner * 16))] * kernel[(((((((n_c_fused_1 * 1152) + (rc_outer_outer * 18)) + (rc_inner * 9)) + (ry_outer * 3)) + (rc_outer_inner * 3)) + (ry_inner * 1)) + rx_inner) + 144]);\n                    conv2d_nchw_local[(((rc_inner * 8) + (rc_inner_inner * 4)) + 2)] = (conv2d_nchw_local[(((rc_inner * 8) + (rc_inner_inner * 4)) + 2)] + (data[((((((n_c_fused * 2097152) + (rc_outer_outer * 131072)) + (rc_inner * 65536)) + (ry_outer * 2048)) + (rc_outer_inner * 1024)) + (ry_inner * 32)) + (rc_inner_inner * 16))] * kernel[(((((((n_c_fused_1 * 1152) + (rc_outer_outer * 18)) + (rc_inner * 9)) + (ry_outer * 3)) + (rc_outer_inner * 3)) + (ry_inner * 1)) + rx_inner) + 288]);\n                    conv2d_nchw_local[(((rc_inner * 8) + (rc_inner_inner * 4)) + 3)] = (conv2d_nchw_local[(((rc_inner * 8) + (rc_inner_inner * 4)) + 3)] + (data[((((((n_c_fused * 2097152) + (rc_outer_outer * 131072)) + (rc_inner * 65536)) + (ry_outer * 2048)) + (rc_outer_inner * 1024)) + (ry_inner * 32)) + (rc_inner_inner * 16))] * kernel[(((((((n_c_fused_1 * 1152) + (rc_outer_outer * 18)) + (rc_inner * 9)) + (ry_outer * 3)) + (rc_outer_inner * 3)) + (ry_inner * 1)) + rx_inner) + 432]);\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    for (int32_t n_inner = 0; n_inner < 2; ++n_inner) {\n      for (int32_t c_inner = 0; c_inner < 8; ++c_inner) {\n        conv2d_nchw[((((n_c_fused * 2064512) + (n_inner * 1032256)) + (c_inner * 254)) + (n_c_fused_1 * 16))] = conv2d_nchw_local[((n_inner * 8) + c_inner)];\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* conv2d_nchw, float* data, float* kernel) {\n  #pragma omp parallel for\n  for (int32_t n_c_fused_ax0_c_fused_ax1_fused = 0; n_c_fused_ax0_c_fused_ax1_fused < 64; ++n_c_fused_ax0_c_fused_ax1_fused) {\n    float conv2d_nchw_local[16];\n    for (int32_t ax1 = 0; ax1 < 32; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 256; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 256; ++ax3) {\n          for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n            conv2d_nchw_local[ax0_ax1_fused_ax2_fused_ax3_fused] = 0.000000e+00f;\n            conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused + 2)] = 0.000000e+00f;\n            conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused + 4)] = 0.000000e+00f;\n            conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused + 6)] = 0.000000e+00f;\n          }\n          for (int32_t rc_outer = 0; rc_outer < 16; ++rc_outer) {\n            for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner) {\n              for (int32_t rc_inner = 0; rc_inner < 2; ++rc_inner) {\n                for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n                  for (int32_t rx_inner = 0; rx_inner < 3; ++rx_inner) {\n                    for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {\n                      conv2d_nchw_local[ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner] = (conv2d_nchw_local[ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner] + (data[((((((((n_c_fused_ax0_c_fused_ax1_fused * 2097152) + (rc_outer * 131072)) + (ax1 * 65536)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner * 32768)) + (ax2 * 2048)) + (ax3 * 1024)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner * 512)) + (rc_inner * 256)) + ((ry_inner * 128) + (rx_inner * 64))) * kernel[((((((n_c_fused_ax0_c_fused_ax1_fused * 1152) + (ax1 * 288)) + (rc_outer * 18)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner * 9)) + (ry_inner * 3)) + rx_inner)]));\n                      conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner + 2)] = (conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner + 2)] + (data[((((((((n_c_fused_ax0_c_fused_ax1_fused * 2097152) + (rc_outer * 131072)) + (ax1 * 65536)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner * 32768)) + (ax2 * 2048)) + (ax3 * 1024)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner * 512)) + (rc_inner * 256)) + ((ry_inner * 128) + (rx_inner * 64))) * kernel[((((((n_c_fused_ax0_c_fused_ax1_fused * 1152) + (ax1 * 288)) + (rc_outer * 18)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner * 9)) + (ry_inner * 3)) + rx_inner) + 144)]);\n                      conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner + 4)] = (conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner + 4)] + (data[((((((((n_c_fused_ax0_c_fused_ax1_fused * 2097152) + (rc_outer * 131072)) + (ax1 * 65536)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner * 32768)) + (ax2 * 2048)) + (ax3 * 1024)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner * 512)) + (rc_inner * 256)) + ((ry_inner * 128) + (rx_inner * 64))) * kernel[((((((n_c_fused_ax0_c_fused_ax1_fused * 1152) + (ax1 * 288)) + (rc_outer * 18)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner * 9)) + (ry_inner * 3)) + rx_inner) + 288)]);\n                      conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner + 6)] = (conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner + 6)] + (data[((((((((n_c_fused_ax0_c_fused_ax1_fused * 2097152) + (rc_outer * 131072)) + (ax1 * 65536)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner * 32768)) + (ax2 * 2048)) + (ax3 * 1024)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner * 512)) + (rc_inner * 256)) + ((ry_inner * 128) + (rx_inner * 64))) * kernel[((((((n_c_fused_ax0_c_fused_ax1_fused * 1152) + (ax1 * 288)) + (rc_outer * 18)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner * 9)) + (ry_inner * 3)) + rx_inner) + 432)]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n          for (int32_t yy_inner_inner_inner = 0; yy_inner_inner_inner < 2; ++yy_inner_inner_inner) {\n            conv2d_nchw[((((((n_c_fused_ax0_c_fused_ax1_fused * 65536) + (ax1 * 32768)) + (ax2 * 2048)) + (ax3 * 1024)) + (yy_inner_inner_inner * 512)) + ((ax0_ax1_fused_ax2_fused_ax3_fused * 256) + ((ry_inner * 128) + (rx_inner * 64))))] = conv2d_nchw_local[yy_inner_inner_inner];\n            conv2d_nchw[((((((n_c_fused_ax0_c_fused_ax1_fused * 65536) + (ax1 * 32768)) + (ax2 * 2048)) + (ax3 * 1024)) + (yy_inner_inner_inner * 512)) + ((ax0_ax1_fused_ax2_fused_ax3_fused * 256) + ((ry_inner * 128) + (rx_inner * 64)))) + 516128] = conv2d_nchw_local[(yy_inner_inner_inner + 2)];\n            conv2d_nchw[((((((n_c_fused_ax0_c_fused_ax1_fused * 65536) + (ax1 * 32768)) + (ax2 * 2048)) + (ax3 * 1024)) + (yy_inner_inner_inner * 512)) + ((ax0_ax1_fused_ax2_fused_ax3_fused * 256) + ((ry_inner * 128) + (rx_inner * 64)))) + 1032256] = conv2d_nchw_local[(yy_inner_inner_inner + 4)];\n            conv2d_nchw[((((((n_c_fused_ax0_c_fused_ax1_fused * 65536) + (ax1 * 32768)) + (ax2 * 2048)) + (ax3 * 1024)) + (yy_inner_inner_inner * 512)) + ((ax0_ax1_fused_ax2_fused_ax3_fused * 256) + ((ry_inner * 128) + (rx_inner * 64)))) + 1548384] = conv2d_nchw_local[(yy_inner_inner_inner + 6)];\n          }\n        }\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "dft",
        "c_code": "void default_function_kernel(float* Im, float* Re, float* dft_cpu, float* dft_cpu_1) {\n  #pragma omp parallel for\n  for (int32_t i = 0; i < 1200; ++i) {\n    for (int32_t j = 0; j < 50; ++j) {\n      dft_cpu[((i * 50) + j)] = 0.000000e+00f;\n      dft_cpu_1[((i * 50) + j)] = 0.000000e+00f;\n      for (int32_t k = 0; k < 50; ++k) {\n        float cse_var_2 = (((-6.283185e+00f * ((float)j)) * 2.000000e-02f) * ((float)k));\n        dft_cpu[((i * 50) + j)] = (dft_cpu[((i * 50) + j)] + ((Re[((i * 50) + k)] * cosf(cse_var_2)) - (Im[((i * 50) + k)] * sinf(cse_var_2))));\n        dft_cpu_1[((i * 50) + j)] = (dft_cpu_1[((i * 50) + j)] + ((Re[((i * 50) + k)] * sinf(cse_var_2)) + (Im[((i * 50) + k)] * cosf(cse_var_2))));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ Im, float* __restrict__ Re, float* __restrict__ dft_cuda, float* __restrict__ dft_cuda_1) {\n  if (((((int)blockIdx.x) * 64) + (((int)threadIdx.x) >> 4)) < 75) {\n    for (int i = 0; i < 50; ++i) {\n      dft_cuda[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + i)] = 0.000000e+00f;\n      dft_cuda_1[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + i)] = 0.000000e+00f;\n      for (int j = 0; j < 50; ++j) {\n        float cse_var_1 = (((-6.283185e+00f * ((float)i)) * 2.000000e-02f) * ((float)j));\n        dft_cuda[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + i)] = (dft_cuda[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + i)] + ((Re[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + j)] * __cosf(cse_var_1)) - (Im[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + j)] * __sinf(cse_var_1))));\n        dft_cuda_1[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + i)] = (dft_cuda_1[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + i)] + ((Re[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + j)] * __sinf(cse_var_1)) + (Im[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + j)] * __cosf(cse_var_1))));\n      }\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(Re: T.Buffer((30, 40, 50), \"float32\"), Im: T.Buffer((30, 40, 50), \"float32\"), dft_cpu: T.Buffer((30, 40, 50), \"float32\"), dft_cpu_1: T.Buffer((30, 40, 50), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T.attr(0, \"extern_scope\", 0)\n        for i in T.parallel(1200):\n            for j in range(50):\n                cse_var_1: T.int32 = i * 50 + j\n                dft_cpu_2 = T.Buffer((60000,), data=dft_cpu.data)\n                dft_cpu_2[cse_var_1] = T.float32(0)\n                dft_cpu_3 = T.Buffer((60000,), data=dft_cpu_1.data)\n                dft_cpu_3[cse_var_1] = T.float32(0)\n                for k in range(50):\n                    cse_var_3: T.int32 = i * 50 + k\n                    cse_var_2: T.float32 = T.float32(-6.2831853071795862) * T.Cast(\"float32\", j) * T.float32(0.02) * T.Cast(\"float32\", k)\n                    Re_1 = T.Buffer((60000,), data=Re.data)\n                    Im_1 = T.Buffer((60000,), data=Im.data)\n                    dft_cpu_2[cse_var_1] = dft_cpu_2[cse_var_1] + (Re_1[cse_var_3] * T.cos(cse_var_2) - Im_1[cse_var_3] * T.sin(cse_var_2))\n                    dft_cpu_3[cse_var_1] = dft_cpu_3[cse_var_1] + (Re_1[cse_var_3] * T.sin(cse_var_2) + Im_1[cse_var_3] * T.cos(cse_var_2))",
        "op_args": "None",
        "input_shape": "[[30, 40, 50], [30, 40, 50]]",
        "output_shape": "[[30, 40, 50], [30, 40, 50]]",
        "input_name": [
            "Re",
            "Im"
        ],
        "output_name": [
            "dft_cpu.v0",
            "dft_cpu.v1"
        ],
        "deepseek_c": "void default_function_kernel(float* dft_cuda, float* dft_cuda_1, float* Re, float* Im) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 60000; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 50; ++ax3) {\n      float cse_var_1 = (-6.283185e+00f * ((float)(ax3)));\n      dft_cuda[((ax0_ax1_fused_ax2_fused * 50) + ax3)] = 0.000000e+00f;\n      dft_cuda_1[((ax0_ax1_fused_ax2_fused * 50) + ax3)] = 0.000000e+00f;\n      for (int32_t j = 0; j < 50; ++j) {\n        dft_cuda[((ax0_ax1_fused_ax2_fused * 50) + ax3)] = (dft_cuda[((ax0_ax1_fused_ax2_fused * 50) + ax3)] + ((Re[((ax0_ax1_fused_ax2_fused * 50) + j)] * cosf(cse_var_1)) - (Im[((ax0_ax1_fused_ax2_fused * 50) + j)] * sinf(cse_var_1))));\n        dft_cuda_1[((ax0_ax1_fused_ax2_fused * 50) + ax3)] = (dft_cuda_1[((ax0_ax1_fused_ax2_fused * 50) + ax3)] + ((Re[((ax0_ax1_fused_ax2_fused * 50) + j)] * sinf(cse_var_1)) + (Im[((ax0_ax1_fused_ax2_fused * 50) + j)] * cosf(cse_var_1))));\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* Im, float* Re, float* dft_cuda, float* dft_cuda_1) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 60000; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 50; ++i3) {\n      dft_cuda[i0_i1_fused_i2_fused] = 0.000000e+00f;\n      dft_cuda_1[i0_i1_fused_i2_fused] = 0.000000e+00f;\n      for (int32_t j = 0; j < 50; ++j) {\n        float cse_var_1 = (((-6.283185e+00f * ((float)i3)) * 2.000000e-02f) * ((float)j));\n        dft_cuda[i0_i1_fused_i2_fused] = (dft_cuda[i0_i1_fused_i2_fused] + ((Re[((i0_i1_fused_i2_fused / 50) * 2000 + (i3 * 50) + j)] * cosf(cse_var_1)) - (Im[((i0_i1_fused_i2_fused / 50) * 2000 + (i3 * 50) + j)] * sinf(cse_var_1))));\n        dft_cuda_1[i0_i1_fused_i2_fused] = (dft_cuda_1[i0_i1_fused_i2_fused] + ((Re[((i0_i1_fused_i2_fused / 50) * 2000 + (i3 * 50) + j)] * sinf(cse_var_1)) + (Im[((i0_i1_fused_i2_fused / 50) * 2000 + (i3 * 50) + j)] * cosf(cse_var_1))));\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* Im, float* Re, float* dft_cuda, float* dft_cuda_1) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 7500; ++i0_i1_fused_i2_fused) {\n    for (int32_t i0_i1_fused = 0; i0_i1_fused < 1200; ++i0_i1_fused) {\n      dft_cuda[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] = 0.000000e+00f;\n      dft_cuda_1[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] = 0.000000e+00f;\n      for (int32_t j = 0; j < 50; ++j) {\n        float cse_var_1 = (((-6.283185e+00f * ((float)i0_i1_fused_i2_fused)) * 2.000000e-02f) * ((float)j));\n        dft_cuda[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] = (dft_cuda[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] + ((Re[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] * __cosf(cse_var_1)) - (Im[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] * __sinf(cse_var_1))));\n        dft_cuda_1[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] = (dft_cuda_1[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] + ((Re[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] * __sinf(cse_var_1)) + (Im[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] * __cosf(cse_var_1))));\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "group_conv2d_opt",
        "c_code": "void default_function_kernel(float* A, float* W, float* output_unpack) {\n  float data_vec[8388608];\n  float kernel_vec[4608];\n  float conv_global[16516096];\n  for (int32_t g = 0; g < 4; ++g) {\n    #pragma omp parallel for\n    for (int32_t n_C_fused_h_fused = 0; n_C_fused_h_fused < 2048; ++n_C_fused_h_fused) {\n      for (int32_t c = 0; c < 4; ++c) {\n        for (int32_t w = 0; w < 256; ++w) {\n          data_vec[((((g * 2097152) + (n_C_fused_h_fused * 1024)) + (c * 256)) + w)] = A[(((((((n_C_fused_h_fused >> 9) * 2097152) + (g * 524288)) + (((n_C_fused_h_fused & 511) >> 8) * 262144)) + (c * 65536)) + ((n_C_fused_h_fused & 255) * 256)) + w)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t g_out_channel_fused_h_fused = 0; g_out_channel_fused_h_fused < 48; ++g_out_channel_fused_h_fused) {\n    for (int32_t in_channel = 0; in_channel < 2; ++in_channel) {\n      for (int32_t w_1 = 0; w_1 < 3; ++w_1) {\n        for (int32_t ci = 0; ci < 4; ++ci) {\n          for (int32_t co = 0; co < 4; ++co) {\n            kernel_vec[(((((((g_out_channel_fused_h_fused / 3) * 288) + (in_channel * 144)) + ((g_out_channel_fused_h_fused % 3) * 48)) + (w_1 * 16)) + (ci * 4)) + co)] = W[(((((((g_out_channel_fused_h_fused / 3) * 288) + (co * 72)) + (in_channel * 36)) + (ci * 9)) + ((g_out_channel_fused_h_fused % 3) * 3)) + w_1)];\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused = 0; g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused < 16256; ++g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused) {\n    for (int32_t ow_c_outer = 0; ow_c_outer < 127; ++ow_c_outer) {\n      for (int32_t oc_block_c_init = 0; oc_block_c_init < 4; ++oc_block_c_init) {\n        conv_global[(((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016) + (ow_c_outer * 8)) + oc_block_c_init)] = 0.000000e+00f;\n      }\n      for (int32_t oc_block_c_init_1 = 0; oc_block_c_init_1 < 4; ++oc_block_c_init_1) {\n        conv_global[((((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016) + (ow_c_outer * 8)) + oc_block_c_init_1) + 4)] = 0.000000e+00f;\n      }\n      for (int32_t ic_outer = 0; ic_outer < 2; ++ic_outer) {\n        for (int32_t kh = 0; kh < 3; ++kh) {\n          for (int32_t kw = 0; kw < 3; ++kw) {\n            for (int32_t ic_inner = 0; ic_inner < 4; ++ic_inner) {\n              for (int32_t oc_block_c = 0; oc_block_c < 4; ++oc_block_c) {\n                conv_global[(((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016) + (ow_c_outer * 8)) + oc_block_c)] = (conv_global[(((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016) + (ow_c_outer * 8)) + oc_block_c)] + (data_vec[((((((((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused / 1016) * 524288) + (ic_outer * 262144)) + (kh * 1024)) + ((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused % 254) * 1024)) + (ic_inner * 256)) + (ow_c_outer * 2)) + kw)] * kernel_vec[((((((((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused / 4064) * 1152) + (((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused % 1016) / 254) * 288)) + (ic_outer * 144)) + (kh * 48)) + (kw * 16)) + (ic_inner * 4)) + oc_block_c)]));\n              }\n              for (int32_t oc_block_c_1 = 0; oc_block_c_1 < 4; ++oc_block_c_1) {\n                conv_global[((((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016) + (ow_c_outer * 8)) + oc_block_c_1) + 4)] = (conv_global[((((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016) + (ow_c_outer * 8)) + oc_block_c_1) + 4)] + (data_vec[(((((((((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused / 1016) * 524288) + (ic_outer * 262144)) + (kh * 1024)) + ((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused % 254) * 1024)) + (ic_inner * 256)) + (ow_c_outer * 2)) + kw) + 1)] * kernel_vec[((((((((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused / 4064) * 1152) + (((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused % 1016) / 254) * 288)) + (ic_outer * 144)) + (kh * 48)) + (kw * 16)) + (ic_inner * 4)) + oc_block_c_1)]));\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n  for (int32_t n = 0; n < 4; ++n) {\n    #pragma omp parallel for\n    for (int32_t c_outer_h_fused = 0; c_outer_h_fused < 4064; ++c_outer_h_fused) {\n      for (int32_t w_outer = 0; w_outer < 127; ++w_outer) {\n        for (int32_t w_inner = 0; w_inner < 2; ++w_inner) {\n          for (int32_t c_inner = 0; c_inner < 4; ++c_inner) {\n            output_unpack[((((((n * 4129024) + ((c_outer_h_fused / 254) * 258064)) + (c_inner * 64516)) + ((c_outer_h_fused % 254) * 254)) + (w_outer * 2)) + w_inner)] = conv_global[(((((((c_outer_h_fused / 1016) * 4129024) + (n * 1032256)) + ((c_outer_h_fused % 1016) * 1016)) + (w_outer * 8)) + (w_inner * 4)) + c_inner)];\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ A, float* __restrict__ W, float* __restrict__ group_conv2d_nchw) {\n  float group_conv2d_nchw_local[1];\n  __shared__ float pad_temp_shared[1];\n  __shared__ float W_shared[1];\n  group_conv2d_nchw_local[0] = 0.000000e+00f;\n  for (int rc_outer = 0; rc_outer < 8; ++rc_outer) {\n    for (int ry_outer = 0; ry_outer < 3; ++ry_outer) {\n      for (int rx_outer = 0; rx_outer < 3; ++rx_outer) {\n        __syncthreads();\n        pad_temp_shared[0] = A[(((((((((int)blockIdx.z) * 2097152) + ((((int)blockIdx.y) >> 4) * 524288)) + (rc_outer * 65536)) + ((((int)blockIdx.x) / 254) * 256)) + (ry_outer * 256)) + rx_outer) + (((int)blockIdx.x) % 254))];\n        W_shared[0] = W[((((((int)blockIdx.y) * 72) + (rc_outer * 9)) + (ry_outer * 3)) + rx_outer)];\n        __syncthreads();\n        group_conv2d_nchw_local[0] = (group_conv2d_nchw_local[0] + (pad_temp_shared[0] * W_shared[0]));\n      }\n    }\n  }\n  group_conv2d_nchw[(((((int)blockIdx.z) * 4129024) + (((int)blockIdx.y) * 64516)) + ((int)blockIdx.x))] = group_conv2d_nchw_local[0];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((4, 32, 256, 256), \"float32\"), W: T.Buffer((64, 8, 3, 3), \"float32\"), output_unpack: T.Buffer((4, 64, 254, 254), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_vec = T.allocate([8388608], \"float32\", \"global\")\n        kernel_vec = T.allocate([4608], \"float32\", \"global\")\n        conv_global = T.allocate([16516096], \"float32\", \"global\")\n        data_vec_1 = T.Buffer((8388608,), data=data_vec)\n        for g in range(4):\n            for n_C_fused_h_fused in T.parallel(2048):\n                for c, w in T.grid(4, 256):\n                    A_1 = T.Buffer((8388608,), data=A.data)\n                    data_vec_1[g * 2097152 + n_C_fused_h_fused * 1024 + c * 256 + w] = A_1[n_C_fused_h_fused // 512 * 2097152 + g * 524288 + n_C_fused_h_fused % 512 // 256 * 262144 + c * 65536 + n_C_fused_h_fused % 256 * 256 + w]\n        kernel_vec_1 = T.Buffer((4608,), data=kernel_vec)\n        for g_out_channel_fused_h_fused in T.parallel(48):\n            for in_channel, w, ci, co in T.grid(2, 3, 4, 4):\n                cse_var_2: T.int32 = g_out_channel_fused_h_fused % 3\n                cse_var_1: T.int32 = g_out_channel_fused_h_fused // 3 * 288\n                W_1 = T.Buffer((4608,), data=W.data)\n                kernel_vec_1[cse_var_1 + in_channel * 144 + cse_var_2 * 48 + w * 16 + ci * 4 + co] = W_1[cse_var_1 + co * 72 + in_channel * 36 + ci * 9 + cse_var_2 * 3 + w]\n        for g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused in T.parallel(16256):\n            for ow_c_outer in range(127):\n                conv_global_1 = T.Buffer((16516096,), data=conv_global)\n                for oc_block_c_init in range(4):\n                    conv_global_1[g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016 + ow_c_outer * 8 + oc_block_c_init] = T.float32(0)\n                for oc_block_c_init in range(4):\n                    conv_global_1[g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016 + ow_c_outer * 8 + oc_block_c_init + 4] = T.float32(0)\n                for ic_outer, kh, kw, ic_inner in T.grid(2, 3, 3, 4):\n                    for oc_block_c in range(4):\n                        cse_var_3: T.int32 = g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016 + ow_c_outer * 8 + oc_block_c\n                        conv_global_1[cse_var_3] = conv_global_1[cse_var_3] + data_vec_1[g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused // 1016 * 524288 + ic_outer * 262144 + kh * 1024 + g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused % 254 * 1024 + ic_inner * 256 + ow_c_outer * 2 + kw] * kernel_vec_1[g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused // 4064 * 1152 + g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused % 1016 // 254 * 288 + ic_outer * 144 + kh * 48 + kw * 16 + ic_inner * 4 + oc_block_c]\n                    for oc_block_c in range(4):\n                        cse_var_4: T.int32 = g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016 + ow_c_outer * 8 + oc_block_c + 4\n                        conv_global_1[cse_var_4] = conv_global_1[cse_var_4] + data_vec_1[g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused // 1016 * 524288 + ic_outer * 262144 + kh * 1024 + g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused % 254 * 1024 + ic_inner * 256 + ow_c_outer * 2 + kw + 1] * kernel_vec_1[g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused // 4064 * 1152 + g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused % 1016 // 254 * 288 + ic_outer * 144 + kh * 48 + kw * 16 + ic_inner * 4 + oc_block_c]\n        for n in range(4):\n            for c_outer_h_fused in T.parallel(4064):\n                for w_outer, w_inner, c_inner in T.grid(127, 2, 4):\n                    output_unpack_1 = T.Buffer((16516096,), data=output_unpack.data)\n                    conv_global_1 = T.Buffer((16516096,), data=conv_global)\n                    output_unpack_1[n * 4129024 + c_outer_h_fused // 254 * 258064 + c_inner * 64516 + c_outer_h_fused % 254 * 254 + w_outer * 2 + w_inner] = conv_global_1[c_outer_h_fused // 1016 * 4129024 + n * 1032256 + c_outer_h_fused % 1016 * 1016 + w_outer * 8 + w_inner * 4 + c_inner]",
        "op_args": "None",
        "input_shape": "[[4, 32, 256, 256], [64, 8, 3, 3]]",
        "output_shape": "[[4, 64, 254, 254]]",
        "input_name": [
            "A",
            "W"
        ],
        "output_name": [
            "output_unpack"
        ],
        "deepseek_c": "void default_function_kernel(float* __restrict__ A, float* __restrict__ W, float* __restrict__ group_conv2d_nchw) {\n  float group_conv2d_nchw_local[1];\n  __shared__ float pad_temp_shared[1];\n  __shared__ float W_shared[1];\n  group_conv2d_nchw_local[0] = 0.000000e+00f;\n  for (int rc_outer = 0; rc_outer < 8; ++rc_outer) {\n    for (int ry_outer = 0; ry_outer < 3; ++ry_outer) {\n      for (int rx_outer = 0; rx_outer < 3; ++rx_outer) {\n        __syncthreads();\n        pad_temp_shared[0] = A[(((((((((int)blockIdx.z) * 2097152) + ((((int)blockIdx.y) >> 4) * 524288)) + (rc_outer * 65536)) + ((((int)blockIdx.x) / 254) * 256)) + (ry_outer * 256)) + rx_outer) + (((int)blockIdx.x) % 254))];\n        W_shared[0] = W[((((((int)blockIdx.y) * 72) + (rc_outer * 9)) + (ry_outer * 3)) + rx_outer)];\n        __syncthreads();\n        group_conv2d_nchw_local[0] = (group_conv2d_nchw_local[0] + (pad_temp_shared[0] * W_shared[0]));\n      }\n    }\n  }\n  group_conv2d_nchw[(((((int)blockIdx.z) * 4129024) + (((int)blockIdx.y) * 64516)) + ((int)blockIdx.x))] = group_conv2d_nchw_local[0];\n}",
        "starcoder_c": "void default_function_kernel(float* __restrict__ A, float* __restrict__ W, float* __restrict__ group_conv2d_nchw) {\n  float group_conv2d_nchw_local[1];\n  __shared__ float pad_temp_shared[1];\n  __shared__ float W_shared[1];\n  group_conv2d_nchw_local[0] = 0.000000e+00f;\n  for (int rc_outer = 0; rc_outer < 8; ++rc_outer) {\n    for (int ry_outer = 0; ry_outer < 3; ++ry_outer) {\n      for (int rx_outer = 0; rx_outer < 3; ++rx_outer) {\n        __syncthreads();\n        pad_temp_shared[0] = A[(((((((((int)blockIdx.z) * 2097152) + ((((int)blockIdx.y) >> 4) * 524288)) + (rc_outer * 65536)) + ((((int)blockIdx.x) / 254) * 256)) + (ry_outer * 256)) + rx_outer) + (((int)blockIdx.x) % 254))];\n        W_shared[0] = W[((((((int)blockIdx.y) * 72) + (rc_outer * 9)) + (ry_outer * 3)) + rx_outer)];\n        __syncthreads();\n        group_conv2d_nchw_local[0] = (group_conv2d_nchw_local[0] + (pad_temp_shared[0] * W_shared[0]));\n      }\n    }\n  }\n  group_conv2d_nchw[(((((int)blockIdx.z) * 4129024) + (((int)blockIdx.y) * 64516)) + ((int)blockIdx.x))] = group_conv2d_nchw_local[0];\n}",
        "codellama_c": "void default_function_kernel(float* __restrict__ A, float* __restrict__ W, float* __restrict__ group_conv2d_nchw) {\n  float group_conv2d_nchw_local[1];\n  __shared__ float pad_temp_shared[1];\n  __shared__ float W_shared[1];\n  group_conv2d_nchw_local[0] = 0.000000e+00f;\n  for (int rc_outer = 0; rc_outer < 8; ++rc_outer) {\n    for (int ry_outer = 0; ry_outer < 3; ++ry_outer) {\n      for (int rx_outer = 0; rx_outer < 3; ++rx_outer) {\n        __syncthreads();\n        pad_temp_shared[0] = A[(((((((((int)blockIdx.z) * 2097152) + ((((int)blockIdx.y) >> 4) * 524288)) + (rc_outer * 65536)) + ((((int)blockIdx.x) / 254) * 256)) + (ry_outer * 256)) + rx_outer) + (((int)blockIdx.x) % 254))];\n        W_shared[0] = W[((((((int)blockIdx.y) * 72) + (rc_outer * 9)) + (ry_outer * 3)) + rx_outer)];\n        __syncthreads();\n        group_conv2d_nchw_local[0] = (group_conv2d_nchw_local[0] + (pad_temp_shared[0] * W_shared[0]));\n      }\n    }\n  }\n  group_conv2d_nchw[(((((int)blockIdx.z) * 4129024) + (((int)blockIdx.y) * 64516)) + ((int)blockIdx.x))] = group_conv2d_nchw_local[0];\n}"
    },
    {
        "op_name": "batch_matmul_opt",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NT, float* x, float* y) {\n  #pragma omp parallel for\n  for (int32_t b_i_outer_fused_j_outer_fused = 0; b_i_outer_fused_j_outer_fused < 1048576; ++b_i_outer_fused_j_outer_fused) {\n    float T_batch_matmul_NT_global_rf[1024];\n    float T_batch_matmul_NT_global[64];\n    for (int32_t k_inner = 0; k_inner < 16; ++k_inner) {\n      for (int32_t i_c_j_c_fused = 0; i_c_j_c_fused < 64; ++i_c_j_c_fused) {\n        T_batch_matmul_NT_global_rf[((k_inner * 64) + i_c_j_c_fused)] = 0.000000e+00f;\n        for (int32_t k_outer = 0; k_outer < 128; ++k_outer) {\n          T_batch_matmul_NT_global_rf[((k_inner * 64) + i_c_j_c_fused)] = (T_batch_matmul_NT_global_rf[((k_inner * 64) + i_c_j_c_fused)] + (x[(((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + ((i_c_j_c_fused >> 3) * 2048)) + (k_outer * 16)) + k_inner)] * y[((((((b_i_outer_fused_j_outer_fused >> 16) * 4194304) + ((b_i_outer_fused_j_outer_fused & 255) * 16384)) + ((i_c_j_c_fused & 7) * 2048)) + (k_outer * 16)) + k_inner)]));\n        }\n      }\n    }\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = 0.000000e+00f;\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[((ax1 * 8) + ax2)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 64)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 128)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 192)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 256)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 320)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 384)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 448)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 512)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 576)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 640)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 704)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 768)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 832)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 896)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 960)]);\n      }\n    }\n    for (int32_t i_inner = 0; i_inner < 8; ++i_inner) {\n      T_batch_matmul_NT[((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + (i_inner * 2048)) + ((b_i_outer_fused_j_outer_fused & 255) * 8))] = T_batch_matmul_NT_global[(i_inner * 8)];\n      T_batch_matmul_NT[(((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + (i_inner * 2048)) + ((b_i_outer_fused_j_outer_fused & 255) * 8)) + 1)] = T_batch_matmul_NT_global[((i_inner * 8) + 1)];\n      T_batch_matmul_NT[(((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + (i_inner * 2048)) + ((b_i_outer_fused_j_outer_fused & 255) * 8)) + 2)] = T_batch_matmul_NT_global[((i_inner * 8) + 2)];\n      T_batch_matmul_NT[(((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + (i_inner * 2048)) + ((b_i_outer_fused_j_outer_fused & 255) * 8)) + 3)] = T_batch_matmul_NT_global[((i_inner * 8) + 3)];\n      T_batch_matmul_NT[(((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + (i_inner * 2048)) + ((b_i_outer_fused_j_outer_fused & 255) * 8)) + 4)] = T_batch_matmul_NT_global[((i_inner * 8) + 4)];\n      T_batch_matmul_NT[(((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + (i_inner * 2048)) + ((b_i_outer_fused_j_outer_fused & 255) * 8)) + 5)] = T_batch_matmul_NT_global[((i_inner * 8) + 5)];\n      T_batch_matmul_NT[(((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + (i_inner * 2048)) + ((b_i_outer_fused_j_outer_fused & 255) * 8)) + 6)] = T_batch_matmul_NT_global[((i_inner * 8) + 6)];\n      T_batch_matmul_NT[(((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + (i_inner * 2048)) + ((b_i_outer_fused_j_outer_fused & 255) * 8)) + 7)] = T_batch_matmul_NT_global[((i_inner * 8) + 7)];\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_batch_matmul_NT, float* __restrict__ x, float* __restrict__ y) {\n  float T_batch_matmul_NT_local[64];\n  __shared__ float x_shared[512];\n  __shared__ float y_shared[512];\n  float x_shared_local[8];\n  float y_shared_local[8];\n  for (int i_c_init = 0; i_c_init < 8; ++i_c_init) {\n    for (int j_c_init = 0; j_c_init < 8; ++j_c_init) {\n      T_batch_matmul_NT_local[((i_c_init * 8) + j_c_init)] = 0.000000e+00f;\n    }\n  }\n  for (int k_outer = 0; k_outer < 256; ++k_outer) {\n    __syncthreads();\n    #pragma unroll\n    for (int ax1_inner = 0; ax1_inner < 8; ++ax1_inner) {\n      x_shared[(((((int)threadIdx.y) * 64) + (ax1_inner * 8)) + ((int)threadIdx.x))] = x[((((((((int)blockIdx.z) * 4194304) + (((int)blockIdx.y) * 131072)) + (((int)threadIdx.y) * 16384)) + (ax1_inner * 2048)) + (k_outer * 8)) + ((int)threadIdx.x))];\n    }\n    #pragma unroll\n    for (int ax1_inner_1 = 0; ax1_inner_1 < 8; ++ax1_inner_1) {\n      y_shared[(((((int)threadIdx.y) * 64) + (ax1_inner_1 * 8)) + ((int)threadIdx.x))] = y[((((((((int)blockIdx.z) * 4194304) + (((int)blockIdx.x) * 131072)) + (((int)threadIdx.y) * 16384)) + (ax1_inner_1 * 2048)) + (k_outer * 8)) + ((int)threadIdx.x))];\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 8; ++k_inner) {\n      #pragma unroll\n      for (int ax1 = 0; ax1 < 8; ++ax1) {\n        x_shared_local[ax1] = x_shared[(((((int)threadIdx.y) * 64) + (ax1 * 8)) + k_inner)];\n      }\n      #pragma unroll\n      for (int ax1_1 = 0; ax1_1 < 8; ++ax1_1) {\n        y_shared_local[ax1_1] = y_shared[(((((int)threadIdx.x) * 64) + (ax1_1 * 8)) + k_inner)];\n      }\n      for (int i_c = 0; i_c < 8; ++i_c) {\n        #pragma unroll\n        for (int j_c = 0; j_c < 8; ++j_c) {\n          T_batch_matmul_NT_local[((i_c * 8) + j_c)] = (T_batch_matmul_NT_local[((i_c * 8) + j_c)] + (x_shared_local[i_c] * y_shared_local[j_c]));\n        }\n      }\n    }\n  }\n  for (int i_inner_inner = 0; i_inner_inner < 8; ++i_inner_inner) {\n    #pragma unroll\n    for (int j_inner_inner = 0; j_inner_inner < 8; ++j_inner_inner) {\n      T_batch_matmul_NT[(((((((((int)blockIdx.z) * 4194304) + (((int)blockIdx.y) * 131072)) + (((int)threadIdx.y) * 16384)) + (i_inner_inner * 2048)) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.x) * 8)) + j_inner_inner)] = T_batch_matmul_NT_local[((i_inner_inner * 8) + j_inner_inner)];\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(x: T.Buffer((16, 2048, 2048), \"float32\"), y: T.Buffer((16, 2048, 2048), \"float32\"), T_batch_matmul_NT: T.Buffer((16, 2048, 2048), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_i_outer_fused_j_outer_fused in T.parallel(1048576):\n            T_batch_matmul_NT_global_rf = T.allocate([1024], \"float32\", \"global\")\n            T_batch_matmul_NT_global = T.allocate([64], \"float32\", \"global\")\n            T_batch_matmul_NT_global_rf_1 = T.Buffer((1024,), data=T_batch_matmul_NT_global_rf)\n            for k_inner, i_c_j_c_fused in T.grid(16, 64):\n                T_batch_matmul_NT_global_rf_1[k_inner * 64 + i_c_j_c_fused] = T.float32(0)\n                for k_outer in range(128):\n                    cse_var_2: T.int32 = k_outer * 16\n                    cse_var_1: T.int32 = k_inner * 64 + i_c_j_c_fused\n                    x_1 = T.Buffer((67108864,), data=x.data)\n                    y_1 = T.Buffer((67108864,), data=y.data)\n                    T_batch_matmul_NT_global_rf_1[cse_var_1] = T_batch_matmul_NT_global_rf_1[cse_var_1] + x_1[b_i_outer_fused_j_outer_fused // 256 * 16384 + i_c_j_c_fused // 8 * 2048 + cse_var_2 + k_inner] * y_1[b_i_outer_fused_j_outer_fused // 65536 * 4194304 + b_i_outer_fused_j_outer_fused % 256 * 16384 + i_c_j_c_fused % 8 * 2048 + cse_var_2 + k_inner]\n            T_batch_matmul_NT_global_1 = T.Buffer((64,), data=T_batch_matmul_NT_global)\n            for ax1, ax2 in T.grid(8, 8):\n                cse_var_3: T.int32 = ax1 * 8 + ax2\n                T_batch_matmul_NT_global_1[cse_var_3] = T.float32(0)\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 64]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 128]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 192]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 256]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 320]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 384]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 448]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 512]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 576]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 640]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 704]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 768]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 832]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 896]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 960]\n            for i_inner in range(8):\n                cse_var_5: T.int32 = i_inner * 8\n                cse_var_4: T.int32 = b_i_outer_fused_j_outer_fused // 256 * 16384 + i_inner * 2048 + b_i_outer_fused_j_outer_fused % 256 * 8\n                T_batch_matmul_NT_1 = T.Buffer((67108864,), data=T_batch_matmul_NT.data)\n                T_batch_matmul_NT_1[cse_var_4] = T_batch_matmul_NT_global_1[cse_var_5]\n                T_batch_matmul_NT_1[cse_var_4 + 1] = T_batch_matmul_NT_global_1[cse_var_5 + 1]\n                T_batch_matmul_NT_1[cse_var_4 + 2] = T_batch_matmul_NT_global_1[cse_var_5 + 2]\n                T_batch_matmul_NT_1[cse_var_4 + 3] = T_batch_matmul_NT_global_1[cse_var_5 + 3]\n                T_batch_matmul_NT_1[cse_var_4 + 4] = T_batch_matmul_NT_global_1[cse_var_5 + 4]\n                T_batch_matmul_NT_1[cse_var_4 + 5] = T_batch_matmul_NT_global_1[cse_var_5 + 5]\n                T_batch_matmul_NT_1[cse_var_4 + 6] = T_batch_matmul_NT_global_1[cse_var_5 + 6]\n                T_batch_matmul_NT_1[cse_var_4 + 7] = T_batch_matmul_NT_global_1[cse_var_5 + 7]",
        "op_args": "None",
        "input_shape": "[[16, 2048, 2048], [16, 2048, 2048]]",
        "output_shape": "[[16, 2048, 2048]]",
        "input_name": [
            "x",
            "y"
        ],
        "output_name": [
            "T_batch_matmul_NT"
        ],
        "deepseek_c": "void default_function_kernel(float* T_batch_matmul_NT, float* x, float* y) {\n  float T_batch_matmul_NT_local[64];\n  for (int i_c_init = 0; i_c_init < 8; ++i_c_init) {\n    for (int j_c_init = 0; j_c_init < 8; ++j_c_init) {\n      T_batch_matmul_NT_local[((i_c_init * 8) + j_c_init)] = 0.000000e+00f;\n    }\n  }\n  for (int k_outer = 0; k_outer < 256; ++k_outer) {\n    for (int i_c_init_1 = 0; i_c_init_1 < 8; ++i_c_init_1) {\n      for (int j_c_init_1 = 0; j_c_init_1 < 8; ++j_c_init_1) {\n        for (int i_inner_inner = 0; i_inner_inner < 8; ++i_inner_inner) {\n          for (int j_inner_inner = 0; j_inner_inner < 8; ++j_inner_inner) {\n            T_batch_matmul_NT_local[((i_inner_inner * 8) + j_inner_inner)] = (T_batch_matmul_NT_local[((i_inner_inner * 8) + j_inner_inner)] + (x[((((((i_c_init_1 * 2048) + (i_inner_inner * 256)) + k_outer) * 8) + j_c_init_1)] * y[((((((j_c_init_1 * 2048) + (j_inner_inner * 256)) + k_outer) * 8) + i_c_init_1)]));\n          }\n        }\n      }\n    }\n  }\n  for (int i_inner_inner_outer = 0; i_inner_inner_outer < 8; ++i_inner_inner_outer) {\n    for (int j_inner_inner_outer = 0; j_inner_inner_outer < 8; ++j_inner_inner_outer) {\n      T_batch_matmul_NT[((((((i_inner_inner_outer * 64) + (i_inner_inner_outer * 64)) + (i_inner_inner_outer * 8)) + (i_inner_inner_outer * 8)) + j_inner_inner_outer) + (i_inner_inner_outer * 64))] = T_batch_matmul_NT_local[((i_inner_inner_outer * 8) + j_inner_inner_outer)];\n    }\n  }\n}",
        "starcoder_c": "",
        "codellama_c": "void default_function_kernel(float* T_batch_matmul_NT, float* x, float* y) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 32768; ++ax0_ax1_fused_ax2_fused) {\n    float T_batch_matmul_NT_local[64];\n    for (int32_t i_c_init = 0; i_c_init < 8; ++i_c_init) {\n      for (int32_t j_c_init = 0; j_c_init < 8; ++j_c_init) {\n        T_batch_matmul_NT_local[((i_c_init * 8) + j_c_init)] = 0.000000e+00f;\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 256; ++k_outer) {\n      for (int32_t k_inner = 0; k_inner < 8; ++k_inner) {\n        float x_shared_local[8];\n        float y_shared_local[8];\n        for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n          x_shared_local[ax1] = x[((((ax0_ax1_fused_ax2_fused * 16384) + (ax1 * 2048)) + (k_outer * 8)) + k_inner)];\n        }\n        for (int32_t ax1_1 = 0; ax1_1 < 8; ++ax1_1) {\n          y_shared_local[ax1_1] = y[((((ax0_ax1_fused_ax2_fused * 16384) + (ax1_1 * 2048)) + (k_outer * 8)) + k_inner)];\n        }\n        for (int32_t i_c = 0; i_c < 8; ++i_c) {\n          for (int32_t j_c = 0; j_c < 8; ++j_c) {\n            T_batch_matmul_NT_local[((i_c * 8) + j_c)] = (T_batch_matmul_NT_local[((i_c * 8) + j_c)] + (x_shared_local[i_c] * y_shared_local[j_c]));\n          }\n        }\n      }\n    }\n    for (int32_t i_inner_inner = 0; i_inner_inner < 8; ++i_inner_inner) {\n      for (int32_t j_inner_inner = 0; j_inner_inner < 8; ++j_inner_inner) {\n        T_batch_matmul_NT[((((ax0_ax1_fused_ax2_fused * 64) + (i_inner_inner * 8)) + (j_inner_inner * 8)) + k_inner)] = T_batch_matmul_NT_local[((i_inner_inner * 8) + j_inner_inner)];\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "lrn",
        "c_code": "void default_function_kernel(float* T_divide, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1560; ++ax0_ax1_fused_ax2_fused) {\n    float tensor[1];\n    for (int32_t ax3 = 0; ax3 < 9; ++ax3) {\n      tensor[0] = 0.000000e+00f;\n      tensor[0] = (tensor[0] + (data[((ax0_ax1_fused_ax2_fused * 9) + ax3)] * data[((ax0_ax1_fused_ax2_fused * 9) + ax3)]));\n      T_divide[((ax0_ax1_fused_ax2_fused * 9) + ax3)] = (data[((ax0_ax1_fused_ax2_fused * 9) + ax3)] / powf((2.000000e+00f + (1.000000e-04f * tensor[0])), 7.500000e-01f));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(26) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  T_divide[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))])), 7.500000e-01f));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 1755) {\n    tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 1755) {\n    tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 10, 13, 9), \"float32\"), T_divide: T.Buffer((12, 10, 13, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(1560):\n            tensor = T.allocate([1], \"float32\", \"global\")\n            for ax3 in range(9):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 9 + ax3\n                tensor_1 = T.Buffer((1,), data=tensor, align=4)\n                tensor_1[0] = T.float32(0)\n                data_1 = T.Buffer((14040,), data=data.data)\n                tensor_1[0] = tensor_1[0] + data_1[cse_var_1] * data_1[cse_var_1]\n                T_divide_1 = T.Buffer((14040,), data=T_divide.data)\n                T_divide_1[cse_var_1] = data_1[cse_var_1] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[0], T.float32(0.75))",
        "op_args": [
            12,
            10,
            13,
            9
        ],
        "input_shape": "[[12, 10, 13, 9]]",
        "output_shape": "[[12, 10, 13, 9]]",
        "deepseek_c": "void default_function_kernel(float* T_divide, float* data, float* tensor) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_outer = 0; ax0_ax1_fused_ax2_fused_outer < 1560; ++ax0_ax1_fused_ax2_fused_outer) {\n    for (int32_t ax3 = 0; ax3 < 9; ++ax3) {\n      tensor[((ax0_ax1_fused_ax2_fused_outer * 9) + ax3)] = 0.000000e+00f;\n    }\n    for (int32_t ax3_1 = 0; ax3_1 < 9; ++ax3_1) {\n      tensor[((ax0_ax1_fused_ax2_fused_outer * 9) + ax3_1)] = (tensor[((ax0_ax1_fused_ax2_fused_outer * 9) + ax3_1)] + (data[((ax0_ax1_fused_ax2_fused_outer * 9) + ax3_1)] * data[((ax0_ax1_fused_ax2_fused_outer * 9) + ax3_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_1 < 1560; ++ax0_ax1_fused_ax2_fused_outer_1) {\n    for (int32_t ax3_2 = 0; ax3_2 < 9; ++ax3_2) {\n      T_divide[((ax0_ax1_fused_ax2_fused_outer_1 * 9) + ax3_2)] = (data[((ax0_ax1_fused_ax2_fused_outer_1 * 9) + ax3_2)] / powf((2.000000e+00f + (1.000000e-04f * tensor[((ax0_ax1_fused_ax2_fused_outer_1 * 9) + ax3_2)])), 7.500000e-01f));\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* T_divide, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1755; ++i0_i1_fused_i2_fused_i3_fused) {\n    float tensor[1];\n    tensor[0] = 0.000000e+00f;\n    tensor[0] = (tensor[0] + (data[i0_i1_fused_i2_fused_i3_fused] * data[i0_i1_fused_i2_fused_i3_fused]));\n    T_divide[i0_i1_fused_i2_fused_i3_fused] = (data[i0_i1_fused_i2_fused_i3_fused] / powf((2.000000e+00f + (1.000000e-04f * tensor[0])), 7.500000e-01f));\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_divide, float* data, float* tensor) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 1560; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    tensor[ax0_ax1_fused_ax2_fused_ax3_fused] = 0.000000e+00f;\n    tensor[ax0_ax1_fused_ax2_fused_ax3_fused] = (tensor[ax0_ax1_fused_ax2_fused_ax3_fused] + (data[ax0_ax1_fused_ax2_fused_ax3_fused] * data[ax0_ax1_fused_ax2_fused_ax3_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_1 < 1560; ++ax0_ax1_fused_ax2_fused_ax3_fused_1) {\n    T_divide[ax0_ax1_fused_ax2_fused_ax3_fused_1] = (data[ax0_ax1_fused_ax2_fused_ax3_fused_1] / powf((2.000000e+00f + (1.000000e-04f * tensor[ax0_ax1_fused_ax2_fused_ax3_fused_1])), 7.500000e-01f));\n  }\n}"
    },
    {
        "op_name": "matmul",
        "c_code": "void default_function_kernel(float* T_matmul, float* left_matrix, float* right_matrix) {\n  for (int32_t ax1_outer_outer_outer = 0; ax1_outer_outer_outer < 2; ++ax1_outer_outer_outer) {\n    for (int32_t ax0_inner_init = 0; ax0_inner_init < 2; ++ax0_inner_init) {\n      T_matmul[((ax0_inner_init * 2) + ax1_outer_outer_outer)] = 0.000000e+00f;\n    }\n    for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n      for (int32_t ax0_inner = 0; ax0_inner < 2; ++ax0_inner) {\n        T_matmul[((ax0_inner * 2) + ax1_outer_outer_outer)] = (T_matmul[((ax0_inner * 2) + ax1_outer_outer_outer)] + (left_matrix[((ax0_inner * 2) + k_inner)] * right_matrix[((k_inner * 2) + ax1_outer_outer_outer)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_matmul, float* __restrict__ left_matrix, float* __restrict__ right_matrix) {\n  float T_matmul_local[2];\n  __shared__ float left_matrix_shared[4];\n  __shared__ float right_matrix_shared[4];\n  T_matmul_local[0] = 0.000000e+00f;\n  T_matmul_local[1] = 0.000000e+00f;\n  for (int ax0_ax1_fused_outer_outer = 0; ax0_ax1_fused_outer_outer < 2; ++ax0_ax1_fused_outer_outer) {\n    left_matrix_shared[((ax0_ax1_fused_outer_outer * 2) + ((int)threadIdx.x))] = left_matrix[((ax0_ax1_fused_outer_outer * 2) + ((int)threadIdx.x))];\n  }\n  for (int ax0_ax1_fused_outer_outer_1 = 0; ax0_ax1_fused_outer_outer_1 < 2; ++ax0_ax1_fused_outer_outer_1) {\n    right_matrix_shared[((ax0_ax1_fused_outer_outer_1 * 2) + ((int)threadIdx.x))] = right_matrix[((ax0_ax1_fused_outer_outer_1 * 2) + ((int)threadIdx.x))];\n  }\n  __syncthreads();\n  for (int k_inner = 0; k_inner < 2; ++k_inner) {\n    T_matmul_local[0] = (T_matmul_local[0] + (left_matrix_shared[((((int)threadIdx.x) * 2) + k_inner)] * right_matrix_shared[(k_inner * 2)]));\n    T_matmul_local[1] = (T_matmul_local[1] + (left_matrix_shared[((((int)threadIdx.x) * 2) + k_inner)] * right_matrix_shared[((k_inner * 2) + 1)]));\n  }\n  T_matmul[(((int)threadIdx.x) * 2)] = T_matmul_local[0];\n  T_matmul[((((int)threadIdx.x) * 2) + 1)] = T_matmul_local[1];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(left_matrix: T.Buffer((2, 2), \"float32\"), right_matrix: T.Buffer((2, 2), \"float32\"), T_matmul: T.Buffer((2, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax1_outer_outer_outer in range(2):\n            T_matmul_1 = T.Buffer((4,), data=T_matmul.data)\n            for ax0_inner_init in range(2):\n                T_matmul_1[ax0_inner_init * 2 + ax1_outer_outer_outer] = T.float32(0)\n            for k_inner, ax0_inner in T.grid(2, 2):\n                cse_var_2: T.int32 = ax0_inner * 2\n                cse_var_1: T.int32 = cse_var_2 + ax1_outer_outer_outer\n                left_matrix_1 = T.Buffer((4,), data=left_matrix.data)\n                right_matrix_1 = T.Buffer((4,), data=right_matrix.data)\n                T_matmul_1[cse_var_1] = T_matmul_1[cse_var_1] + left_matrix_1[cse_var_2 + k_inner] * right_matrix_1[k_inner * 2 + ax1_outer_outer_outer]",
        "op_args": [
            7,
            2,
            2,
            2
        ],
        "input_shape": "[[2, 2], [2, 2]]",
        "output_shape": "[[2, 2]]",
        "deepseek_c": "void default_function_kernel(float* T_matmul, float* left_matrix, float* right_matrix) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_outer_outer = 0; ax0_ax1_fused_outer_outer < 4; ++ax0_ax1_fused_outer_outer) {\n    float T_matmul_local[2];\n    T_matmul_local[0] = 0.000000e+00f;\n    T_matmul_local[1] = 0.000000e+00f;\n    for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n      T_matmul_local[0] = (T_matmul_local[0] + (left_matrix[((ax0_ax1_fused_outer_outer * 2) + k_inner)] * right_matrix[k_inner]));\n      T_matmul_local[1] = (T_matmul_local[1] + (left_matrix[((ax0_ax1_fused_outer_outer * 2) + k_inner)] * right_matrix[((k_inner * 2) + 1)]));\n    }\n    T_matmul[((ax0_ax1_fused_outer_outer * 2) + 0)] = T_matmul_local[0];\n    T_matmul[((ax0_ax1_fused_outer_outer * 2) + 1)] = T_matmul_local[1];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* T_matmul, float* left_matrix, float* right_matrix) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 2; ++ax0_ax1_fused) {\n    float T_matmul_local[2];\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      T_matmul_local[ax1] = 0.000000e+00f;\n      for (int32_t k = 0; k < 2; ++k) {\n        T_matmul_local[ax1] = (T_matmul_local[ax1] + (left_matrix[((ax0_ax1_fused * 2) + k)] * right_matrix[((k * 2) + ax1)]));\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 2; ++ax1_1) {\n      T_matmul[((ax0_ax1_fused * 2) + ax1_1)] = T_matmul_local[ax1_1];\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_matmul, float* left_matrix, float* right_matrix) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 4; ++ax0_ax1_fused) {\n    float T_matmul_local[1];\n    T_matmul_local[0] = 0.000000e+00f;\n    for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n      T_matmul_local[0] = (T_matmul_local[0] + (left_matrix[((ax0_ax1_fused * 2) + k_inner)] * right_matrix[k_inner]));\n    }\n    T_matmul[ax0_ax1_fused] = T_matmul_local[0];\n  }\n}"
    }
]