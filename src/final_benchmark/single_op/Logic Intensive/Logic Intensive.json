[
    {
        "op_name": "resize2d",
        "c_code": "void default_function_kernel(float* A, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 64; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 50; ++i2) {\n      for (int32_t i3_outer = 0; i3_outer < 4; ++i3_outer) {\n        for (int32_t i3_inner = 0; i3_inner < 16; ++i3_inner) {\n          if (((i3_outer * 8) + (i3_inner >> 1)) < 25) {\n            float cse_var_2 = (((((float)i2) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f);\n            float cse_var_1 = (((((float)((i3_outer * 16) + i3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f);\n            resize[((((i0_i1_fused * 2500) + (i2 * 50)) + (i3_outer * 16)) + i3_inner)] = ((((A[(((i0_i1_fused * 1024) + (max(min(((int32_t)floorf(cse_var_2)), 31), 0) * 32)) + max(min(((int32_t)floorf(cse_var_1)), 31), 0))] * (1.000000e+00f - (cse_var_1 - ((float)((int32_t)floorf(cse_var_1)))))) + (A[(((i0_i1_fused * 1024) + (max(min(((int32_t)floorf(cse_var_2)), 31), 0) * 32)) + max(min((((int32_t)floorf(cse_var_1)) + 1), 31), 0))] * (cse_var_1 - ((float)((int32_t)floorf(cse_var_1)))))) * (1.000000e+00f - (cse_var_2 - ((float)((int32_t)floorf(cse_var_2)))))) + (((A[(((i0_i1_fused * 1024) + (max(min((((int32_t)floorf(cse_var_2)) + 1), 31), 0) * 32)) + max(min(((int32_t)floorf(cse_var_1)), 31), 0))] * (1.000000e+00f - (cse_var_1 - ((float)((int32_t)floorf(cse_var_1)))))) + (A[(((i0_i1_fused * 1024) + (max(min((((int32_t)floorf(cse_var_2)) + 1), 31), 0) * 32)) + max(min((((int32_t)floorf(cse_var_1)) + 1), 31), 0))] * (cse_var_1 - ((float)((int32_t)floorf(cse_var_1)))))) * (cse_var_2 - ((float)((int32_t)floorf(cse_var_2))))));\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ A, float* __restrict__ resize) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 625) {\n    resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((((A[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 625) * 1024) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (A[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 625) * 1024) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (((A[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 625) * 1024) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (A[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 625) * 1024) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) * ((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((4, 16, 32, 32), \"float32\"), resize: T.Buffer((4, 16, 50, 50), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(64):\n            for i2, i3_outer, i3_inner in T.grid(50, 4, 16):\n                if T.likely(i3_outer * 8 + i3_inner // 2 < 25):\n                    cse_var_4: T.int32 = i3_outer * 16\n                    cse_var_3: T.int32 = i0_i1_fused * 1024\n                    cse_var_2: T.float32 = (T.Cast(\"float32\", i2) + T.float32(0.5)) * T.float32(0.63999998569488525) - T.float32(0.5)\n                    cse_var_1: T.float32 = (T.Cast(\"float32\", cse_var_4 + i3_inner) + T.float32(0.5)) * T.float32(0.63999998569488525) - T.float32(0.5)\n                    resize_1 = T.Buffer((160000,), data=resize.data)\n                    A_1 = T.Buffer((65536,), data=A.data)\n                    resize_1[i0_i1_fused * 2500 + i2 * 50 + cse_var_4 + i3_inner] = (A_1[cse_var_3 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)), 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)), 31), 0)] * (T.float32(1) - (cse_var_1 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_1))))) + A_1[cse_var_3 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)), 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)) + 1, 31), 0)] * (cse_var_1 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_1))))) * (T.float32(1) - (cse_var_2 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_2))))) + (A_1[cse_var_3 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)) + 1, 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)), 31), 0)] * (T.float32(1) - (cse_var_1 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_1))))) + A_1[cse_var_3 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)) + 1, 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)) + 1, 31), 0)] * (cse_var_1 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_1))))) * (cse_var_2 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_2))))",
        "op_args": "None",
        "input_shape": "[[4, 16, 32, 32]]",
        "output_shape": "[[4, 16, 50, 50]]",
        "input_name": [
            "A"
        ],
        "output_name": [
            "resize"
        ],
        "deepseek_c": "void default_function_kernel(float* __restrict__ A, float* __restrict__ resize) {\n  for (int32_t ax0_ax1_fused_ax2_fused_outer = 0; ax0_ax1_fused_ax2_fused_outer < 1024; ++ax0_ax1_fused_ax2_fused_outer) {\n    for (int32_t ax3_inner = 0; ax3_inner < 32; ++ax3_inner) {\n      resize[((((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner) * 1024) + (((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) * 32) + max(min(((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) + 1), 31), 0)))] = ((((A[((((((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner) / 625) * 1024) + (((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) * 32) + max(min(((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))), 31), 0)))] * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))))) + (A[((((((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner) / 625) * 1024) + (((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) * 32) + max(min(((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) + 1), 31), 0)))] * (((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))))) * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))))))) + (((A[((((((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner) / 625) * 1024) + (((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) * 32) + max(min(((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) + 1), 31), 0)))] * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))))) + (A[((((((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner) / 625) * 1024) + (((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) * 32) + max(min(((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) + 1), 31), 0)))] * (((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))))) * (((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))))))) * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))))))));\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* A, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 204800; ++i0_i1_fused_i2_fused_i3_fused) {\n    resize[i0_i1_fused_i2_fused_i3_fused] = ((((A[((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (max(min(((i0_i1_fused_i2_fused_i3_fused % 1250) / 25), 31), 0) * 32)) + max(min(i0_i1_fused_i2_fused_i3_fused % 50, 31), 0)] * (1.000000f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (A[((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (max(min(((i0_i1_fused_i2_fused_i3_fused % 1250) / 25), 31), 0) * 32) + max(min(((int)floorf((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f), 31), 0)] * ((((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) * (1.000000f - ((((((float)(i0_i1_fused_i2_fused_i3_fused % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(i0_i1_fused_i2_fused_i3_fused % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (((A[((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (max(min(((i0_i1_fused_i2_fused_i3_fused % 1250) / 25) + 1, 31), 0) * 32) + max(min(((int)floorf((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f), 31), 0)] * (1.000000f - ((((((float)(i0_i1_fused_i2_fused_i3_fused % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(i0_i1_fused_i2_fused_i3_fused % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (A[((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (max(min(((i0_i1_fused_i2_fused_i3_fused % 1250) / 25) + 1, 31), 0) * 32) + max(min(((int)floorf((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) + 1, 31), 0)] * ((((((float)(i0_i1_fused_i2_fused_i3_fused % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(i0_i1_fused_i2_fused_i3_fused % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) * ((((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))));\n  }\n}",
        "codellama_c": "void default_function_kernel(float* A, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 262144; ++i0_i1_fused_i2_fused_i3_fused) {\n    resize[i0_i1_fused_i2_fused_i3_fused] = ((((A[((((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) * 1024) + ((((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) % 16) * 32))] * (1.000000e+00f - ((((((float)((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) / 16)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) / 16)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (A[(((((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) * 1024) + (((((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) % 16) * 32)) + 16))] * (1.000000e+00f - ((((((float)((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) / 16)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) / 16)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (A[(((((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) * 1024) + (((((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) % 16) * 32)) + 16))] * (1.000000e+00f - ((((((float)((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) / 16)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) / 16)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) * ((((((float)((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) / 16)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) / 16)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))));\n  }\n}"
    },
    {
        "op_name": "resize3d",
        "c_code": "void default_function_kernel(float* A, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 10; ++i3) {\n      for (int32_t i4_inner = 0; i4_inner < 10; ++i4_inner) {\n        float cse_var_3 = (((((float)i4_inner) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f);\n        float cse_var_2 = (((((float)i3) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f);\n        float cse_var_1 = (((((float)(i0_i1_fused_i2_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f);\n        resize[(((i0_i1_fused_i2_fused * 100) + (i3 * 10)) + i4_inner)] = ((((((A[(((((i0_i1_fused_i2_fused / 10) * 32768) + (max(min(((int32_t)floorf(cse_var_1)), 31), 0) * 1024)) + (max(min(((int32_t)floorf(cse_var_2)), 31), 0) * 32)) + max(min(((int32_t)floorf(cse_var_3)), 31), 0))] * (1.000000e+00f - (cse_var_3 - ((float)((int32_t)floorf(cse_var_3)))))) + (A[(((((i0_i1_fused_i2_fused / 10) * 32768) + (max(min(((int32_t)floorf(cse_var_1)), 31), 0) * 1024)) + (max(min(((int32_t)floorf(cse_var_2)), 31), 0) * 32)) + max(min((((int32_t)floorf(cse_var_3)) + 1), 31), 0))] * (cse_var_3 - ((float)((int32_t)floorf(cse_var_3)))))) * (1.000000e+00f - (cse_var_2 - ((float)((int32_t)floorf(cse_var_2)))))) + (((A[(((((i0_i1_fused_i2_fused / 10) * 32768) + (max(min(((int32_t)floorf(cse_var_1)), 31), 0) * 1024)) + (max(min((((int32_t)floorf(cse_var_2)) + 1), 31), 0) * 32)) + max(min(((int32_t)floorf(cse_var_3)), 31), 0))] * (1.000000e+00f - (cse_var_3 - ((float)((int32_t)floorf(cse_var_3)))))) + (A[(((((i0_i1_fused_i2_fused / 10) * 32768) + (max(min(((int32_t)floorf(cse_var_1)), 31), 0) * 1024)) + (max(min((((int32_t)floorf(cse_var_2)) + 1), 31), 0) * 32)) + max(min((((int32_t)floorf(cse_var_3)) + 1), 31), 0))] * (cse_var_3 - ((float)((int32_t)floorf(cse_var_3)))))) * (cse_var_2 - ((float)((int32_t)floorf(cse_var_2)))))) * (1.000000e+00f - (cse_var_1 - ((float)((int32_t)floorf(cse_var_1)))))) + (((((A[(((((i0_i1_fused_i2_fused / 10) * 32768) + (max(min((((int32_t)floorf(cse_var_1)) + 1), 31), 0) * 1024)) + (max(min(((int32_t)floorf(cse_var_2)), 31), 0) * 32)) + max(min(((int32_t)floorf(cse_var_3)), 31), 0))] * (1.000000e+00f - (cse_var_3 - ((float)((int32_t)floorf(cse_var_3)))))) + (A[(((((i0_i1_fused_i2_fused / 10) * 32768) + (max(min((((int32_t)floorf(cse_var_1)) + 1), 31), 0) * 1024)) + (max(min(((int32_t)floorf(cse_var_2)), 31), 0) * 32)) + max(min((((int32_t)floorf(cse_var_3)) + 1), 31), 0))] * (cse_var_3 - ((float)((int32_t)floorf(cse_var_3)))))) * (1.000000e+00f - (cse_var_2 - ((float)((int32_t)floorf(cse_var_2)))))) + (((A[(((((i0_i1_fused_i2_fused / 10) * 32768) + (max(min((((int32_t)floorf(cse_var_1)) + 1), 31), 0) * 1024)) + (max(min((((int32_t)floorf(cse_var_2)) + 1), 31), 0) * 32)) + max(min(((int32_t)floorf(cse_var_3)), 31), 0))] * (1.000000e+00f - (cse_var_3 - ((float)((int32_t)floorf(cse_var_3)))))) + (A[(((((i0_i1_fused_i2_fused / 10) * 32768) + (max(min((((int32_t)floorf(cse_var_1)) + 1), 31), 0) * 1024)) + (max(min((((int32_t)floorf(cse_var_2)) + 1), 31), 0) * 32)) + max(min((((int32_t)floorf(cse_var_3)) + 1), 31), 0))] * (cse_var_3 - ((float)((int32_t)floorf(cse_var_3)))))) * (cse_var_2 - ((float)((int32_t)floorf(cse_var_2)))))) * (cse_var_1 - ((float)((int32_t)floorf(cse_var_1))))));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ A, float* __restrict__ resize) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 375) {\n    resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((((((A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * ((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((((A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 1024)) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 1024)) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * ((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * ((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f)))))));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((3, 16, 32, 32, 32), \"float32\"), resize: T.Buffer((3, 16, 10, 10, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            for i3, i4_inner in T.grid(10, 10):\n                cse_var_4: T.int32 = i0_i1_fused_i2_fused // 10 * 32768\n                cse_var_3: T.float32 = (T.Cast(\"float32\", i4_inner) + T.float32(0.5)) * T.float32(3.2000000476837158) - T.float32(0.5)\n                cse_var_2: T.float32 = (T.Cast(\"float32\", i3) + T.float32(0.5)) * T.float32(3.2000000476837158) - T.float32(0.5)\n                cse_var_1: T.float32 = (T.Cast(\"float32\", i0_i1_fused_i2_fused % 10) + T.float32(0.5)) * T.float32(3.2000000476837158) - T.float32(0.5)\n                resize_1 = T.Buffer((48000,), data=resize.data)\n                A_1 = T.Buffer((1572864,), data=A.data)\n                resize_1[i0_i1_fused_i2_fused * 100 + i3 * 10 + i4_inner] = ((A_1[cse_var_4 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)), 31), 0) * 1024 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)), 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_3)), 31), 0)] * (T.float32(1) - (cse_var_3 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_3))))) + A_1[cse_var_4 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)), 31), 0) * 1024 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)), 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_3)) + 1, 31), 0)] * (cse_var_3 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_3))))) * (T.float32(1) - (cse_var_2 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_2))))) + (A_1[cse_var_4 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)), 31), 0) * 1024 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)) + 1, 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_3)), 31), 0)] * (T.float32(1) - (cse_var_3 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_3))))) + A_1[cse_var_4 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)), 31), 0) * 1024 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)) + 1, 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_3)) + 1, 31), 0)] * (cse_var_3 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_3))))) * (cse_var_2 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_2))))) * (T.float32(1) - (cse_var_1 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_1))))) + ((A_1[cse_var_4 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)) + 1, 31), 0) * 1024 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)), 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_3)), 31), 0)] * (T.float32(1) - (cse_var_3 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_3))))) + A_1[cse_var_4 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)) + 1, 31), 0) * 1024 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)), 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_3)) + 1, 31), 0)] * (cse_var_3 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_3))))) * (T.float32(1) - (cse_var_2 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_2))))) + (A_1[cse_var_4 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)) + 1, 31), 0) * 1024 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)) + 1, 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_3)), 31), 0)] * (T.float32(1) - (cse_var_3 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_3))))) + A_1[cse_var_4 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)) + 1, 31), 0) * 1024 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)) + 1, 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_3)) + 1, 31), 0)] * (cse_var_3 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_3))))) * (cse_var_2 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_2))))) * (cse_var_1 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_1))))",
        "op_args": "None",
        "input_shape": "[[3, 16, 32, 32, 32]]",
        "output_shape": "[[3, 16, 10, 10, 10]]",
        "input_name": [
            "A"
        ],
        "output_name": [
            "resize"
        ],
        "deepseek_c": "void default_function_kernel(float* A, float* resize) {\n  for (int32_t ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 1536; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    for (int32_t ax3_inner = 0; ax3_inner < 32; ++ax3_inner) {\n      for (int32_t ax4_inner = 0; ax4_inner < 32; ++ax4_inner) {\n        resize[((((ax0_ax1_fused_ax2_fused_outer_outer * 1024) + (ax3_inner * 32)) + ax4_inner) * 32) + 0)] = ((((((A[((((((ax0_ax1_fused_ax2_fused_outer_outer / 128) * 32768) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((ax0_ax1_fused_ax2_fused_outer_outer / 128) * 32768) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[((((((ax0_ax1_fused_ax2_fused_outer_outer / 128) * 32768) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((ax0_ax1_fused_ax2_fused_outer_outer / 128) * 32768) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((((A[((((((ax0_ax1_fused_ax2_fused_outer_outer / 128) * 32768) + (max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((ax0_ax1_fused_ax2_fused_outer_outer / 128) * 32768) + (max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((((A[((((((ax0_ax1_fused_ax2_fused_outer_outer / 128) * 32768) + (max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((ax0_ax1_fused_ax2_fused_outer_outer / 128) * 32768) + (max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f)))))));\n      }\n    }\n  }\n}",
        "starcoder_c": "#include <math.h>\n#include <stdlib.h>\nvoid default_function_kernel(float* A, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 375; ++i0_i1_fused_i2_fused_i3_fused) {\n    for (int32_t i4 = 0; i4 < 32; ++i4) {\n      resize[((i0_i1_fused_i2_fused_i3_fused * 1024) + (i4 * 32))] = ((((((A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 10), 0.000000e+00f), 31), 0))] * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf((((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 0.000000e+00f), 31), 0))] * ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 10), 0.000000e+00f), 31), 0))] * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf((((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 0.000000e+00f), 31), 0))] * ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 10), 0.000000e+00f), 31), 0))] * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf((((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 0.000000e+00f), 31), 0))] * ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 10), 0.000000e+00f), 31), 0))] * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf((((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 0.000000e+00f), 31), 0))] * ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 10), 0.000000e+00f), 31), 0))] * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf((((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 0.000000e+00f), 31), 0))] * ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 10), 0.000000e+00f), 31), 0))] * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf((((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 0.000000e+00f), 31), 0))] * ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 10), 0.000000e+00f), 31), 0))] * (1.000000e+",
        "codellama_c": "#pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 128; ++i0_i1_fused_i2_fused_i3_fused) {\n    for (int32_t i4 = 0; i4 < 10; ++i4) {\n      for (int32_t i5 = 0; i5 < 32; ++i5) {\n        resize[(((i0_i1_fused_i2_fused_i3_fused * 320) + (i4 * 32)) + i5)] = ((((((A[((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024"
    },
    {
        "op_name": "grid_sample",
        "c_code": "void default_function_kernel(float* compute, float* data, float* grid) {\n  #pragma omp parallel for\n  for (int32_t n_c_fused = 0; n_c_fused < 16; ++n_c_fused) {\n    for (int32_t h = 0; h < 16; ++h) {\n      for (int32_t w_inner = 0; w_inner < 16; ++w_inner) {\n        compute[(((n_c_fused * 256) + (h * 16)) + w_inner)] = ((((((((((0 <= ((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) ? data[(((n_c_fused * 64) + (((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))] : 0.000000e+00f) * (1.000000e+00f - ((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * (1.000000e+00f - ((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((0 <= ((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[((((n_c_fused * 64) + (((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 1)] : 0.000000e+00f) * (1.000000e+00f - ((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * ((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((-1 <= ((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) ? data[((((n_c_fused * 64) + (((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 8)] : 0.000000e+00f) * ((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))) * (1.000000e+00f - ((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))))) + (((((((-1 <= ((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[((((n_c_fused * 64) + (((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 9)] : 0.000000e+00f) * ((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))) * ((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ grid) {\n  compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((((((((((0 <= ((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) ? data[((((((int)blockIdx.x) * 256) + ((((int)threadIdx.x) >> 8) * 64)) + (((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))] : 0.000000e+00f) * (1.000000e+00f - ((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * (1.000000e+00f - ((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((0 <= ((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[(((((((int)blockIdx.x) * 256) + ((((int)threadIdx.x) >> 8) * 64)) + (((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 1)] : 0.000000e+00f) * (1.000000e+00f - ((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * ((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((-1 <= ((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) ? data[(((((((int)blockIdx.x) * 256) + ((((int)threadIdx.x) >> 8) * 64)) + (((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 8)] : 0.000000e+00f) * ((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))) * (1.000000e+00f - ((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))))) + (((((((-1 <= ((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[(((((((int)blockIdx.x) * 256) + ((((int)threadIdx.x) >> 8) * 64)) + (((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 9)] : 0.000000e+00f) * ((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))) * ((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 4, 8, 8), \"float32\"), grid: T.Buffer((4, 2, 16, 16), \"float32\"), compute: T.Buffer((4, 4, 16, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for n_c_fused in T.parallel(16):\n            for h, w_inner in T.grid(16, 16):\n                cse_var_4: T.int32 = h * 16\n                cse_var_3: T.int32 = n_c_fused * 64\n                cse_var_2: T.int32 = n_c_fused // 4 * 512 + cse_var_4 + w_inner\n                cse_var_1: T.int32 = cse_var_2 + 256\n                compute_1 = T.Buffer((4096,), data=compute.data)\n                grid_1 = T.Buffer((2048,), data=grid.data)\n                data_1 = T.Buffer((1024,), data=data.data)\n                compute_1[n_c_fused * 256 + cse_var_4 + w_inner] = T.if_then_else(0 <= T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) and 0 <= T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) and T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) < 8 and T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) < 8, data_1[cse_var_3 + T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) * 8 + T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5)))], T.float32(0)) * (T.float32(1) - ((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5) - T.Cast(\"float32\", T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5)))))) * (T.float32(1) - ((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5) - T.Cast(\"float32\", T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5)))))) + T.if_then_else(0 <= T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) and -1 <= T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) and T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) < 8 and T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) < 7, data_1[cse_var_3 + T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) * 8 + T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) + 1], T.float32(0)) * (T.float32(1) - ((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5) - T.Cast(\"float32\", T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5)))))) * ((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5) - T.Cast(\"float32\", T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))))) + T.if_then_else(-1 <= T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) and 0 <= T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) and T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) < 7 and T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) < 8, data_1[cse_var_3 + T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) * 8 + T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) + 8], T.float32(0)) * ((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5) - T.Cast(\"float32\", T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))))) * (T.float32(1) - ((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5) - T.Cast(\"float32\", T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5)))))) + T.if_then_else(-1 <= T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) and -1 <= T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) and T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) < 7 and T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) < 7, data_1[cse_var_3 + T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) * 8 + T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) + 9], T.float32(0)) * ((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5) - T.Cast(\"float32\", T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))))) * ((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5) - T.Cast(\"float32\", T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5)))))",
        "op_args": "None",
        "input_shape": "[[4, 4, 8, 8], [4, 2, 16, 16]]",
        "output_shape": "[[4, 4, 16, 16]]",
        "input_name": [
            "data",
            "grid"
        ],
        "output_name": [
            "compute"
        ],
        "deepseek_c": "#include <stdint.h>\n#include <stddef.h>\n#include <stdlib.h>\n#include <string.h>\n\nvoid default_function_kernel(float* compute, float* data, float* grid) {\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1024; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((((((((((0 <= ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) ? data[((((((i0_i1_fused_i2_fused_i3_fused >> 6) * 256) + (((i0_i1_fused_i2_fused_i3_fused & 63) >> 1) * 64)) + (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))] : 0.000000e+00f) * (1.000000e+00f - ((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * (1.000000e+00f - ((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))))) + (((((((0 <= ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[((((((i0_i1_fused_i2_fused_i3_fused >> 6) * 256) + (((i0_i1_fused_i2_fused_i3_fused & 63) >> 1) * 64)) + (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 1)] : 0.000000e+00f) * (1.000000e+00f - ((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * ((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((0 <= ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[((((((i0_i1_fused_i2_fused_i3_fused >> 6) * 256) + (((i0_i1_fused_i2_fused_i3_fused & 63) >> 1) * 64)) + (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 8)] : 0.000000e+00f) * ((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * (1.000000e+00f - ((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))))) + (((((((0 <= ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[((((((i0_i1_fused_i2_fused_i3_fused >> 6) * 256) + (((i0_i1_fused_i2_fused_i3_fused & 63) >> 1) * 64)) + (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 9)] : 0.000000e+00f) * ((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6)",
        "starcoder_c": "void default_function_kernel(float* compute, float* data, float* grid) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2048; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((((((((((0 <= ((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8))? data[((((i0_i1_fused_i2_fused_i3_fused >> 8) * 64) + (((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))] : 0.000000e+00f) * (1.000000e+00f - ((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * (1.000000e+00f - ((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((0 <= ((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7))? data[(((((i0_i1_fused_i2_fused_i3_fused >> 8) * 64) + (((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 1)] : 0.000000e+00f) * (1.000000e+00f - ((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * ((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((-1 <= ((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8))? data[(((((i0_i1_fused_i2_fused_i3_fused >> 8) * 64) + (((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 8)] : 0.000000e+00f) * ((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))) * (1.000000e+00f - ((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))))) + (((((((-1 <= ((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7))? data[(((((i0_i1_fused_i2_fused_i3_fused >> 8) * 64) + (((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 9)] : 0.000000e+00f) * ((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))) * ((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))));\n  }\n}",
        "codellama_c": "#pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 256; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((((((((((0 <= ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) ? data[((((((((((i0_i1_fused_i2_fused_i3_fused >> 4) * 128) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3) * 8)) + (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))] : 0.000000e+00f) * (1.000000e+00f - ((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * (1.000000e+00f - ((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((0 <= ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[((((((((((i0_i1_fused_i2_fused_i3_fused >> 4) * 128) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3) * 8)) + (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 1)] : 0.000000e+00f) * (1.000000e+00f - ((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * ((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((-1 <= ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) ? data[((((((((((i0_i1_fused_i2_fused_i3_fused >> 4) * 128) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3) * 8)) + (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 8)] : 0.000000e+00f) * ((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))) * (1.000000e+00f - ((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))))) + (((((((-1 <= ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[((((((((((i0_i1_fused_i2_fused_i3_fused >> 4) * 128) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3) * 8)) + (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 9)] : 0.000000e+00f) * ((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid"
    },
    {
        "op_name": "argsort",
        "c_code": "void default_function_kernel(float* data, int32_t* extern_1) {\n  float temp_data[1];\n  int32_t temp_index[1];\n  #pragma omp parallel for\n  for (int32_t i = 0; i < 64; ++i) {\n    for (int32_t j = 0; j < 256; ++j) {\n      extern_1[((i * 512) + (j * 2))] = (j * 2);\n    }\n    for (int32_t k = 0; k < 256; ++k) {\n      extern_1[(((i * 512) + (k * 2)) + 1)] = ((k * 2) + 1);\n    }\n    for (int32_t i_0 = 0; i_0 < 512; ++i_0) {\n      for (int32_t i_1 = 0; i_1 < 256; ++i_1) {\n        if (((((i * 512) + (i_1 * 2)) + (i_0 & 1)) < 511) && (data[(((i * 512) + (i_1 * 2)) + (i_0 & 1))] < data[((((i * 512) + (i_1 * 2)) + (i_0 & 1)) + 1)])) {\n          temp_data[0] = data[(((i * 512) + (i_1 * 2)) + (i_0 & 1))];\n          data[(((i * 512) + (i_1 * 2)) + (i_0 & 1))] = data[((((i * 512) + (i_1 * 2)) + (i_0 & 1)) + 1)];\n          data[((((i * 512) + (i_1 * 2)) + (i_0 & 1)) + 1)] = temp_data[0];\n          temp_index[0] = extern_1[(((i * 512) + (i_1 * 2)) + (i_0 & 1))];\n          extern_1[(((i * 512) + (i_1 * 2)) + (i_0 & 1))] = extern_1[((((i * 512) + (i_1 * 2)) + (i_0 & 1)) + 1)];\n          extern_1[((((i * 512) + (i_1 * 2)) + (i_0 & 1)) + 1)] = temp_index[0];\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3, int64_t cse_var_1, int64_t i_0) {\n  int64_t first[1];\n  int64_t last[1];\n  int64_t first_1[1];\n  int64_t last_1[1];\n  if ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) < (int64_t)512) {\n    if (i_0 == (int64_t)0) {\n      first[0] = max((int64_t)0, ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1))));\n      last[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), (((int64_t)2 << cse_var_1) >> (int64_t)1));\n      while ((first[0] < last[0])) {\n        if (argsort_gpu_v0[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first[0] + last[0]) >> (int64_t)1))] <= argsort_gpu_v0[((((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) >> (int64_t)1)) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first[0] + last[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first[0] = (((first[0] + last[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last[0] = ((first[0] + last[0]) >> (int64_t)1);\n        }\n      }\n      first[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first[0]);\n      last[0] = ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last[0]);\n      for (int i_1 = 0; i_1 < ((int)min((((int64_t)2 << cse_var_1) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_1) {\n        if ((((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) && (last[0] < (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1)))) && (last[0] < (int64_t)512)) {\n          if (argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])] <= argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])]) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        } else {\n          if ((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        }\n      }\n    } else {\n      first_1[0] = max((int64_t)0, ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (int64_t)512));\n      last_1[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), min((((int64_t)2 << cse_var_1) >> (int64_t)1), ((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))));\n      while ((first_1[0] < last_1[0])) {\n        if (argsort_gpu_v2[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first_1[0] + last_1[0]) >> (int64_t)1))] <= argsort_gpu_v2[(((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512)) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first_1[0] + last_1[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first_1[0] = (((first_1[0] + last_1[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last_1[0] = ((first_1[0] + last_1[0]) >> (int64_t)1);\n        }\n      }\n      first_1[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first_1[0]);\n      last_1[0] = ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last_1[0]);\n      for (int i_2 = 0; i_2 < ((int)min((((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_2) {\n        if (((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) && (last_1[0] < (int64_t)512)) {\n          if (argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])] <= argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])]) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        } else {\n          if ((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        }\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ data) {\n  if (((int)threadIdx.x) < 512) {\n    argsort_gpu_v0[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))] = data[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))];\n    argsort_gpu[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))] = ((float)((int)threadIdx.x));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3) {\n  __shared__ float temp_keys_swap[128];\n  __shared__ float temp_values_swap[128];\n  float temp_cond1[1];\n  float temp_cond2[1];\n  float temp_keys[1];\n  float temp_values[1];\n  for (int i = 0; i < 2; ++i) {\n    temp_keys_swap[((((int)threadIdx.x) * 2) + i)] = argsort_gpu_v0[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + i)];\n    temp_values_swap[((((int)threadIdx.x) * 2) + i)] = argsort_gpu[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + i)];\n  }\n  __syncthreads();\n  for (int j = 0; j < 128; ++j) {\n    if (((((int)threadIdx.x) * 2) + (j & 1)) < 127) {\n      temp_cond1[0] = temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))];\n      temp_cond2[0] = temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];\n      if (temp_cond2[0] < temp_cond1[0]) {\n        temp_keys[0] = temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))];\n        temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))] = temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];\n        temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)] = temp_keys[0];\n        temp_values[0] = temp_values_swap[((((int)threadIdx.x) * 2) + (j & 1))];\n        temp_values_swap[((((int)threadIdx.x) * 2) + (j & 1))] = temp_values_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];\n        temp_values_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)] = temp_values[0];\n      }\n    }\n    __syncthreads();\n  }\n  for (int k = 0; k < 2; ++k) {\n    argsort_gpu_v0[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_keys_swap[((((int)threadIdx.x) * 2) + k)];\n    argsort_gpu_v2[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_keys_swap[((((int)threadIdx.x) * 2) + k)];\n    argsort_gpu[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_values_swap[((((int)threadIdx.x) * 2) + k)];\n    argsort_gpu_v3[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_values_swap[((((int)threadIdx.x) * 2) + k)];\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((64, 512), \"float32\"), extern: T.Buffer((64, 512), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        temp_data = T.allocate([1], \"float32\", \"global\")\n        temp_index = T.allocate([1], \"int32\", \"global\")\n        T.attr(0, \"extern_scope\", 0)\n        for i in T.parallel(64):\n            extern_1 = T.Buffer((32768,), \"int32\", data=extern.data)\n            for j in range(256):\n                cse_var_1: T.int32 = j * 2\n                extern_1[i * 512 + cse_var_1] = cse_var_1\n            for k in range(256):\n                cse_var_2: T.int32 = k * 2\n                extern_1[i * 512 + cse_var_2 + 1] = cse_var_2 + 1\n            for i_0, i_1 in T.grid(512, 256):\n                cse_var_4: T.int32 = i * 512 + i_1 * 2 + i_0 % 2\n                cse_var_3: T.int32 = cse_var_4 + 1\n                data_1 = T.Buffer((32768,), data=data.data)\n                if cse_var_4 < 511 and data_1[cse_var_4] < data_1[cse_var_3]:\n                    temp_data_1 = T.Buffer((1,), data=temp_data)\n                    temp_data_1[0] = data_1[cse_var_4]\n                    data_1[cse_var_4] = data_1[cse_var_3]\n                    data_1[cse_var_3] = temp_data_1[0]\n                    temp_index_1 = T.Buffer((1,), \"int32\", data=temp_index)\n                    temp_index_1[0] = extern_1[cse_var_4]\n                    extern_1[cse_var_4] = extern_1[cse_var_3]\n                    extern_1[cse_var_3] = temp_index_1[0]",
        "op_args": "None",
        "input_shape": "[[64, 512]]",
        "output_shape": "[[64, 512]]",
        "input_name": [
            "data"
        ],
        "output_name": [
            "extern"
        ],
        "deepseek_c": "void default_function_kernel_2(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3, int64_t cse_var_1, int64_t i_0) {\n  int64_t first[1];\n  int64_t last[1];\n  int64_t first_1[1];\n  int64_t last_1[1];\n  if ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) < (int64_t)512) {\n    if (i_0 == (int64_t)0) {\n      first[0] = max((int64_t)0, ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1))));\n      last[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), (((int64_t)2 << cse_var_1) >> (int64_t)1));\n      while ((first[0] < last[0])) {\n        if (argsort_gpu_v0[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first[0] + last[0]) >> (int64_t)1))] <= argsort_gpu_v0[((((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) >> (int64_t)1)) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first[0] + last[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first[0] = (((first[0] + last[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last[0] = ((first[0] + last[0]) >> (int64_t)1);\n        }\n      }\n      first[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first[0]);\n      last[0] = ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last[0]);\n      for (int i_1 = 0; i_1 < ((int)min((((int64_t)2 << cse_var_1) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_1) {\n        if ((((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) && (last[0] < (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1)))) && (last[0] < (int64_t)512)) {\n          if (argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])] <= argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])]) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        } else {\n          if ((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        }\n      }\n    } else {\n      first_1[0] = max((int64_t)0, ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (int64_t)512));\n      last_1[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), min((((int64_t)2 << cse_var_1) >> (int64_t)1), ((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))));\n      while ((first_1[0] < last_1[0])) {\n        if (argsort_gpu_v2[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first_1[0] + last_1[0]) >> (int64_t)1))] <= argsort_gpu_v2[(((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512)) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first_1[0] + last_1[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first_1[0] = (((first_1[0] + last_1[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last_1[0] = ((first_1[0] + last_1[0]) >> (int64_t)1);\n        }\n      }\n      first_1[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first_1[0]);\n      last_1[0] = ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last_1[0]);\n      for (int i_2 = 0; i_2 < ((int)min((((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_2) {\n        if (((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) && (last_1[0] < (int64_t)512)) {\n          if (argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])] <= argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])]) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        } else {\n          if ((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel_2(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3, int64_t cse_var_1, int64_t i_0) {\n  int64_t first[1];\n  int64_t last[1];\n  int64_t first_1[1];\n  int64_t last_1[1];\n  if ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) < (int64_t)512) {\n    if (i_0 == (int64_t)0) {\n      first[0] = max((int64_t)0, ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1))));\n      last[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), (((int64_t)2 << cse_var_1) >> (int64_t)1));\n      while ((first[0] < last[0])) {\n        if (argsort_gpu_v0[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first[0] + last[0]) >> (int64_t)1))] <= argsort_gpu_v0[((((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) >> (int64_t)1)) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first[0] + last[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first[0] = (((first[0] + last[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last[0] = ((first[0] + last[0]) >> (int64_t)1);\n        }\n      }\n      first[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first[0]);\n      last[0] = ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last[0]);\n      for (int i_1 = 0; i_1 < ((int)min((((int64_t)2 << cse_var_1) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_1) {\n        if ((((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) && (last[0] < (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1)))) && (last[0] < (int64_t)512)) {\n          if (argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])] <= argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])]) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        } else {\n          if ((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        }\n      }\n    } else {\n      first_1[0] = max((int64_t)0, ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (int64_t)512));\n      last_1[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), min((((int64_t)2 << cse_var_1) >> (int64_t)1), ((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))));\n      while ((first_1[0] < last_1[0])) {\n        if (argsort_gpu_v2[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first_1[0] + last_1[0]) >> (int64_t)1))] <= argsort_gpu_v2[(((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512)) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first_1[0] + last_1[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first_1[0] = (((first_1[0] + last_1[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last_1[0] = ((first_1[0] + last_1[0]) >> (int64_t)1);\n        }\n      }\n      first_1[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first_1[0]);\n      last_1[0] = ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last_1[0]);\n      for (int i_2 = 0; i_2 < ((int)min((((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_2) {\n        if (((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) && (last_1[0] < (int64_t)512)) {\n          if (argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])] <= argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])]) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        } else {\n          if ((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        }\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel_2(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3, int64_t cse_var_1, int64_t i_0) {\n  int64_t first[1];\n  int64_t last[1];\n  int64_t first_1[1];\n  int64_t last_1[1];\n  if ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) < (int64_t)512) {\n    if (i_0 == (int64_t)0) {\n      first[0] = max((int64_t)0, ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1))));\n      last[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), (((int64_t)2 << cse_var_1) >> (int64_t)1));\n      while ((first[0] < last[0])) {\n        if (argsort_gpu_v0[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first[0] + last[0]) >> (int64_t)1))] <= argsort_gpu_v0[((((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) >> (int64_t)1)) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first[0] + last[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first[0] = (((first[0] + last[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last[0] = ((first[0] + last[0]) >> (int64_t)1);\n        }\n      }\n      first[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first[0]);\n      last[0] = ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last[0]);\n      for (int i_1 = 0; i_1 < ((int)min((((int64_t)2 << cse_var_1) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_1) {\n        if ((((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) && (last[0] < (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1)))) && (last[0] < (int64_t)512)) {\n          if (argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])] <= argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])]) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        } else {\n          if ((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        }\n      }\n    } else {\n      first_1[0] = max((int64_t)0, ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (int64_t)512));\n      last_1[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), min((((int64_t)2 << cse_var_1) >> (int64_t)1), ((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))));\n      while ((first_1[0] < last_1[0])) {\n        if (argsort_gpu_v2[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first_1[0] + last_1[0]) >> (int64_t)1))] <= argsort_gpu_v2[(((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512)) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first_1[0] + last_1[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first_1[0] = (((first_1[0] + last_1[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last_1[0] = ((first_1[0] + last_1[0]) >> (int64_t)1);\n        }\n      }\n      first_1[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first_1[0]);\n      last_1[0] = ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last_1[0]);\n      for (int i_2 = 0; i_2 < ((int)min((((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_2) {\n        if (((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) && (last_1[0] < (int64_t)512)) {\n          if (argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])] <= argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])]) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        } else {\n          if ((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        }\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "fifo_buffer",
        "c_code": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused = 0; i_j_fused < 255; ++i_j_fused) {\n    for (int32_t k = 0; k < 7; ++k) {\n      for (int32_t l = 0; l < 18; ++l) {\n        new_buffer[(((i_j_fused * 126) + (k * 18)) + l)] = data[(((i_j_fused * 126) + (k * 18)) + l)];\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(34) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 17, 7, 18), \"float32\"), buffer: T.Buffer((15, 17, 7, 18), \"float32\"), new_buffer: T.Buffer((15, 17, 7, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused in T.parallel(255):\n            for k, l in T.grid(7, 18):\n                cse_var_1: T.int32 = i_j_fused * 126 + k * 18 + l\n                new_buffer_1 = T.Buffer((32130,), data=new_buffer.data)\n                data_1 = T.Buffer((32130,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]",
        "op_args": [
            15,
            17,
            7,
            18
        ],
        "input_shape": "[[15, 17, 7, 18], [15, 17, 7, 18]]",
        "output_shape": "[[15, 17, 7, 18]]",
        "deepseek_c": "void default_function_kernel(float* data, float* new_buffer) {\n  for (int32_t i = 0; i < 15; ++i) {\n    for (int32_t j = 0; j < 17; ++j) {\n      for (int32_t k = 0; k < 7; ++k) {\n        for (int32_t l = 0; l < 18; ++l) {\n          new_buffer[((((i * 238) + (j * 14)) + (k * 2)) + l)] = data[((((i * 238) + (j * 14)) + (k * 2)) + l)];\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* data, float* new_buffer) {\n  for (int32_t i1 = 0; i1 < 17; ++i1) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      for (int32_t i3 = 0; i3 < 18; ++i3) {\n        new_buffer[(((i1 * 126) + (i2 * 18)) + i3)] = data[(((i1 * 126) + (i2 * 18)) + i3)];\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 4550; ++i0_i1_fused_i2_fused_i3_fused) {\n    new_buffer[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}"
    },
    {
        "op_name": "shape",
        "c_code": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_shape[ax0] = ((ax0 == 3) ? 11 : ((ax0 == 2) ? 1 : ((ax0 == 1) ? 5 : 14)));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 11 : ((((int)threadIdx.x) == 2) ? 1 : ((((int)threadIdx.x) == 1) ? 5 : 14)));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 5, 1, 11), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 11, T.if_then_else(ax0 == 2, 1, T.if_then_else(ax0 == 1, 5, 14)))",
        "op_args": [
            14,
            5,
            1,
            11
        ],
        "input_shape": "[[14, 5, 1, 11]]",
        "output_shape": "[[4]]",
        "deepseek_c": "void default_function_kernel(int* T_shape) {\n  T_shape[0] = 14;\n  T_shape[1] = 5;\n  T_shape[2] = 1;\n  T_shape[3] = 11;\n}",
        "starcoder_c": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_shape[ax0] = ((ax0 == 3)? 11 : ((ax0 == 2)? 1 : ((ax0 == 1)? 5 : 14)));\n  }\n}",
        "codellama_c": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 70; ++ax0_ax1_fused_ax2_fused) {\n    T_shape[ax0_ax1_fused_ax2_fused] = ((ax0_ax1_fused_ax2_fused == 69) ? 11 : (((ax0_ax1_fused_ax2_fused == 68) ? 1 : (((ax0_ax1_fused_ax2_fused == 67) ? 5 : 14))));\n  }\n}"
    },
    {
        "op_name": "multi_out_op",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        for (int32_t i3 = 0; i3 < 17; ++i3) {\n          compute[((((i0 * 3468) + (i1 * 204)) + (i2 * 17)) + i3)] = sqrtf((data[((((i0 * 3468) + (i1 * 204)) + (i2 * 17)) + i3)] + data_1[((((i0 * 3468) + (i1 * 204)) + (i2 * 17)) + i3)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 68; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 12; ++i2_1) {\n      for (int32_t i3_1 = 0; i3_1 < 17; ++i3_1) {\n        compute_1[(((i0_i1_fused * 204) + (i2_1 * 17)) + i3_1)] = cosf((data[(((i0_i1_fused * 204) + (i2_1 * 17)) + i3_1)] + data_1[(((i0_i1_fused * 204) + (i2_1 * 17)) + i3_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __cosf((data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = sqrtf((data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 17, 12, 17), \"float32\"), data_1: T.Buffer((4, 17, 12, 17), \"float32\"), compute: T.Buffer((4, 17, 12, 17), \"float32\"), compute_1: T.Buffer((4, 17, 12, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_2 = T.Buffer((13872,), data=data.data)\n        data_3 = T.Buffer((13872,), data=data_1.data)\n        for i0 in T.parallel(4):\n            for i1, i2, i3 in T.grid(17, 12, 17):\n                cse_var_1: T.int32 = i0 * 3468 + i1 * 204 + i2 * 17 + i3\n                compute_2 = T.Buffer((13872,), data=compute.data)\n                compute_2[cse_var_1] = T.sqrt(data_2[cse_var_1] + data_3[cse_var_1])\n        for i0_i1_fused in T.parallel(68):\n            for i2, i3 in T.grid(12, 17):\n                cse_var_2: T.int32 = i0_i1_fused * 204 + i2 * 17 + i3\n                compute_2 = T.Buffer((13872,), data=compute_1.data)\n                compute_2[cse_var_2] = T.cos(data_2[cse_var_2] + data_3[cse_var_2])",
        "op_args": [
            4,
            17,
            12,
            17
        ],
        "input_shape": "[[4, 17, 12, 17], [4, 17, 12, 17], [4, 17, 12, 17]]",
        "output_shape": "[[4, 17, 12, 17]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        for (int32_t i3 = 0; i3 < 17; ++i3) {\n          compute[((((i0 * 3468) + (i1 * 198)) + (i2 * 17)) + i3)] = sqrtf((data[((((i0 * 3468) + (i1 * 198)) + (i2 * 17)) + i3)] + data_1[((((i0 * 3468) + (i1 * 198)) + (i2 * 17)) + i3)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 4; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 17; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 12; ++i2_1) {\n        for (int32_t i3_1 = 0; i3_1 < 17; ++i3_1) {\n          compute[((((i0_1 * 3468) + (i1_1 * 198)) + (i2_1 * 17)) + i3_1)] = cosf((data[((((i0_1 * 3468) + (i1_1 * 198)) + (i2_1 * 17)) + i3_1)] + data_1[((((i0_1 * 3468) + (i1_1 * 198)) + (i2_1 * 17)) + i3_1)]));\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* compute_1, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 16320; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sqrtf((data[i0_i1_fused_i2_fused_i3_fused] + data_1[i0_i1_fused_i2_fused_i3_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused_1 = 0; i0_i1_fused_i2_fused_i3_fused_1 < 16320; ++i0_i1_fused_i2_fused_i3_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_i3_fused_1] = cosf((data[i0_i1_fused_i2_fused_i3_fused_1] + data_1[i0_i1_fused_i2_fused_i3_fused_1]));\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* compute_1, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 864; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 17; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 17) + i3)] = sqrtf((data[((i0_i1_fused_i2_fused * 17) + i3)] + data_1[((i0_i1_fused_i2_fused * 17) + i3)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 864; ++i0_i1_fused_i2_fused_1) {\n    for (int32_t i3_1 = 0; i3_1 < 17; ++i3_1) {\n      compute_1[((i0_i1_fused_i2_fused_1 * 17) + i3_1)] = cosf((data[((i0_i1_fused_i2_fused_1 * 17) + i3_1)] + data_1[((i0_i1_fused_i2_fused_1 * 17) + i3_1)]));\n    }\n  }\n}"
    },
    {
        "op_name": "upsampling",
        "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i2 = 0; i2 < 40; ++i2) {\n      for (int32_t i3 = 0; i3 < 36; ++i3) {\n        resize[(((i0 * 1440) + (i2 * 36)) + i3)] = data[(((i0 * 360) + ((i2 / 2) * 18)) + (i3 / 2))];\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) / 40) * 360) + (((((int)blockIdx.x) % 40) / 2) * 18)) + (((int)threadIdx.x) / 2))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 1, 20, 18), \"float32\"), resize: T.Buffer((11, 1, 40, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(11):\n            for i2, i3 in T.grid(40, 36):\n                resize_1 = T.Buffer((15840,), data=resize.data)\n                data_1 = T.Buffer((3960,), data=data.data)\n                resize_1[i0 * 1440 + i2 * 36 + i3] = data_1[i0 * 360 + T.Div(i2, 2) * 18 + T.Div(i3, 2)]",
        "op_args": [
            11,
            1,
            10,
            9
        ],
        "input_shape": "[[11, 1, 20, 18]]",
        "output_shape": "[[11, 1, 40, 36]]",
        "deepseek_c": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 7920; ++i0_i1_fused_i2_fused_i3_fused) {\n    resize[i0_i1_fused_i2_fused_i3_fused] = data[((((i0_i1_fused_i2_fused_i3_fused / 36) / 2) * 360) + ((i0_i1_fused_i2_fused_i3_fused % 36) / 2))];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 7920; ++i0_i1_fused_i2_fused_i3_fused) {\n    resize[i0_i1_fused_i2_fused_i3_fused] = data[((((i0_i1_fused_i2_fused_i3_fused / 360) * 360) + (((i0_i1_fused_i2_fused_i3_fused % 360) / 18) * 18)) + ((i0_i1_fused_i2_fused_i3_fused % 18) / 2))];\n  }\n}",
        "codellama_c": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 7200; ++i0_i1_fused_i2_fused_i3_fused) {\n    resize[i0_i1_fused_i2_fused_i3_fused] = data[((((i0_i1_fused_i2_fused_i3_fused / 360) * 360) + (((i0_i1_fused_i2_fused_i3_fused % 360) / 2) * 18)) + ((i0_i1_fused_i2_fused_i3_fused % 360) / 2))];\n  }\n}"
    }
]