[
    {
        "op_name": "sin",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2048; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sinf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 2, 16, 4), \"float32\"), compute: T.Buffer((16, 2, 16, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(2048):\n            compute_1 = T.Buffer((2048,), data=compute.data)\n            data_1 = T.Buffer((2048,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sin(data_1[i0_i1_fused_i2_fused_i3_fused])",
        "op_args": [
            16,
            2,
            16,
            4
        ],
        "input_shape": "[[16, 2, 16, 4]]",
        "output_shape": "[[16, 2, 16, 4]]",
        "cluade1_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < 16; i++) {\n        for (int j = 0; j < 2; j++) {\n            for (int k = 0; k < 16; k++) {\n                for (int l = 0; l < 4; l++) {\n                    int blockIdx_x = i * 2 * 16 + j * 16 + k;\n                    int threadIdx_x = l;\n                    compute[blockIdx_x * 4 + threadIdx_x] = sinf(data[blockIdx_x * 4 + threadIdx_x]);\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_kernel_cpu(float* compute, float* data) {\n    for (int i = 0; i < 16; i++) {\n        for (int j = 0; j < 2; j++) {\n            for (int k = 0; k < 16; k++) {\n                for (int l = 0; l < 4; l++) {\n                    int idx = i * 2 * 16 * 4 + j * 16 * 4 + k * 4 + l;\n                    compute[i * 2 * 16 * 4 + j * 16 * 4 + k * 4 + l] = sinf(data[idx]);\n                }\n            }\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  const int block_size = 4;\n  const int num_threads = 4;\n  #pragma omp parallel for\n  for (int i = 0; i < 16 * 2 * 16 * 4; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    compute[i] = sinf(data[i]);\n  }\n}\n\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Input tensor shape: [16, 2, 16, 4]\n  // compute shape: [16, 2, 16, 4]\n  // data shape: [16, 2, 16, 4]\n\n  for (int blockIdx_x = 0; blockIdx_x < 16; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 4; ++threadIdx_x) {\n      for (int i = 0; i < 2; ++i) {\n        for (int j = 0; j < 16; ++j) {\n          for (int k = 0; k < 4; ++k) {\n            int index = ((blockIdx_x * 2048) + (threadIdx_x * 512) + (i * 256) + (j * 16) + k);\n            compute[index] = sinf(data[index]);\n          }\n        }\n      }\n    }\n  }\n}\n\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        compute[i] = sinf(data[i]);\n    }\n}\n\nint main() {\n    int n = 16, c = 2, h = 16, w = 4;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 2.0f * M_PI; // Random values between 0 and 2*PI\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = sinf(data[i]);\n    }\n}\n\nint main() {\n    int size = 16 * 2 * 16 * 4;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i / 10.0); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "abs",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 12; ++i3) {\n        compute[(((i0_i1_fused * 48) + (i2 * 12)) + i3)] = fabsf(data[(((i0_i1_fused * 48) + (i2 * 12)) + i3)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 3, 4, 12), \"float32\"), compute: T.Buffer((16, 3, 4, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(48):\n            for i2, i3 in T.grid(4, 12):\n                cse_var_1: T.int32 = i0_i1_fused * 48 + i2 * 12 + i3\n                compute_1 = T.Buffer((2304,), data=compute.data)\n                data_1 = T.Buffer((2304,), data=data.data)\n                compute_1[cse_var_1] = T.fabs(data_1[cse_var_1])",
        "op_args": [
            16,
            3,
            4,
            12
        ],
        "input_shape": "[[16, 3, 4, 12]]",
        "output_shape": "[[16, 3, 4, 12]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < 16; blockIdx_x++) {\n        for (int threadIdx_x = 0; threadIdx_x < 64; threadIdx_x++) {\n            compute[(blockIdx_x * 64) + threadIdx_x] = fabsf(data[(blockIdx_x * 64) + threadIdx_x]);\n        }\n    }\n}\n",
        "cluade2_c": "#include <stdio.h>\n#include <math.h>\n\n#define N 16\n#define C 3\n#define H 4\n#define W 12\n#define BLOCK_SIZE 64\n\nvoid default_function_kernel(float* compute, float* data) {\n    for (int n = 0; n < N; ++n) {\n        for (int c = 0; c < C; ++c) {\n            for (int h = 0; h < H; ++h) {\n                for (int w = 0; w < W; ++w) {\n                    int index = n * C * H * W + c * H * W + h * W + w;\n                    compute[index] = fabsf(data[index]);\n                }\n            }\n        }\n    }\n}\n\n// Call the function like this:\n// default_function_kernel(compute, data);\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data, int size) {\n  #pragma omp parallel for\n  for (int i = 0; i < size; i++) {\n    compute[i] = fabsf(data[i]);\n  }\n}\n//End-of-Code|\n\n",
        "gemini2_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Calculate the number of threads per block (64 in this case)\n  const int threadsPerBlock = 64;\n\n  // Calculate the number of blocks needed for the input tensor shape\n  const int numBlocks = (16 * 3 * 4 * 12) / threadsPerBlock;\n\n  // Use OpenMP to parallelize the loop\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int globalIdx = (blockIdx * threadsPerBlock) + threadIdx;\n      compute[globalIdx] = fabsf(data[globalIdx]);\n    }\n  }\n}\n//|End-of-Code|\n\n",
        "gpt_1": "#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        compute[i] = fabsf(data[i]);\n    }\n}\n\nint main() {\n    int n = 16, c = 3, h = 4, w = 12;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration (this should be replaced with actual data initialization)\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = i * 0.1f;  // Example data\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = fabsf(data[i]);\n    }\n}\n\nint main() {\n    float data[16 * 3 * 4 * 12];\n    float compute[16 * 3 * 4 * 12];\n\n    // Initialize data array with some values\n    for (int i = 0; i < 16 * 3 * 4 * 12; i++) {\n        data[i] = (float)(i - (16 * 3 * 4 * 6)); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, 16 * 3 * 4 * 12);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "cos",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3094; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = cosf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 2, 13, 7), \"float32\"), compute: T.Buffer((17, 2, 13, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3094):\n            compute_1 = T.Buffer((3094,), data=compute.data)\n            data_1 = T.Buffer((3094,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.cos(data_1[i0_i1_fused_i2_fused_i3_fused])",
        "op_args": [
            17,
            2,
            13,
            7
        ],
        "input_shape": "[[17, 2, 13, 7]]",
        "output_shape": "[[17, 2, 13, 7]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < 17; blockIdx_x++) {\n        for (int blockIdx_y = 0; blockIdx_y < 2; blockIdx_y++) {\n            for (int blockIdx_z = 0; blockIdx_z < 13; blockIdx_z++) {\n                for (int threadIdx_x = 0; threadIdx_x < 7; threadIdx_x++) {\n                    int index = ((blockIdx_x * 2 * 13 * 7) + (blockIdx_y * 13 * 7) + (blockIdx_z * 7) + threadIdx_x);\n                    compute[index] = cosf(data[index]);\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_kernel_CPU(float* compute, float* data) {\n    int gridDim_x = 17;\n    int blockDim_x = 7;\n\n    for (int blockIdx_x = 0; blockIdx_x < gridDim_x; blockIdx_x++) {\n        for (int threadIdx_x = 0; threadIdx_x < blockDim_x; threadIdx_x++) {\n            int index = blockIdx_x * blockDim_x + threadIdx_x;\n\n            if (index < 17 * 2 * 13 * 7) {\n                compute[index] = cosf(data[index]);\n            }\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data, int size) {\n  #pragma omp parallel for\n  for (int i = 0; i < size; i++) {\n    compute[i] = cosf(data[i]);\n  }\n}\n//End-of-Code|\n\n",
        "gemini2_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Calculate the number of threads per block (7 in this case)\n  const int threadsPerBlock = 7;\n\n  // Calculate the number of blocks needed for the input tensor shape\n  const int numBlocks = (17 * 2 * 13 * 7) / threadsPerBlock;\n\n  // Use OpenMP to parallelize the loop\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int globalIdx = (blockIdx * threadsPerBlock) + threadIdx;\n      compute[globalIdx] = cosf(data[globalIdx]);\n    }\n  }\n}\n//|End-of-Code|\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        compute[i] = cosf(data[i]);\n    }\n}\n\nint main() {\n    int n = 17, c = 2, h = 13, w = 7;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 2.0f * M_PI; // Random values between 0 and 2*PI\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = cosf(data[i]);\n    }\n}\n\nint main() {\n    int size = 17 * 2 * 13 * 7;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i - (size / 2)); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "atan",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1428; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 9; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 9) + i3_s)] = atanf(data[((i0_i1_fused_i2_fused * 9) + i3_s)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 12, 7, 9), \"float32\"), compute: T.Buffer((17, 12, 7, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1428):\n            for i3_s in range(9):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 9 + i3_s\n                compute_1 = T.Buffer((12852,), data=compute.data)\n                data_1 = T.Buffer((12852,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])",
        "op_args": [
            17,
            12,
            7,
            9
        ],
        "input_shape": "[[17, 12, 7, 9]]",
        "output_shape": "[[17, 12, 7, 9]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < 17; blockIdx_x++) {\n        for (int blockIdx_y = 0; blockIdx_y < 12; blockIdx_y++) {\n            for (int blockIdx_z = 0; blockIdx_z < 7; blockIdx_z++) {\n                for (int threadIdx_x = 0; threadIdx_x < 9; threadIdx_x++) {\n                    int index = ((blockIdx_x * 12 * 7 * 9) + (blockIdx_y * 7 * 9) + (blockIdx_z * 9) + threadIdx_x);\n                    compute[index] = atanf(data[index]);\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_kernel_cpu(float* compute, float* data) {\n    int dim0 = 17, dim1 = 12, dim2 = 7, dim3 = 9;\n    \n    for (int i = 0; i < dim0; i++) {\n        for (int j = 0; j < dim1; j++) {\n            for (int k = 0; k < dim2; k++) {\n                for (int l = 0; l < dim3; l++) {\n                    int index = (((i * dim1 + j) * dim2 + k) * dim3 + l);\n                    compute[index] = atanf(data[index]);\n                }\n            }\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data, int size) {\n  #pragma omp parallel for\n  for (int i = 0; i < size; i++) {\n    compute[i] = atanf(data[i]);\n  }\n}\n//End-of-Code|\n\n",
        "gemini2_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Calculate the number of threads per block (18 in this case)\n  const int threadsPerBlock = 18;\n\n  // Calculate the number of blocks needed for the input tensor shape\n  const int numBlocks = (17 * 12 * 7 * 9) / threadsPerBlock;\n\n  // Use OpenMP to parallelize the loop\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int globalIdx = (blockIdx * threadsPerBlock) + threadIdx;\n      compute[globalIdx] = atanf(data[globalIdx]);\n    }\n  }\n}\n//|End-of-Code|\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        compute[i] = atanf(data[i]);\n    }\n}\n\nint main() {\n    int n = 17, c = 12, h = 7, w = 9;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 10.0f - 5.0f; // Random values between -5 and 5\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = atanf(data[i]);\n    }\n}\n\nint main() {\n    int size = 17 * 12 * 7 * 9;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i - (17 * 12 * 7 * 4)); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "add",
        "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 18; ++ax3) {\n          T_add[((((ax0 * 1620) + (ax1 * 162)) + (ax2 * 18)) + ax3)] = (data[((((ax0 * 1620) + (ax1 * 162)) + (ax2 * 18)) + ax3)] + data_1[((((ax0 * 1620) + (ax1 * 162)) + (ax2 * 18)) + ax3)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 10, 9, 18), \"float32\"), data_1: T.Buffer((20, 10, 9, 18), \"float32\"), T_add: T.Buffer((20, 10, 9, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(20):\n            for ax1, ax2, ax3 in T.grid(10, 9, 18):\n                cse_var_1: T.int32 = ax0 * 1620 + ax1 * 162 + ax2 * 18 + ax3\n                T_add_1 = T.Buffer((32400,), data=T_add.data)\n                data_2 = T.Buffer((32400,), data=data.data)\n                data_3 = T.Buffer((32400,), data=data_1.data)\n                T_add_1[cse_var_1] = data_2[cse_var_1] + data_3[cse_var_1]",
        "op_args": [
            20,
            10,
            9,
            18
        ],
        "input_shape": "[[20, 10, 9, 18], [20, 10, 9, 18]]",
        "output_shape": "[[20, 10, 9, 18]]",
        "cluade1_c": "#include <stdio.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* T_add, float* data, float* data_1) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < 20; blockIdx_x++) {\n        for (int blockIdx_y = 0; blockIdx_y < 10; blockIdx_y++) {\n            for (int blockIdx_z = 0; blockIdx_z < 9; blockIdx_z++) {\n                for (int threadIdx_x = 0; threadIdx_x < 18; threadIdx_x++) {\n                    int index = ((blockIdx_x * 10 * 9 * 18) + (blockIdx_y * 9 * 18) + (blockIdx_z * 18) + threadIdx_x);\n                    T_add[index] = data[index] + data_1[index];\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "void default_function_cpu(float* T_add, float* data, float* data_1) {\n    int block_x = 20;\n    int block_y = 10;\n    int block_z = 9;\n    int block_w = 18;\n\n    for (int i = 0; i < block_x; i++) {\n        for (int j = 0; j < block_y; j++) {\n            for (int k = 0; k < block_z; k++) {\n                for (int l = 0; l < block_w; l++) {\n                    int index = (((i * block_y + j) * block_z + k) * block_w + l);\n                    T_add[index] = data[index] + data_1[index];\n                }\n            }\n        }\n    }\n}\n\n// End-of-Code",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* T_add, float* data, float* data_1, int size) {\n  #pragma omp parallel for\n  for (int i = 0; i < size; i++) {\n    T_add[i] = (data[i] + data_1[i]);\n  }\n}\n//End-of-Code|\n\n",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel(float* T_add, float* data, float* data_1) {\n  // Calculate the number of threads per block (50 in this case)\n  const int threadsPerBlock = 50;\n\n  // Calculate the number of blocks needed for the input tensor shape\n  const int numBlocks = (20 * 10 * 9 * 18) / threadsPerBlock;\n\n  // Use OpenMP to parallelize the loop\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int globalIdx = (blockIdx * threadsPerBlock) + threadIdx;\n      T_add[globalIdx] = data[globalIdx] + data_1[globalIdx];\n    }\n  }\n}\n//|End-of-Code|\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\nvoid default_function(float* T_add, float* data, float* data_1, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        T_add[i] = data[i] + data_1[i];\n    }\n}\n\nint main() {\n    int n = 20, c = 10, h = 9, w = 18;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* data_1 = (float*)malloc(total_elements * sizeof(float));\n    float* T_add = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data arrays with example values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = i * 0.01f;  // Incremental values as example data\n        data_1[i] = i * 0.02f; // Incremental values as example data_1\n    }\n\n    default_function(T_add, data, data_1, total_elements);\n\n    // Optionally print the result array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", T_add[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(data_1);\n    free(T_add);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1, int size) {\n    for (int i = 0; i < size; i++) {\n        T_add[i] = data[i] + data_1[i];\n    }\n}\n\nint main() {\n    int size = 20 * 10 * 9 * 18;\n    float data[size];\n    float data_1[size];\n    float T_add[size];\n\n    // Initialize data and data_1 arrays with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)i;\n        data_1[i] = (float)(i + 1);\n    }\n\n    default_function_kernel(T_add, data, data_1, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"T_add[%d] = %f\\n\", i, T_add[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "fast_softmax",
        "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 260; ++i0_i1_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      T_softmax_maxelem[0] = -3.402823e+38f;\n      for (int32_t k = 0; k < 15; ++k) {\n        T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[(((i0_i1_fused * 45) + (i2 * 15)) + k)]);\n      }\n      T_softmax_expsum[0] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 15; ++k_1) {\n          int32_t v_ = ((int32_t)(floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0])));\n      }\n      for (int32_t i3_s = 0; i3_s < 15; ++i3_s) {\n          int32_t v__1 = ((int32_t)(floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_norm[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 195) {\n    T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 15; ++k) {\n    if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 195) {\n        int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(52) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)])) / T_softmax_expsum[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) < 195) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 15; ++k) {\n    if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) < 195) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 960) + (((int)threadIdx.x) * 15)) + k)]);\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 20, 3, 15), \"float32\"), T_softmax_norm: T.Buffer((13, 20, 3, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(260):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            for i2 in range(3):\n                T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n                T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((11700,), data=data.data)\n                for k in range(15):\n                    T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused * 45 + i2 * 15 + k])\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(15):\n                    cse_var_1: T.int32 = i0_i1_fused * 45 + i2 * 15 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0])\n                for i3_s in range(15):\n                    cse_var_2: T.int32 = i0_i1_fused * 45 + i2 * 15 + i3_s\n                    T_softmax_norm_1 = T.Buffer((11700,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]",
        "op_args": [
            13,
            20,
            3,
            15
        ],
        "input_shape": "[[13, 20, 3, 15]]",
        "output_shape": "[[13, 20, 3, 15]]",
        "cluade1_c": "void default_function_kernel(float* T_softmax_norm, float* data, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 260; ++i0_i1_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      T_softmax_maxelem[0] = -3.402823e+38f;\n      for (int32_t k = 0; k < 15; ++k) {\n        T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[(((i0_i1_fused * 45) + (i2 * 15)) + k)]);\n      }\n      T_softmax_expsum[0] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 15; ++k_1) {\n          int32_t v_ = ((int32_t)(floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0])));\n      }\n      for (int32_t i3_s = 0; i3_s < 15; ++i3_s) {\n          int32_t v__1 = ((int32_t)(floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_norm[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n      }\n    }\n  }\n}\n\n",
        "cluade2_c": "void default_function_kernel(float* T_softmax_norm, float*) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 260; ++i0_i1_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      T_softmax_maxelem[0] = -3.402823e+38f;\n      for (int32_t k = 0; k < 15; ++k) {\n        T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[(((i0_i1_fused * 45) + (i2 * 15)) + k)]);\n      }\n      T_softmax_expsum[0] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 15; ++k_1) {\n          int32_t v_ = ((int32_t)(floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0])));\n      }\n      for (int32_t i3_s = 0; i3_s < 15; ++i3_s) {\n          int32_t v__1 = ((int32_t)(floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_norm[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n      }\n    }\n  }\n}\n\n",
        "gemini1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\n#define NUM_THREADS 64\n\n// Function to calculate the maximum of two values\nfloat max(float a, float b) {\n  return (a > b) ? a : b;\n}\n\n// Function to calculate the minimum of two values\nfloat min(float a, float b) {\n  return (a < b) ? a : b;\n}\n\n// Function to perform the softmax operation on a 2D array\nvoid default_function_kernel(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data) {\n  // Calculate the maximum element for each row\n  #pragma omp parallel for num_threads(NUM_THREADS)\n  for (int i = 0; i < 195; i++) {\n    T_softmax_maxelem[i] = -3.402823e+38f;\n    for (int k = 0; k < 15; k++) {\n      T_softmax_maxelem[i] = max(T_softmax_maxelem[i], data[(i * 960) + (k * 20) + (i % 20)]);\n    }\n  }\n\n  // Calculate the exponential sum for each row\n  #pragma omp parallel for num_threads(NUM_THREADS)\n  for (int i = 0; i < 195; i++) {\n    T_softmax_expsum[i] = 0.0f;\n    for (int k = 0; k < 15; k++) {\n      float val = data[(i * 960) + (k * 20) + (i % 20)] - T_softmax_maxelem[i];\n      val = max(min(val, 8.837627e+01f), -8.837626e+01f);\n      float exp_val = expf((val * 1.442695e+00f) - floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f);\n      T_softmax_expsum[i] += exp_val;\n    }\n  }\n\n  // Normalize the exponential sum for each row\n  #pragma omp parallel for num_threads(NUM_THREADS)\n  for (int i = 0; i < 195; i++) {\n    for (int k = 0; k < 15; k++) {\n      float val = data[(i * 960) + (k * 20) + (i % 20)] - T_softmax_maxelem[i];\n      val = max(min(val, 8.837627e+01f), -8.837626e+01f);\n      float exp_val = expf((val * 1.442695e+00f) - floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f);\n      T_softmax_norm[(i * 960) + (k * 20) + (i % 20)] = exp_val / T_softmax_expsum[i];\n    }\n  }\n}\n\nint main() {\n  // Allocate memory for the input data and output arrays\n  float* data = (float*)malloc(13 * 20 * 3 * 15 * sizeof(float));\n  float* T_softmax_expsum = (float*)malloc(195 * sizeof(float));\n  float* T_softmax_maxelem = (float*)malloc(195 * sizeof(float));\n  float* T_softmax_norm = (float*)malloc(13 * 20 * 3 * 15 * sizeof(float));\n\n  // Initialize the input data (replace with your actual data)\n  for (int i = 0; i < 13 * 20 * 3 * 15; i++) {\n    data[i] = (float)i;\n  }\n\n  // Call the softmax function\n  default_function_kernel(T_softmax_expsum, T_softmax_maxelem, T_softmax_norm, data);\n\n  // Print the normalized softmax output (replace with your desired usage)\n  // for (int i = 0; i < 13 * 20 * 3 * 15; i++) {\n  //   printf(\"%f \", T_softmax_norm[i]);\n  // }\n\n  // Free allocated memory\n  free(data);\n  free(T_softmax_expsum);\n  free(T_softmax_maxelem);\n  free(T_softmax_norm);\n\n  return 0;\n}\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n#include <float.h>\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n  // Input tensor shape: [13, 20, 3, 15]\n  // T_softmax_maxelem shape: [13, 20, 3]\n  // data shape: [13, 20, 3, 15]\n\n  for (int blockIdx_x = 0; blockIdx_x < 13; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 20; ++threadIdx_x) {\n      for (int k = 0; k < 3; ++k) {\n        T_softmax_maxelem[((blockIdx_x * 60) + (threadIdx_x * 3) + k)] = -FLT_MAX;\n        for (int i = 0; i < 15; ++i) {\n          T_softmax_maxelem[((blockIdx_x * 60) + (threadIdx_x * 3) + k)] = fmaxf(T_softmax_maxelem[((blockIdx_x * 60) + (threadIdx_x * 3) + k)], data[((blockIdx_x * 960) + (threadIdx_x * 45) + (k * 15) + i)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid kernel_1(float* T_softmax_expsum, float* T_softmax_maxelem, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        int blockIdx_x = i / 32;\n        int threadIdx_x = i % 32;\n        if ((blockIdx_x * 8 + (threadIdx_x >> 2)) < 195) {\n            T_softmax_expsum[i] = 0.0f;\n        }\n        for (int k = 0; k < 15; ++k) {\n            if ((blockIdx_x * 8 + (threadIdx_x >> 2)) < 195) {\n                float x = data[(blockIdx_x * 480) + (threadIdx_x * 15) + k] - T_softmax_maxelem[i];\n                x = fmaxf(fminf(x, 8.837627e+01f), -8.837626e+01f);\n                int v_ = ((int)(floorf(x * 1.442695e+00f + 5.000000e-01f) + 1.270000e+02f)) << 23;\n                float base = *((float *)(&v_));\n                float diff = x - floorf(x * 1.442695e+00f + 5.000000e-01f) * 6.931472e-01f;\n                float result = base *\n                               (((((((((((((1.987569e-04f * diff +\n                                            1.398200e-03f) * diff +\n                                            8.333452e-03f) * diff +\n                                            4.166580e-02f) * diff +\n                                            1.666667e-01f) * diff +\n                                            5.000000e-01f) * diff +\n                                            1.000000e+00f) * diff +\n                                            1.000000e+00f))));\n                T_softmax_expsum[i] += fmaxf(result, x);\n            }\n        }\n    }\n}\nvoid kernel_2(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        float x = data[i] - T_softmax_maxelem[i / 15];\n        x = fmaxf(fminf(x, 8.837627e+01f), -8.837626e+01f);\n        int v_ = ((int)(floorf(x * 1.442695e+00f + 5.000000e-01f) + 1.270000e+02f)) << 23;\n        float base = *((float *)(&v_));\n        float diff = x - floorf(x * 1.442695e+00f + 5.000000e-01f) * 6.931472e-01f;\n        float result = base *\n                       (((((((((((((1.987569e-04f * diff +\n                                    1.398200e-03f) * diff +\n                                    8.333452e-03f) * diff +\n                                    4.166580e-02f) * diff +\n                                    1.666667e-01f) * diff +\n                                    5.000000e-01f) * diff +\n                                    1.000000e+00f) * diff +\n                                    1.000000e+00f))));\n        T_softmax_norm[i] = fmaxf(result, x) / T_softmax_expsum[i / 15];\n    }\n}\nvoid kernel_3(float* T_softmax_maxelem, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        int blockIdx_x = i / 64;\n        int threadIdx_x = i % 64;\n        if ((blockIdx_x * 16 + (threadIdx_x >> 2)) < 195) {\n            T_softmax_maxelem[i] = -3.402823e+38f;\n        }\n        for (int k = 0; k < 15; ++k) {\n            if ((blockIdx_x * 16 + (threadIdx_x >> 2)) < 195) {\n                T_softmax_maxelem[i] = fmaxf(T_softmax_maxelem[i], data[(blockIdx_x * 960) + (threadIdx_x * 15) + k]);\n            }\n        }\n    }\n}\nint main() {\n    int n = 13, c = 20, h = 3, w = 15;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* T_softmax_expsum = (float*)malloc(total_elements * sizeof(float));\n    float* T_softmax_maxelem = (float*)malloc(total_elements * sizeof(float));\n    float* T_softmax_norm = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 20.0f - 10.0f; // Random values between -10 and 10\n    }\n\n    kernel_3(T_softmax_maxelem, data, total_elements);\n    kernel_1(T_softmax_expsum, T_softmax_maxelem, data, total_elements);\n    kernel_2(T_softmax_expsum, T_softmax_maxelem, T_softmax_norm, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", T_softmax_norm[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(T_softmax_expsum);\n    free(T_softmax_maxelem);\n    free(T_softmax_norm);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data, int size) {\n    int num_elements = 195; // derived from the shape [[13, 20, 3, 15]]\n\n    // Initialize T_softmax_expsum to 0\n    for (int i = 0; i < size; i++) {\n        if ((i / 8) < num_elements) {\n            T_softmax_expsum[i] = 0.0f;\n        }\n    }\n\n    // Calculate T_softmax_expsum\n    for (int k = 0; k < 15; ++k) {\n        for (int i = 0; i < size; i++) {\n            if ((i / 8) < num_elements) {\n                float val = data[(i * 15) + k] - T_softmax_maxelem[i];\n                float clamped_val = fmaxf(fminf(val, 8.837627e+01f), -8.837626e+01f);\n                float exp_val = clamped_val * 1.442695e+00f;\n                int v_ = ((int)(floorf(exp_val + 5.000000e-01f)) + 1.270000e+02f) << 23;\n\n                float mantissa = clamped_val - floorf(exp_val) * 6.931472e-01f;\n                float result = (*(float *)&v_);\n                result *= (((((((((((((1.987569e-04f * mantissa) + 1.398200e-03f) * mantissa) + 8.333452e-03f) * mantissa) + 4.166580e-02f) * mantissa) + 1.666667e-01f) * mantissa) + 5.000000e-01f) * mantissa) + 1.000000e+00f);\n\n                T_softmax_expsum[i] += fmaxf(result, val);\n            }\n        }\n    }\n}\n\nvoid default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        float val = data[i] - T_softmax_maxelem[i / 15];\n        float clamped_val = fmaxf(fminf(val, 8.837627e+01f), -8.837626e+01f);\n        float exp_val = clamped_val * 1.442695e+00f;\n        int v_ = ((int)(floorf(exp_val + 5.000000e-01f)) + 1.270000e+02f) << 23;\n\n        float mantissa = clamped_val - floorf(exp_val) * 6.931472e-01f;\n        float result = (*(float *)&v_);\n        result *= (((((((((((((1.987569e-04f * mantissa) + 1.398200e-03f) * mantissa) + 8.333452e-03f) * mantissa) + 4.166580e-02f) * mantissa) + 1.666667e-01f) * mantissa) + 5.000000e-01f) * mantissa) + 1.000000e+00f);\n\n        T_softmax_norm[i] = fmaxf(result, val) / T_softmax_expsum[i / 15];\n    }\n}\n\nvoid default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data, int size) {\n    int num_elements = 195; // derived from the shape [[13, 20, 3, 15]]\n\n    // Initialize T_softmax_maxelem to a very small value\n    for (int i = 0; i < size; i++) {\n        if ((i / 16) < num_elements) {\n            T_softmax_maxelem[i] = -3.402823e+38f;\n        }\n    }\n\n    // Calculate T_softmax_maxelem\n    for (int k = 0; k < 15; ++k) {\n        for (int i = 0; i < size; i++) {\n            if ((i / 16) < num_elements) {\n                T_softmax_maxelem[i] = fmaxf(T_softmax_maxelem[i], data[(i * 15) + k]);\n            }\n        }\n    }\n}"
    },
    {
        "op_name": "acos",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1496; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = acosf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(17) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 17) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 17) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 17, 1, 11), \"float32\"), compute: T.Buffer((8, 17, 1, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(1496):\n            compute_1 = T.Buffer((1496,), data=compute.data)\n            data_1 = T.Buffer((1496,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.acos(data_1[i0_i1_fused_i2_fused_i3_fused])",
        "op_args": [
            8,
            17,
            1,
            11
        ],
        "input_shape": "[[8, 17, 1, 11]]",
        "output_shape": "[[8, 17, 1, 11]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < 8; blockIdx_x++) {\n        for (int threadIdx_x = 0; threadIdx_x < 17; threadIdx_x++) {\n            compute[(blockIdx_x * 17) + threadIdx_x] = acosf(data[(blockIdx_x * 17) + threadIdx_x]);\n        }\n    }\n}\n",
        "cluade2_c": "#include <stdio.h>\n#include <math.h>\n\n#define N 8\n#define C 17\n#define H 1\n#define W 11\n#define BLOCK_SIZE 17\n\nvoid default_function_kernel(float* compute, float* data) {\n    for (int n = 0; n < N; ++n) {\n        for (int c = 0; c < C; ++c) {\n            for (int h = 0; h < H; ++h) {\n                for (int w = 0; w < W; ++w) {\n                    int index = n * C * H * W + c * H * W + h * W + w;\n                    compute[index] = acosf(data[index]);\n                }\n            }\n        }\n    }\n}\n\n// Call the function like this:\n// default_function_kernel(compute, data);\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data, int size) {\n  #pragma omp parallel for\n  for (int i = 0; i < size; i++) {\n    compute[i] = acosf(data[i]);\n  }\n}\n//End-of-Code|\n\n",
        "gemini2_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Calculate the number of threads per block (17 in this case)\n  const int threadsPerBlock = 17;\n\n  // Calculate the number of blocks needed for the input tensor shape\n  const int numBlocks = (8 * 17 * 1 * 11) / threadsPerBlock;\n\n  // Use OpenMP to parallelize the loop\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int globalIdx = (blockIdx * threadsPerBlock) + threadIdx;\n      compute[globalIdx] = acosf(data[globalIdx]);\n    }\n  }\n}\n//|End-of-Code|\n\n",
        "gpt_1": "#include <math.h>\n#include <stdlib.h>\n#include <stdio.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        // Ensure data[i] is within the domain of acosf, which is [-1, 1]\n        if (data[i] > 1) data[i] = 1;\n        else if (data[i] < -1) data[i] = -1;\n        compute[i] = acosf(data[i]);\n    }\n}\n\nint main() {\n    int n = 8, c = 17, h = 1, w = 11;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration (ensure values are in [-1, 1])\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 2.0f - 1.0f; // Random values between -1 and 1\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = acosf(data[i]);\n    }\n}\n\nint main() {\n    float data[8 * 17 * 1 * 11];\n    float compute[8 * 17 * 1 * 11];\n\n    // Initialize data array with some values within the range of [-1, 1] for acosf\n    for (int i = 0; i < 8 * 17 * 1 * 11; i++) {\n        data[i] = (float)(i % 2 == 0 ? -1.0 : 1.0); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, 8 * 17 * 1 * 11);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "asin",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        for (int32_t i3_s = 0; i3_s < 7; ++i3_s) {\n          compute[((((i0 * 847) + (i1 * 77)) + (i2 * 7)) + i3_s)] = asinf(data[((((i0 * 847) + (i1 * 77)) + (i2 * 7)) + i3_s)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 7623) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 11, 11, 7), \"float32\"), compute: T.Buffer((9, 11, 11, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(9):\n            for i1, i2, i3_s in T.grid(11, 11, 7):\n                cse_var_1: T.int32 = i0 * 847 + i1 * 77 + i2 * 7 + i3_s\n                compute_1 = T.Buffer((7623,), data=compute.data)\n                data_1 = T.Buffer((7623,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])",
        "op_args": [
            9,
            11,
            11,
            7
        ],
        "input_shape": "[[9, 11, 11, 7]]",
        "output_shape": "[[9, 11, 11, 7]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < 9; blockIdx_x++) {\n        for (int blockIdx_y = 0; blockIdx_y < 11; blockIdx_y++) {\n            for (int blockIdx_z = 0; blockIdx_z < 11; blockIdx_z++) {\n                for (int threadIdx_x = 0; threadIdx_x < 7; threadIdx_x++) {\n                    int index = ((blockIdx_x * 11 * 11 * 7) + (blockIdx_y * 11 * 7) + (blockIdx_z * 7) + threadIdx_x);\n                    if ((blockIdx_x * 32 + threadIdx_x) < 7623) {\n                        compute[index] = asinf(data[index]);\n                    }\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_cpu(float* compute, float* data) {\n    int block_x = 9;\n    int block_y = 11;\n    int block_z = 11;\n    int block_w = 7;\n    int total_elements = block_x * block_y * block_z * block_w;\n    \n    for (int i = 0; i < total_elements; i++) {\n        if (i < 7623) {\n            compute[i] = asinf(data[i]);\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data, int size) {\n  #pragma omp parallel for\n  for (int i = 0; i < size; i++) {\n    if (i < 7623) {\n      compute[i] = asinf(data[i]);\n    }\n  }\n}\n//End-of-Code|\n\n",
        "gemini2_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Calculate the number of threads per block (32 in this case)\n  const int threadsPerBlock = 32;\n\n  // Calculate the number of blocks needed for the input tensor shape\n  const int numBlocks = (9 * 11 * 11 * 7) / threadsPerBlock;\n\n  // Use OpenMP to parallelize the loop\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int globalIdx = (blockIdx * threadsPerBlock) + threadIdx;\n      if (globalIdx < 7623) {\n        compute[globalIdx] = asinf(data[globalIdx]);\n      }\n    }\n  }\n}\n//|End-of-Code|\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        // Ensure data[i] is within the domain of asinf, which is [-1, 1]\n        if (data[i] > 1) data[i] = 1;\n        else if (data[i] < -1) data[i] = -1;\n        \n        compute[i] = asinf(data[i]);\n    }\n}\n\nint main() {\n    int n = 9, c = 11, h = 11, w = 7;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration (ensure values are in [-1, 1])\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 2.0f - 1.0f; // Random values between -1 and 1\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        if (i < 7623) {\n            compute[i] = asinf(data[i]);\n        }\n    }\n}\n\nint main() {\n    int size = 9 * 11 * 11 * 7;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values within the range of [-1, 1] for asinf\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i % 2 == 0 ? -1.0 : 1.0); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "asinh",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 240; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 19; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 19) + i3)] = asinhf(data[((i0_i1_fused_i2_fused * 19) + i3)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 4, 15, 19), \"float32\"), compute: T.Buffer((4, 4, 15, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(240):\n            for i3 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 19 + i3\n                compute_1 = T.Buffer((4560,), data=compute.data)\n                data_1 = T.Buffer((4560,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])",
        "op_args": [
            4,
            4,
            15,
            19
        ],
        "input_shape": "[[4, 4, 15, 19]]",
        "output_shape": "[[4, 4, 15, 19]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < 4; blockIdx_x++) {\n        for (int blockIdx_y = 0; blockIdx_y < 4; blockIdx_y++) {\n            for (int blockIdx_z = 0; blockIdx_z < 15; blockIdx_z++) {\n                for (int threadIdx_x = 0; threadIdx_x < 19; threadIdx_x++) {\n                    int index = ((blockIdx_x * 4 * 15 * 19) + (blockIdx_y * 15 * 19) + (blockIdx_z * 19) + threadIdx_x);\n                    compute[index] = asinhf(data[index]);\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_cpu(float* compute, float* data) {\n    int block_x = 4;\n    int block_y = 4;\n    int block_z = 15;\n    int block_w = 19;\n    int total_elements = block_x * block_y * block_z * block_w;\n\n    for (int i = 0; i < total_elements; i++) {\n        compute[i] = asinhf(data[i]);\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data, int size) {\n  #pragma omp parallel for\n  for (int i = 0; i < size; i++) {\n    compute[i] = asinhf(data[i]);\n  }\n}\n//End-of-Code|\n\n",
        "gemini2_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Calculate the number of threads per block (20 in this case)\n  const int threadsPerBlock = 20;\n\n  // Calculate the number of blocks needed for the input tensor shape\n  const int numBlocks = (4 * 4 * 15 * 19) / threadsPerBlock;\n\n  // Use OpenMP to parallelize the loop\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int globalIdx = (blockIdx * threadsPerBlock) + threadIdx;\n      compute[globalIdx] = asinhf(data[globalIdx]);\n    }\n  }\n}\n//|End-of-Code|\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        compute[i] = asinhf(data[i]);\n    }\n}\n\nint main() {\n    int n = 4, c = 4, h = 15, w = 19;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 10.0f - 5.0f; // Random values between -5 and 5\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = asinhf(data[i]);\n    }\n}\n\nint main() {\n    int size = 4 * 4 * 15 * 19;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i - (4 * 4 * 15 * 9)); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "atanh",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3 = 0; i3 < 11; ++i3) {\n          compute[((((i0 * 297) + (i1 * 33)) + (i2 * 11)) + i3)] = atanhf(data[((((i0 * 297) + (i1 * 33)) + (i2 * 11)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(33) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 9, 3, 11), \"float32\"), compute: T.Buffer((18, 9, 3, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(18):\n            for i1, i2, i3 in T.grid(9, 3, 11):\n                cse_var_1: T.int32 = i0 * 297 + i1 * 33 + i2 * 11 + i3\n                compute_1 = T.Buffer((5346,), data=compute.data)\n                data_1 = T.Buffer((5346,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])",
        "op_args": [
            18,
            9,
            3,
            11
        ],
        "input_shape": "[[18, 9, 3, 11]]",
        "output_shape": "[[18, 9, 3, 11]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < 18; blockIdx_x++) {\n        for (int blockIdx_y = 0; blockIdx_y < 9; blockIdx_y++) {\n            for (int blockIdx_z = 0; blockIdx_z < 3; blockIdx_z++) {\n                for (int threadIdx_x = 0; threadIdx_x < 11; threadIdx_x++) {\n                    int index = ((blockIdx_x * 9 * 3 * 11) + (blockIdx_y * 3 * 11) + (blockIdx_z * 11) + threadIdx_x);\n                    compute[index] = atanhf(data[index]);\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_kernel_cpu(float* compute, float* data) {\n    int dim0 = 18, dim1 = 9, dim2 = 3, dim3 = 11;\n    \n    for (int i = 0; i < dim0; i++) {\n        for (int j = 0; j < dim1; j++) {\n            for (int k = 0; k < dim2; k++) {\n                for (int l = 0; l < dim3; l++) {\n                    int index = (((i * dim1 + j) * dim2 + k) * dim3 + l);\n                    compute[index] = atanhf(data[index]);\n                }\n            }\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data, int size) {\n  #pragma omp parallel for\n  for (int i = 0; i < size; i++) {\n    compute[i] = atanhf(data[i]);\n  }\n}\n//End-of-Code|\n\n",
        "gemini2_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Calculate the number of threads per block (33 in this case)\n  const int threadsPerBlock = 33;\n\n  // Calculate the number of blocks needed for the input tensor shape\n  const int numBlocks = (18 * 9 * 3 * 11) / threadsPerBlock;\n\n  // Use OpenMP to parallelize the loop\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int globalIdx = (blockIdx * threadsPerBlock) + threadIdx;\n      compute[globalIdx] = atanhf(data[globalIdx]);\n    }\n  }\n}\n//|End-of-Code|\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        compute[i] = atanhf(data[i]);\n    }\n}\n\nint main() {\n    int n = 18, c = 9, h = 3, w = 11;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 2.0f - 1.0f; // Random values between -1 and 1\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = atanhf(data[i]);\n    }\n}\n\nint main() {\n    int size = 18 * 9 * 3 * 11;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values within the range of [-1, 1] for atanhf\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i % 2 == 0 ? -0.5 : 0.5); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "ceil",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 88; ++i0_i1_fused) {\n    for (int32_t i3 = 0; i3 < 17; ++i3) {\n      compute[((i0_i1_fused * 17) + i3)] = ceilf(data[((i0_i1_fused * 17) + i3)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 187) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 11, 1, 17), \"float32\"), compute: T.Buffer((8, 11, 1, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(88):\n            for i3 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused * 17 + i3\n                compute_1 = T.Buffer((1496,), data=compute.data)\n                data_1 = T.Buffer((1496,), data=data.data)\n                compute_1[cse_var_1] = T.ceil(data_1[cse_var_1])",
        "op_args": [
            8,
            11,
            1,
            17
        ],
        "input_shape": "[[8, 11, 1, 17]]",
        "output_shape": "[[8, 11, 1, 17]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < 8; blockIdx_x++) {\n        for (int blockIdx_y = 0; blockIdx_y < 11; blockIdx_y++) {\n            for (int blockIdx_z = 0; blockIdx_z < 1; blockIdx_z++) {\n                for (int threadIdx_x = 0; threadIdx_x < 17; threadIdx_x++) {\n                    int global_idx = (blockIdx_x * 11 * 17) + (blockIdx_y * 17) + threadIdx_x;\n                    int check_idx = (blockIdx_x * 4) + (threadIdx_x >> 3);\n                    if (check_idx < 187) {\n                        compute[global_idx] = ceilf(data[global_idx]);\n                    }\n                }\n            }\n        }\n    }\n}",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_kernel_CPU(float* compute, float* data) {\n    int gridDim_x = 8;\n    int blockDim_x = 32;\n\n    for (int blockIdx_x = 0; blockIdx_x < gridDim_x; blockIdx_x++) {\n        for (int threadIdx_x = 0; threadIdx_x < blockDim_x; threadIdx_x++) {\n            if ((blockIdx_x * 4 + (threadIdx_x >> 3)) < 187) {\n                int compute_index = blockIdx_x * blockDim_x + threadIdx_x;\n                int data_index = blockIdx_x * blockDim_x + threadIdx_x;\n\n                compute[compute_index] = ceilf(data[data_index]);\n            }\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data, int size) {\n  #pragma omp parallel for\n  for (int i = 0; i < size; i++) {\n    if ((i / 8) < 187) {\n      compute[i] = ceilf(data[i]);\n    }\n  }\n}\n//End-of-Code|\n",
        "gemini2_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Calculate the number of threads per block (32 in this case)\n  const int threadsPerBlock = 32;\n\n  // Calculate the number of blocks needed for the input tensor shape\n  const int numBlocks = (8 * 11 * 1 * 17) / threadsPerBlock;\n\n  // Use OpenMP to parallelize the loop\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int globalIdx = (blockIdx * threadsPerBlock) + threadIdx;\n      if ((blockIdx * 4 + (threadIdx >> 3)) < 187) {\n        compute[globalIdx] = ceilf(data[globalIdx]);\n      }\n    }\n  }\n}\n//|End-of-Code|\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        int blockIdx_x = i / 32;\n        int threadIdx_x = i % 32;\n        if ((blockIdx_x * 4 + (threadIdx_x >> 3)) < 187) {\n            compute[i] = ceilf(data[i]);\n        }\n    }\n}\n\nint main() {\n    int n = 8, c = 11, h = 1, w = 17;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 100.0f; // Random values between 0 and 100\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        if ((i / 8) < 187) {\n            compute[i] = ceilf(data[i]);\n        }\n    }\n}\n\nint main() {\n    int size = 8 * 11 * 1 * 17;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i - (size / 2)); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "erf",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 312; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = erff(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 6, 1, 4), \"float32\"), compute: T.Buffer((13, 6, 1, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(312):\n            compute_1 = T.Buffer((312,), data=compute.data)\n            data_1 = T.Buffer((312,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.erf(data_1[i0_i1_fused_i2_fused_i3_fused])",
        "op_args": [
            13,
            6,
            1,
            4
        ],
        "input_shape": "[[13, 6, 1, 4]]",
        "output_shape": "[[13, 6, 1, 4]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < 1; blockIdx_x++) {\n        for (int blockIdx_y = 0; blockIdx_y < 20; blockIdx_y++) {\n            for (int blockIdx_z = 0; blockIdx_z < 14; blockIdx_z++) {\n                for (int threadIdx_x = 0; threadIdx_x < 13; threadIdx_x++) {\n                    int index = ((blockIdx_x * 20 * 14 * 13) + (blockIdx_y * 14 * 13) + (blockIdx_z * 13) + threadIdx_x);\n                    compute[index] = tanhf(data[index]);\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_kernel_CPU(float* compute, float* data) {\n    int gridDim_x = 13;\n    int blockDim_x = 2;\n\n    for (int blockIdx_x = 0; blockIdx_x < gridDim_x; blockIdx_x++) {\n        for (int threadIdx_x = 0; threadIdx_x < blockDim_x; threadIdx_x++) {\n            int index = blockIdx_x * blockDim_x + threadIdx_x;\n\n            if (index < 13 * 6 * 1 * 4) {\n                compute[index] = erff(data[index]);\n            }\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data, int size) {\n  #pragma omp parallel for\n  for (int i = 0; i < size; i++) {\n    compute[i] = erff(data[i]);\n  }\n}\n//End-of-Code|\n\n",
        "gemini2_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Calculate the number of threads per block (2 in this case)\n  const int threadsPerBlock = 2;\n\n  // Calculate the number of blocks needed for the input tensor shape\n  const int numBlocks = (13 * 6 * 1 * 4) / threadsPerBlock;\n\n  // Use OpenMP to parallelize the loop\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int globalIdx = (blockIdx * threadsPerBlock) + threadIdx;\n      compute[globalIdx] = erff(data[globalIdx]);\n    }\n  }\n}\n//|End-of-Code|\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        compute[i] = erff(data[i]);\n    }\n}\n\nint main() {\n    int n = 13, c = 6, h = 1, w = 4;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 2.0f - 1.0f; // Random values between -1 and 1\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = erff(data[i]);\n    }\n}\n\nint main() {\n    int size = 13 * 6 * 1 * 4;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i - (size / 2)); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "exp",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 19; ++i1) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 10; ++i3) {\n        compute[(((i1 * 40) + (i2 * 10)) + i3)] = expf(data[(((i1 * 40) + (i2 * 10)) + i3)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = __expf(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 19, 4, 10), \"float32\"), compute: T.Buffer((1, 19, 4, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(19, 4, 10):\n            cse_var_1: T.int32 = i1 * 40 + i2 * 10 + i3\n            compute_1 = T.Buffer((760,), data=compute.data)\n            data_1 = T.Buffer((760,), data=data.data)\n            compute_1[cse_var_1] = T.exp(data_1[cse_var_1])",
        "op_args": [
            1,
            19,
            4,
            10
        ],
        "input_shape": "[[1, 19, 4, 10]]",
        "output_shape": "[[1, 19, 4, 10]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < 1; blockIdx_x++) {\n        for (int blockIdx_y = 0; blockIdx_y < 19; blockIdx_y++) {\n            for (int blockIdx_z = 0; blockIdx_z < 4; blockIdx_z++) {\n                for (int threadIdx_x = 0; threadIdx_x < 10; threadIdx_x++) {\n                    int index = ((blockIdx_x * 19 * 4 * 10) + (blockIdx_y * 4 * 10) + (blockIdx_z * 10) + threadIdx_x);\n                    compute[index] = expf(data[index]);\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_kernel_CPU(float* compute, float* data) {\n    int gridDim_x = 1;\n    int blockDim_x = 20;\n\n    for (int blockIdx_x = 0; blockIdx_x < gridDim_x; blockIdx_x++) {\n        for (int threadIdx_x = 0; threadIdx_x < blockDim_x; threadIdx_x++) {\n            int index = blockIdx_x * blockDim_x + threadIdx_x;\n\n            if (index < 1 * 19 * 4 * 10) {\n                compute[index] = expf(data[index]);\n            }\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data, int size) {\n  #pragma omp parallel for\n  for (int i = 0; i < size; i++) {\n    compute[i] = expf(data[i]);\n  }\n}\n//End-of-Code|\n\n",
        "gemini2_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Calculate the number of threads per block (20 in this case)\n  const int threadsPerBlock = 20;\n\n  // Calculate the number of blocks needed for the input tensor shape\n  const int numBlocks = (1 * 19 * 4 * 10) / threadsPerBlock;\n\n  // Use OpenMP to parallelize the loop\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int globalIdx = (blockIdx * threadsPerBlock) + threadIdx;\n      compute[globalIdx] = expf(data[globalIdx]);\n    }\n  }\n}\n//|End-of-Code|\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        compute[i] = expf(data[i]);\n    }\n}\n\nint main() {\n    int n = 1, c = 19, h = 4, w = 10;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 10.0f; // Random values between 0 and 10\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = expf(data[i]);\n    }\n}\n\nint main() {\n    int size = 1 * 19 * 4 * 10;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i - (size / 2)); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "fast_exp",
        "c_code": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 19; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n            int32_t v_ = ((int32_t)(floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_fast_exp[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 19, 2, 16), \"float32\"), T_fast_exp: T.Buffer((15, 19, 2, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(15):\n            for ax1, ax2, ax3 in T.grid(19, 2, 16):\n                cse_var_1: T.int32 = ax0 * 608 + ax1 * 32 + ax2 * 16 + ax3\n                T_fast_exp_1 = T.Buffer((9120,), data=T_fast_exp.data)\n                data_1 = T.Buffer((9120,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])",
        "op_args": [
            15,
            19,
            2,
            16
        ],
        "input_shape": "[[15, 19, 2, 16]]",
        "output_shape": "[[15, 19, 2, 16]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* T_fast_exp, float* data) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < 15; blockIdx_x++) {\n        for (int blockIdx_y = 0; blockIdx_y < 19; blockIdx_y++) {\n            for (int blockIdx_z = 0; blockIdx_z < 2; blockIdx_z++) {\n                for (int threadIdx_x = 0; threadIdx_x < 16; threadIdx_x++) {\n                    int index = ((blockIdx_x * 19 * 2 * 16) + (blockIdx_y * 2 * 16) + (blockIdx_z * 16) + threadIdx_x);\n\n                    float v_ = floorf(fmaxf(fminf(data[index], 88.37627f), -88.37626f) * 1.442695f + 0.5f) + 127.0f;\n                    int v_int = ((int)v_) << 23;\n\n                    float result = fmaxf(*(float*)(&v_int) *\n                        (\n                            (\n                                1.987569e-04f * (fmaxf(fminf(data[index], 88.37627f), -88.37626f) - (floorf(fmaxf(fminf(data[index], 88.37627f), -88.37626f) * 1.442695f) * 0.6931472f))\n                                + 1.398200e-03f * (fmaxf(fminf(data[index], 88.37627f), -88.37626f) - (floorf(fmaxf(fminf(data[index], 88.37627f), -88.37626f) * 1.442695f) * 0.6931472f))\n                                + 8.333452e-03f * (fmaxf(fminf(data[index], 88.37627f), -88.37626f) - (floorf(fmaxf(fminf(data[index], 88.37627f), -88.37626f) * 1.442695f) * 0.6931472f))\n                                + 4.166580e-02f * (fmaxf(fminf(data[index], 88.37627f), -88.37626f) - (floorf(fmaxf(fminf(data[index], 88.37627f), -88.37626f) * 1.442695f) * 0.6931472f))\n                                + 1.666667e-01f * (fmaxf(fminf(data[index], 88.37627f), -88.37626f) - (floorf(fmaxf(fminf(data[index], 88.37627f), -88.37626f) * 1.442695f) * 0.6931472f))\n                                + 0.5f * (fmaxf(fminf(data[index], 88.37627f), -88.37626f) - (floorf(fmaxf(fminf(data[index], 88.37627f), -88.37626f) * 1.442695f) * 0.6931472f))\n                            ) \n                            * (fmaxf(fminf(data[index], 88.37627f), -88.37626f) - (floorf(fmaxf(fminf(data[index], 88.37627f), -88.37626f) * 1.442695f) * 0.6931472f))\n                            + (fmaxf(fminf(data[index], 88.37627f), -88.37626f) - (floorf(fmaxf(fminf(data[index], 88.37627f), -88.37626f) * 1.442695f) * 0.6931472f))\n                            + 1.0f\n                        ), data[index]);\n\n                    T_fast_exp[index] = result;\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_kernel_CPU(float* T_fast_exp, float* data) {\n    int gridDim_x = 15;\n    int blockDim_x = 24;\n\n    for (int blockIdx_x = 0; blockIdx_x < gridDim_x; blockIdx_x++) {\n        for (int threadIdx_x = 0; threadIdx_x < blockDim_x; threadIdx_x++) {\n            int index = blockIdx_x * blockDim_x + threadIdx_x;\n\n            if (index < 15 * 19 * 2 * 16) {\n                float value = data[index];\n\n                float clamped_value = fmaxf(fminf(value, 88.37627f), -88.37626f);\n                float exp_value = expf(clamped_value);\n\n                // Complex expression, simplified for readability\n                float result = exp_value * (1 + clamped_value - floorf((clamped_value * 1.442695f + 0.5f)) * 0.6931472f);\n\n                T_fast_exp[index] = result;\n            }\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n#include <float.h>\n\nvoid default_function_kernel(float* T_fast_exp, float* data, int size) {\n  #pragma omp parallel for\n  for (int i = 0; i < size; i++) {\n    float x = fmaxf(fminf(data[i], 8.837627e+01f), -8.837626e+01f);\n    int v_ = (int)(floorf((fmaxf(fminf(x, 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f) << 23;\n    float exp_val = (*(float *)(&(v_))) * (((((((((((((1.987569e-04f * (x - (floorf((x * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (x - (floorf((x * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (x - (floorf((x * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (x - (floorf((x * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (x - (floorf((x * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (x - (floorf((x * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (x - (floorf((x * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (x - (floorf((x * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n    T_fast_exp[i] = fmaxf(exp_val, data[i]);\n  }\n}\n//End-of-Code|\n\n",
        "gemini2_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* T_fast_exp, float* data) {\n  // Calculate the number of threads per block (24 in this case)\n  const int threadsPerBlock = 24;\n\n  // Calculate the number of blocks needed for the input tensor shape\n  const int numBlocks = (15 * 19 * 2 * 16) / threadsPerBlock;\n\n  // Use OpenMP to parallelize the loop\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int globalIdx = (blockIdx * threadsPerBlock) + threadIdx;\n      float x = data[globalIdx];\n      x = fmaxf(fminf(x, 8.837627e+01f), -8.837626e+01f); // Clamp x to [-88.37626, 88.37627]\n      int v_ = (int)(floorf((x * 1.442695e+00f) + 5.000000e-01f) + 1.270000e+02f) << 23;\n      float exp_approx = (*(float *)(&(v_))) * (\n          (1.987569e-04f * (x - (floorf((x * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))) +\n          (1.398200e-03f * (x - (floorf((x * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))) +\n          (8.333452e-03f * (x - (floorf((x * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))) +\n          (4.166580e-02f * (x - (floorf((x * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))) +\n          (1.666667e-01f * (x - (floorf((x * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))) +\n          (5.000000e-01f * (x - (floorf((x * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))) *\n          (x - (floorf((x * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f)) +\n          (x - (floorf((x * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f)) + 1.000000e+00f);\n      T_fast_exp[globalIdx] = fmaxf(exp_approx, data[globalIdx]);\n    }\n  }\n}\n//|End-of-Code|\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* T_fast_exp, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        float x = data[i];\n        x = fmaxf(fminf(x, 8.837627e+01f), -8.837626e+01f);\n        float v_ = ((int)(floorf(x * 1.442695e+00f + 5.000000e-01f) + 1.270000e+02f)) << 23;\n        float base = *((float *)(&v_));\n        float diff = x - floorf(x * 1.442695e+00f + 5.000000e-01f) * 6.931472e-01f;\n        float result = base *\n                       (((((((((((((1.987569e-04f * diff +\n                                    1.398200e-03f) * diff +\n                                    8.333452e-03f) * diff +\n                                    4.166580e-02f) * diff +\n                                    1.666667e-01f) * diff +\n                                    5.000000e-01f) * diff +\n                                    1.000000e+00f) * diff + \n                                    1.000000e+00f))));\n        T_fast_exp[i] = fmaxf(result, x);\n    }\n}\n\nint main() {\n    int n = 15, c = 19, h = 2, w = 16;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* T_fast_exp = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 20.0f - 10.0f; // Random values between -10 and 10\n    }\n\n    default_function(T_fast_exp, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", T_fast_exp[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(T_fast_exp);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        float val = data[i];\n        float clamped_val = fmaxf(fminf(val, 8.837627e+01f), -8.837626e+01f);\n        float exp_val = clamped_val * 1.442695e+00f;\n        int v_ = ((int)(floorf(exp_val + 5.000000e-01f)) + 1.270000e+02f) << 23;\n\n        float mantissa = clamped_val - floorf(exp_val) * 6.931472e-01f;\n        float result = (*(float *)&v_);\n        result *= (((((((((((((1.987569e-04f * mantissa) + 1.398200e-03f) * mantissa) + 8.333452e-03f) * mantissa) + 4.166580e-02f) * mantissa) + 1.666667e-01f) * mantissa) + 5.000000e-01f) * mantissa) + 1.000000e+00f);\n\n        T_fast_exp[i] = fmaxf(result, val);\n    }\n}\n\nint main() {\n    int size = 15 * 19 * 2 * 16;\n    float data[size];\n    float T_fast_exp[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i - (size / 2)); // Some sample initialization\n    }\n\n    default_function_kernel(T_fast_exp, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"T_fast_exp[%d] = %f\\n\", i, T_fast_exp[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "fast_tanh",
        "c_code": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 252; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 12; ++ax3) {\n        T_fast_tanh[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)] = ((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 14, 2, 12), \"float32\"), T_fast_tanh: T.Buffer((18, 14, 2, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(252):\n            for ax2, ax3 in T.grid(2, 12):\n                cse_var_1: T.int32 = ax0_ax1_fused * 24 + ax2 * 12 + ax3\n                T_fast_tanh_1 = T.Buffer((6048,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((6048,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))",
        "op_args": [
            18,
            14,
            2,
            12
        ],
        "input_shape": "[[18, 14, 2, 12]]",
        "output_shape": "[[18, 14, 2, 12]]",
        "cluade1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\n#define BLOCK_SIZE 18\n#define GRID_SIZE_X 14\n#define GRID_SIZE_Y 2\n#define GRID_SIZE_Z 12\n\nvoid default_function_kernel(float* T_fast_tanh, float* data) {\n    #pragma omp parallel for collapse(3)\n    for (int i = 0; i < GRID_SIZE_X; i++) {\n        for (int j = 0; j < GRID_SIZE_Y; j++) {\n            for (int k = 0; k < GRID_SIZE_Z; k++) {\n                for (int l = 0; l < BLOCK_SIZE; l++) {\n                    int idx = ((((i * GRID_SIZE_Y + j) * GRID_SIZE_Z + k) * BLOCK_SIZE + l));\n                    float val = data[idx];\n                    float temp = fminf(9.0f, fmaxf(-9.0f, val));\n                    T_fast_tanh[idx] = temp * (temp * (temp * (temp * (temp * (temp * (temp * (temp * (temp * (temp * (temp * (temp * (temp * -2.760768e-16f + 2.000188e-13f) - 8.604672e-11f) + 5.122297e-08f) + 1.485722e-05f) + 6.372619e-04f) + 4.893525e-03f) / (temp * (temp * (temp * (temp * (temp * (temp * (temp * (temp * (temp * (1.198258e-06f + 1.185347e-04f) + 2.268435e-03f) + 4.893525e-03f)))))))))));\n                }\n            }\n        }\n    }\n}\n\nint main() {\n    float* T_fast_tanh = (float*)malloc(sizeof(float) * 18 * 14 * 2 * 12);\n    float* data = (float*)malloc(sizeof(float) * 18 * 14 * 2 * 12);\n\n    // Fill data with values\n\n    default_function_kernel(T_fast_tanh, data);\n\n    // Print or use T_fast_tanh\n\n    free(T_fast_tanh);\n    free(data);\n\n    return 0;\n}\n",
        "cluade2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel_cpu(float* T_fast_tanh, float* data) {\n    const int shape[4] = {18, 14, 2, 12};\n\n    for (int i = 0; i < shape[0]; i++) {\n        for (int j = 0; j < shape[1]; j++) {\n            for (int k = 0; k < shape[2]; k++) {\n                for (int l = 0; l < shape[3]; l++) {\n                    int index = i * shape[1] * shape[2] * shape[3] + j * shape[2] * shape[3] + k * shape[3] + l;\n\n                    float value = fmaxf(-9.0f, fminf(9.0f, data[index]));\n                    float result = value * (value * (value * (value * (value * (value * (value * (value * -2.760768e-16f + 2.000188e-13f) - 8.604672e-11f) + 5.122297e-08f) + 1.485722e-05f) + 6.372619e-04f) + 4.893525e-03f) / (value * (value * (value * (value * (value * (value * 1.198258e-06f + 1.185347e-04f) + 2.268435e-03f) + 4.893525e-03f))));\n\n                    T_fast_tanh[index] = result;\n                }\n            }\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* T_fast_tanh, float* data) {\n  const int block_size = 18;\n  const int num_threads = 18;\n  #pragma omp parallel for\n  for (int i = 0; i < 18 * 14 * 2 * 12; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    float value = data[i];\n    value = fmaxf(-9.0f, fminf(9.0f, value));\n    T_fast_tanh[i] = (value * value * value * value * value * value * value * value * value * value * value * value * value * value * value * value * value * value * -2.760768e-16f + 2.000188e-13f - 8.604672e-11f + 5.122297e-08f + 1.485722e-05f + 6.372619e-04f + 4.893525e-03f) / (value * value * value * value * value * value * value * value * value * value * value * value * 1.198258e-06f + 1.185347e-04f + 2.268435e-03f + 4.893525e-03f);\n  }\n}\n\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* T_fast_tanh, float* data) {\n  // Input tensor shape: [18, 14, 2, 12]\n  // T_fast_tanh shape: [18, 14, 2, 12]\n  // data shape: [18, 14, 2, 12]\n\n  for (int blockIdx_x = 0; blockIdx_x < 18; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 14; ++threadIdx_x) {\n      for (int i = 0; i < 2; ++i) {\n        for (int j = 0; j < 12; ++j) {\n          float x = data[((blockIdx_x * 336) + (threadIdx_x * 24) + (i * 12) + j)];\n          x = fmaxf(-9.0f, fminf(9.0f, x));\n          T_fast_tanh[((blockIdx_x * 336) + (threadIdx_x * 24) + (i * 12) + j)] = (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * (x * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f))));\n        }\n      }\n    }\n  }\n}\n\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* T_fast_tanh, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        float x = fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[i]));\n        float x2 = x * x;\n        float x4 = x2 * x2;\n        float x8 = x4 * x4;\n\n        float numerator = ((((((((x * x2 * x4 * x8 * -2.760768e-16f) + 2.000188e-13f) * x2 * x4 * x8) + -8.604672e-11f) * x2 * x4) + 5.122297e-08f) * x2) + 1.485722e-05f) * x2 + 6.372619e-04f) * x2 + 4.893525e-03f;\n\n        float denominator = (((((x2 * x4 * x8 * 1.198258e-06f) + 1.185347e-04f) * x2 * x4) + 2.268435e-03f) * x2 + 4.893525e-03f);\n\n        T_fast_tanh[i] = numerator / denominator;\n    }\n}\n\nint main() {\n    int n = 18, c = 14, h = 2, w = 12;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* T_fast_tanh = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 18.0f - 9.0f; // Random values between -9 and 9\n    }\n\n    default_function(T_fast_tanh, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", T_fast_tanh[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(T_fast_tanh);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nfloat fast_tanh(float x) {\n    float clamped_val = fmaxf(-9.000000e+00f, fminf(9.000000e+00f, x));\n    float x2 = clamped_val * clamped_val;\n    float x4 = x2 * x2;\n    float x8 = x4 * x4;\n\n    float numerator = clamped_val * (x2 * (x4 * (x8 * -2.760768e-16f + 2.000188e-13f) + -8.604672e-11f) + 5.122297e-08f) + 1.485722e-05f;\n    numerator = numerator * x2 + 6.372619e-04f;\n    numerator = numerator * x2 + 4.893525e-03f;\n\n    float denominator = (x2 * (x4 * 1.198258e-06f + 1.185347e-04f) + 2.268435e-03f) + 4.893525e-03f;\n\n    return numerator / denominator;\n}\n\nvoid default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        T_fast_tanh[i] = fast_tanh(data[i]);\n    }\n}\n\nint main() {\n    int size = 18 * 14 * 2 * 12;\n    float data[size];\n    float T_fast_tanh[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i - (size / 2)); // Some sample initialization\n    }\n\n    default_function_kernel(T_fast_tanh, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"T_fast_tanh[%d] = %f\\n\", i, T_fast_tanh[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "floor",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 594; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 10; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 10) + i3)] = floorf(data[((i0_i1_fused_i2_fused * 10) + i3)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 18, 11, 10), \"float32\"), compute: T.Buffer((3, 18, 11, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(594):\n            for i3 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 10 + i3\n                compute_1 = T.Buffer((5940,), data=compute.data)\n                data_1 = T.Buffer((5940,), data=data.data)\n                compute_1[cse_var_1] = T.floor(data_1[cse_var_1])",
        "op_args": [
            3,
            18,
            11,
            10
        ],
        "input_shape": "[[3, 18, 11, 10]]",
        "output_shape": "[[3, 18, 11, 10]]",
        "cluade1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\n#define BLOCK_SIZE 15\n#define GRID_SIZE_X 18\n#define GRID_SIZE_Y 11\n#define GRID_SIZE_Z 10\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for collapse(3)\n    for (int i = 0; i < GRID_SIZE_X; i++) {\n        for (int j = 0; j < GRID_SIZE_Y; j++) {\n            for (int k = 0; k < GRID_SIZE_Z; k++) {\n                for (int l = 0; l < BLOCK_SIZE; l++) {\n                    int idx = ((((i * GRID_SIZE_Y + j) * GRID_SIZE_Z + k) * BLOCK_SIZE + l));\n                    compute[idx] = floorf(data[idx]);\n                }\n            }\n        }\n    }\n}\n\nint main() {\n    float* compute = (float*)malloc(sizeof(float) * 3 * 18 * 11 * 10);\n    float* data = (float*)malloc(sizeof(float) * 3 * 18 * 11 * 10);\n\n    // Fill data with values\n\n    default_function_kernel(compute, data);\n\n    // Print or use compute\n\n    free(compute);\n    free(data);\n\n    return 0;\n}\n",
        "cluade2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel_cpu(float* compute, float* data) {\n    const int shape[4] = {3, 18, 11, 10};\n\n    for (int i = 0; i < shape[0]; i++) {\n        for (int j = 0; j < shape[1]; j++) {\n            for (int k = 0; k < shape[2]; k++) {\n                for (int l = 0; l < shape[3]; l++) {\n                    int index = i * shape[1] * shape[2] * shape[3] + j * shape[2] * shape[3] + k * shape[3] + l;\n\n                    compute[index] = floorf(data[index]);\n                }\n            }\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  const int block_size = 15;\n  const int num_threads = 15;\n  #pragma omp parallel for\n  for (int i = 0; i < 3 * 18 * 11 * 10; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    compute[i] = floorf(data[i]);\n  }\n}\n\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Input tensor shape: [3, 18, 11, 10]\n  // compute shape: [3, 18, 11, 10]\n  // data shape: [3, 18, 11, 10]\n\n  for (int blockIdx_x = 0; blockIdx_x < 3; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 18; ++threadIdx_x) {\n      for (int i = 0; i < 11; ++i) {\n        for (int j = 0; j < 10; ++j) {\n          compute[((blockIdx_x * 1980) + (threadIdx_x * 110) + (i * 10) + j)] = floorf(data[((blockIdx_x * 1980) + (threadIdx_x * 110) + (i * 10) + j)]);\n        }\n      }\n    }\n  }\n}\n\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        compute[i] = floorf(data[i]);\n    }\n}\n\nint main() {\n    int n = 3, c = 18, h = 11, w = 10;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 20.0f - 10.0f; // Random values between -10 and 10\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = floorf(data[i]);\n    }\n}\n\nint main() {\n    int size = 3 * 18 * 11 * 10;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i - (size / 2)); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "isnan",
        "c_code": "void default_function_kernel(int8_t* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 112; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 5; ++i3_s) {\n        compute[(((i0_i1_fused * 95) + (i2 * 5)) + i3_s)] = ((int8_t)(data[(((i0_i1_fused * 95) + (i2 * 5)) + i3_s)] != data[(((i0_i1_fused * 95) + (i2 * 5)) + i3_s)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 665) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 8, 19, 5), \"float32\"), compute: T.Buffer((14, 8, 19, 5), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(112):\n            for i2, i3_s in T.grid(19, 5):\n                cse_var_1: T.int32 = i0_i1_fused * 95 + i2 * 5 + i3_s\n                compute_1 = T.Buffer((10640,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((10640,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))",
        "op_args": [
            14,
            8,
            19,
            5
        ],
        "input_shape": "[[14, 8, 19, 5]]",
        "output_shape": "[[14, 8, 19, 5]]",
        "cluade1_c": "#include <stdio.h>\n#include <omp.h>\n\nvoid default_function_kernel(signed char* compute, float* data) {\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < 14; ++i) {\n        for (int j = 0; j < 8; ++j) {\n            for (int k = 0; k < 19; ++k) {\n                for (int l = 0; l < 5; ++l) {\n                    int blockIdx_x = i * 8 * 19 * 5 + j * 19 * 5 + k * 5 + l;\n                    int threadIdx_x = omp_get_thread_num();\n\n                    if ((blockIdx_x * 2 + (threadIdx_x >> 4)) < 665) {\n                        compute[blockIdx_x * 32 + threadIdx_x] = (signed char)(data[blockIdx_x * 32 + threadIdx_x] != data[blockIdx_x * 32 + threadIdx_x]);\n                    }\n                }\n            }\n        }\n    }\n}\n\n// End of Code",
        "cluade2_c": "#include <stdio.h>\n\nvoid default_function_kernel_cpu(signed char* compute, float* data) {\n    const int shape[4] = {14, 8, 19, 5};\n    const int total_elements = shape[0] * shape[1] * shape[2] * shape[3];\n\n    for (int i = 0; i < total_elements; i++) {\n        int block_idx = i / 32;\n        int thread_idx = i % 32;\n\n        if ((block_idx * 2 + (thread_idx >> 4)) < 665) {\n            compute[i] = (signed char)(data[i] != data[i]);\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(signed char* compute, float* data) {\n  const int block_size = 32;\n  const int num_threads = 32;\n  #pragma omp parallel for\n  for (int i = 0; i < 14 * 8 * 19 * 5; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    if ((block_idx * 2 + thread_idx >> 4) < 665) {\n      compute[i] = (signed char)(data[i] != data[i]);\n    }\n  }\n}\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(signed char* compute, float* data) {\n  // Input tensor shape: [14, 8, 19, 5]\n  // compute shape: [14, 8, 19, 5]\n  // data shape: [14, 8, 19, 5]\n\n  for (int blockIdx_x = 0; blockIdx_x < 14; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 32; ++threadIdx_x) {\n      if (((blockIdx_x * 2) + (threadIdx_x >> 4)) < 665) {\n        compute[((blockIdx_x * 560) + threadIdx_x)] = (signed char)(data[((blockIdx_x * 560) + threadIdx_x)] != data[((blockIdx_x * 560) + threadIdx_x)]);\n      }\n    }\n  }\n}\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\nvoid default_function(signed char* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        if ((i / 32 * 2 + (i % 32) / 16) < 665) {\n            compute[i] = (signed char)(data[i] != data[i]);\n        }\n    }\n}\n\nint main() {\n    int n = 14, c = 8, h = 19, w = 5;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    signed char* compute = (signed char*)malloc(total_elements * sizeof(signed char));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 20.0f - 10.0f; // Random values between -10 and 10\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%d \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <stdio.h>\n\nvoid default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data, int size) {\n    int limit = 665; // Derived from the shape and the condition in the CUDA kernel\n    for (int i = 0; i < size; i++) {\n        if ((i / 16) < limit) {\n            compute[i] = (signed char)(data[i] != data[i]);\n        }\n    }\n}\n\nint main() {\n    int size = 14 * 8 * 19 * 5;\n    float data[size];\n    signed char compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i - (size / 2)); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %d\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "log",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        for (int32_t i3 = 0; i3 < 17; ++i3) {\n          compute[((((i0 * 1836) + (i1 * 102)) + (i2 * 17)) + i3)] = logf(data[((((i0 * 1836) + (i1 * 102)) + (i2 * 17)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(51) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] = __logf(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 18, 6, 17), \"float32\"), compute: T.Buffer((3, 18, 6, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(3):\n            for i1, i2, i3 in T.grid(18, 6, 17):\n                cse_var_1: T.int32 = i0 * 1836 + i1 * 102 + i2 * 17 + i3\n                compute_1 = T.Buffer((5508,), data=compute.data)\n                data_1 = T.Buffer((5508,), data=data.data)\n                compute_1[cse_var_1] = T.log(data_1[cse_var_1])",
        "op_args": [
            3,
            18,
            6,
            17
        ],
        "input_shape": "[[3, 18, 6, 17]]",
        "output_shape": "[[3, 18, 6, 17]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < 3; ++i) {\n        for (int j = 0; j < 18; ++j) {\n            for (int k = 0; k < 6; ++k) {\n                for (int l = 0; l < 17; ++l) {\n                    int blockIdx_x = i * 18 * 6 * 17 + j * 6 * 17 + k * 17 + l;\n                    int threadIdx_x = omp_get_thread_num();\n\n                    if (blockIdx_x * 51 + threadIdx_x < 3 * 18 * 6 * 17) {\n                        compute[blockIdx_x * 51 + threadIdx_x] = logf(data[blockIdx_x * 51 + threadIdx_x]);\n                    }\n                }\n            }\n        }\n    }\n}\n\n// End of Code",
        "cluade2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel_cpu(float* compute, float* data) {\n    const int shape[4] = {3, 18, 6, 17};\n    const int total_elements = shape[0] * shape[1] * shape[2] * shape[3];\n\n    for (int i = 0; i < total_elements; i++) {\n        int block_idx = i / 51;\n        int thread_idx = i % 51;\n\n        compute[i] = logf(data[i]);\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  const int block_size = 51;\n  const int num_threads = 51;\n  #pragma omp parallel for\n  for (int i = 0; i < 3 * 18 * 6 * 17; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    compute[i] = logf(data[i]);\n  }\n}\n\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Input tensor shape: [3, 18, 6, 17]\n  // compute shape: [3, 18, 6, 17]\n  // data shape: [3, 18, 6, 17]\n\n  for (int blockIdx_x = 0; blockIdx_x < 3; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 18; ++threadIdx_x) {\n      for (int i = 0; i < 6; ++i) {\n        for (int j = 0; j < 17; ++j) {\n          compute[((blockIdx_x * 1836) + (threadIdx_x * 102) + (i * 17) + j)] = logf(data[((blockIdx_x * 1836) + (threadIdx_x * 102) + (i * 17) + j)]);\n        }\n      }\n    }\n  }\n}\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        compute[i] = logf(data[i]);\n    }\n}\n\nint main() {\n    int n = 3, c = 18, h = 6, w = 17;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 9.0f + 1.0f; // Random values between 1 and 10\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = (0.000000e+00f < data[i]) ? data[i] : (data[i] * 5.000000e-01f);\n    }\n}\n\nint main() {\n    int size = 3 * 12 * 18 * 4;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i - (size / 2)); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "log10",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 42; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      for (int32_t i3 = 0; i3 < 6; ++i3) {\n        compute[(((i0_i1_fused * 18) + (i2 * 6)) + i3)] = log10f(data[(((i0_i1_fused * 18) + (i2 * 6)) + i3)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 6, 3, 6), \"float32\"), compute: T.Buffer((7, 6, 3, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(42):\n            for i2, i3 in T.grid(3, 6):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2 * 6 + i3\n                compute_1 = T.Buffer((756,), data=compute.data)\n                data_1 = T.Buffer((756,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])",
        "op_args": [
            7,
            6,
            3,
            6
        ],
        "input_shape": "[[7, 6, 3, 6]]",
        "output_shape": "[[7, 6, 3, 6]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < 7; ++i) {\n        for (int j = 0; j < 6; ++j) {\n            for (int k = 0; k < 3; ++k) {\n                for (int l = 0; l < 6; ++l) {\n                    int blockIdx_x = i * 6 * 3 * 6 + j * 3 * 6 + k * 6 + l;\n                    int threadIdx_x = omp_get_thread_num();\n\n                    if (blockIdx_x * 54 + threadIdx_x < 7 * 6 * 3 * 6) {\n                        compute[blockIdx_x * 54 + threadIdx_x] = log10f(data[blockIdx_x * 54 + threadIdx_x]);\n                    }\n                }\n            }\n        }\n    }\n}\n\n// End of Code",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    for (int blockIdx_x = 0; blockIdx_x < 7; blockIdx_x++) {\n        for (int threadIdx_x = 0; threadIdx_x < 6; threadIdx_x++) {\n            compute[blockIdx_x * 54 + threadIdx_x] = log10f(data[blockIdx_x * 54 + threadIdx_x]);\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  const int block_size = 54;\n  const int num_threads = 54;\n  #pragma omp parallel for\n  for (int i = 0; i < 7 * 6 * 3 * 6; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    compute[i] = log10f(data[i]);\n  }\n}\n\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Input tensor shape: [7, 6, 3, 6]\n  // compute shape: [7, 6, 3, 6]\n  // data shape: [7, 6, 3, 6]\n\n  for (int blockIdx_x = 0; blockIdx_x < 7; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 6; ++threadIdx_x) {\n      for (int i = 0; i < 3; ++i) {\n        for (int j = 0; j < 6; ++j) {\n          compute[((blockIdx_x * 756) + (threadIdx_x * 18) + (i * 6) + j)] = log10f(data[((blockIdx_x * 756) + (threadIdx_x * 18) + (i * 6) + j)]);\n        }\n      }\n    }\n  }\n}\n\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        compute[i] = log10f(data[i]);\n    }\n}\n\nint main() {\n    int n = 7, c = 6, h = 3, w = 6;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 9.0f + 1.0f; // Random values between 1 and 10\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = log10f(data[i]);\n    }\n}\n\nint main() {\n    int size = 7 * 6 * 3 * 6;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i + 1); // Some sample initialization to avoid log10f(0)\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "log2",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i3 = 0; i3 < 13; ++i3) {\n        compute[(((i0 * 104) + (i1 * 13)) + i3)] = log2f(data[(((i0 * 104) + (i1 * 13)) + i3)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 143) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 8, 1, 13), \"float32\"), compute: T.Buffer((11, 8, 1, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(11):\n            for i1, i3 in T.grid(8, 13):\n                cse_var_1: T.int32 = i0 * 104 + i1 * 13 + i3\n                compute_1 = T.Buffer((1144,), data=compute.data)\n                data_1 = T.Buffer((1144,), data=data.data)\n                compute_1[cse_var_1] = T.log2(data_1[cse_var_1])",
        "op_args": [
            11,
            8,
            1,
            13
        ],
        "input_shape": "[[11, 8, 1, 13]]",
        "output_shape": "[[11, 8, 1, 13]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < 11; ++i) {\n        for (int j = 0; j < 8; ++j) {\n            for (int k = 0; k < 1; ++k) {\n                for (int l = 0; l < 13; ++l) {\n                    int blockIdx_x = i * 8 * 1 * 13 + j * 1 * 13 + k * 13 + l;\n                    int threadIdx_x = omp_get_thread_num();\n\n                    if ((blockIdx_x * 4 + (threadIdx_x >> 3)) < 143) {\n                        compute[blockIdx_x * 32 + threadIdx_x] = log2f(data[blockIdx_x * 32 + threadIdx_x]);\n                    }\n                }\n            }\n        }\n    }\n}\n\n// End of Code",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    for (int blockIdx_x = 0; blockIdx_x < 11; blockIdx_x++) {\n        for (int threadIdx_x = 0; threadIdx_x < 8; threadIdx_x++) {\n            if ((blockIdx_x * 4 + (threadIdx_x >> 3)) < 143) {\n                compute[blockIdx_x * 32 + threadIdx_x] = log2f(data[blockIdx_x * 32 + threadIdx_x]);\n            }\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  const int block_size = 32;\n  const int num_threads = 32;\n  #pragma omp parallel for\n  for (int i = 0; i < 11 * 8 * 1 * 13; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    if ((block_idx * 4 + thread_idx >> 3) < 143) {\n      compute[i] = log2f(data[i]);\n    }\n  }\n}\n\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Input tensor shape: [11, 8, 1, 13]\n  // compute shape: [11, 8, 1, 13]\n  // data shape: [11, 8, 1, 13]\n\n  for (int blockIdx_x = 0; blockIdx_x < 11; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 32; ++threadIdx_x) {\n      if (((blockIdx_x * 4) + (threadIdx_x >> 3)) < 143) {\n        compute[((blockIdx_x * 1056) + threadIdx_x)] = log2f(data[((blockIdx_x * 1056) + threadIdx_x)]);\n      }\n    }\n  }\n}\n\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        if ((i / 32 * 4 + (i % 32) / 8) < 143) {\n            compute[i] = log2f(data[i]);\n        }\n    }\n}\n\nint main() {\n    int n = 11, c = 8, h = 1, w = 13;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 19.0f + 1.0f; // Random values between 1 and 20\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    int limit = 143; // Derived from the shape and the condition in the CUDA kernel\n    for (int i = 0; i < size; i++) {\n        if ((i / 8) < limit) {\n            compute[i] = log2f(data[i]);\n        }\n    }\n}\n\nint main() {\n    int size = 11 * 8 * 1 * 13;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i + 1); // Some sample initialization to avoid log2f(0)\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "leaky_relu",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2592; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < data[i0_i1_fused_i2_fused_i3_fused]) ? data[i0_i1_fused_i2_fused_i3_fused] : (data[i0_i1_fused_i2_fused_i3_fused] * 5.000000e-01f));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] * 5.000000e-01f));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 12, 18, 4), \"float32\"), compute: T.Buffer((3, 12, 18, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(2592):\n            compute_1 = T.Buffer((2592,), data=compute.data)\n            data_1 = T.Buffer((2592,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.if_then_else(T.float32(0) < data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused] * T.float32(0.5))",
        "op_args": [
            3,
            12,
            18,
            4
        ],
        "input_shape": "[[3, 12, 18, 4]]",
        "output_shape": "[[3, 12, 18, 4]]",
        "cluade1_c": "#include <stdio.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < 3; ++i) {\n        for (int j = 0; j < 12; ++j) {\n            for (int k = 0; k < 18; ++k) {\n                for (int l = 0; l < 4; ++l) {\n                    int blockIdx_x = i * 12 * 18 * 4 + j * 18 * 4 + k * 4 + l;\n                    int threadIdx_x = omp_get_thread_num();\n\n                    if (0.000000e+00f < data[blockIdx_x * 24 + threadIdx_x]) {\n                        compute[blockIdx_x * 24 + threadIdx_x] = data[blockIdx_x * 24 + threadIdx_x];\n                    } else {\n                        compute[blockIdx_x * 24 + threadIdx_x] = data[blockIdx_x * 24 + threadIdx_x] * 5.000000e-01f;\n                    }\n                }\n            }\n        }\n    }\n}\n\n// End of Code",
        "cluade2_c": "#include <stdio.h>\n\nvoid default_function_kernel_cpu(float* compute, float* data) {\n    const int shape[4] = {3, 12, 18, 4};\n    const int total_elements = shape[0] * shape[1] * shape[2] * shape[3];\n\n    for (int i = 0; i < total_elements; i++) {\n        int block_idx = i / 24;\n        int thread_idx = i % 24;\n\n        float value = data[i];\n        compute[i] = (0.0f < value) ? value : (value * 0.5f);\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  const int block_size = 24;\n  const int num_threads = 24;\n  #pragma omp parallel for\n  for (int i = 0; i < 3 * 12 * 18 * 4; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    compute[i] = (0.0f < data[i]) ? data[i] : (data[i] * 0.5f);\n  }\n}\n\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Input tensor shape: [3, 12, 18, 4]\n  // compute shape: [3, 12, 18, 4]\n  // data shape: [3, 12, 18, 4]\n\n  for (int blockIdx_x = 0; blockIdx_x < 3; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 12; ++threadIdx_x) {\n      for (int i = 0; i < 18; ++i) {\n        for (int j = 0; j < 4; ++j) {\n          int index = ((blockIdx_x * 1296) + (threadIdx_x * 72) + (i * 4) + j);\n          compute[index] = (0.0f < data[index]) ? data[index] : (data[index] * 0.5f);\n        }\n      }\n    }\n  }\n}\n\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        compute[i] = (0.0f < data[i]) ? data[i] : (data[i] * 0.5f);\n    }\n}\n\nint main() {\n    int n = 3, c = 12, h = 18, w = 4;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 20.0f - 10.0f; // Random values between -10 and 10\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = (0.000000e+00f < data[i]) ? data[i] : (data[i] * 5.000000e-01f);\n    }\n}\n\nint main() {\n    int size = 3 * 12 * 18 * 4;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i - (size / 2)); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "log_softmax",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  float T_softmax_maxelem[495];\n  float compute_1[1];\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        T_softmax_maxelem[(((i0 * 33) + (i1 * 3)) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 4; ++k) {\n          T_softmax_maxelem[(((i0 * 33) + (i1 * 3)) + i2)] = max(T_softmax_maxelem[(((i0 * 33) + (i1 * 3)) + i2)], data[((((i0 * 132) + (i1 * 12)) + (i2 * 4)) + k)]);\n        }\n      }\n    }\n  }\n  for (int32_t i0_outer_outer_inner = 0; i0_outer_outer_inner < 5; ++i0_outer_outer_inner) {\n    for (int32_t i3_outer_outer_inner = 0; i3_outer_outer_inner < 4; ++i3_outer_outer_inner) {\n      for (int32_t i0_outer_inner = 0; i0_outer_inner < 3; ++i0_outer_inner) {\n        for (int32_t i1_outer_inner = 0; i1_outer_inner < 11; ++i1_outer_inner) {\n          for (int32_t i2_outer_inner = 0; i2_outer_inner < 3; ++i2_outer_inner) {\n            compute_1[0] = 0.000000e+00f;\n            for (int32_t k_1 = 0; k_1 < 4; ++k_1) {\n              compute_1[0] = (compute_1[0] + expf((data[(((((i0_outer_outer_inner * 396) + (i0_outer_inner * 132)) + (i1_outer_inner * 12)) + (i2_outer_inner * 4)) + k_1)] - T_softmax_maxelem[((((i0_outer_outer_inner * 99) + (i0_outer_inner * 33)) + (i1_outer_inner * 3)) + i2_outer_inner)])));\n            }\n            compute[(((((i0_outer_outer_inner * 396) + (i0_outer_inner * 132)) + (i1_outer_inner * 12)) + (i2_outer_inner * 4)) + i3_outer_outer_inner)] = ((data[(((((i0_outer_outer_inner * 396) + (i0_outer_inner * 132)) + (i1_outer_inner * 12)) + (i2_outer_inner * 4)) + i3_outer_outer_inner)] - T_softmax_maxelem[((((i0_outer_outer_inner * 99) + (i0_outer_inner * 33)) + (i1_outer_inner * 3)) + i2_outer_inner)]) - logf(compute_1[0]));\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) < 495) {\n    compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 4; ++k) {\n    if (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) < 495) {\n      compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 495) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2))]) - __logf(compute_1[((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) < 495) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 4; ++k) {\n    if (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) < 495) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 4)) + k)]);\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 11, 3, 4), \"float32\"), compute: T.Buffer((15, 11, 3, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_softmax_maxelem = T.allocate([495], \"float32\", \"global\")\n        compute_1 = T.allocate([1], \"float32\", \"global\")\n        T_softmax_maxelem_1 = T.Buffer((495,), data=T_softmax_maxelem)\n        data_1 = T.Buffer((1980,), data=data.data)\n        for i0, i1, i2 in T.grid(15, 11, 3):\n            T_softmax_maxelem_1[i0 * 33 + i1 * 3 + i2] = T.float32(-3.4028234663852886e+38)\n            for k in range(4):\n                cse_var_1: T.int32 = i0 * 33 + i1 * 3 + i2\n                T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 132 + i1 * 12 + i2 * 4 + k])\n        for i0_outer_outer_inner, i3_outer_outer_inner, i0_outer_inner, i1_outer_inner, i2_outer_inner in T.grid(5, 4, 3, 11, 3):\n            cse_var_2: T.int32 = i0_outer_outer_inner * 396 + i0_outer_inner * 132 + i1_outer_inner * 12 + i2_outer_inner * 4 + i3_outer_outer_inner\n            compute_2 = T.Buffer((1,), data=compute_1, align=4)\n            compute_2[0] = T.float32(0)\n            for k in range(4):\n                compute_2[0] = compute_2[0] + T.exp(data_1[i0_outer_outer_inner * 396 + i0_outer_inner * 132 + i1_outer_inner * 12 + i2_outer_inner * 4 + k] - T_softmax_maxelem_1[i0_outer_outer_inner * 99 + i0_outer_inner * 33 + i1_outer_inner * 3 + i2_outer_inner])\n            compute_3 = T.Buffer((1980,), data=compute.data)\n            compute_3[cse_var_2] = data_1[cse_var_2] - T_softmax_maxelem_1[i0_outer_outer_inner * 99 + i0_outer_inner * 33 + i1_outer_inner * 3 + i2_outer_inner] - T.log(compute_2[0])",
        "op_args": [
            15,
            11,
            3,
            4
        ],
        "input_shape": "[[15, 11, 3, 4]]",
        "output_shape": "[[15, 11, 3, 4]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < 15; ++i) {\n        for (int j = 0; j < 11; ++j) {\n            for (int k = 0; k < 3; ++k) {\n                for (int l = 0; l < 4; ++l) {\n                    int blockIdx_x = i * 11 * 3 * 4 + j * 3 * 4 + k * 4 + l;\n                    int threadIdx_x = omp_get_thread_num();\n\n                    if (blockIdx_x * 8 + threadIdx_x < 495) {\n                        T_softmax_maxelem[blockIdx_x * 8 + threadIdx_x] = -3.402823e+38f;\n                    }\n                    for (int m = 0; m < 4; ++m) {\n                        if (blockIdx_x * 8 + threadIdx_x < 495) {\n                            T_softmax_maxelem[blockIdx_x * 8 + threadIdx_x] = fmaxf(T_softmax_maxelem[blockIdx_x * 8 + threadIdx_x], data[((i * 32 + (j * 4 + k)) * 4 + m)]);\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\n// End of Code",
        "cluade2_c": "#include <math.h>\n\n#define max(a, b) ((a) > (b) ? (a) : (b))\n\nvoid default_function_kernel_1(float* T_softmax_maxelem, float* compute, float* data) {\n    for (int blockIdx_x = 0; blockIdx_x < 15; blockIdx_x++) {\n        for (int threadIdx_x = 0; threadIdx_x < 11; threadIdx_x++) {\n            if ((blockIdx_x * 2 + threadIdx_x) < 495) {\n                compute[blockIdx_x * 2 + threadIdx_x] = 0.0f;\n            }\n            for (int k = 0; k < 4; ++k) {\n                if ((blockIdx_x * 2 + threadIdx_x) < 495) {\n                    compute[blockIdx_x * 2 + threadIdx_x] += expf(data[((blockIdx_x * 8 + threadIdx_x * 4) + k)] - T_softmax_maxelem[blockIdx_x * 2 + threadIdx_x]);\n                }\n            }\n        }\n    }\n} \n\nvoid default_function_kernel_2(float* T_softmax_maxelem, float* compute, float* compute_1, float* data) {\n    for (int blockIdx_x = 0; blockIdx_x < 15; blockIdx_x++) {\n        for (int threadIdx_x = 0; threadIdx_x < 11; threadIdx_x++) {\n            if ((blockIdx_x * 8 + (threadIdx_x >> 2)) < 495) {\n                compute[blockIdx_x * 32 + threadIdx_x] = (data[blockIdx_x * 32 + threadIdx_x] - T_softmax_maxelem[blockIdx_x * 8 + (threadIdx_x >> 2)] - logf(compute_1[blockIdx_x * 8 + (threadIdx_x >> 2)]));\n            }\n        }\n    }\n}\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n    for (int blockIdx_x = 0; blockIdx_x < 15; blockIdx_x++) {\n        for (int threadIdx_x = 0; threadIdx_x < 11; threadIdx_x++) {\n            if ((blockIdx_x * 8 + threadIdx_x) < 495) {\n                T_softmax_maxelem[blockIdx_x * 8 + threadIdx_x] = -3.402823e+38f;\n            }\n            for (int k = 0; k < 4; ++k) {\n                if ((blockIdx_x * 8 + threadIdx_x) < 495) {\n                    T_softmax_maxelem[blockIdx_x * 8 + threadIdx_x] = max(T_softmax_maxelem[blockIdx_x * 8 + threadIdx_x], data[((blockIdx_x * 32 + threadIdx_x * 4) + k)]);\n                }\n            }\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel_1(float* T_softmax_maxelem, float* compute, float* data) {\n  const int block_size = 2;\n  const int num_threads = 2;\n  #pragma omp parallel for\n  for (int i = 0; i < 495; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    compute[i] = 0.0f;\n    for (int k = 0; k < 4; ++k) {\n      compute[i] = compute[i] + expf(data[(block_idx * 8 + thread_idx * 4) + k] - T_softmax_maxelem[i]);\n    }\n  }\n}\n\nvoid default_function_kernel_2(float* T_softmax_maxelem, float* compute, float* compute_1, float* data) {\n  const int block_size = 32;\n  const int num_threads = 32;\n  #pragma omp parallel for\n  for (int i = 0; i < 15 * 11 * 3 * 4; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    if ((block_idx * 8 + thread_idx >> 2) < 495) {\n      compute[i] = (data[i] - T_softmax_maxelem[block_idx * 8 + thread_idx >> 2]) - logf(compute_1[block_idx * 8 + thread_idx >> 2]);\n    }\n  }\n}\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n  const int block_size = 8;\n  const int num_threads = 8;\n  #pragma omp parallel for\n  for (int i = 0; i < 495; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    T_softmax_maxelem[i] = -3.402823e+38f;\n    for (int k = 0; k < 4; ++k) {\n      T_softmax_maxelem[i] = fmaxf(T_softmax_maxelem[i], data[(block_idx * 32 + thread_idx * 4) + k]);\n    }\n  }\n}\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n#include <float.h>\n\n// Function to calculate the maximum of two floats\nfloat max(float a, float b) {\n  return (a > b) ? a : b;\n}\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n  // Input tensor shape: [15, 11, 3, 4]\n  // T_softmax_maxelem shape: [15, 11, 3]\n  // data shape: [15, 11, 3, 4]\n\n  for (int blockIdx_x = 0; blockIdx_x < 15; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 11; ++threadIdx_x) {\n      for (int i = 0; i < 3; ++i) {\n        int index = ((blockIdx_x * 330) + (threadIdx_x * 3) + i);\n        T_softmax_maxelem[index] = -FLT_MAX; // Initialize to negative infinity\n        for (int k = 0; k < 4; ++k) {\n          T_softmax_maxelem[index] = max(T_softmax_maxelem[index], data[((blockIdx_x * 1320) + (threadIdx_x * 12) + (i * 4) + k)]);\n        }\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* T_softmax_maxelem, float* compute, float* data) {\n  // Input tensor shape: [15, 11, 3, 4]\n  // T_softmax_maxelem shape: [15, 11, 3]\n  // compute shape: [15, 11, 3]\n  // data shape: [15, 11, 3, 4]\n\n  for (int blockIdx_x = 0; blockIdx_x < 15; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 11; ++threadIdx_x) {\n      for (int i = 0; i < 3; ++i) {\n        int index = ((blockIdx_x * 330) + (threadIdx_x * 3) + i);\n        if (index < 495) {\n          compute[index] = 0.0f;\n          for (int k = 0; k < 4; ++k) {\n            compute[index] += expf(data[((blockIdx_x * 1320) + (threadIdx_x * 12) + (i * 4) + k)] - T_softmax_maxelem[index]);\n          }\n        }\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_2(float* T_softmax_maxelem, float* compute, float* compute_1, float* data) {\n  // Input tensor shape: [15, 11, 3, 4]\n  // T_softmax_maxelem shape: [15, 11, 3]\n  // compute shape: [15, 11, 3, 4]\n  // compute_1 shape: [15, 11, 3]\n  // data shape: [15, 11, 3, 4]\n\n  for (int blockIdx_x = 0; blockIdx_x < 15; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 32; ++threadIdx_x) {\n      for (int i = 0; i < 4; ++i) {\n        int index = ((blockIdx_x * 128) + (threadIdx_x * 4) + i);\n        if (index < 495) {\n          compute[index] = data[index] - T_softmax_maxelem[((blockIdx_x * 33) + (threadIdx_x >> 2))] - logf(compute_1[((blockIdx_x * 33) + (threadIdx_x >> 2))]);\n        }\n      }\n    }\n  }\n}\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid kernel_1(float* T_softmax_maxelem, float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        if (i < 495) {\n            compute[i] = 0.0f;\n        }\n        for (int k = 0; k < 4; ++k) {\n            if (i < 495) {\n                compute[i] += expf(data[i * 4 + k] - T_softmax_maxelem[i]);\n            }\n        }\n    }\n}\n\nvoid kernel_2(float* T_softmax_maxelem, float* compute, float* compute_1, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        int idx = (i / 32) * 8 + ((i % 32) >> 2);\n        if (idx < 495) {\n            compute[i] = (data[i] - T_softmax_maxelem[idx]) - logf(compute_1[idx]);\n        }\n    }\n}\n\nvoid kernel_3(float* T_softmax_maxelem, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        if (i < 495) {\n            T_softmax_maxelem[i] = -3.402823e+38f;\n        }\n        for (int k = 0; k < 4; ++k) {\n            if (i < 495) {\n                T_softmax_maxelem[i] = fmaxf(T_softmax_maxelem[i], data[i * 4 + k]);\n            }\n        }\n    }\n}\n\nint main() {\n    int n = 15, c = 11, h = 3, w = 4;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* T_softmax_maxelem = (float*)malloc(495 * sizeof(float)); // Adjusted size\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n    float* compute_1 = (float*)malloc(495 * sizeof(float)); // Adjusted size\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 20.0f - 10.0f; // Random values between -10 and 10\n    }\n\n    kernel_3(T_softmax_maxelem, data, total_elements);\n    kernel_1(T_softmax_maxelem, compute_1, data, 495);\n    kernel_2(T_softmax_maxelem, compute, compute_1, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(T_softmax_maxelem);\n    free(compute);\n    free(compute_1);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data, int size) {\n    int limit = 495; // Derived from the shape and the condition in the CUDA kernel\n\n    // Initialize compute to 0\n    for (int i = 0; i < size; i++) {\n        if ((i / 2) < limit) {\n            compute[i] = 0.0f;\n        }\n    }\n\n    // Calculate the exponent sum\n    for (int k = 0; k < 4; ++k) {\n        for (int i = 0; i < size; i++) {\n            if ((i / 2) < limit) {\n                compute[i] += expf(data[(i * 4) + k] - T_softmax_maxelem[i]);\n            }\n        }\n    }\n}\n\nvoid default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data, int size) {\n    int limit = 495; // Derived from the shape and the condition in the CUDA kernel\n\n    for (int i = 0; i < size; i++) {\n        if ((i / 8) < limit) {\n            compute[i] = (data[i] - T_softmax_maxelem[i / 4]) - logf(compute_1[i / 4]);\n        }\n    }\n}\n\nvoid default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data, int size) {\n    int limit = 495; // Derived from the shape and the condition in the CUDA kernel\n\n    // Initialize T_softmax_maxelem to a very small value\n    for (int i = 0; i < size; i++) {\n        if ((i / 8) < limit) {\n            T_softmax_maxelem[i] = -3.402823e+38f;\n        }\n    }\n\n    // Calculate T_softmax_maxelem\n    for (int k = 0; k < 4; ++k) {\n        for (int i = 0; i < size; i++) {\n            if ((i / 8) < limit) {\n                T_softmax_maxelem[i] = fmaxf(T_softmax_maxelem[i], data[(i * 4) + k]);\n            }\n        }\n    }\n}\n\nint main() {\n    int size = 15 * 11 * 3 * 4;\n    float data[size];\n    float T_softmax_maxelem[size];\n    float compute[size];\n    float compute_1[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i - (size / 2)); // Some sample initialization\n    }\n\n    default_function_kernel(T_softmax_maxelem, data, size);\n    default_function_kernel_1(T_softmax_maxelem, compute, data, size);\n    default_function_kernel_2(T_softmax_maxelem, compute, compute_1, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "negative",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          compute[((((i0 * 540) + (i1 * 60)) + (i2 * 10)) + i3)] = (data[((((i0 * 540) + (i1 * 60)) + (i2 * 10)) + i3)] * -1.000000e+00f);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 9, 6, 10), \"float32\"), compute: T.Buffer((4, 9, 6, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(4):\n            for i1, i2, i3 in T.grid(9, 6, 10):\n                cse_var_1: T.int32 = i0 * 540 + i1 * 60 + i2 * 10 + i3\n                compute_1 = T.Buffer((2160,), data=compute.data)\n                data_1 = T.Buffer((2160,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)",
        "op_args": [
            4,
            9,
            6,
            10
        ],
        "input_shape": "[[4, 9, 6, 10]]",
        "output_shape": "[[4, 9, 6, 10]]",
        "cluade1_c": "#include <stdio.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for\n    for (int i = 0; i < 4; i++) {\n        for (int j = 0; j < 9; j++) {\n            for (int k = 0; k < 6; k++) {\n                #pragma omp simd\n                for (int l = 0; l < 10; l++) {\n                    int idx = ((((i * 9 + j) * 6 + k) * 10) + l);\n                    compute[idx] = data[idx] * -1.0f;\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "#include <stdio.h>\n\nvoid default_function_kernel_CPU(float* compute, float* data) {\n    int blockId_x = 0; // Assuming blockIdx.x = 0 for simplicity\n    int threadIdx_x = 0; // Assuming threadIdx.x = 0 for simplicity\n    int tensor_shape[] = {4, 9, 6, 10};\n    int index = ((blockId_x * 8) + threadIdx_x);\n\n    // Flattening the 4D tensor to a 1D array\n    int flattened_index = index;\n    int stride = 1;\n    for (int i = 3; i >= 0; i--) {\n        flattened_index += stride * (index % tensor_shape[i]);\n        stride *= tensor_shape[i];\n        index /= tensor_shape[i];\n    }\n\n    compute[flattened_index] = data[flattened_index] * -1.0f;\n}\n\n// Print the single C code function implementation\ndefault_function_kernel_CPU(compute, data);  // Function call\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  const int block_size = 8;\n  const int num_threads = 8;\n  #pragma omp parallel for\n  for (int i = 0; i < 4 * 9 * 6 * 10; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    compute[i] = data[i] * -1.0f;\n  }\n}\n\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Input tensor shape: [4, 9, 6, 10]\n  // compute shape: [4, 9, 6, 10]\n  // data shape: [4, 9, 6, 10]\n\n  for (int blockIdx_x = 0; blockIdx_x < 4; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 8; ++threadIdx_x) {\n      for (int i = 0; i < 6; ++i) {\n        for (int j = 0; j < 10; ++j) {\n          compute[((blockIdx_x * 2160) + (threadIdx_x * 60) + (i * 10) + j)] = data[((blockIdx_x * 2160) + (threadIdx_x * 60) + (i * 10) + j)] * -1.0f;\n        }\n      }\n    }\n  }\n}\n\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        compute[i] = data[i] * -1.0f;\n    }\n}\n\nint main() {\n    int n = 4, c = 9, h = 6, w = 10;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 20.0f - 10.0f; // Random values between -10 and 10\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = data[i] * -1.000000e+00f;\n    }\n}\n\nint main() {\n    int size = 4 * 9 * 6 * 10;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "prod",
        "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[21];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 21; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 1.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 300; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 21; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 21) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 1.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 21; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] * data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 1.000000e+00f;\n  for (int k0 = 0; k0 < 15; ++k0) {\n    for (int k1 = 0; k1 < 20; ++k1) {\n      for (int k2 = 0; k2 < 3; ++k2) {\n        for (int k3 = 0; k3 < 7; ++k3) {\n          data_red[0] = (data_red[0] * data[((((k0 * 420) + (k1 * 21)) + (k2 * 7)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 20, 3, 7), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([21], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((21,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(21):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(300, 21):\n            data_1 = T.Buffer((6300,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] * data_1[k0_k1_fused_k2_fused_k3_fused_outer * 21 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(21):\n            data_red_1[0] = data_red_1[0] * data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]",
        "op_args": [
            15,
            20,
            3,
            7
        ],
        "input_shape": "[[15, 20, 3, 7]]",
        "output_shape": "[[]]",
        "cluade1_c": "#include <stdio.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* data, float* data_red) {\n    data_red[0] = 1.0f;\n\n    #pragma omp parallel for\n    for (int k0 = 0; k0 < 15; ++k0) {\n        for (int k1 = 0; k1 < 20; ++k1) {\n            for (int k2 = 0; k2 < 3; ++k2) {\n                for (int k3 = 0; k3 < 7; ++k3) {\n                    #pragma omp atomic\n                    data_red[0] *= data[((((k0 * 420) + (k1 * 21)) + (k2 * 7)) + k3)];\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "void default_function_kernel_cpu(float* data, float* data_red) {\n    data_red[0] = 1.000000e+00f;\n    for (int k0 = 0; k0 < 15; ++k0) {\n        for (int k1 = 0; k1 < 20; ++k1) {\n            for (int k2 = 0; k2 < 3; ++k2) {\n                for (int k3 = 0; k3 < 7; ++k3) {\n                    data_red[0] *= data[((k0 * 20 * 3 * 7) + (k1 * 3 * 7) + (k2 * 7) + k3)];\n                }\n            }\n        }\n    }\n}\n",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* data, float* data_red) {\n  data_red[0] = 1.0f;\n  for (int k0 = 0; k0 < 15; ++k0) {\n    for (int k1 = 0; k1 < 20; ++k1) {\n      for (int k2 = 0; k2 < 3; ++k2) {\n        for (int k3 = 0; k3 < 7; ++k3) {\n          data_red[0] *= data[((k0 * 420) + (k1 * 21) + (k2 * 7) + k3)];\n        }\n      }\n    }\n  }\n}\n\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* data, float* data_red) {\n  // Input tensor shape: [15, 20, 3, 7]\n  // data shape: [15, 20, 3, 7]\n  // data_red shape: [1]\n\n  data_red[0] = 1.0f;\n  for (int k0 = 0; k0 < 15; ++k0) {\n    for (int k1 = 0; k1 < 20; ++k1) {\n      for (int k2 = 0; k2 < 3; ++k2) {\n        for (int k3 = 0; k3 < 7; ++k3) {\n          data_red[0] *= data[((k0 * 420) + (k1 * 21) + (k2 * 7) + k3)];\n        }\n      }\n    }\n  }\n}\n\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\nvoid default_function(float* data, float* data_red, int total_elements) {\n    data_red[0] = 1.0f;\n    \n    #pragma omp parallel for reduction(*:data_red[0])\n    for (int i = 0; i < total_elements; i++) {\n        data_red[0] *= data[i];\n    }\n}\n\nint main() {\n    int k0 = 15, k1 = 20, k2 = 3, k3 = 7;\n    int total_elements = k0 * k1 * k2 * k3; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* data_red = (float*)malloc(1 * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 2.0f; // Random values between 0 and 2\n    }\n\n    default_function(data, data_red, total_elements);\n\n    // Print the reduction result\n    printf(\"Reduction result: %f\\n\", data_red[0]);\n\n    free(data);\n    free(data_red);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n    data_red[0] = 1.000000e+00f;\n    for (int k0 = 0; k0 < 15; ++k0) {\n        for (int k1 = 0; k1 < 20; ++k1) {\n            for (int k2 = 0; k2 < 3; ++k2) {\n                for (int k3 = 0; k3 < 7; ++k3) {\n                    data_red[0] *= data[((k0 * 420) + (k1 * 21) + (k2 * 7) + k3)];\n                }\n            }\n        }\n    }\n}\n\nint main() {\n    int size = 15 * 20 * 3 * 7;\n    float data[size];\n    float data_red[1];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i % 10 + 1); // Some sample initialization to avoid zero values\n    }\n\n    default_function_kernel(data, data_red);\n\n    // Print the reduced value\n    printf(\"data_red[0] = %f\\n\", data_red[0]);\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "round",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      for (int32_t i3 = 0; i3 < 11; ++i3) {\n        compute[(((i0 * 22) + (i2 * 11)) + i3)] = roundf(data[(((i0 * 22) + (i2 * 11)) + i3)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) < 374) {\n    compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 1, 2, 11), \"float32\"), compute: T.Buffer((17, 1, 2, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(17):\n            for i2, i3 in T.grid(2, 11):\n                cse_var_1: T.int32 = i0 * 22 + i2 * 11 + i3\n                compute_1 = T.Buffer((374,), data=compute.data)\n                data_1 = T.Buffer((374,), data=data.data)\n                compute_1[cse_var_1] = T.round(data_1[cse_var_1])",
        "op_args": [
            17,
            1,
            2,
            11
        ],
        "input_shape": "[[17, 1, 2, 11]]",
        "output_shape": "[[17, 1, 2, 11]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for\n    for (int i = 0; i < 17; i++) {\n        for (int j = 0; j < 1; j++) {\n            for (int k = 0; k < 2; k++) {\n                #pragma omp simd\n                for (int l = 0; l < 3; l++) {\n                    int idx = (((i * 2 + j) * 2 + k) * 11 + l);\n                    if ((i * 3 + l) < 374) {\n                        compute[i * 3 + l] = roundf(data[idx]);\n                    }\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "void default_function_kernel_cpu(float* compute, float* data) {\n    for (int blockIdx_x = 0; blockIdx_x < 17; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < 3; ++threadIdx_x) {\n            int index = (blockIdx_x * 3) + threadIdx_x;\n            if (index < 374) {\n                compute[index] = roundf(data[index]);\n            }\n        }\n    }\n}\n",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  const int block_size = 3;\n  const int num_threads = 3;\n  #pragma omp parallel for\n  for (int i = 0; i < 17 * 1 * 2 * 11; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    if ((block_idx * 3 + thread_idx) < 374) {\n      compute[i] = roundf(data[i]);\n    }\n  }\n}\n\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Input tensor shape: [17, 1, 2, 11]\n  // compute shape: [17, 1, 2, 11]\n  // data shape: [17, 1, 2, 11]\n\n  for (int blockIdx_x = 0; blockIdx_x < 17; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 3; ++threadIdx_x) {\n      for (int i = 0; i < 2; ++i) {\n        for (int j = 0; j < 11; ++j) {\n          int index = ((blockIdx_x * 66) + (threadIdx_x * 22) + (i * 11) + j);\n          if (index < 374) {\n            compute[index] = roundf(data[index]);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        if (i < 374) {\n            compute[i] = roundf(data[i]);\n        }\n    }\n}\n\nint main() {\n    int n = 17, c = 1, h = 2, w = 11;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 20.0f - 10.0f; // Random values between -10 and 10\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    int limit = 374; // Derived from the shape and the condition in the CUDA kernel\n    for (int i = 0; i < size; i++) {\n        if ((i / 3) < limit) {\n            compute[i] = roundf(data[i]);\n        }\n    }\n}\n\nint main() {\n    int size = 17 * 1 * 2 * 11;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i + 0.5); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "sigmoid",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 7; ++i1) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      for (int32_t i3 = 0; i3 < 4; ++i3) {\n        compute[(((i1 * 68) + (i2 * 4)) + i3)] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - data[(((i1 * 68) + (i2 * 4)) + i3)]))));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))]))));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 7, 17, 4), \"float32\"), compute: T.Buffer((1, 7, 17, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(7, 17, 4):\n            cse_var_1: T.int32 = i1 * 68 + i2 * 4 + i3\n            compute_1 = T.Buffer((476,), data=compute.data)\n            data_1 = T.Buffer((476,), data=data.data)\n            compute_1[cse_var_1] = T.sigmoid(data_1[cse_var_1])",
        "op_args": [
            1,
            7,
            17,
            4
        ],
        "input_shape": "[[1, 7, 17, 4]]",
        "output_shape": "[[1, 7, 17, 4]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < 1; i++) {\n        for (int j = 0; j < 7; j++) {\n            for (int k = 0; k < 17; k++) {\n                for (int l = 0; l < 4; l++) {\n                    #pragma omp simd\n                    for (int m = 0; m < 7; m++) {\n                        int blockIdx_x = i * 119 + j * 17 + k * 4 + l * 1 + m;\n                        compute[blockIdx_x] = 1.0f / (1.0f + expf(0.0f - data[blockIdx_x]));\n                    }\n                }\n            }\n        }\n    }\n}\n\n// End-of-Code",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_kernel_cpu(float* compute, float* data) {\n    for (int blockIdx_x = 0; blockIdx_x < 1; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < 7; ++threadIdx_x) {\n            int index = (blockIdx_x * 7) + threadIdx_x;\n            compute[index] = 1.000000e+00f / (1.000000e+00f + expf(0.000000e+00f - data[index]));\n        }\n    }\n}\n",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  const int block_size = 7;\n  const int num_threads = 7;\n  #pragma omp parallel for\n  for (int i = 0; i < 1 * 7 * 17 * 4; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    compute[i] = 1.0f / (1.0f + expf(0.0f - data[i]));\n  }\n}\n\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Input tensor shape: [1, 7, 17, 4]\n  // compute shape: [1, 7, 17, 4]\n  // data shape: [1, 7, 17, 4]\n\n  for (int blockIdx_x = 0; blockIdx_x < 1; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 7; ++threadIdx_x) {\n      for (int i = 0; i < 17; ++i) {\n        for (int j = 0; j < 4; ++j) {\n          int index = ((blockIdx_x * 476) + (threadIdx_x * 68) + (i * 4) + j);\n          compute[index] = 1.0f / (1.0f + expf(0.0f - data[index]));\n        }\n      }\n    }\n  }\n}\n\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        compute[i] = 1.0f / (1.0f + expf(-data[i]));\n    }\n}\n\nint main() {\n    int n = 1, c = 7, h = 17, w = 4;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 10.0f - 5.0f; // Random values between -5 and 5\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = 1.000000e+00f / (1.000000e+00f + expf(-data[i]));\n    }\n}\n\nint main() {\n    int size = 1 * 7 * 17 * 4;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i - (size / 2)); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "sign",
        "c_code": "void default_function_kernel(float* T_sign, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 1248; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_sign[ax0_ax1_fused_ax2_fused_ax3_fused] = ((0.000000e+00f < data[ax0_ax1_fused_ax2_fused_ax3_fused]) ? 1.000000e+00f : ((data[ax0_ax1_fused_ax2_fused_ax3_fused] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 2, 13, 3), \"float32\"), T_sign: T.Buffer((16, 2, 13, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(1248):\n            T_sign_1 = T.Buffer((1248,), data=T_sign.data)\n            data_1 = T.Buffer((1248,), data=data.data)\n            T_sign_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.if_then_else(T.float32(0) < data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(1), T.Select(data_1[ax0_ax1_fused_ax2_fused_ax3_fused] < T.float32(0), T.float32(-1), T.float32(0)))",
        "op_args": [
            16,
            2,
            13,
            3
        ],
        "input_shape": "[[16, 2, 13, 3]]",
        "output_shape": "[[16, 2, 13, 3]]",
        "cluade1_c": "#include <stdio.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* T_sign, float* data) {\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < 16; i++) {\n        for (int j = 0; j < 2; j++) {\n            for (int k = 0; k < 13; k++) {\n                for (int l = 0; l < 3; l++) {\n                    #pragma omp simd\n                    for (int m = 0; m < 16; m++) {\n                        int blockIdx_x = i * 78 + j * 39 + k * 3 + l * 1 + m;\n                        float val = data[blockIdx_x];\n                        T_sign[blockIdx_x] = (val > 0.0f) ? 1.0f : ((val < 0.0f) ? -1.0f : 0.0f);\n                    }\n                }\n            }\n        }\n    }\n}\n\n// End-of-Code",
        "cluade2_c": "void default_function_kernel_cpu(float* T_sign, float* data) {\n    // Assuming that blockIdx.x and threadIdx.x are replaced with appropriate loop indices\n\n    for (int i = 0; i < 16; i++) {\n        for (int j = 0; j < 2; j++) {\n            for (int k = 0; k < 13; k++) {\n                for (int l = 0; l < 3; l++) {\n                    int index = i * 2 * 13 * 3 + j * 13 * 3 + k * 3 + l;\n                    T_sign[index] = (data[index] > 0.0f) ? 1.0f : ((data[index] < 0.0f) ? -1.0f : 0.0f);\n                }\n            }\n        }\n    }\n\n} // |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* T_sign, float* data) {\n  const int block_size = 16;\n  const int num_threads = 16;\n  #pragma omp parallel for\n  for (int i = 0; i < 16 * 2 * 13 * 3; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    T_sign[i] = (0.0f < data[i]) ? 1.0f : ((data[i] < 0.0f) ? -1.0f : 0.0f);\n  }\n}\n\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* T_sign, float* data) {\n  // Input tensor shape: [16, 2, 13, 3]\n  // T_sign shape: [16, 2, 13, 3]\n  // data shape: [16, 2, 13, 3]\n\n  for (int blockIdx_x = 0; blockIdx_x < 16; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 16; ++threadIdx_x) {\n      for (int i = 0; i < 2; ++i) {\n        for (int j = 0; j < 13; ++j) {\n          for (int k = 0; k < 3; ++k) {\n            int index = ((blockIdx_x * 104) + (threadIdx_x * 26) + (i * 39) + (j * 3) + k);\n            T_sign[index] = (data[index] > 0.0f) ? 1.0f : ((data[index] < 0.0f) ? -1.0f : 0.0f);\n          }\n        }\n      }\n    }\n  }\n}\n\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\nvoid default_function(float* T_sign, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        if (data[i] > 0.0f) {\n            T_sign[i] = 1.0f;\n        } else if (data[i] < 0.0f) {\n            T_sign[i] = -1.0f;\n        } else {\n            T_sign[i] = 0.0f;\n        }\n    }\n}\n\nint main() {\n    int n = 16, c = 2, h = 13, w = 3;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* T_sign = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 20.0f - 10.0f; // Random values between -10 and 10\n    }\n\n    default_function(T_sign, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", T_sign[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(T_sign);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        if (data[i] > 0.000000e+00f) {\n            T_sign[i] = 1.000000e+00f;\n        } else if (data[i] < 0.000000e+00f) {\n            T_sign[i] = -1.000000e+00f;\n        } else {\n            T_sign[i] = 0.000000e+00f;\n        }\n    }\n}\n\nint main() {\n    int size = 16 * 2 * 13 * 3;\n    float data[size];\n    float T_sign[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i - (size / 2)); // Some sample initialization\n    }\n\n    default_function_kernel(T_sign, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"T_sign[%d] = %f\\n\", i, T_sign[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "relu",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 6930; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = max(data[i0_i1_fused_i2_fused_i3_fused], 0.000000e+00f);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 0.000000e+00f);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 14, 3, 11), \"float32\"), compute: T.Buffer((15, 14, 3, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(6930):\n            compute_1 = T.Buffer((6930,), data=compute.data)\n            data_1 = T.Buffer((6930,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.max(data_1[i0_i1_fused_i2_fused_i3_fused], T.float32(0))",
        "op_args": [
            15,
            14,
            3,
            11
        ],
        "input_shape": "[[15, 14, 3, 11]]",
        "output_shape": "[[15, 14, 3, 11]]",
        "cluade1_c": "#include <stdio.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for\n    for (int i = 0; i < 15; i++) {\n        for (int j = 0; j < 14; j++) {\n            for (int k = 0; k < 3; k++) {\n                #pragma omp simd\n                for (int l = 0; l < 11; l++) {\n                    int idx = ((((i * 14 + j) * 3 + k) * 11) + l);\n                    compute[idx] = data[idx] > 0.0f ? data[idx] : 0.0f;\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "void default_function_kernel_cpu(float* compute, float* data) {\n    for (int blockIdx_x = 0; blockIdx_x < 15; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < 14; ++threadIdx_x) {\n            int index = (blockIdx_x * 14) + threadIdx_x;\n            compute[index] = fmaxf(data[index], 0.000000e+00f);\n        }\n    }\n}\n",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  const int block_size = 63;\n  const int num_threads = 63;\n  #pragma omp parallel for\n  for (int i = 0; i < 15 * 14 * 3 * 11; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    compute[i] = fmaxf(data[i], 0.0f);\n  }\n}\n\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n#include <float.h>\n\n// Function to calculate the maximum of two floats\nfloat max(float a, float b) {\n  return (a > b) ? a : b;\n}\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Input tensor shape: [15, 14, 3, 11]\n  // compute shape: [15, 14, 3, 11]\n  // data shape: [15, 14, 3, 11]\n\n  for (int blockIdx_x = 0; blockIdx_x < 15; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 14; ++threadIdx_x) {\n      for (int i = 0; i < 3; ++i) {\n        for (int j = 0; j < 11; ++j) {\n          int index = ((blockIdx_x * 462) + (threadIdx_x * 33) + (i * 11) + j);\n          compute[index] = max(data[index], 0.0f);\n        }\n      }\n    }\n  }\n}\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\nvoid default_function(float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        compute[i] = fmaxf(data[i], 0.0f);\n    }\n}\n\nint main() {\n    int n = 15, c = 14, h = 3, w = 11;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 20.0f - 10.0f; // Random values between -10 and 10\n    }\n\n    default_function(compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = fmaxf(data[i], 0.000000e+00f);\n    }\n}\n\nint main() {\n    int size = 15 * 14 * 3 * 11;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i - (size / 2)); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "sqrt",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 180) + (i1 * 9)) + i2)] = sqrtf(data[(((i0 * 180) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 20, 9, 1), \"float32\"), compute: T.Buffer((7, 20, 9, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(20, 9):\n                cse_var_1: T.int32 = i0 * 180 + i1 * 9 + i2\n                compute_1 = T.Buffer((1260,), data=compute.data)\n                data_1 = T.Buffer((1260,), data=data.data)\n                compute_1[cse_var_1] = T.sqrt(data_1[cse_var_1])",
        "op_args": [
            7,
            20,
            9,
            1
        ],
        "input_shape": "[[7, 20, 9, 1]]",
        "output_shape": "[[7, 20, 9, 1]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\n#define BLOCK_SIZE 36\n#define GRID_SIZE_X 7\n#define GRID_SIZE_Y 20\n#define GRID_SIZE_Z 9\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for\n    for (int x = 0; x < GRID_SIZE_X; x++) {\n        for (int y = 0; y < GRID_SIZE_Y; y++) {\n            for (int z = 0; z < GRID_SIZE_Z; z++) {\n                int index = (x * GRID_SIZE_Y * GRID_SIZE_Z) + (y * GRID_SIZE_Z) + z;\n                compute[index * BLOCK_SIZE + omp_get_thread_num()] = sqrt(data[index * BLOCK_SIZE + omp_get_thread_num()]);\n            }\n        }\n    }\n}\n",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    int blockId = 0;\n    int threadId = 0;\n    int blockStride = 36;\n\n    for (int i = 0; i < 7; i++) {\n        for (int j = 0; j < 20; j++) {\n            for (int k = 0; k < 9; k++) {\n                int dataIdx = (((blockId * blockStride) + threadId));\n                compute[dataIdx] = sqrtf(data[dataIdx]);\n\n                threadId++;\n                if (threadId == 36) {\n                    threadId = 0;\n                    blockId++;\n                }\n            }\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Calculate the number of threads per block (36 in this case)\n  const int threadsPerBlock = 36;\n\n  // Calculate the number of blocks needed for the input tensor shape\n  const int numBlocks = (7 * 20 * 9 * 1) / threadsPerBlock;\n\n  // Use OpenMP to parallelize the loop\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int globalIdx = (blockIdx * threadsPerBlock) + threadIdx;\n      compute[globalIdx] = sqrtf(data[globalIdx]);\n    }\n  }\n}\n//|End-of-Code|\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Input tensor shape: [7, 20, 9, 1]\n  // compute: [7, 20, 9, 1]\n  // data: [7, 20, 9, 1]\n  for (int blockIdx_x = 0; blockIdx_x < 7; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 20; ++threadIdx_x) {\n      for (int i = 0; i < 9; ++i) {\n        compute[((blockIdx_x * 36) + (threadIdx_x * 9)) + i] = sqrtf(data[((blockIdx_x * 36) + (threadIdx_x * 9)) + i]);\n      }\n    }\n  }\n}\n\n//|End-of-Code|\n",
        "gpt_1": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int num_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < num_elements; ++i) {\n        compute[i] = sqrtf(data[i]);\n    }\n}\n\nint main() {\n    int num_elements = 7 * 20 * 9 * 1;\n    float compute[num_elements];\n    float data[num_elements];\n\n    // Initialize data (for example purposes)\n    for (int i = 0; i < num_elements; ++i) {\n        data[i] = (float)i;\n    }\n\n    // Call the function\n    default_function_kernel(compute, data, num_elements);\n\n    // Print results (for example purposes)\n    for (int i = 0; i < num_elements; ++i) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = sqrtf(data[i]);\n    }\n}\n\nint main() {\n    int size = 7 * 20 * 9 * 1;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i + 1); // Some sample initialization to avoid sqrtf(0)\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "tan",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        for (int32_t i3 = 0; i3 < 20; ++i3) {\n          compute[((((i0 * 1320) + (i1 * 220)) + (i2 * 20)) + i3)] = tanf(data[((((i0 * 1320) + (i1 * 220)) + (i2 * 20)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(55) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 55) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 55) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 6, 11, 20), \"float32\"), compute: T.Buffer((13, 6, 11, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(13):\n            for i1, i2, i3 in T.grid(6, 11, 20):\n                cse_var_1: T.int32 = i0 * 1320 + i1 * 220 + i2 * 20 + i3\n                compute_1 = T.Buffer((17160,), data=compute.data)\n                data_1 = T.Buffer((17160,), data=data.data)\n                compute_1[cse_var_1] = T.tan(data_1[cse_var_1])",
        "op_args": [
            13,
            6,
            11,
            20
        ],
        "input_shape": "[[13, 6, 11, 20]]",
        "output_shape": "[[13, 6, 11, 20]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\n#define BLOCK_SIZE 55\n#define GRID_SIZE_X 13\n#define GRID_SIZE_Y 6\n#define GRID_SIZE_Z 11\n#define GRID_SIZE_W 20\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for\n    for (int x = 0; x < GRID_SIZE_X; x++) {\n        for (int y = 0; y < GRID_SIZE_Y; y++) {\n            for (int z = 0; z < GRID_SIZE_Z; z++) {\n                for (int w = 0; w < GRID_SIZE_W; w++) {\n                    int index = (x * GRID_SIZE_Y * GRID_SIZE_Z * GRID_SIZE_W) + (y * GRID_SIZE_Z * GRID_SIZE_W) + (z * GRID_SIZE_W) + w;\n                    compute[index * BLOCK_SIZE + omp_get_thread_num()] = tan(data[index * BLOCK_SIZE + omp_get_thread_num()]);\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    int blockId = 0;\n    int threadId = 0;\n    int blockStride = 55;\n\n    for (int i = 0; i < 13; i++) {\n        for (int j = 0; j < 6; j++) {\n            for (int k = 0; k < 11; k++) {\n                for (int l = 0; l < 20; l++) {\n                    int dataIdx = (((blockId * blockStride) + threadId));\n                    compute[dataIdx] = tanf(data[dataIdx]);\n\n                    threadId++;\n                    if (threadId == 55) {\n                        threadId = 0;\n                        blockId++;\n                    }\n                }\n            }\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Calculate the number of threads per block (55 in this case)\n  const int threadsPerBlock = 55;\n\n  // Calculate the number of blocks needed for the input tensor shape\n  const int numBlocks = (13 * 6 * 11 * 20) / threadsPerBlock;\n\n  // Use OpenMP to parallelize the loop\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int globalIdx = (blockIdx * threadsPerBlock) + threadIdx;\n      compute[globalIdx] = tanf(data[globalIdx]);\n    }\n  }\n}\n//|End-of-Code|\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Input tensor shape: [13, 6, 11, 20]\n  // compute: [13, 6, 11, 20]\n  // data: [13, 6, 11, 20]\n  for (int blockIdx_x = 0; blockIdx_x < 13; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 6; ++threadIdx_x) {\n      for (int i = 0; i < 11; ++i) {\n        for (int j = 0; j < 20; ++j) {\n          compute[((blockIdx_x * 55) + (threadIdx_x * 11 * 20)) + (i * 20) + j] = tanf(data[((blockIdx_x * 55) + (threadIdx_x * 11 * 20)) + (i * 20) + j]);\n        }\n      }\n    }\n  }\n}\n\n//|End-of-Code|\n",
        "gpt_1": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int num_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < num_elements; ++i) {\n        compute[i] = tanf(data[i]);\n    }\n}\n\nint main() {\n    int num_elements = 13 * 6 * 11 * 20;\n    float compute[num_elements];\n    float data[num_elements];\n\n    // Initialize data (for example purposes)\n    for (int i = 0; i < num_elements; ++i) {\n        data[i] = (float)i;\n    }\n\n    // Call the function\n    default_function_kernel(compute, data, num_elements);\n\n    // Print results (for example purposes)\n    for (int i = 0; i < num_elements; ++i) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = tanf(data[i]);\n    }\n}\n\nint main() {\n    int size = 13 * 6 * 11 * 20;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i / 10.0); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "prelu",
        "c_code": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 46512; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < data[i0_i1_fused_i2_fused_i3_fused]) ? data[i0_i1_fused_i2_fused_i3_fused] : (data[i0_i1_fused_i2_fused_i3_fused] * Scale[(i0_i1_fused_i2_fused_i3_fused % 19)]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(38) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] * Scale[(((int)threadIdx.x) % 19)]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 17, 16, 19), \"float32\"), Scale: T.Buffer((19,), \"float32\"), compute: T.Buffer((9, 17, 16, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(46512):\n            compute_1 = T.Buffer((46512,), data=compute.data)\n            data_1 = T.Buffer((46512,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.if_then_else(T.float32(0) < data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused] * Scale[i0_i1_fused_i2_fused_i3_fused % 19])",
        "op_args": [
            9,
            17,
            16,
            19
        ],
        "input_shape": "[[9, 17, 16, 19], [19]]",
        "output_shape": "[[9, 17, 16, 19]]",
        "cluade1_c": "#include <stdio.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* Scale, float* compute, float* data) {\n    #pragma omp parallel for\n    for (int i = 0; i < 9; i++) {\n        for (int j = 0; j < 17; j++) {\n            for (int k = 0; k < 16; k++) {\n                #pragma omp simd\n                for (int l = 0; l < 19; l++) {\n                    int idx = ((((i * 17 + j) * 16 + k) * 19) + l);\n                    compute[idx] = (0.0f < data[idx]) ? data[idx] : (data[idx] * Scale[l % 19]);\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "#include <stdio.h>\n\nvoid default_function_kernel_CPU(float* Scale, float* compute, float* data) {\n    int blockId_x = 0; // Assuming blockIdx.x = 0 for simplicity\n    int threadIdx_x = 0; // Assuming threadIdx.x = 0 for simplicity\n    int tensor_shape[] = {9, 17, 16, 19};\n    int scale_shape[] = {19};\n    int index = ((blockId_x * 38) + threadIdx_x);\n\n    // Flattening the 4D tensor to a 1D array\n    int flattened_index = index;\n    int stride = 1;\n    for (int i = 3; i >= 0; i--) {\n        flattened_index += stride * (index % tensor_shape[i]);\n        stride *= tensor_shape[i];\n        index /= tensor_shape[i];\n    }\n\n    // Adjusting for Scale array\n    int scale_index = threadIdx_x % scale_shape[0];\n\n    // Performing the computation\n    float value = data[flattened_index];\n    compute[flattened_index] = (value < 0.0f) ? value : (value * Scale[scale_index]);\n}\n\n// Print the single C code function implementation\ndefault_function_kernel_CPU(Scale, compute, data);  // Function call\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* Scale, float* compute, float* data) {\n  const int block_size = 38;\n  const int num_threads = 38;\n  #pragma omp parallel for\n  for (int i = 0; i < 9 * 17 * 16 * 19; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    compute[i] = (0.0f < data[i]) ? data[i] : (data[i] * Scale[thread_idx % 19]);\n  }\n}\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* Scale, float* compute, float* data) {\n  // Input tensor shape: [9, 17, 16, 19], [19]\n  // Scale shape: [19]\n  // compute shape: [9, 17, 16, 19]\n  // data shape: [9, 17, 16, 19]\n\n  for (int blockIdx_x = 0; blockIdx_x < 9; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 17; ++threadIdx_x) {\n      for (int i = 0; i < 16; ++i) {\n        for (int j = 0; j < 19; ++j) {\n          int index = ((blockIdx_x * 5440) + (threadIdx_x * 304) + (i * 19) + j);\n          compute[index] = (0.0f < data[index]) ? data[index] : (data[index] * Scale[j % 19]);\n        }\n      }\n    }\n  }\n}\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\nvoid default_function(float* Scale, float* compute, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        if (data[i] > 0.0f) {\n            compute[i] = data[i];\n        } else {\n            compute[i] = data[i] * Scale[i % 19];\n        }\n    }\n}\n\nint main() {\n    int n = 9, c = 17, h = 16, w = 19;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* compute = (float*)malloc(total_elements * sizeof(float));\n    float* Scale = (float*)malloc(19 * sizeof(float));\n\n    // Initialize data and Scale with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 20.0f - 10.0f; // Random values between -10 and 10\n    }\n    for (int i = 0; i < 19; i++) {\n        Scale[i] = (float)rand() / (float)RAND_MAX * 2.0f; // Random values between 0 and 2\n    }\n\n    default_function(Scale, compute, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(compute);\n    free(Scale);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data, int data_size, int scale_size) {\n    for (int i = 0; i < data_size; i++) {\n        int thread_id = i % 38; // Simulating threadIdx.x in CPU\n        int scale_idx = thread_id % scale_size;\n        compute[i] = (0.000000e+00f < data[i]) ? data[i] : (data[i] * Scale[scale_idx]);\n    }\n}\n\nint main() {\n    int data_size = 9 * 17 * 16 * 19;\n    int scale_size = 19;\n    float data[data_size];\n    float compute[data_size];\n    float Scale[scale_size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < data_size; i++) {\n        data[i] = (float)(i - (data_size / 2)); // Some sample initialization\n    }\n\n    // Initialize Scale array with some values\n    for (int i = 0; i < scale_size; i++) {\n        Scale[i] = (float)(i + 1); // Some sample initialization\n    }\n\n    default_function_kernel(Scale, compute, data, data_size, scale_size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "tanh",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 20; ++i1) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      for (int32_t i3 = 0; i3 < 13; ++i3) {\n        compute[(((i1 * 182) + (i2 * 13)) + i3)] = tanhf(data[(((i1 * 182) + (i2 * 13)) + i3)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = tanhf(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 20, 14, 13), \"float32\"), compute: T.Buffer((1, 20, 14, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(20, 14, 13):\n            cse_var_1: T.int32 = i1 * 182 + i2 * 13 + i3\n            compute_1 = T.Buffer((3640,), data=compute.data)\n            data_1 = T.Buffer((3640,), data=data.data)\n            compute_1[cse_var_1] = T.tanh(data_1[cse_var_1])",
        "op_args": [
            1,
            20,
            14,
            13
        ],
        "input_shape": "[[1, 20, 14, 13]]",
        "output_shape": "[[1, 20, 14, 13]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\n#define BLOCK_SIZE 28\n#define GRID_SIZE_X 1\n#define GRID_SIZE_Y 20\n#define GRID_SIZE_Z 14\n#define GRID_SIZE_W 13\n\nvoid default_function_kernel(float* compute, float* data) {\n    #pragma omp parallel for\n    for (int x = 0; x < GRID_SIZE_X; x++) {\n        for (int y = 0; y < GRID_SIZE_Y; y++) {\n            for (int z = 0; z < GRID_SIZE_Z; z++) {\n                for (int w = 0; w < GRID_SIZE_W; w++) {\n                    int index = (x * GRID_SIZE_Y * GRID_SIZE_Z * GRID_SIZE_W) + (y * GRID_SIZE_Z * GRID_SIZE_W) + (z * GRID_SIZE_W) + w;\n                    compute[index * BLOCK_SIZE + omp_get_thread_num()] = tanh(data[index * BLOCK_SIZE + omp_get_thread_num()]);\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_kernel_cpu(float* compute, float* data) {\n    int gridDim = 20 * 14 * 13;\n    int blockDim = 28;\n\n    for (int blockIdx_x = 0; blockIdx_x < gridDim; blockIdx_x++) {\n        for (int threadIdx_x = 0; threadIdx_x < blockDim; threadIdx_x++) {\n            int index = blockIdx_x * blockDim + threadIdx_x;\n            if (index < gridDim * blockDim) {\n                compute[index] = tanh(data[index]);\n            }\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Calculate the number of threads per block (28 in this case)\n  const int threadsPerBlock = 28;\n\n  // Calculate the number of blocks needed for the input tensor shape\n  const int numBlocks = (1 * 20 * 14 * 13) / threadsPerBlock;\n\n  // Use OpenMP to parallelize the loop\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int globalIdx = (blockIdx * threadsPerBlock) + threadIdx;\n      compute[globalIdx] = tanhf(data[globalIdx]);\n    }\n  }\n}\n//|End-of-Code|\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  // Input tensor shape: [1, 20, 14, 13]\n  // compute: [1, 20, 14, 13]\n  // data: [1, 20, 14, 13]\n  for (int blockIdx_x = 0; blockIdx_x < 1; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 20; ++threadIdx_x) {\n      for (int i = 0; i < 14; ++i) {\n        for (int j = 0; j < 13; ++j) {\n          compute[((blockIdx_x * 28) + (threadIdx_x * 14 * 13)) + (i * 13) + j] = tanhf(data[((blockIdx_x * 28) + (threadIdx_x * 14 * 13)) + (i * 13) + j]);\n        }\n      }\n    }\n  }\n}\n\n//|End-of-Code|\n\n",
        "gpt_1": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int num_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < num_elements; ++i) {\n        compute[i] = tanhf(data[i]);\n    }\n}\n\nint main() {\n    int num_elements = 1 * 20 * 14 * 13;\n    float compute[num_elements];\n    float data[num_elements];\n\n    // Initialize data (for example purposes)\n    for (int i = 0; i < num_elements; ++i) {\n        data[i] = (float)i / num_elements;  // Example data initialization\n    }\n\n    // Call the function\n    default_function_kernel(compute, data, num_elements);\n\n    // Print results (for example purposes)\n    for (int i = 0; i < num_elements; ++i) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ compute, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        compute[i] = tanhf(data[i]);\n    }\n}\n\nint main() {\n    int size = 1 * 20 * 14 * 13;\n    float data[size];\n    float compute[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i / 10.0); // Some sample initialization\n    }\n\n    default_function_kernel(compute, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"compute[%d] = %f\\n\", i, compute[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "scale_shift_nchwc",
        "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused < 196; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused) {\n    for (int32_t cb_outer_inner = 0; cb_outer_inner < 7; ++cb_outer_inner) {\n      for (int32_t i_inner = 0; i_inner < 16; ++i_inner) {\n        ScaleShift[((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused / 28) * 3136) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused & 1) * 1568)) + (i_inner * 98)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 28) >> 1) * 7)) + cb_outer_inner)] = ((data[((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused / 28) * 3136) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused & 1) * 1568)) + (i_inner * 98)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 28) >> 1) * 7)) + cb_outer_inner)] * Scale[(((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused & 1) * 7) + cb_outer_inner)]) + Shift[(((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused & 1) * 7) + cb_outer_inner)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[((((((((int)blockIdx.x) % 49) * 2) + (((int)threadIdx.x) >> 5)) / 49) * 7) + ((((int)blockIdx.x) + ((int)threadIdx.x)) % 7))]) + Shift[((((((((int)blockIdx.x) % 49) * 2) + (((int)threadIdx.x) >> 5)) / 49) * 7) + ((((int)blockIdx.x) + ((int)threadIdx.x)) % 7))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 2, 16, 14, 7), \"float32\"), Scale: T.Buffer((2, 7), \"float32\"), Shift: T.Buffer((2, 7), \"float32\"), ScaleShift: T.Buffer((7, 2, 16, 14, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused in T.parallel(196):\n            for cb_outer_inner, i_inner in T.grid(7, 16):\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 2\n                cse_var_2: T.int32 = cse_var_3 * 7 + cb_outer_inner\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused // 28 * 3136 + cse_var_3 * 1568 + i_inner * 98 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 28 // 2 * 7 + cb_outer_inner\n                ScaleShift_1 = T.Buffer((21952,), data=ScaleShift.data)\n                data_1 = T.Buffer((21952,), data=data.data)\n                Scale_1 = T.Buffer((14,), data=Scale.data)\n                Shift_1 = T.Buffer((14,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]",
        "op_args": [
            7,
            7,
            16,
            14
        ],
        "input_shape": "[[7, 2, 16, 14, 7], [2, 7], [2, 7]]",
        "output_shape": "[[7, 2, 16, 14, 7]]",
        "cluade1_c": "#include <stdio.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < 7; i++) {\n        for (int j = 0; j < 2; j++) {\n            for (int k = 0; k < 16; k++) {\n                for (int l = 0; l < 14; l++) {\n                    #pragma omp simd\n                    for (int m = 0; m < 7; m++) {\n                        int blockIdx_x = i * 448 + j * 224 + k * 14 + l * 7 + m;\n                        ScaleShift[blockIdx_x] = (data[blockIdx_x] * Scale[((i % 49) * 2 + (j >> 5) / 49) * 7 + (i + j) % 7]) +\n                                                  Shift[((i % 49) * 2 + (j >> 5) / 49) * 7 + (i + j) % 7];\n                    }\n                }\n            }\n        }\n    }\n}\n\n// End-of-Code",
        "cluade2_c": "void default_function_kernel_cpu(float* Scale, float* ScaleShift, float* Shift, float* data) {\n    for (int blockIdx_x = 0; blockIdx_x < 7; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < 64; ++threadIdx_x) {\n            int idx = (blockIdx_x * 64) + threadIdx_x;\n            int idx_mod_49 = (blockIdx_x % 49);\n            int thread_idx_shifted = threadIdx_x >> 5;\n            int scale_idx = (((idx_mod_49 * 2) + thread_idx_shifted) / 49) * 7 + ((blockIdx_x + threadIdx_x) % 7);\n            ScaleShift[idx] = (data[idx] * Scale[scale_idx]) + Shift[scale_idx];\n        }\n    }\n}\n",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  const int block_size = 64;\n  const int num_threads = 64;\n  #pragma omp parallel for\n  for (int i = 0; i < 7 * 2 * 16 * 14 * 7; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    int scale_idx = (((block_idx % 49) * 2 + (thread_idx >> 5)) / 49) * 7 + ((block_idx + thread_idx) % 7);\n    ScaleShift[i] = (data[i] * Scale[scale_idx]) + Shift[scale_idx];\n  }\n}\n\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  // Input tensor shape: [7, 2, 16, 14, 7], [2, 7], [2, 7]\n  // Scale shape: [2, 7]\n  // ScaleShift shape: [7, 2, 16, 14, 7]\n  // Shift shape: [2, 7]\n  // data shape: [7, 2, 16, 14, 7]\n\n  for (int blockIdx_x = 0; blockIdx_x < 7; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 64; ++threadIdx_x) {\n      for (int i = 0; i < 16; ++i) {\n        for (int j = 0; j < 14; ++j) {\n          for (int k = 0; k < 7; ++k) {\n            int index = ((blockIdx_x * 17920) + (threadIdx_x * 280) + (i * 196) + (j * 14) + k);\n            int scale_index = (((((blockIdx_x % 49) * 2) + (threadIdx_x >> 5)) / 49) * 7) + ((blockIdx_x + threadIdx_x) % 7);\n            ScaleShift[index] = (data[index] * Scale[scale_index]) + Shift[scale_index];\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\nvoid default_function(float* Scale, float* ScaleShift, float* Shift, float* data, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        int idx1 = (((((i / 64) % 49) * 2) + ((i % 64) >> 5)) / 49) * 7;\n        int idx2 = ((i / 64) + (i % 64)) % 7;\n        int idx = idx1 + idx2;\n\n        ScaleShift[i] = (data[i] * Scale[idx]) + Shift[idx];\n    }\n}\n\nint main() {\n    int n = 7, c = 2, d = 16, h = 14, w = 7;\n    int total_elements = n * c * d * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* ScaleShift = (float*)malloc(total_elements * sizeof(float));\n    float* Scale = (float*)malloc(14 * sizeof(float));\n    float* Shift = (float*)malloc(14 * sizeof(float));\n\n    // Initialize data, Scale, and Shift with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 20.0f - 10.0f; // Random values between -10 and 10\n    }\n    for (int i = 0; i < 14; i++) {\n        Scale[i] = (float)rand() / (float)RAND_MAX * 2.0f; // Random values between 0 and 2\n        Shift[i] = (float)rand() / (float)RAND_MAX * 5.0f - 2.5f; // Random values between -2.5 and 2.5\n    }\n\n    default_function(Scale, ScaleShift, Shift, data, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", ScaleShift[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(ScaleShift);\n    free(Scale);\n    free(Shift);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data, int data_size, int scale_shift_size, int shift_size) {\n    for (int i = 0; i < data_size; i++) {\n        int blockIdx_x = i / 64;\n        int threadIdx_x = i % 64;\n\n        int scale_shift_index = (((((blockIdx_x % 49) * 2) + (threadIdx_x >> 5)) / 49) * 7) + ((blockIdx_x + threadIdx_x) % 7);\n\n        ScaleShift[i] = (data[i] * Scale[scale_shift_index]) + Shift[scale_shift_index];\n    }\n}\n\nint main() {\n    int data_size = 7 * 2 * 16 * 14 * 7;\n    int scale_shift_size = 2 * 7;\n    int shift_size = 2 * 7;\n\n    float data[data_size];\n    float ScaleShift[data_size];\n    float Scale[scale_shift_size];\n    float Shift[shift_size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < data_size; i++) {\n        data[i] = (float)(i % 10); // Some sample initialization\n    }\n\n    // Initialize Scale and Shift arrays with some values\n    for (int i = 0; i < scale_shift_size; i++) {\n        Scale[i] = (float)(i + 1); // Some sample initialization\n    }\n    for (int i = 0; i < shift_size; i++) {\n        Shift[i] = (float)(i + 1); // Some sample initialization\n    }\n\n    default_function_kernel(Scale, ScaleShift, Shift, data, data_size, scale_shift_size, shift_size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"ScaleShift[%d] = %f\\n\", i, ScaleShift[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "softmax",
        "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    float T_softmax_maxelem[4];\n    float T_softmax_expsum[4];\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        T_softmax_maxelem[i2] = -3.402823e+38f;\n        T_softmax_maxelem[i2] = max(T_softmax_maxelem[i2], data[(((i0 * 16) + (i1 * 4)) + i2)]);\n      }\n      for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n        T_softmax_maxelem[i2_1] = expf((data[(((i0 * 16) + (i1 * 4)) + i2_1)] - T_softmax_maxelem[i2_1]));\n      }\n      for (int32_t i2_2 = 0; i2_2 < 4; ++i2_2) {\n        T_softmax_expsum[i2_2] = 0.000000e+00f;\n        T_softmax_expsum[i2_2] = (T_softmax_expsum[i2_2] + T_softmax_maxelem[i2_2]);\n      }\n      for (int32_t i2_3 = 0; i2_3 < 4; ++i2_3) {\n        T_softmax_norm[(((i0 * 16) + (i1 * 4)) + i2_3)] = (T_softmax_maxelem[i2_3] / T_softmax_expsum[i2_3]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 17) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 17) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) < 17) {\n    T_softmax_norm[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__expf((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) / T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + __expf((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 4, 4, 1), \"float32\"), T_softmax_norm: T.Buffer((17, 4, 4, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(17):\n            T_softmax_maxelem = T.allocate([4], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([4], \"float32\", \"global\")\n            for i1 in range(4):\n                T_softmax_maxelem_1 = T.Buffer((4,), data=T_softmax_maxelem, align=16)\n                data_1 = T.Buffer((272,), data=data.data)\n                for i2 in range(4):\n                    T_softmax_maxelem_1[i2] = T.float32(-3.4028234663852886e+38)\n                    T_softmax_maxelem_1[i2] = T.max(T_softmax_maxelem_1[i2], data_1[i0 * 16 + i1 * 4 + i2])\n                T_softmax_maxelem_2 = T.Buffer((4,), data=T_softmax_maxelem, align=16)\n                for i2 in range(4):\n                    T_softmax_maxelem_2[i2] = T.exp(data_1[i0 * 16 + i1 * 4 + i2] - T_softmax_maxelem_1[i2])\n                T_softmax_expsum_1 = T.Buffer((4,), data=T_softmax_expsum, align=16)\n                for i2 in range(4):\n                    T_softmax_expsum_1[i2] = T.float32(0)\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_maxelem_2[i2]\n                for i2 in range(4):\n                    T_softmax_norm_1 = T.Buffer((272,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[i0 * 16 + i1 * 4 + i2] = T_softmax_maxelem_2[i2] / T_softmax_expsum_1[i2]",
        "op_args": [
            17,
            4,
            4,
            1
        ],
        "input_shape": "[[17, 4, 4, 1]]",
        "output_shape": "[[17, 4, 4, 1]]",
        "cluade1_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n    #pragma omp parallel for\n    for (int i = 0; i < 17; i++) {\n        for (int j = 0; j < 4; j++) {\n            for (int k = 0; k < 4; k++) {\n                for (int l = 0; l < 1; l++) {\n                    int blockIdx_x = i * 4 * 4 + j * 4 + k;\n                    int threadIdx_x = l;\n                    if (((i * 2) + (threadIdx_x >> 4)) < 17) {\n                        T_softmax_maxelem[blockIdx_x * 32 + threadIdx_x] = -3.402823e+38f;\n                    }\n                    if (((i * 2) + (threadIdx_x >> 4)) < 17) {\n                        T_softmax_maxelem[blockIdx_x * 32 + threadIdx_x] = fmaxf(T_softmax_maxelem[blockIdx_x * 32 + threadIdx_x], data[blockIdx_x * 32 + threadIdx_x]);\n                    }\n                }\n            }\n        }\n    }\n}\n\nvoid default_function_kernel_2(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data) {\n    #pragma omp parallel for\n    for (int i = 0; i < 17; i++) {\n        for (int j = 0; j < 4; j++) {\n            for (int k = 0; k < 4; k++) {\n                for (int l = 0; l < 1; l++) {\n                    int blockIdx_x = i * 4 * 4 + j * 4 + k;\n                    int threadIdx_x = l;\n                    if (((i * 4) + (threadIdx_x >> 4)) < 17) {\n                        T_softmax_norm[blockIdx_x * 64 + threadIdx_x] = (expf(data[blockIdx_x * 64 + threadIdx_x] - T_softmax_maxelem[blockIdx_x * 64 + threadIdx_x]) / T_softmax_expsum[blockIdx_x * 64 + threadIdx_x]);\n                    }\n                }\n            }\n        }\n    }\n}\n\nvoid default_function_kernel_1(float* T_softmax_expsum, float* T_softmax_maxelem, float* data) {\n    #pragma omp parallel for\n    for (int i = 0; i < 17; i++) {\n        for (int j = 0; j < 4; j++) {\n            for (int k = 0; k < 1; k++) {\n                int blockIdx_x = i * 4 + j;\n                int threadIdx_x = k;\n                T_softmax_expsum[blockIdx_x * 8 + threadIdx_x] = 0.000000e+00f;\n                T_softmax_expsum[blockIdx_x * 8 + threadIdx_x] += expf(data[blockIdx_x * 8 + threadIdx_x] - T_softmax_maxelem[blockIdx_x * 8 + threadIdx_x]);\n            }\n        }\n    }\n}\n",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_kernel_cpu(float* T_softmax_maxelem, float* data) {\n    for (int i = 0; i < 17; i++) {\n        for (int j = 0; j < 4; j++) {\n            for (int k = 0; k < 4; k++) {\n                for (int l = 0; l < 1; l++) {\n                    int idx = i * 4 * 4 * 1 + j * 4 * 1 + k * 1 + l;\n                    T_softmax_maxelem[i * 4 * 4 * 1 + j * 4 * 1 + k * 1 + l] = -3.402823e+38f;\n                    if (i * 2 + (j * 16 + k) < 17) {\n                        T_softmax_maxelem[i * 4 * 4 * 1 + j * 4 * 1 + k * 1 + l] = fmaxf(T_softmax_maxelem[i * 4 * 4 * 1 + j * 4 * 1 + k * 1 + l], data[idx]);\n                    }\n                }\n            }\n        }\n    }\n}\n\nvoid default_function_kernel_2_cpu(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data) {\n    for (int i = 0; i < 17; i++) {\n        for (int j = 0; j < 4; j++) {\n            for (int k = 0; k < 4; k++) {\n                for (int l = 0; l < 1; l++) {\n                    int idx = i * 4 * 4 * 1 + j * 4 * 1 + k * 1 + l;\n                    T_softmax_norm[i * 4 * 4 * 1 + j * 4 * 1 + k * 1 + l] = expf((data[idx] - T_softmax_maxelem[idx])) / T_softmax_expsum[idx];\n                }\n            }\n        }\n    }\n}\n\nvoid default_function_kernel_1_cpu(float* T_softmax_expsum, float* T_softmax_maxelem, float* data) {\n    for (int i = 0; i < 17; i++) {\n        for (int j = 0; j < 4; j++) {\n            for (int k = 0; k < 4; k++) {\n                for (int l = 0; l < 1; l++) {\n                    int idx = i * 4 * 4 * 1 + j * 4 * 1 + k * 1 + l;\n                    T_softmax_expsum[i * 4 * 4 * 1 + j * 4 * 1 + k * 1 + l] = 0.000000e+00f;\n                    T_softmax_expsum[i * 4 * 4 * 1 + j * 4 * 1 + k * 1 + l] += expf((data[idx] - T_softmax_maxelem[idx]));\n                }\n            }\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n  const int block_size = 32;\n  const int num_threads = 32;\n  #pragma omp parallel for\n  for (int i = 0; i < 17 * 4 * 4 * 1; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    if ((block_idx * 2 + (thread_idx >> 4)) < 17) {\n      T_softmax_maxelem[i] = -3.402823e+38f;\n      T_softmax_maxelem[i] = fmaxf(T_softmax_maxelem[i], data[i]);\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* T_softmax_expsum, float* T_softmax_maxelem, float* data) {\n  const int block_size = 8;\n  const int num_threads = 8;\n  #pragma omp parallel for\n  for (int i = 0; i < 17 * 4 * 4 * 1; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    T_softmax_expsum[i] = 0.0f;\n    T_softmax_expsum[i] += expf(data[i] - T_softmax_maxelem[i]);\n  }\n}\n\nvoid default_function_kernel_2(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data) {\n  const int block_size = 64;\n  const int num_threads = 64;\n  #pragma omp parallel for\n  for (int i = 0; i < 17 * 4 * 4 * 1; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    if ((block_idx * 4 + (thread_idx >> 4)) < 17) {\n      T_softmax_norm[i] = expf((data[i] - T_softmax_maxelem[i])) / T_softmax_expsum[i];\n    }\n  }\n}\n\n\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n#include <float.h>\n\n// Function to calculate the maximum of two floats\nfloat max(float a, float b) {\n  return (a > b) ? a : b;\n}\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n  // Input tensor shape: [17, 4, 4, 1]\n  // T_softmax_maxelem shape: [17, 4, 4, 1]\n  // data shape: [17, 4, 4, 1]\n\n  for (int blockIdx_x = 0; blockIdx_x < 17; ++blockIdx_x) {\n    for (int i = 0; i < 4; ++i) {\n      for (int j = 0; j < 4; ++j) {\n        for (int k = 0; k < 1; ++k) {\n          int index = ((blockIdx_x * 64) + (i * 16) + (j * 4) + k);\n          T_softmax_maxelem[index] = -FLT_MAX;\n          T_softmax_maxelem[index] = max(T_softmax_maxelem[index], data[index]);\n        }\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* T_softmax_expsum, float* T_softmax_maxelem, float* data) {\n  // Input tensor shape: [17, 4, 4, 1]\n  // T_softmax_expsum shape: [17, 4, 4, 1]\n  // T_softmax_maxelem shape: [17, 4, 4, 1]\n  // data shape: [17, 4, 4, 1]\n\n  for (int blockIdx_x = 0; blockIdx_x < 17; ++blockIdx_x) {\n    for (int i = 0; i < 4; ++i) {\n      for (int j = 0; j < 4; ++j) {\n        for (int k = 0; k < 1; ++k) {\n          int index = ((blockIdx_x * 64) + (i * 16) + (j * 4) + k);\n          T_softmax_expsum[index] = 0.0f;\n          T_softmax_expsum[index] += expf(data[index] - T_softmax_maxelem[index]);\n        }\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_2(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data) {\n  // Input tensor shape: [17, 4, 4, 1]\n  // T_softmax_expsum shape: [17, 4, 4, 1]\n  // T_softmax_maxelem shape: [17, 4, 4, 1]\n  // T_softmax_norm shape: [17, 4, 4, 1]\n  // data shape: [17, 4, 4, 1]\n\n  for (int blockIdx_x = 0; blockIdx_x < 17; ++blockIdx_x) {\n    for (int i = 0; i < 4; ++i) {\n      for (int j = 0; j < 4; ++j) {\n        for (int k = 0; k < 1; ++k) {\n          int index = ((blockIdx_x * 64) + (i * 16) + (j * 4) + k);\n          T_softmax_norm[index] = expf(data[index] - T_softmax_maxelem[index]) / T_softmax_expsum[index];\n        }\n      }\n    }\n  }\n}\n\n\n",
        "gpt_1": "#include <stdio.h>\n#include <math.h>\n#include <float.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data, int num_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < num_elements; ++i) {\n        int block_x = i / 32;\n        int thread_x = i % 32;\n\n        if ((block_x * 2) + (thread_x >> 4) < 17) {\n            T_softmax_maxelem[i] = -FLT_MAX;\n        }\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < num_elements; ++i) {\n        int block_x = i / 32;\n        int thread_x = i % 32;\n\n        if ((block_x * 2) + (thread_x >> 4) < 17) {\n            T_softmax_maxelem[i] = fmax(T_softmax_maxelem[i], data[i]);\n        }\n    }\n}\n\nvoid default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data, int num_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < num_elements; ++i) {\n        int block_x = i / 8;\n        int thread_x = i % 8;\n\n        T_softmax_expsum[i] = 0.0f;\n        T_softmax_expsum[i] += expf(data[i] - T_softmax_maxelem[i]);\n    }\n}\n\nvoid default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data, int num_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < num_elements; ++i) {\n        int block_x = i / 64;\n        int thread_x = i % 64;\n\n        if ((block_x * 4) + (thread_x >> 4) < 17) {\n            T_softmax_norm[i] = expf(data[i] - T_softmax_maxelem[i]) / T_softmax_expsum[i];\n        }\n    }\n}\n\nint main() {\n    int num_elements = 17 * 4 * 4 * 1;\n    float T_softmax_maxelem[num_elements];\n    float data[num_elements];\n    float T_softmax_expsum[num_elements];\n    float T_softmax_norm[num_elements];\n\n    // Initialize data (for example purposes)\n    for (int i = 0; i < num_elements; ++i) {\n        data[i] = (float)i;\n    }\n\n    // Call the functions\n    default_function_kernel(T_softmax_maxelem, data, num_elements);\n    default_function_kernel_1(T_softmax_expsum, T_softmax_maxelem, data, num_elements);\n    default_function_kernel_2(T_softmax_expsum, T_softmax_maxelem, T_softmax_norm, data, num_elements);\n\n    // Print results (for example purposes)\n    for (int i = 0; i < num_elements; ++i) {\n        printf(\"T_softmax_norm[%d] = %f\\n\", i, T_softmax_norm[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n#include <float.h>\n\nvoid default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data, int size) {\n    int limit = 17; // Derived from the shape and the condition in the CUDA kernel\n    for (int i = 0; i < size; i++) {\n        if ((i / 16) < limit) {\n            T_softmax_maxelem[i] = -FLT_MAX;\n        }\n    }\n\n    for (int i = 0; i < size; i++) {\n        if ((i / 16) < limit) {\n            T_softmax_maxelem[i] = fmaxf(T_softmax_maxelem[i], data[i]);\n        }\n    }\n}\n\nvoid default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        T_softmax_expsum[i] = 0.000000e+00f;\n    }\n\n    for (int i = 0; i < size; i++) {\n        T_softmax_expsum[i] += expf(data[i] - T_softmax_maxelem[i]);\n    }\n}\n\nvoid default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data, int size) {\n    int limit = 17; // Derived from the shape and the condition in the CUDA kernel\n\n    for (int i = 0; i < size; i++) {\n        if ((i / 16) < limit) {\n            T_softmax_norm[i] = expf(data[i] - T_softmax_maxelem[i]) / T_softmax_expsum[i];\n        }\n    }\n}\n\nint main() {\n    int size = 17 * 4 * 4 * 1;\n    float data[size];\n    float T_softmax_maxelem[size];\n    float T_softmax_expsum[size];\n    float T_softmax_norm[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i % 10); // Some sample initialization\n    }\n\n    default_function_kernel(T_softmax_maxelem, data, size);\n    default_function_kernel_1(T_softmax_expsum, T_softmax_maxelem, data, size);\n    default_function_kernel_2(T_softmax_expsum, T_softmax_maxelem, T_softmax_norm, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"T_softmax_norm[%d] = %f\\n\", i, T_softmax_norm[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "combination_op",
        "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 392; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n      T_add[((ax0_ax1_fused_ax2_fused * 16) + ax3)] = (sqrtf(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)]) + cosf(data_1[((ax0_ax1_fused_ax2_fused * 16) + ax3)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 4, 7, 16), \"float32\"), data_1: T.Buffer((14, 4, 7, 16), \"float32\"), T_add: T.Buffer((14, 4, 7, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(392):\n            for ax3 in range(16):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 16 + ax3\n                T_add_1 = T.Buffer((6272,), data=T_add.data)\n                data_2 = T.Buffer((6272,), data=data.data)\n                data_3 = T.Buffer((6272,), data=data_1.data)\n                T_add_1[cse_var_1] = T.sqrt(data_2[cse_var_1]) + T.cos(data_3[cse_var_1])",
        "op_args": [
            14,
            4,
            7,
            16
        ],
        "input_shape": "[[14, 4, 7, 16], [14, 4, 7, 16]]",
        "output_shape": "[[14, 4, 7, 16]]",
        "cluade1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* T_add, float* data, float* data_1) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < 14; blockIdx_x++) {\n        for (int blockIdx_y = 0; blockIdx_y < 4; blockIdx_y++) {\n            for (int blockIdx_z = 0; blockIdx_z < 7; blockIdx_z++) {\n                for (int threadIdx_x = 0; threadIdx_x < 16; threadIdx_x++) {\n                    int index = ((blockIdx_x * 4 * 7 * 16) + (blockIdx_y * 7 * 16) + (blockIdx_z * 16) + threadIdx_x);\n                    T_add[index] = sqrtf(data[index]) + cosf(data_1[index]);\n                }\n            }\n        }\n    }\n}\n",
        "cluade2_c": "#include <math.h>\n\nvoid default_function_kernel_CPU(float* T_add, float* data, float* data_1) {\n    int gridDim_x = 14;\n    int blockDim_x = 32;\n\n    for (int blockIdx_x = 0; blockIdx_x < gridDim_x; blockIdx_x++) {\n        for (int threadIdx_x = 0; threadIdx_x < blockDim_x; threadIdx_x++) {\n            int index = blockIdx_x * blockDim_x + threadIdx_x;\n\n            if (index < 14 * 4 * 7 * 16) {\n                T_add[index] = sqrtf(data[index]) + cosf(data_1[index]);\n            }\n        }\n    }\n}\n\n// |End-of-Code|",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* T_add, float* data, float* data_1, int size) {\n  #pragma omp parallel for\n  for (int i = 0; i < size; i++) {\n    T_add[i] = (sqrtf(data[i]) + cosf(data_1[i]));\n  }\n}\n//End-of-Code|\n\n",
        "gemini2_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* T_add, float* data, float* data_1) {\n  // Calculate the number of threads per block (32 in this case)\n  const int threadsPerBlock = 32;\n\n  // Calculate the number of blocks needed for the input tensor shape\n  const int numBlocks = (14 * 4 * 7 * 16) / threadsPerBlock;\n\n  // Use OpenMP to parallelize the loop\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int globalIdx = (blockIdx * threadsPerBlock) + threadIdx;\n      T_add[globalIdx] = sqrtf(data[globalIdx]) + cosf(data_1[globalIdx]);\n    }\n  }\n}\n//|End-of-Code|\n\n",
        "gpt_1": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function(float* T_add, float* data, float* data_1, int total_elements) {\n    #pragma omp parallel for\n    for (int i = 0; i < total_elements; i++) {\n        T_add[i] = sqrtf(data[i]) + cosf(data_1[i]);\n    }\n}\n\nint main() {\n    int n = 14, c = 4, h = 7, w = 16;\n    int total_elements = n * c * h * w; // Calculating the total number of elements in the tensor\n    float* data = (float*)malloc(total_elements * sizeof(float));\n    float* data_1 = (float*)malloc(total_elements * sizeof(float));\n    float* T_add = (float*)malloc(total_elements * sizeof(float));\n\n    // Initialize data arrays with some values for demonstration\n    for (int i = 0; i < total_elements; i++) {\n        data[i] = (float)rand() / (float)RAND_MAX * 100.0f; // Random values between 0 and 100\n        data_1[i] = (float)rand() / (float)RAND_MAX * 100.0f; // Random values between 0 and 100\n    }\n\n    default_function(T_add, data, data_1, total_elements);\n\n    // Optionally print the computed array\n    for (int i = 0; i < total_elements; i++) {\n        printf(\"%f \", T_add[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(data_1);\n    free(T_add);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1, int size) {\n    for (int i = 0; i < size; i++) {\n        T_add[i] = sqrtf(data[i]) + cosf(data_1[i]);\n    }\n}\n\nint main() {\n    int size = 14 * 4 * 7 * 16;\n    float data[size];\n    float data_1[size];\n    float T_add[size];\n\n    // Initialize data arrays with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i % 100);       // Some sample initialization\n        data_1[i] = (float)(i % 50);      // Some sample initialization\n    }\n\n    default_function_kernel(T_add, data, data_1, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"T_add[%d] = %f\\n\", i, T_add[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    },
    {
        "op_name": "softmax_common",
        "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    float T_softmax_maxelem[168];\n    float T_softmax_expsum[14];\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        T_softmax_maxelem[((i1 * 14) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 3; ++k) {\n          T_softmax_maxelem[((i1 * 14) + i2)] = max(T_softmax_maxelem[((i1 * 14) + i2)], data[((((i0 * 504) + (i1 * 42)) + (i2 * 3)) + k)]);\n        }\n      }\n    }\n    for (int32_t i1_1 = 0; i1_1 < 12; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n        T_softmax_expsum[i2_1] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 3; ++k_1) {\n            int32_t v_ = ((int32_t)(floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)])));\n        }\n      }\n      for (int32_t i2_2 = 0; i2_2 < 14; ++i2_2) {\n        for (int32_t i3_s = 0; i3_s < 3; ++i3_s) {\n            int32_t v__1 = ((int32_t)(floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_norm[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)])) / T_softmax_expsum[i2_2]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 189) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 3; ++k) {\n    if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 189) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 96) + (((int)threadIdx.x) * 3)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 567) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)])) / T_softmax_expsum[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 3; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 12, 14, 3), \"float32\"), T_softmax_norm: T.Buffer((18, 12, 14, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(18):\n            T_softmax_maxelem = T.allocate([168], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([14], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((168,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((9072,), data=data.data)\n            for i1, i2 in T.grid(12, 14):\n                T_softmax_maxelem_1[i1 * 14 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(3):\n                    cse_var_1: T.int32 = i1 * 14 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 504 + i1 * 42 + i2 * 3 + k])\n            for i1 in range(12):\n                T_softmax_expsum_1 = T.Buffer((14,), data=T_softmax_expsum, align=32)\n                for i2 in range(14):\n                    T_softmax_expsum_1[i2] = T.float32(0)\n                    for k in range(3):\n                        cse_var_3: T.int32 = i1 * 14 + i2\n                        cse_var_2: T.int32 = i0 * 504 + i1 * 42 + i2 * 3 + k\n                        T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3])\n                for i2, i3_s in T.grid(14, 3):\n                    cse_var_5: T.int32 = i1 * 14 + i2\n                    cse_var_4: T.int32 = i0 * 504 + i1 * 42 + i2 * 3 + i3_s\n                    T_softmax_norm_1 = T.Buffer((9072,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_4] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5]) / T_softmax_expsum_1[i2]",
        "op_args": [
            18,
            12,
            14,
            3
        ],
        "input_shape": "[[18, 12, 14, 3]]",
        "output_shape": "[[18, 12, 14, 3]]",
        "cluade1_c": "void default_function_kernel(float* T_softmax_norm, float* data, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    float T_softmax_maxelem[168];\n    float T_softmax_expsum[14];\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        T_softmax_maxelem[((i1 * 14) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 3; ++k) {\n          T_softmax_maxelem[((i1 * 14) + i2)] = max(T_softmax_maxelem[((i1 * 14) + i2)], data[((((i0 * 504) + (i1 * 42)) + (i2 * 3)) + k)]);\n        }\n      }\n    }\n    for (int32_t i1_1 = 0; i1_1 < 12; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n        T_softmax_expsum[i2_1] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 3; ++k_1) {\n            int32_t v_ = ((int32_t)(floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)])));\n        }\n      }\n      for (int32_t i2_2 = 0; i2_2 < 14; ++i2_2) {\n        for (int32_t i3_s = 0; i3_s < 3; ++i3_s) {\n            int32_t v__1 = ((int32_t)(floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_norm[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)])) / T_softmax_expsum[i2_2]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cluade2_c": "void default_function_kernel(float* T_softmax_norm, float*) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    float T_softmax_maxelem[168];\n    float T_softmax_expsum[14];\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        T_softmax_maxelem[((i1 * 14) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 3; ++k) {\n          T_softmax_maxelem[((i1 * 14) + i2)] = max(T_softmax_maxelem[((i1 * 14) + i2)], data[((((i0 * 504) + (i1 * 42)) + (i2 * 3)) + k)]);\n        }\n      }\n    }\n    for (int32_t i1_1 = 0; i1_1 < 12; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n        T_softmax_expsum[i2_1] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 3; ++k_1) {\n            int32_t v_ = ((int32_t)(floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)])));\n        }\n      }\n      for (int32_t i2_2 = 0; i2_2 < 14; ++i2_2) {\n        for (int32_t i3_s = 0; i3_s < 3; ++i3_s) {\n            int32_t v__1 = ((int32_t)(floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_norm[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)])) / T_softmax_expsum[i2_2]);\n        }\n      }\n    }\n  }\n}\n\n",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n  const int block_size = 32;\n  const int num_threads = 32;\n  #pragma omp parallel for\n  for (int i = 0; i < 18 * 12 * 14 * 3; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    if ((block_idx * 2 + (thread_idx >> 4)) < 189) {\n      T_softmax_maxelem[i] = -3.402823e+38f;\n      for (int k = 0; k < 3; ++k) {\n        if ((block_idx * 2 + (thread_idx >> 4)) < 189) {\n          T_softmax_maxelem[i] = fmaxf(T_softmax_maxelem[i], data[(block_idx * 96 + thread_idx * 3) + k]);\n        }\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* T_softmax_expsum, float* T_softmax_maxelem, float* data) {\n  const int block_size = 4;\n  const int num_threads = 4;\n  #pragma omp parallel for\n  for (int i = 0; i < 18 * 12 * 14 * 3; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    T_softmax_expsum[i] = 0.0f;\n    for (int k = 0; k < 3; ++k) {\n      float val = data[(block_idx * 12 + thread_idx * 3) + k] - T_softmax_maxelem[i];\n      val = fmaxf(fminf(val, 8.837627e+01f), -8.837626e+01f);\n      val = (floorf((val * 1.442695e+00f) + 5.000000e-01f) + 1.270000e+02f) << 23;\n      T_softmax_expsum[i] += (\n        (\n          (\n            (\n              (\n                (\n                  (\n                    1.987569e-04f * (val - (floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))\n                  ) + 1.398200e-03f\n                ) * (val - (floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))\n              ) + 8.333452e-03f\n            ) * (val - (floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))\n          ) + 4.166580e-02f\n        ) * (val - (floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))\n      ) + (\n        (\n          (\n            (\n              (\n                (\n                  1.666667e-01f * (val - (floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))\n                ) + 5.000000e-01f\n              ) * (val - (floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))\n            ) + (val - (floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))\n          ) + (val - (floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))\n        ) + 1.000000e+00f\n      );\n    }\n  }\n}\n\nvoid default_function_kernel_2(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data) {\n  const int block_size = 32;\n  const int num_threads = 32;\n  #pragma omp parallel for\n  for (int i = 0; i < 18 * 12 * 14 * 3; i++) {\n    int block_idx = i / block_size;\n    int thread_idx = i % block_size;\n    if ((block_idx * 2 + (thread_idx >> 4)) < 567) {\n      float val = data[i] - T_softmax_maxelem[i / 3];\n      val = fmaxf(fminf(val, 8.837627e+01f), -8.837626e+01f);\n      val = (floorf((val * 1.442695e+00f) + 5.000000e-01f) + 1.270000e+02f) << 23;\n      T_softmax_norm[i] = fmaxf(\n        (\n          (\n            (\n              (\n                (\n                  (\n                    1.987569e-04f * (val - (floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))\n                  ) + 1.398200e-03f\n                ) * (val - (floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))\n              ) + 8.333452e-03f\n            ) * (val - (floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))\n          ) + 4.166580e-02f\n        ) * (val - (floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))\n      ) + (\n        (\n          (\n            (\n              (\n                (\n                  1.666667e-01f * (val - (floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))\n                ) + 5.000000e-01f\n              ) * (val - (floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))\n            ) + (val - (floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))\n          ) + (val - (floorf((val * 1.442695e+00f) + 5.000000e-01f) * 6.931472e-01f))\n        ) + 1.000000e+00f\n      ), val / T_softmax_expsum[i / 3]);\n    }\n  }\n}\n",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n#include <float.h>\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n  // Input tensor shape: [18, 12, 14, 3]\n  // T_softmax_maxelem: [18, 12, 14]\n  // data: [18, 12, 14, 3]\n  for (int blockIdx_x = 0; blockIdx_x < 18; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 12; ++threadIdx_x) {\n      if (((blockIdx_x * 2) + (threadIdx_x >> 4)) < 189) {\n        T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x] = -FLT_MAX;\n        for (int k = 0; k < 3; ++k) {\n          if (((blockIdx_x * 2) + (threadIdx_x >> 4)) < 189) {\n            T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x] = fmaxf(T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x], data[((blockIdx_x * 96) + (threadIdx_x * 3)) + k]);\n          }\n        }\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* T_softmax_expsum, float* T_softmax_maxelem, float* data) {\n  // Input tensor shape: [18, 12, 14, 3]\n  // T_softmax_expsum: [18, 12, 14]\n  // T_softmax_maxelem: [18, 12, 14]\n  // data: [18, 12, 14, 3]\n  for (int blockIdx_x = 0; blockIdx_x < 18; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 12; ++threadIdx_x) {\n      T_softmax_expsum[(blockIdx_x * 4) + threadIdx_x] = 0.0f;\n      for (int k = 0; k < 3; ++k) {\n        float exp_val = expf(fminf(fmaxf(data[((blockIdx_x * 12) + (threadIdx_x * 3)) + k] - T_softmax_maxelem[(blockIdx_x * 4) + threadIdx_x], -88.37626f), 88.37627f));\n        T_softmax_expsum[(blockIdx_x * 4) + threadIdx_x] += exp_val;\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_2(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data) {\n  // Input tensor shape: [18, 12, 14, 3]\n  // T_softmax_expsum: [18, 12, 14]\n  // T_softmax_maxelem: [18, 12, 14]\n  // T_softmax_norm: [18, 12, 14]\n  // data: [18, 12, 14, 3]\n  for (int blockIdx_x = 0; blockIdx_x < 18; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 12; ++threadIdx_x) {\n      if (((blockIdx_x * 2) + (threadIdx_x >> 4)) < 567) {\n        float exp_val = expf(fminf(fmaxf(data[(blockIdx_x * 32) + threadIdx_x] - T_softmax_maxelem[((blockIdx_x * 32) + threadIdx_x) / 3], -88.37626f), 88.37627f));\n        T_softmax_norm[(blockIdx_x * 32) + threadIdx_x] = fmaxf(exp_val / T_softmax_expsum[((blockIdx_x * 32) + threadIdx_x) / 3], (data[(blockIdx_x * 32) + threadIdx_x] - T_softmax_maxelem[((blockIdx_x * 32) + threadIdx_x) / 3]));\n      }\n    }\n  }\n}\n\n//|End-of-Code|",
        "gpt_1": "#include <stdio.h>\n#include <math.h>\n#include <float.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data, int num_elements) {\n    int num_blocks = num_elements / 32;\n    \n    #pragma omp parallel for\n    for (int i = 0; i < num_blocks; ++i) {\n        for (int j = 0; j < 32; ++j) {\n            int idx = i * 32 + j;\n            int block_x = idx / 32;\n            int thread_x = idx % 32;\n\n            if ((block_x * 2) + (thread_x >> 4) < 189) {\n                T_softmax_maxelem[idx] = -FLT_MAX;\n            }\n        }\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < num_blocks; ++i) {\n        for (int j = 0; j < 32; ++j) {\n            int idx = i * 32 + j;\n            int block_x = idx / 32;\n            int thread_x = idx % 32;\n\n            for (int k = 0; k < 3; ++k) {\n                if ((block_x * 2) + (thread_x >> 4) < 189) {\n                    T_softmax_maxelem[idx] = fmax(T_softmax_maxelem[idx], data[(i * 96 + j * 3) + k]);\n                }\n            }\n        }\n    }\n}\n\nint main() {\n    int num_elements = 18 * 12 * 14 * 3;\n    float T_softmax_maxelem[num_elements];\n    float data[num_elements];\n\n    // Initialize data (for example purposes)\n    for (int i = 0; i < num_elements; ++i) {\n        data[i] = (float)i;\n    }\n\n    // Call the function\n    default_function_kernel(T_softmax_maxelem, data, num_elements);\n\n    // Print results (for example purposes)\n    for (int i = 0; i < num_elements; ++i) {\n        printf(\"T_softmax_maxelem[%d] = %f\\n\", i, T_softmax_maxelem[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt_2": "#include <math.h>\n#include <stdio.h>\n#include <float.h>\n\nvoid default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ data, int size) {\n    int limit = 189; // Derived from the shape and the condition in the CUDA kernel\n    for (int i = 0; i < size; i++) {\n        if ((i / 16) < limit) {\n            T_softmax_maxelem[i] = -FLT_MAX;\n        }\n    }\n\n    for (int k = 0; k < 3; ++k) {\n        for (int i = 0; i < size; i++) {\n            if ((i / 16) < limit) {\n                int index = (i * 3) + k;\n                T_softmax_maxelem[i] = fmaxf(T_softmax_maxelem[i], data[index]);\n            }\n        }\n    }\n}\n\nvoid default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data, int size) {\n    int limit = 567; // Derived from the shape and the condition in the CUDA kernel\n\n    for (int i = 0; i < size; i++) {\n        if ((i / 16) < limit) {\n            float val = data[i] - T_softmax_maxelem[i / 3];\n            float clamped_val = fmaxf(fminf(val, 8.837627e+01f), -8.837626e+01f);\n            float exp_val = clamped_val * 1.442695e+00f;\n            int v_ = ((int)(floorf(exp_val + 5.000000e-01f)) + 1.270000e+02f) << 23;\n\n            float mantissa = clamped_val - floorf(exp_val) * 6.931472e-01f;\n            float result = (*(float *)&v_);\n            result *= (((((((((((((1.987569e-04f * mantissa) + 1.398200e-03f) * mantissa) + 8.333452e-03f) * mantissa) + 4.166580e-02f) * mantissa) + 1.666667e-01f) * mantissa) + 5.000000e-01f) * mantissa) + 1.000000e+00f);\n\n            T_softmax_norm[i] = fmaxf(result, val) / T_softmax_expsum[i / 3];\n        }\n    }\n}\n\nvoid default_function_kernel_3(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data, int size) {\n    for (int i = 0; i < size; i++) {\n        T_softmax_expsum[i] = 0.0f;\n    }\n\n    for (int k = 0; k < 3; ++k) {\n        for (int i = 0; i < size; i++) {\n            float val = data[(i * 3) + k] - T_softmax_maxelem[i];\n            float clamped_val = fmaxf(fminf(val, 8.837627e+01f), -8.837626e+01f);\n            float exp_val = clamped_val * 1.442695e+00f;\n            int v_ = ((int)(floorf(exp_val + 5.000000e-01f)) + 1.270000e+02f) << 23;\n\n            float mantissa = clamped_val - floorf(exp_val) * 6.931472e-01f;\n            float result = (*(float *)&v_);\n            result *= (((((((((((((1.987569e-04f * mantissa) + 1.398200e-03f) * mantissa) + 8.333452e-03f) * mantissa) + 4.166580e-02f) * mantissa) + 1.666667e-01f) * mantissa) + 5.000000e-01f) * mantissa) + 1.000000e+00f);\n\n            T_softmax_expsum[i] += fmaxf(result, val);\n        }\n    }\n}\n\nint main() {\n    int size = 18 * 12 * 14 * 3;\n    float data[size];\n    float T_softmax_maxelem[size];\n    float T_softmax_expsum[size];\n    float T_softmax_norm[size];\n\n    // Initialize data array with some values\n    for (int i = 0; i < size; i++) {\n        data[i] = (float)(i % 10); // Some sample initialization\n    }\n\n    default_function_kernel_1(T_softmax_maxelem, data, size);\n    default_function_kernel_3(T_softmax_expsum, T_softmax_maxelem, data, size);\n    default_function_kernel_2(T_softmax_expsum, T_softmax_maxelem, T_softmax_norm, data, size);\n\n    // Print a few values for verification\n    for (int i = 0; i < 10; i++) {\n        printf(\"T_softmax_norm[%d] = %f\\n\", i, T_softmax_norm[i]);\n    }\n\n    return 0;\n}\n\n// |End-of-Code|\n"
    }
]