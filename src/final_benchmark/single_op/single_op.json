[
    {
        "op_name": "reorg",
        "c_code": "void default_function_kernel(float* A, float* T_reshape) {\n  float tensor[16777216];\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 64; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 128; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 128; ++ax3) {\n          tensor[((((ax0 * 1048576) + (ax1 * 16384)) + (ax2 * 128)) + ax3)] = A[((((((ax0 * 4194304) + ((ax1 & 15) * 65536)) + (ax2 * 512)) + ((ax1 >> 5) * 256)) + (ax3 * 2)) + ((ax1 & 31) >> 4))];\n        }\n      }\n    }\n  }\n  for (int32_t ax0_1 = 0; ax0_1 < 16; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 256; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 64; ++ax2_1) {\n        for (int32_t ax3_1 = 0; ax3_1 < 64; ++ax3_1) {\n          T_reshape[((((ax0_1 * 1048576) + (ax1_1 * 4096)) + (ax2_1 * 64)) + ax3_1)] = tensor[((((ax0_1 * 1048576) + (ax1_1 * 4096)) + (ax2_1 * 64)) + ax3_1)];\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ A, float* __restrict__ T_reshape) {\n  T_reshape[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = A[(((((((((int)blockIdx.x) >> 10) * 4194304) + ((((int)blockIdx.x) & 255) * 4096)) + ((((int)threadIdx.x) >> 7) * 512)) + (((((int)blockIdx.x) & 1023) >> 9) * 256)) + ((((int)threadIdx.x) & 127) * 2)) + ((((int)blockIdx.x) & 511) >> 8))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((16, 64, 128, 128), \"float32\"), T_reshape: T.Buffer((16, 256, 64, 64), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        tensor = T.allocate([16777216], \"float32\", \"global\")\n        tensor_1 = T.Buffer((16777216,), data=tensor)\n        for ax0, ax1, ax2, ax3 in T.grid(16, 64, 128, 128):\n            A_1 = T.Buffer((16777216,), data=A.data)\n            tensor_1[ax0 * 1048576 + ax1 * 16384 + ax2 * 128 + ax3] = A_1[ax0 * 4194304 + ax1 % 16 * 65536 + ax2 * 512 + ax1 // 32 * 256 + ax3 * 2 + ax1 % 32 // 16]\n        for ax0, ax1, ax2, ax3 in T.grid(16, 256, 64, 64):\n            cse_var_1: T.int32 = ax0 * 1048576 + ax1 * 4096 + ax2 * 64 + ax3\n            T_reshape_1 = T.Buffer((16777216,), data=T_reshape.data)\n            T_reshape_1[cse_var_1] = tensor_1[cse_var_1]",
        "op_args": "None",
        "input_shape": "[[16, 64, 128, 128]]",
        "output_shape": "[[16, 256, 64, 64]]",
        "input_name": [
            "A"
        ],
        "output_name": [
            "T_reshape"
        ],
        "deepseek_c": "void default_function_kernel(float* A, float* T_reshape) {\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 16384; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_reshape[ax0_ax1_fused_ax2_fused_ax3_fused] = A[((((((ax0_ax1_fused_ax2_fused_ax3_fused >> 10) * 4194304) + (((ax0_ax1_fused_ax2_fused_ax3_fused & 255) * 4096) + (((ax0_ax1_fused_ax2_fused_ax3_fused & 1023) >> 9) * 256))) + (((ax0_ax1_fused_ax2_fused_ax3_fused & 1023) & 255) >> 8)) + (((ax0_ax1_fused_ax2_fused_ax3_fused & 127) * 2)) + (((ax0_ax1_fused_ax2_fused_ax3_fused & 511) >> 8))))];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* A, float* T_reshape) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 16384; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 256; ++ax3) {\n      T_reshape[((ax0_ax1_fused_ax2_fused * 256) + ax3)] = A[((((((ax0_ax1_fused_ax2_fused >> 10) * 4194304) + ((ax0_ax1_fused_ax2_fused & 1023) * 4096)) + (((ax3 >> 7) * 512) + (((ax0_ax1_fused_ax2_fused & 511) >> 9) * 256))) + ((ax3 & 127) * 2)) + (ax0_ax1_fused_ax2_fused & 511))];\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* A, float* T_reshape) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1048576; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 256; ++ax3) {\n      T_reshape[((ax0_ax1_fused_ax2_fused * 256) + ax3)] = A[(((((ax0_ax1_fused_ax2_fused >> 11) * 4194304) + ((ax0_ax1_fused_ax2_fused & 2047) * 512)) + (ax3 * 2)) + ((ax0_ax1_fused_ax2_fused & 2047) >> 11))];\n    }\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* A, float* T_reshape) {\n  #pragma omp parallel for\n  for (int blockIdx_x = 0; blockIdx_x < 16; blockIdx_x++) {\n    for (int threadIdx_x = 0; threadIdx_x < 1024; threadIdx_x++) {\n      T_reshape[((blockIdx_x * 1024) + threadIdx_x)] = A[(((((blockIdx_x >> 10) * 4194304) + ((blockIdx_x & 255) * 4096)) + ((threadIdx_x >> 7) * 512)) + (((blockIdx_x & 1023) >> 9) * 256)) + ((threadIdx_x & 127) * 2) + ((blockIdx_x & 511) >> 8))];\n    }\n  }\n}\n",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel(float* A, float* T_reshape) {\n  #pragma omp parallel for\n  for (int i = 0; i < 16; i++) {\n    for (int j = 0; j < 1024; j++) {\n      T_reshape[(i * 1024) + j] = A[(((((i >> 10) * 4194304) + (((i & 255) * 4096)) + (((j >> 7) * 512)) + (((i & 1023) >> 9) * 256)) + (((j & 127) * 2)) + (((i & 511) >> 8))];\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* A, float* T_reshape) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 16 * 64 * 128; // Adjusted for the given tensor shape\n    int blockDim_x = 1024;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int src_index = (((((blockIdx_x >> 10) * 4194304) +\n                               ((blockIdx_x & 255) * 4096)) +\n                              ((threadIdx_x >> 7) * 512)) +\n                             (((blockIdx_x & 1023) >> 9) * 256)) +\n                            ((threadIdx_x & 127) * 2) +\n                            ((blockIdx_x & 511) >> 8);\n            T_reshape[idx] = A[src_index];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* A, float* T_reshape) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 16 * 64 * 128 * 128 / 1024; // Adjusted for the given tensor shape (total elements divided by blockDim_x)\n    int blockDim_x = 1024;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int src_idx = (((((blockIdx_x >> 10) * 4194304) +\n                             ((blockIdx_x & 255) * 4096)) +\n                            ((threadIdx_x >> 7) * 512)) +\n                           (((blockIdx_x & 1023) >> 9) * 256)) +\n                          ((threadIdx_x & 127) * 2) +\n                          ((blockIdx_x & 511) >> 8);\n            T_reshape[idx] = A[src_idx];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* A, float* T_reshape) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2097152; ++i0_i1_fused_i2_fused_i3_fused) {\n    T_reshape[i0_i1_fused_i2_fused_i3_fused] = A[((((((i0_i1_fused_i2_fused_i3_fused >> 10) * 4194304) + ((i0_i1_fused_i2_fused_i3_fused & 255) * 4096)) + ((i0_i1_fused_i2_fused_i3_fused >> 7) * 512)) + (((i0_i1_fused_i2_fused_i3_fused & 1023) >> 9) * 256)) + ((i0_i1_fused_i2_fused_i3_fused & 127) * 2)) + ((i0_i1_fused_i2_fused_i3_fused & 511) >> 8))];\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "scatter_nd",
        "c_code": "void default_function_kernel(float* data, int32_t* indices, float* scatter_nd_generic, float* updates) {\n  for (int32_t i = 0; i < 3120; ++i) {\n    scatter_nd_generic[i] = data[i];\n  }\n  for (int32_t j = 0; j < 3; ++j) {\n    #pragma omp parallel for\n    for (int32_t k = 0; k < 1560; ++k) {\n      scatter_nd_generic[((indices[j] * 1560) + k)] = (scatter_nd_generic[((indices[j] * 1560) + k)] + updates[((j * 1560) + k)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel_1(float* __restrict__ atomic_add_return, int* __restrict__ indices, float* __restrict__ scatter_nd_cuda, float* __restrict__ updates) {\n  if (((((int)blockIdx.y) * 128) + (((int)threadIdx.x) >> 3)) < 195) {\n    atomic_add_return[0] = atomicAdd((&(scatter_nd_cuda[(((indices[((int)blockIdx.x)] * 1560) + (((int)blockIdx.y) * 1024)) + ((int)threadIdx.x))])), updates[(((((int)blockIdx.x) * 1560) + (((int)blockIdx.y) * 1024)) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ data, float* __restrict__ scatter_nd_cuda) {\n  if (((((int)blockIdx.x) * 64) + (((int)threadIdx.x) >> 4)) < 195) {\n    scatter_nd_cuda[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 1560), \"float32\"), indices: T.Buffer((1, 3), \"int32\"), updates: T.Buffer((3, 1560), \"float32\"), scatter_nd_generic: T.Buffer((2, 1560), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T.attr(0, \"extern_scope\", 0)\n        scatter_nd_generic_1 = T.Buffer((3120,), data=scatter_nd_generic.data)\n        for i in range(3120):\n            data_1 = T.Buffer((3120,), data=data.data)\n            scatter_nd_generic_1[i] = data_1[i]\n        for j in range(3):\n            for k in T.parallel(1560):\n                indices_1 = T.Buffer((3,), \"int32\", data=indices.data)\n                updates_1 = T.Buffer((4680,), data=updates.data)\n                scatter_nd_generic_1[indices_1[j] * 1560 + k] = scatter_nd_generic_1[indices_1[j] * 1560 + k] + updates_1[j * 1560 + k]",
        "op_args": "None",
        "input_shape": "[[2, 1560], [1, 3], [3, 1560]]",
        "output_shape": "[[2, 1560]]",
        "input_name": [
            "data",
            "indices",
            "updates"
        ],
        "output_name": [
            "scatter_nd.generic"
        ],
        "deepseek_c": "void default_function_kernel(float* data, float* scatter_nd_cuda, float* updates) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3120; ++i0_i1_fused_i2_fused) {\n    scatter_nd_cuda[i0_i1_fused_i2_fused] = data[i0_i1_fused_i2_fused];\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3120; ++i0_i1_fused_i2_fused_1) {\n    float atomic_add_return[1];\n    atomic_add_return[0] = 0.000000e+00f;\n    for (int32_t i0_i1_fused_i2_fused_inner = 0; i0_i1_fused_i2_fused_inner < 3; ++i0_i1_fused_i2_fused_inner) {\n      atomic_add_return[0] = (atomic_add_return[0] + scatter_nd_cuda[((((i0_i1_fused_i2_fused_inner * 1040) + (i0_i1_fused_i2_fused % 1040)) + (i0_i1_fused_i2_fused_inner * 1040)) + (i0_i1_fused_i2_fused % 1040)))]);\n    }\n    scatter_nd_cuda[i0_i1_fused_i2_fused_1] = atomic_add_return[0];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* data, int* indices, float* scatter_nd_cuda, float* updates) {\n  float atomic_add_return[1];\n  for (int32_t i1 = 0; i1 < 2; ++i1) {\n    for (int32_t i2 = 0; i2 < 1560; ++i2) {\n      scatter_nd_cuda[((i1 * 1560) + i2)] = data[((i1 * 1560) + i2)];\n    }\n  }\n  for (int32_t i1_1 = 0; i1_1 < 1; ++i1_1) {\n    for (int32_t i2_1 = 0; i2_1 < 3; ++i2_1) {\n      for (int32_t i3 = 0; i3 < 1560; ++i3) {\n        atomic_add_return[0] = 0.000000e+00f;\n        for (int32_t k = 0; k < 1; ++k) {\n          atomic_add_return[0] = (atomic_add_return[0] + updates[(((k * 3) + i2_1) * 1560) + i3]);\n        }\n        scatter_nd_cuda[(((i1_1 * 4680) + (i2_1 * 1560)) + i3)] = (scatter_nd_cuda[(((i1_1 * 4680) + (i2_1 * 1560)) + i3)] + atomic_add_return[0]);\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* data, float* scatter_nd_cuda, float* updates, int* indices) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3120; ++i0_i1_fused_i2_fused) {\n    scatter_nd_cuda[i0_i1_fused_i2_fused] = data[i0_i1_fused_i2_fused];\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 3120; ++i0_i1_fused) {\n    scatter_nd_cuda[i0_i1_fused] = (scatter_nd_cuda[i0_i1_fused] + updates[i0_i1_fused]);\n  }\n}",
        "gemini1_c": "#include <omp.h>\n#include <atomic>\n\nvoid default_function_kernel_1(float* atomic_add_return, int* indices, float* scatter_nd_cuda, float* updates) {\n  #pragma omp parallel for\n  for (int blockIdx_y = 0; blockIdx_y < 3; blockIdx_y++) {\n    for (int blockIdx_x = 0; blockIdx_x < 2; blockIdx_x++) {\n      for (int threadIdx_x = 0; threadIdx_x < 1024; threadIdx_x++) {\n        int idx = (blockIdx_y * 128) + (threadIdx_x >> 3);\n        if (idx < 195) {\n          int scatter_nd_cuda_idx = (indices[blockIdx_x] * 1560) + (blockIdx_y * 1024) + threadIdx_x;\n          int updates_idx = (blockIdx_x * 1560) + (blockIdx_y * 1024) + threadIdx_x;\n          atomic_add_return[0] = std::atomic_fetch_add(&scatter_nd_cuda[scatter_nd_cuda_idx], updates[updates_idx]);\n        }\n      }\n    }\n  }\n}\n\nvoid default_function_kernel(float* data, float* scatter_nd_cuda) {\n  #pragma omp parallel for\n  for (int blockIdx_x = 0; blockIdx_x < 2; blockIdx_x++) {\n    for (int threadIdx_x = 0; threadIdx_x < 1024; threadIdx_x++) {\n      int idx = (blockIdx_x * 64) + (threadIdx_x >> 4);\n      if (idx < 195) {\n        scatter_nd_cuda[(blockIdx_x * 1024) + threadIdx_x] = data[(blockIdx_x * 1024) + threadIdx_x];\n      }\n    }\n  }\n}\n",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel_1(float* atomic_add_return, int* indices, float* scatter_nd_cuda, float* updates) {\n  #pragma omp parallel for\n  for (int i = 0; i < 3; i++) {\n    for (int j = 0; j < 128; j++) {\n      if ((i * 128) + (j >> 3) < 195) {\n        atomic_add_return[0] += updates[((i * 1024) + j)];\n        scatter_nd_cuda[((indices[i] * 1560) + (i * 1024) + j)] += updates[((i * 1024) + j)];\n      }\n    }\n  }\n}\n\nvoid default_function_kernel(float* data, float* scatter_nd_cuda) {\n  #pragma omp parallel for\n  for (int i = 0; i < 3; i++) {\n    for (int j = 0; j < 1024; j++) {\n      if ((i * 64) + (j >> 4) < 195) {\n        scatter_nd_cuda[(i * 1024) + j] = data[(i * 1024) + j];\n      }\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel_1(float* atomic_add_return, int* indices, float* scatter_nd_cuda, float* updates) {\n    int blockIdx_x, blockIdx_y, threadIdx_x;\n    int gridDim_x = 3; // Based on input tensor shape\n    int gridDim_y = 1560 / 128 + 1;\n    int blockDim_x = 1024;\n\n    #pragma omp parallel for collapse(2) private(blockIdx_x, blockIdx_y, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (blockIdx_y = 0; blockIdx_y < gridDim_y; ++blockIdx_y) {\n            for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n                int idx = (blockIdx_y * 128) + (threadIdx_x >> 3);\n                if (idx < 195) {\n                    int scatter_idx = (indices[blockIdx_x] * 1560) + (blockIdx_y * 1024) + threadIdx_x;\n                    int updates_idx = (blockIdx_x * 1560) + (blockIdx_y * 1024) + threadIdx_x;\n                    #pragma omp atomic\n                    scatter_nd_cuda[scatter_idx] += updates[updates_idx];\n                    #pragma omp critical\n                    {\n                        atomic_add_return[0] = scatter_nd_cuda[scatter_idx];\n                    }\n                }\n            }\n        }\n    }\n}\n\nvoid default_function_kernel(float* data, float* scatter_nd_cuda) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 3; // Based on input tensor shape\n    int blockDim_x = 1024;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * 64) + (threadIdx_x >> 4);\n            if (idx < 195) {\n                int scatter_idx = (blockIdx_x * 1024) + threadIdx_x;\n                scatter_nd_cuda[scatter_idx] = data[scatter_idx];\n            }\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel_1(float* atomic_add_return, int* indices, float* scatter_nd_cuda, float* updates) {\n    int blockIdx_x, blockIdx_y, threadIdx_x;\n    int gridDim_x = 3; // Based on input tensor shape\n    int gridDim_y = 1560 / 128 + 1; // 13\n    int blockDim_x = 1024;\n\n    #pragma omp parallel for collapse(2) private(blockIdx_x, blockIdx_y, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (blockIdx_y = 0; blockIdx_y < gridDim_y; ++blockIdx_y) {\n            for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n                int idx_y = (blockIdx_y * 128) + (threadIdx_x >> 3);\n                if (idx_y < 195) {\n                    int scatter_idx = (indices[blockIdx_x] * 1560) + (blockIdx_y * 1024) + threadIdx_x;\n                    int updates_idx = (blockIdx_x * 1560) + (blockIdx_y * 1024) + threadIdx_x;\n                    #pragma omp atomic\n                    scatter_nd_cuda[scatter_idx] += updates[updates_idx];\n                    #pragma omp critical\n                    {\n                        atomic_add_return[0] = scatter_nd_cuda[scatter_idx];\n                    }\n                }\n            }\n        }\n    }\n}\n\nvoid default_function_kernel(float* data, float* scatter_nd_cuda) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 3; // Based on input tensor shape\n    int blockDim_x = 1024;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * 64) + (threadIdx_x >> 4);\n            if (idx < 195) {\n                int scatter_idx = (blockIdx_x * 1024) + threadIdx_x;\n                scatter_nd_cuda[scatter_idx] = data[scatter_idx];\n            }\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* data, float* scatter_nd_cuda) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 195; ++i0_i1_fused) {\n    for (int32_t i2_s = 0; i2_s < 1024; ++i2_s) {\n      if (((i0_i1_fused * 1024) + i2_s) < 195 * 1024) {\n        scatter_nd_cuda[((i0_i1_fused * 1024) + i2_s)] = data[((i0_i1_fused * 1024) + i2_s)];\n      }\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "gather_nd",
        "c_code": "void default_function_kernel(float* A, float* T_gather_nd, float* indices) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 8; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 64; ++ax3) {\n      for (int32_t ax4_outer = 0; ax4_outer < 4; ++ax4_outer) {\n        for (int32_t ax4_inner = 0; ax4_inner < 16; ++ax4_inner) {\n          if (((ax4_outer * 8) + (ax4_inner >> 1)) < 25) {\n            T_gather_nd[((((ax0_ax1_fused_ax2_fused * 3200) + (ax3 * 50)) + (ax4_outer * 16)) + ax4_inner)] = A[(((((((int32_t)indices[ax0_ax1_fused_ax2_fused]) * 204800) + (((int32_t)indices[(ax0_ax1_fused_ax2_fused + 8)]) * 3200)) + (ax3 * 50)) + (ax4_outer * 16)) + ax4_inner)];\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ A, float* __restrict__ T_gather_nd, float* __restrict__ indices) {\n  T_gather_nd[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = A[(((((int)indices[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) / 25)]) * 204800) + (((int)indices[((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) / 25) + 8)]) * 3200)) + (((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) % 3200))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((32, 64, 64, 50), \"float32\"), indices: T.Buffer((2, 2, 2, 2), \"float32\"), T_gather_nd: T.Buffer((2, 2, 2, 64, 50), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(8):\n            for ax3, ax4_outer, ax4_inner in T.grid(64, 4, 16):\n                if T.likely(ax4_outer * 8 + ax4_inner // 2 < 25):\n                    cse_var_2: T.int32 = ax4_outer * 16\n                    cse_var_1: T.int32 = ax3 * 50\n                    T_gather_nd_1 = T.Buffer((25600,), data=T_gather_nd.data)\n                    A_1 = T.Buffer((6553600,), data=A.data)\n                    indices_1 = T.Buffer((16,), data=indices.data)\n                    T_gather_nd_1[ax0_ax1_fused_ax2_fused * 3200 + cse_var_1 + cse_var_2 + ax4_inner] = A_1[T.Cast(\"int32\", indices_1[ax0_ax1_fused_ax2_fused]) * 204800 + T.Cast(\"int32\", indices_1[ax0_ax1_fused_ax2_fused + 8]) * 3200 + cse_var_1 + cse_var_2 + ax4_inner]",
        "op_args": "None",
        "input_shape": "[[32, 64, 64, 50], [2, 2, 2, 2]]",
        "output_shape": "[[2, 2, 2, 64, 50]]",
        "input_name": [
            "A",
            "indices"
        ],
        "output_name": [
            "T_gather_nd"
        ],
        "deepseek_c": "void default_function_kernel(float* A, float* T_gather_nd, float* indices) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 204800; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_gather_nd[ax0_ax1_fused_ax2_fused_ax3_fused] = A[(((((ax0_ax1_fused_ax2_fused_ax3_fused / 3200) * 204800) + (((ax0_ax1_fused_ax2_fused_ax3_fused % 3200) / 8) * 3200)) + (((ax0_ax1_fused_ax2_fused_ax3_fused % 8) * 400) + (((ax0_ax1_fused_ax2_fused_ax3_fused % 3200) % 8) * 50))) + (((ax0_ax1_fused_ax2_fused_ax3_fused / 3200) * 50) + (((ax0_ax1_fused_ax2_fused_ax3_fused % 3200) / 8) * 50)))];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* A, float* T_gather_nd, float* indices) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 128; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 2; ++ax3) {\n        T_gather_nd[(((ax0_ax1_fused * 4) + (ax2 * 2)) + ax3)] = A[(((((int)indices[ax0_ax1_fused]) * 204800) + (((int)indices[((ax0_ax1_fused * 4) + (ax2 * 2))]) * 3200)) + (((ax0_ax1_fused % 2) * 1600) + (ax3 * 800)))];\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* A, float* T_gather_nd, float* indices) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 12288; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 50; ++ax3) {\n      T_gather_nd[((ax0_ax1_fused_ax2_fused * 50) + ax3)] = A[((((((int32_t)indices[((ax0_ax1_fused_ax2_fused / 100) * 2) + 16]) * 204800) + ((int32_t)indices[(((ax0_ax1_fused_ax2_fused / 100) * 2) + 17)] * 3200)) + ((ax0_ax1_fused_ax2_fused % 100) / 50 * 500)) + ((ax0_ax1_fused_ax2_fused % 50) * 2)) + ax3)];\n    }\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* A, float* T_gather_nd, float* indices) {\n  #pragma omp parallel for\n  for (int blockIdx_x = 0; blockIdx_x < 32; blockIdx_x++) {\n    for (int threadIdx_x = 0; threadIdx_x < 1024; threadIdx_x++) {\n      T_gather_nd[((blockIdx_x * 1024) + threadIdx_x)] = A[(((((int)indices[(((((blockIdx_x * 8) + (threadIdx_x >> 7)) / 25)]) * 204800) + (((int)indices[((((((blockIdx_x * 8) + (threadIdx_x >> 7)) / 25) + 8)]) * 3200)) + (((((blockIdx_x * 1024) + threadIdx_x)) % 3200)))];\n    }\n  }\n}\n",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel(float* A, float* T_gather_nd, float* indices) {\n  #pragma omp parallel for\n  for (int i = 0; i < 2; i++) {\n    for (int j = 0; j < 1024; j++) {\n      T_gather_nd[(i * 1024) + j] = A[(((((int)indices[((i * 8) + (j >> 7)) / 25)]) * 204800) + (((int)indices[((i * 8) + (j >> 7) / 25) + 8)]) * 3200)) + (((i * 1024) + j) % 3200))];\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* A, float* T_gather_nd, float* indices) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 32;\n    int blockDim_x = 1024;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int index1 = (int)indices[((blockIdx_x * 8) + (threadIdx_x >> 7)) / 25];\n            int index2 = (int)indices[(((blockIdx_x * 8) + (threadIdx_x >> 7)) / 25) + 8];\n            int source_idx = (index1 * 204800) + (index2 * 3200) + (idx % 3200);\n            T_gather_nd[idx] = A[source_idx];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* A, float* T_gather_nd, float* indices) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 32 * 64; // Based on input tensor shape (32 * 64)\n    int blockDim_x = 1024;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int index1 = (int)indices[((blockIdx_x * 8) + (threadIdx_x >> 7)) / 25];\n            int index2 = (int)indices[(((blockIdx_x * 8) + (threadIdx_x >> 7)) / 25) + 8];\n            int source_idx = (index1 * 204800) + (index2 * 3200) + (idx % 3200);\n            T_gather_nd[idx] = A[source_idx];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* A, float* T_gather_nd, float* indices) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 8; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 1024; ++i3_s) {\n      T_gather_nd[((i0_i1_fused_i2_fused * 1024) + i3_s)] = A[(((((int)indices[((i0_i1_fused_i2_fused * 8) + (i3_s >> 7)) / 25)]) * 204800) + (((int)indices[(((i0_i1_fused_i2_fused * 8) + (i3_s >> 7)) / 25) + 8)]) * 3200)) + (((i0_i1_fused_i2_fused * 1024) + i3_s) % 3200)];\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "reshape",
        "c_code": "void default_function_kernel(float* A, float* T_reshape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 128; ++ax0) {\n    for (int32_t ax1_outer = 0; ax1_outer < 20; ++ax1_outer) {\n      for (int32_t ax1_inner = 0; ax1_inner < 16; ++ax1_inner) {\n        T_reshape[(((ax0 * 320) + (ax1_outer * 16)) + ax1_inner)] = A[(((ax0 * 320) + (ax1_outer * 16)) + ax1_inner)];\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ A, float* __restrict__ T_reshape) {\n  T_reshape[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = A[((((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 6)) / 5) * 320) + (((((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) % 160) / 5) * 10)) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((2, 64, 32, 10), \"float32\"), T_reshape: T.Buffer((128, 320), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(128):\n            for ax1_outer, ax1_inner in T.grid(20, 16):\n                cse_var_1: T.int32 = ax0 * 320 + ax1_outer * 16 + ax1_inner\n                T_reshape_1 = T.Buffer((40960,), data=T_reshape.data)\n                A_1 = T.Buffer((40960,), data=A.data)\n                T_reshape_1[cse_var_1] = A_1[cse_var_1]",
        "op_args": "None",
        "input_shape": "[[2, 64, 32, 10]]",
        "output_shape": "[[128, 320]]",
        "input_name": [
            "A"
        ],
        "output_name": [
            "T_reshape"
        ],
        "deepseek_c": "void default_function_kernel(float* A, float* T_transpose) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 20480; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_transpose[(((((((int)blockIdx.x) / 20) * 20480) + ((((int)threadIdx.x) & 63) * 320)) + ((((int)blockIdx.x) % 20) * 16)) + (((int)threadIdx.x) >> 6))] = A[(((((((int)blockIdx.x) / 20) * 20480) + ((((int)threadIdx.x) & 63) * 320)) + ((((int)blockIdx.x) % 20) * 16)) + (((int)threadIdx.x) >> 6))];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* A, float* T_transpose) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1280; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n      T_transpose[((ax0_ax1_fused_ax2_fused * 16) + ax3)] = A[((((ax0_ax1_fused_ax2_fused / 32) * 20480) + ((ax3 * 320) + (ax0_ax1_fused_ax2_fused % 32))) + (ax0_ax1_fused_ax2_fused / 32))];\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* A, float* T_transpose) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1280; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 10; ++ax3) {\n      T_transpose[((ax0_ax1_fused_ax2_fused * 10) + ax3)] = A[((((ax0_ax1_fused_ax2_fused & 127) * 10) + ax3) + (((ax0_ax1_fused_ax2_fused & 192) >> 3) * 16))];\n    }\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* A, float* T_transpose) {\n  #pragma omp parallel for\n  for (int i = 0; i < 20; i++) {\n    for (int j = 0; j < 1024; j++) {\n      T_transpose[(i * 1024) + j] = A[(((((i / 20) * 20480) + (((j & 63) * 320)) + ((i % 20) * 16)) + (j >> 6))];\n    }\n  }\n}\n//End-of-Code|",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel(float* A, float* T_transpose) {\n  #pragma omp parallel for\n  for (int i = 0; i < 2; i++) {\n    for (int j = 0; j < 1024; j++) {\n      T_transpose[(i * 1024) + j] = A[(((((i / 20) * 20480) + (((j & 63) * 320)) + (((i % 20) * 16)) + (j >> 6))];\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* A, float* T_transpose) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 2 * 64 * 32 * 10 / 1024; // Adjusted for the given tensor shape\n    int blockDim_x = 1024;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int src_idx = ((((blockIdx_x / 20) * 20480) + ((threadIdx_x & 63) * 320)) + ((blockIdx_x % 20) * 16)) + (threadIdx_x >> 6);\n            T_transpose[idx] = A[src_idx];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* A, float* T_transpose) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 2 * 64 * 32 * 10 / 1024; // Adjusted for the given tensor shape\n    int blockDim_x = 1024;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int src_idx = (((blockIdx_x / 20) * 20480) +\n                           ((threadIdx_x & 63) * 320) +\n                           ((blockIdx_x % 20) * 16) +\n                           (threadIdx_x >> 6));\n            T_transpose[idx] = A[src_idx];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* A, float* T_transpose) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4096; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 1024; ++i3_s) {\n      T_transpose[((i0_i1_fused_i2_fused * 1024) + i3_s)] = A[((((((i0_i1_fused_i2_fused / 20) * 20480) + (((i3_s & 63) * 320)) + (((i0_i1_fused_i2_fused % 20) * 16)) + (i3_s >> 6))];\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "transpose",
        "c_code": "void default_function_kernel(float* A, float* T_transpose) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 64; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n      for (int32_t ax3_outer = 0; ax3_outer < 4; ++ax3_outer) {\n        for (int32_t ax3_inner = 0; ax3_inner < 16; ++ax3_inner) {\n          T_transpose[((((ax0_ax1_fused * 640) + (ax2 * 64)) + (ax3_outer * 16)) + ax3_inner)] = A[((((((ax0_ax1_fused >> 5) * 20480) + (ax3_outer * 5120)) + (ax3_inner * 320)) + ((ax0_ax1_fused & 31) * 10)) + ax2)];\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ A, float* __restrict__ T_transpose) {\n  T_transpose[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = A[(((((((int)blockIdx.x) / 20) * 20480) + ((((int)threadIdx.x) & 63) * 320)) + ((((int)blockIdx.x) % 20) * 16)) + (((int)threadIdx.x) >> 6))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((2, 64, 32, 10), \"float32\"), T_transpose: T.Buffer((2, 32, 10, 64), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(64):\n            for ax2, ax3_outer, ax3_inner in T.grid(10, 4, 16):\n                T_transpose_1 = T.Buffer((40960,), data=T_transpose.data)\n                A_1 = T.Buffer((40960,), data=A.data)\n                T_transpose_1[ax0_ax1_fused * 640 + ax2 * 64 + ax3_outer * 16 + ax3_inner] = A_1[ax0_ax1_fused // 32 * 20480 + ax3_outer * 5120 + ax3_inner * 320 + ax0_ax1_fused % 32 * 10 + ax2]",
        "op_args": "None",
        "input_shape": "[[2, 64, 32, 10]]",
        "output_shape": "[[2, 32, 10, 64]]",
        "input_name": [
            "A"
        ],
        "output_name": [
            "T_transpose"
        ]
    },
    {
        "op_name": "resize2d",
        "c_code": "void default_function_kernel(float* A, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 64; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 50; ++i2) {\n      for (int32_t i3_outer = 0; i3_outer < 4; ++i3_outer) {\n        for (int32_t i3_inner = 0; i3_inner < 16; ++i3_inner) {\n          if (((i3_outer * 8) + (i3_inner >> 1)) < 25) {\n            float cse_var_2 = (((((float)i2) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f);\n            float cse_var_1 = (((((float)((i3_outer * 16) + i3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f);\n            resize[((((i0_i1_fused * 2500) + (i2 * 50)) + (i3_outer * 16)) + i3_inner)] = ((((A[(((i0_i1_fused * 1024) + (max(min(((int32_t)floorf(cse_var_2)), 31), 0) * 32)) + max(min(((int32_t)floorf(cse_var_1)), 31), 0))] * (1.000000e+00f - (cse_var_1 - ((float)((int32_t)floorf(cse_var_1)))))) + (A[(((i0_i1_fused * 1024) + (max(min(((int32_t)floorf(cse_var_2)), 31), 0) * 32)) + max(min((((int32_t)floorf(cse_var_1)) + 1), 31), 0))] * (cse_var_1 - ((float)((int32_t)floorf(cse_var_1)))))) * (1.000000e+00f - (cse_var_2 - ((float)((int32_t)floorf(cse_var_2)))))) + (((A[(((i0_i1_fused * 1024) + (max(min((((int32_t)floorf(cse_var_2)) + 1), 31), 0) * 32)) + max(min(((int32_t)floorf(cse_var_1)), 31), 0))] * (1.000000e+00f - (cse_var_1 - ((float)((int32_t)floorf(cse_var_1)))))) + (A[(((i0_i1_fused * 1024) + (max(min((((int32_t)floorf(cse_var_2)) + 1), 31), 0) * 32)) + max(min((((int32_t)floorf(cse_var_1)) + 1), 31), 0))] * (cse_var_1 - ((float)((int32_t)floorf(cse_var_1)))))) * (cse_var_2 - ((float)((int32_t)floorf(cse_var_2))))));\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ A, float* __restrict__ resize) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 8)) < 625) {\n    resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((((A[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 625) * 1024) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (A[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 625) * 1024) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (((A[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 625) * 1024) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (A[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 625) * 1024) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) * ((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((4, 16, 32, 32), \"float32\"), resize: T.Buffer((4, 16, 50, 50), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(64):\n            for i2, i3_outer, i3_inner in T.grid(50, 4, 16):\n                if T.likely(i3_outer * 8 + i3_inner // 2 < 25):\n                    cse_var_4: T.int32 = i3_outer * 16\n                    cse_var_3: T.int32 = i0_i1_fused * 1024\n                    cse_var_2: T.float32 = (T.Cast(\"float32\", i2) + T.float32(0.5)) * T.float32(0.63999998569488525) - T.float32(0.5)\n                    cse_var_1: T.float32 = (T.Cast(\"float32\", cse_var_4 + i3_inner) + T.float32(0.5)) * T.float32(0.63999998569488525) - T.float32(0.5)\n                    resize_1 = T.Buffer((160000,), data=resize.data)\n                    A_1 = T.Buffer((65536,), data=A.data)\n                    resize_1[i0_i1_fused * 2500 + i2 * 50 + cse_var_4 + i3_inner] = (A_1[cse_var_3 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)), 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)), 31), 0)] * (T.float32(1) - (cse_var_1 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_1))))) + A_1[cse_var_3 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)), 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)) + 1, 31), 0)] * (cse_var_1 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_1))))) * (T.float32(1) - (cse_var_2 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_2))))) + (A_1[cse_var_3 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)) + 1, 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)), 31), 0)] * (T.float32(1) - (cse_var_1 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_1))))) + A_1[cse_var_3 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)) + 1, 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)) + 1, 31), 0)] * (cse_var_1 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_1))))) * (cse_var_2 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_2))))",
        "op_args": "None",
        "input_shape": "[[4, 16, 32, 32]]",
        "output_shape": "[[4, 16, 50, 50]]",
        "input_name": [
            "A"
        ],
        "output_name": [
            "resize"
        ],
        "deepseek_c": "void default_function_kernel(float* __restrict__ A, float* __restrict__ resize) {\n  for (int32_t ax0_ax1_fused_ax2_fused_outer = 0; ax0_ax1_fused_ax2_fused_outer < 1024; ++ax0_ax1_fused_ax2_fused_outer) {\n    for (int32_t ax3_inner = 0; ax3_inner < 32; ++ax3_inner) {\n      resize[((((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner) * 1024) + (((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) * 32) + max(min(((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) + 1), 31), 0)))] = ((((A[((((((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner) / 625) * 1024) + (((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) * 32) + max(min(((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))), 31), 0)))] * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))))) + (A[((((((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner) / 625) * 1024) + (((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) * 32) + max(min(((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) + 1), 31), 0)))] * (((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))))) * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))))))) + (((A[((((((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner) / 625) * 1024) + (((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) * 32) + max(min(((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) + 1), 31), 0)))] * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))))) + (A[((((((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner) / 625) * 1024) + (((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) * 32) + max(min(((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))) + 1), 31), 0)))] * (((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))))) * (((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))))))) * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int32_t)(floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer * 32) + ax3_inner)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))))))));\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* A, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 204800; ++i0_i1_fused_i2_fused_i3_fused) {\n    resize[i0_i1_fused_i2_fused_i3_fused] = ((((A[((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (max(min(((i0_i1_fused_i2_fused_i3_fused % 1250) / 25), 31), 0) * 32)) + max(min(i0_i1_fused_i2_fused_i3_fused % 50, 31), 0)] * (1.000000f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (A[((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (max(min(((i0_i1_fused_i2_fused_i3_fused % 1250) / 25), 31), 0) * 32) + max(min(((int)floorf((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f), 31), 0)] * ((((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) * (1.000000f - ((((((float)(i0_i1_fused_i2_fused_i3_fused % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(i0_i1_fused_i2_fused_i3_fused % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (((A[((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (max(min(((i0_i1_fused_i2_fused_i3_fused % 1250) / 25) + 1, 31), 0) * 32) + max(min(((int)floorf((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f), 31), 0)] * (1.000000f - ((((((float)(i0_i1_fused_i2_fused_i3_fused % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(i0_i1_fused_i2_fused_i3_fused % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (A[((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (max(min(((i0_i1_fused_i2_fused_i3_fused % 1250) / 25) + 1, 31), 0) * 32) + max(min(((int)floorf((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) + 1, 31), 0)] * ((((((float)(i0_i1_fused_i2_fused_i3_fused % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(i0_i1_fused_i2_fused_i3_fused % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) * ((((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))));\n  }\n}",
        "codellama_c": "void default_function_kernel(float* A, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 262144; ++i0_i1_fused_i2_fused_i3_fused) {\n    resize[i0_i1_fused_i2_fused_i3_fused] = ((((A[((((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) * 1024) + ((((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) % 16) * 32))] * (1.000000e+00f - ((((((float)((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) / 16)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) / 16)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (A[(((((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) * 1024) + (((((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) % 16) * 32)) + 16))] * (1.000000e+00f - ((((((float)((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) / 16)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) / 16)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (A[(((((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) * 1024) + (((((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) % 16) * 32)) + 16))] * (1.000000e+00f - ((((((float)((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) / 16)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) / 16)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) * ((((((float)((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) / 16)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused / 1024) * 1024) + (((i0_i1_fused_i2_fused_i3_fused % 1024) / 32) / 16)) / 16)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))));\n  }\n}",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* A, float* resize) {\n  // Calculate the number of threads per block\n  const int threadsPerBlock = 1024;\n\n  // Calculate the number of blocks needed based on the input tensor shape\n  const int numBlocks = 4 * 16 * 32 * 32 / threadsPerBlock;\n\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int index = (blockIdx * threadsPerBlock) + threadIdx;\n\n      // Check the conditional statement\n      if (((blockIdx * 4) + (threadIdx >> 8)) < 625) {\n        // Calculate the indices for accessing A\n        int A_index_1 = (((blockIdx * 256) + (threadIdx >> 2)) / 625) * 1024 + (max(min((int)floorf(((((float)((blockIdx * 512) + (threadIdx >> 1)) % 1250) / 25) + 0.5f) * 0.64f - 0.5f)), 31), 0) * 32) + max(min((int)floorf(((((float)((blockIdx * 24) + threadIdx) % 50) + 0.5f) * 0.64f - 0.5f)), 31), 0);\n        int A_index_2 = (((blockIdx * 256) + (threadIdx >> 2)) / 625) * 1024 + (max(min((int)floorf(((((float)((blockIdx * 512) + (threadIdx >> 1)) % 1250) / 25) + 0.5f) * 0.64f - 0.5f)), 31), 0) * 32) + max(min((int)floorf(((((float)((blockIdx * 24) + threadIdx) % 50) + 0.5f) * 0.64f - 0.5f)) + 1), 31), 0);\n\n        // Calculate the interpolation weights\n        float weight_1 = 1.0f - (((((float)((blockIdx * 24) + threadIdx) % 50) + 0.5f) * 0.64f - 0.5f) - (float)((int)floorf(((((float)((blockIdx * 24) + threadIdx) % 50) + 0.5f) * 0.64f - 0.5f))));\n        float weight_2 = (((((float)((blockIdx * 24) + threadIdx) % 50) + 0.5f) * 0.64f - 0.5f) - (float)((int)floorf(((((float)((blockIdx * 24) + threadIdx) % 50) + 0.5f) * 0.64f - 0.5f))));\n        float weight_3 = 1.0f - (((((float)((blockIdx * 512) + (threadIdx >> 1)) % 1250) / 25) + 0.5f) * 0.64f - 0.5f) - (float)((int)floorf(((((float)((blockIdx * 512) + (threadIdx >> 1)) % 1250) / 25) + 0.5f) * 0.64f - 0.5f))));\n        float weight_4 = (((((float)((blockIdx * 512) + (threadIdx >> 1)) % 1250) / 25) + 0.5f) * 0.64f - 0.5f) - (float)((int)floorf(((((float)((blockIdx * 512) + (threadIdx >> 1)) % 1250) / 25) + 0.5f) * 0.64f - 0.5f))));\n\n        // Perform the bilinear interpolation\n        resize[index] = A[A_index_1] * weight_1 * weight_3 + A[A_index_2] * weight_2 * weight_3 + A[A_index_1 + 1] * weight_1 * weight_4 + A[A_index_2 + 1] * weight_2 * weight_4;\n      }\n    }\n  }\n}\n//|End-of-Code|",
        "gemini2_c": "void default_function_kernel(float* A, float* resize, int blockIdx_x, int threadIdx_x) {\n  if (((blockIdx_x * 4) + (threadIdx_x >> 8)) < 625) {\n    int idx_x = (blockIdx_x * 512 + (threadIdx_x >> 1)) % 1250;\n    int idx_y = (blockIdx_x * 24 + threadIdx_x) % 50;\n    int x_coord = (int)floorf(((float)(idx_x / 25) + 0.5f) * 0.64f - 0.5f);\n    int y_coord = (int)floorf(((float)(idx_y) + 0.5f) * 0.64f - 0.5f);\n    x_coord = max(min(x_coord, 31), 0);\n    y_coord = max(min(y_coord, 31), 0);\n    int A_idx = (((blockIdx_x * 256 + (threadIdx_x >> 2)) / 625) * 1024) + (x_coord * 32) + y_coord;\n    resize[(blockIdx_x * 1024) + threadIdx_x] = A[A_idx] * (1.0f - ((float)(idx_y) + 0.5f) * 0.64f - 0.5f - (float)y_coord) + \n                                                    A[A_idx + 32] * (((float)(idx_y) + 0.5f) * 0.64f - 0.5f - (float)y_coord) * (1.0f - ((float)(idx_x / 25) + 0.5f) * 0.64f - 0.5f - (float)x_coord) + \n                                                    A[A_idx + 33] * (((float)(idx_y) + 0.5f) * 0.64f - 0.5f - (float)y_coord) * ((float)(idx_x / 25) + 0.5f) * 0.64f - 0.5f - (float)x_coord) + \n                                                    A[A_idx + 1] * (1.0f - ((float)(idx_y) + 0.5f) * 0.64f - 0.5f - (float)y_coord) * ((float)(idx_x / 25) + 0.5f) * 0.64f - 0.5f - (float)x_coord);\n  }\n}\n",
        "gpt1_c": "#include <math.h>\n#include <omp.h>\n#include <stdio.h>\n#include <stdlib.h>\n\nvoid default_function_kernel(float* A, float* resize, int dim1, int dim2, int dim3, int dim4) {\n    int total_threads = dim1 * dim2 * dim3 * dim4;\n    int block_size = 1024;\n    int num_blocks = (total_threads + block_size - 1) / block_size;\n\n    #pragma omp parallel for\n    for (int block_idx = 0; block_idx < num_blocks; ++block_idx) {\n        for (int thread_idx = 0; thread_idx < block_size; ++thread_idx) {\n            int idx = block_idx * block_size + thread_idx;\n            if (idx >= total_threads) break;\n\n            int blockDim = 1024;\n            int gridDim = (total_threads + blockDim - 1) / blockDim;\n\n            if (((block_idx * 4) + (thread_idx >> 8)) < 625) {\n                int base_idx = ((block_idx * 256) + (thread_idx >> 2)) / 625;\n                int A_idx1 = base_idx * 1024 + (max(min((int)floor((((((float)((block_idx * 512) + (thread_idx >> 1)) % 1250) / 25) + 0.5f) * 0.64f) - 0.5f), 31), 0) * 32) + max(min((int)floor((((((float)(((block_idx * 24) + thread_idx) % 50)) + 0.5f) * 0.64f) - 0.5f), 31), 0);\n\n                int A_idx2 = base_idx * 1024 + (max(min((int)floor((((((float)((block_idx * 512) + (thread_idx >> 1)) % 1250) / 25) + 0.5f) * 0.64f) - 0.5f), 31), 0) * 32) + max(min((int)floor((((((float)(((block_idx * 24) + thread_idx) % 50)) + 0.5f) * 0.64f) - 0.5f) + 1), 31), 0);\n\n                int A_idx3 = base_idx * 1024 + (max(min((int)floor((((((float)((block_idx * 512) + (thread_idx >> 1)) % 1250) / 25) + 0.5f) * 0.64f) - 0.5f) + 1), 31), 0) * 32) + max(min((int)floor((((((float)(((block_idx * 24) + thread_idx) % 50)) + 0.5f) * 0.64f) - 0.5f), 31), 0);\n\n                int A_idx4 = base_idx * 1024 + (max(min((int)floor((((((float)((block_idx * 512) + (thread_idx >> 1)) % 1250) / 25) + 0.5f) * 0.64f) - 0.5f) + 1), 31), 0) * 32) + max(min((int)floor((((((float)(((block_idx * 24) + thread_idx) % 50)) + 0.5f) * 0.64f) - 0.5f) + 1), 31), 0);\n\n                float w1 = 1.0f - (((((float)(((block_idx * 24) + thread_idx) % 50)) + 0.5f) * 0.64f) - 0.5f - (float)((int)floor(((((float)(((block_idx * 24) + thread_idx) % 50)) + 0.5f) * 0.64f) - 0.5f)));\n                float w2 = (((((float)(((block_idx * 24) + thread_idx) % 50)) + 0.5f) * 0.64f) - 0.5f - (float)((int)floor(((((float)(((block_idx * 24) + thread_idx) % 50)) + 0.5f) * 0.64f) - 0.5f)));\n\n                float h1 = 1.0f - (((((float)(((block_idx * 512) + (thread_idx >> 1)) % 1250) / 25) + 0.5f) * 0.64f) - 0.5f - (float)((int)floor(((((float)(((block_idx * 512) + (thread_idx >> 1)) % 1250) / 25) + 0.5f) * 0.64f) - 0.5f)));\n                float h2 = (((((float)(((block_idx * 512) + (thread_idx >> 1)) % 1250) / 25) + 0.5f) * 0.64f) - 0.5f - (float)((int)floor(((((float)(((block_idx * 512) + (thread_idx >> 1)) % 1250) / 25) + 0.5f) * 0.64f) - 0.5f)));\n\n                resize[(block_idx * 1024) + thread_idx] = (A[A_idx1] * h1 * w1) + (A[A_idx2] * h1 * w2) + (A[A_idx3] * h2 * w1) + (A[A_idx4] * h2 * w2);\n            }\n        }\n    }\n}\n\nint main() {\n    int dim1 = 4, dim2 = 16, dim3 = 32, dim4 = 32;\n    int size = dim1 * dim2 * dim3 * dim4;\n\n    float* A = (float*)malloc(size * sizeof(float));\n    float* resize = (float*)malloc(size * sizeof(float));\n\n    // Initialize A with some values (for example purposes)\n    for (int i = 0; i < size; ++i) {\n        A[i] = (float)i / size;\n    }\n\n    // Call the kernel function\n    default_function_kernel(A, resize, dim1, dim2, dim3, dim4);\n\n    // Print some results (for verification purposes)\n    for (int i = 0; i < 10; ++i) {\n        printf(\"resize[%d] = %f\\n\", i, resize[i]);\n    }\n\n    // Free the allocated memory\n    free(A);\n    free(resize);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <math.h>\n#include <stdio.h>\n#include <stdlib.h>\n\nvoid default_function_kernel(float* A, float* resize, int blocks, int threads) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads; ++threadIdx_x) {\n            int idx = blockIdx_x * threads + threadIdx_x;\n            if ((blockIdx_x * 4 + (threadIdx_x >> 8)) < 625) {\n                int A_idx = ((blockIdx_x * 256 + (threadIdx_x >> 2)) / 625) * 1024 +\n                            (max(min((int)floorf((((float)((blockIdx_x * 512 + (threadIdx_x >> 1)) % 1250) / 25) + 0.5f) * 0.64f - 0.5f), 31), 0) * 32) +\n                            max(min((int)floorf((((float)((blockIdx_x * 24 + threadIdx_x) % 50) + 0.5f) * 0.64f - 0.5f), 31), 0);\n\n                float factor1 = (1.0f - (((((float)((blockIdx_x * 24 + threadIdx_x) % 50) + 0.5f) * 0.64f - 0.5f) -\n                                        (float)((int)floorf((((float)((blockIdx_x * 24 + threadIdx_x) % 50) + 0.5f) * 0.64f - 0.5f))))));\n\n                float factor2 = (1.0f - (((((float)((blockIdx_x * 512 + (threadIdx_x >> 1)) % 1250) / 25) + 0.5f) * 0.64f - 0.5f) -\n                                        (float)((int)floorf((((float)((blockIdx_x * 512 + (threadIdx_x >> 1)) % 1250) / 25) + 0.5f) * 0.64f - 0.5f))));\n\n                resize[idx] = A[A_idx] * factor1 +\n                              A[A_idx + 1] * ((factor1 - (float)((int)floorf(factor1))) * (1.0f - factor2)) +\n                              A[A_idx + 32] * (factor2 * (1.0f - factor1)) +\n                              A[A_idx + 33] * (factor1 * factor2);\n            }\n        }\n    }\n}\n\nint main() {\n    int blocks = 4 * 16 * 32 * 32;  // As per the tensor shape provided\n    int threads_per_block = 1024;   // Launch bounds\n\n    int total_elements = blocks * threads_per_block;\n    float *A = (float *)malloc(total_elements * sizeof(float));\n    float *resize = (float *)malloc(total_elements * sizeof(float));\n\n    // Initialize A (example initialization)\n    for (int i = 0; i < total_elements; ++i) {\n        A[i] = (float)i * 0.01f;\n    }\n\n    // Call the function\n    default_function_kernel(A, resize, blocks, threads_per_block);\n\n    // Print the result\n    for (int i = 0; i < total_elements; ++i) {\n        printf(\"%f \", resize[i]);\n    }\n    printf(\"\\n\");\n\n    free(A);\n    free(resize);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* A, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 625; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 1024; ++i3_s) {\n      if (((i0_i1_fused_i2_fused * 4) + (i3_s >> 8)) < 625) {\n        resize[((i0_i1_fused_i2_fused * 1024) + i3_s)] = ((((A[((((i0_i1_fused_i2_fused * 256) + (i3_s >> 2)) / 625) * 1024) + (max(min(((int)floorf((((((float)((((i0_i1_fused_i2_fused * 512) + (i3_s >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))), 31), 0) * 32)) + max(min(((int)floorf((((((float)((((i0_i1_fused_i2_fused * 24) + i3_s) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)((((i0_i1_fused_i2_fused * 24) + i3_s) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused * 24) + i3_s) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (A[((((i0_i1_fused_i2_fused * 256) + (i3_s >> 2)) / 625) * 1024) + (max(min(((int)floorf((((((float)((((i0_i1_fused_i2_fused * 512) + (i3_s >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))), 31), 0) * 32)) + max(min(((int)floorf((((((float)((((i0_i1_fused_i2_fused * 24) + i3_s) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)((((i0_i1_fused_i2_fused * 24) + i3_s) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused * 24) + i3_s) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((((i0_i1_fused_i2_fused * 512) + (i3_s >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused * 512) + (i3_s >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (((A[((((i0_i1_fused_i2_fused * 256) + (i3_s >> 2)) / 625) * 1024) + (max(min(((int)floorf((((((float)((((i0_i1_fused_i2_fused * 512) + (i3_s >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)((((i0_i1_fused_i2_fused * 24) + i3_s) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)((((i0_i1_fused_i2_fused * 24) + i3_s) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused * 24) + i3_s) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) + (A[((((i0_i1_fused_i2_fused * 256) + (i3_s >> 2)) / 625) * 1024) + (max(min(((int)floorf((((((float)((((i0_i1_fused_i2_fused * 512) + (i3_s >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)((((i0_i1_fused_i2_fused * 24) + i3_s) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)((((i0_i1_fused_i2_fused * 24) + i3_s) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused * 24) + i3_s) % 50)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f))))))) * ((((((float)((((i0_i1_fused_i2_fused * 512) + (i3_s >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused * 512) + (i3_s >> 1)) % 1250) / 25)) + 5.000000e-01f) * 6.400000e-01f) - 5.000000e-01f)))))));\n      }\n    }\n  }\n}\n"
    },
    {
        "op_name": "resize3d",
        "c_code": "void default_function_kernel(float* A, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 10; ++i3) {\n      for (int32_t i4_inner = 0; i4_inner < 10; ++i4_inner) {\n        float cse_var_3 = (((((float)i4_inner) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f);\n        float cse_var_2 = (((((float)i3) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f);\n        float cse_var_1 = (((((float)(i0_i1_fused_i2_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f);\n        resize[(((i0_i1_fused_i2_fused * 100) + (i3 * 10)) + i4_inner)] = ((((((A[(((((i0_i1_fused_i2_fused / 10) * 32768) + (max(min(((int32_t)floorf(cse_var_1)), 31), 0) * 1024)) + (max(min(((int32_t)floorf(cse_var_2)), 31), 0) * 32)) + max(min(((int32_t)floorf(cse_var_3)), 31), 0))] * (1.000000e+00f - (cse_var_3 - ((float)((int32_t)floorf(cse_var_3)))))) + (A[(((((i0_i1_fused_i2_fused / 10) * 32768) + (max(min(((int32_t)floorf(cse_var_1)), 31), 0) * 1024)) + (max(min(((int32_t)floorf(cse_var_2)), 31), 0) * 32)) + max(min((((int32_t)floorf(cse_var_3)) + 1), 31), 0))] * (cse_var_3 - ((float)((int32_t)floorf(cse_var_3)))))) * (1.000000e+00f - (cse_var_2 - ((float)((int32_t)floorf(cse_var_2)))))) + (((A[(((((i0_i1_fused_i2_fused / 10) * 32768) + (max(min(((int32_t)floorf(cse_var_1)), 31), 0) * 1024)) + (max(min((((int32_t)floorf(cse_var_2)) + 1), 31), 0) * 32)) + max(min(((int32_t)floorf(cse_var_3)), 31), 0))] * (1.000000e+00f - (cse_var_3 - ((float)((int32_t)floorf(cse_var_3)))))) + (A[(((((i0_i1_fused_i2_fused / 10) * 32768) + (max(min(((int32_t)floorf(cse_var_1)), 31), 0) * 1024)) + (max(min((((int32_t)floorf(cse_var_2)) + 1), 31), 0) * 32)) + max(min((((int32_t)floorf(cse_var_3)) + 1), 31), 0))] * (cse_var_3 - ((float)((int32_t)floorf(cse_var_3)))))) * (cse_var_2 - ((float)((int32_t)floorf(cse_var_2)))))) * (1.000000e+00f - (cse_var_1 - ((float)((int32_t)floorf(cse_var_1)))))) + (((((A[(((((i0_i1_fused_i2_fused / 10) * 32768) + (max(min((((int32_t)floorf(cse_var_1)) + 1), 31), 0) * 1024)) + (max(min(((int32_t)floorf(cse_var_2)), 31), 0) * 32)) + max(min(((int32_t)floorf(cse_var_3)), 31), 0))] * (1.000000e+00f - (cse_var_3 - ((float)((int32_t)floorf(cse_var_3)))))) + (A[(((((i0_i1_fused_i2_fused / 10) * 32768) + (max(min((((int32_t)floorf(cse_var_1)) + 1), 31), 0) * 1024)) + (max(min(((int32_t)floorf(cse_var_2)), 31), 0) * 32)) + max(min((((int32_t)floorf(cse_var_3)) + 1), 31), 0))] * (cse_var_3 - ((float)((int32_t)floorf(cse_var_3)))))) * (1.000000e+00f - (cse_var_2 - ((float)((int32_t)floorf(cse_var_2)))))) + (((A[(((((i0_i1_fused_i2_fused / 10) * 32768) + (max(min((((int32_t)floorf(cse_var_1)) + 1), 31), 0) * 1024)) + (max(min((((int32_t)floorf(cse_var_2)) + 1), 31), 0) * 32)) + max(min(((int32_t)floorf(cse_var_3)), 31), 0))] * (1.000000e+00f - (cse_var_3 - ((float)((int32_t)floorf(cse_var_3)))))) + (A[(((((i0_i1_fused_i2_fused / 10) * 32768) + (max(min((((int32_t)floorf(cse_var_1)) + 1), 31), 0) * 1024)) + (max(min((((int32_t)floorf(cse_var_2)) + 1), 31), 0) * 32)) + max(min((((int32_t)floorf(cse_var_3)) + 1), 31), 0))] * (cse_var_3 - ((float)((int32_t)floorf(cse_var_3)))))) * (cse_var_2 - ((float)((int32_t)floorf(cse_var_2)))))) * (cse_var_1 - ((float)((int32_t)floorf(cse_var_1))))));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ A, float* __restrict__ resize) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 375) {\n    resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((((((A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * ((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((((A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 1024)) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 1024)) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * ((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * ((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f)))))));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((3, 16, 32, 32, 32), \"float32\"), resize: T.Buffer((3, 16, 10, 10, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            for i3, i4_inner in T.grid(10, 10):\n                cse_var_4: T.int32 = i0_i1_fused_i2_fused // 10 * 32768\n                cse_var_3: T.float32 = (T.Cast(\"float32\", i4_inner) + T.float32(0.5)) * T.float32(3.2000000476837158) - T.float32(0.5)\n                cse_var_2: T.float32 = (T.Cast(\"float32\", i3) + T.float32(0.5)) * T.float32(3.2000000476837158) - T.float32(0.5)\n                cse_var_1: T.float32 = (T.Cast(\"float32\", i0_i1_fused_i2_fused % 10) + T.float32(0.5)) * T.float32(3.2000000476837158) - T.float32(0.5)\n                resize_1 = T.Buffer((48000,), data=resize.data)\n                A_1 = T.Buffer((1572864,), data=A.data)\n                resize_1[i0_i1_fused_i2_fused * 100 + i3 * 10 + i4_inner] = ((A_1[cse_var_4 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)), 31), 0) * 1024 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)), 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_3)), 31), 0)] * (T.float32(1) - (cse_var_3 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_3))))) + A_1[cse_var_4 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)), 31), 0) * 1024 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)), 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_3)) + 1, 31), 0)] * (cse_var_3 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_3))))) * (T.float32(1) - (cse_var_2 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_2))))) + (A_1[cse_var_4 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)), 31), 0) * 1024 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)) + 1, 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_3)), 31), 0)] * (T.float32(1) - (cse_var_3 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_3))))) + A_1[cse_var_4 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)), 31), 0) * 1024 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)) + 1, 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_3)) + 1, 31), 0)] * (cse_var_3 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_3))))) * (cse_var_2 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_2))))) * (T.float32(1) - (cse_var_1 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_1))))) + ((A_1[cse_var_4 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)) + 1, 31), 0) * 1024 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)), 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_3)), 31), 0)] * (T.float32(1) - (cse_var_3 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_3))))) + A_1[cse_var_4 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)) + 1, 31), 0) * 1024 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)), 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_3)) + 1, 31), 0)] * (cse_var_3 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_3))))) * (T.float32(1) - (cse_var_2 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_2))))) + (A_1[cse_var_4 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)) + 1, 31), 0) * 1024 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)) + 1, 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_3)), 31), 0)] * (T.float32(1) - (cse_var_3 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_3))))) + A_1[cse_var_4 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_1)) + 1, 31), 0) * 1024 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_2)) + 1, 31), 0) * 32 + T.max(T.min(T.Cast(\"int32\", T.floor(cse_var_3)) + 1, 31), 0)] * (cse_var_3 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_3))))) * (cse_var_2 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_2))))) * (cse_var_1 - T.Cast(\"float32\", T.Cast(\"int32\", T.floor(cse_var_1))))",
        "op_args": "None",
        "input_shape": "[[3, 16, 32, 32, 32]]",
        "output_shape": "[[3, 16, 10, 10, 10]]",
        "input_name": [
            "A"
        ],
        "output_name": [
            "resize"
        ],
        "deepseek_c": "void default_function_kernel(float* A, float* resize) {\n  for (int32_t ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 1536; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    for (int32_t ax3_inner = 0; ax3_inner < 32; ++ax3_inner) {\n      for (int32_t ax4_inner = 0; ax4_inner < 32; ++ax4_inner) {\n        resize[((((ax0_ax1_fused_ax2_fused_outer_outer * 1024) + (ax3_inner * 32)) + ax4_inner) * 32) + 0)] = ((((((A[((((((ax0_ax1_fused_ax2_fused_outer_outer / 128) * 32768) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((ax0_ax1_fused_ax2_fused_outer_outer / 128) * 32768) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[((((((ax0_ax1_fused_ax2_fused_outer_outer / 128) * 32768) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((ax0_ax1_fused_ax2_fused_outer_outer / 128) * 32768) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((((A[((((((ax0_ax1_fused_ax2_fused_outer_outer / 128) * 32768) + (max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((ax0_ax1_fused_ax2_fused_outer_outer / 128) * 32768) + (max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((((A[((((((ax0_ax1_fused_ax2_fused_outer_outer / 128) * 32768) + (max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((ax0_ax1_fused_ax2_fused_outer_outer / 128) * 32768) + (max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - (((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((ax0_ax1_fused_ax2_fused_outer_outer % 128)) / 32)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f)))))));\n      }\n    }\n  }\n}",
        "starcoder_c": "#include <math.h>\n#include <stdlib.h>\nvoid default_function_kernel(float* A, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 375; ++i0_i1_fused_i2_fused_i3_fused) {\n    for (int32_t i4 = 0; i4 < 32; ++i4) {\n      resize[((i0_i1_fused_i2_fused_i3_fused * 1024) + (i4 * 32))] = ((((((A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 10), 0.000000e+00f), 31), 0))] * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf((((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 0.000000e+00f), 31), 0))] * ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 10), 0.000000e+00f), 31), 0))] * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf((((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 0.000000e+00f), 31), 0))] * ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 10), 0.000000e+00f), 31), 0))] * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf((((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 0.000000e+00f), 31), 0))] * ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 10), 0.000000e+00f), 31), 0))] * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf((((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 0.000000e+00f), 31), 0))] * ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 10), 0.000000e+00f), 31), 0))] * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf((((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 0.000000e+00f), 31), 0))] * ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 10), 0.000000e+00f), 31), 0))] * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf((((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 0.000000e+00f), 31), 0))] * ((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((i0_i1_fused_i2_fused_i3_fused % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 125) / 25), 0.000000e+00f), 31) * 1024)) + (fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 50) / 5), 0.000000e+00f), 31) * 32)) + fminf(fmaxf(((i0_i1_fused_i2_fused_i3_fused % 10), 0.000000e+00f), 31), 0))] * (1.000000e+",
        "codellama_c": "#pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 128; ++i0_i1_fused_i2_fused_i3_fused) {\n    for (int32_t i4 = 0; i4 < 10; ++i4) {\n      for (int32_t i5 = 0; i5 < 32; ++i5) {\n        resize[(((i0_i1_fused_i2_fused_i3_fused * 320) + (i4 * 32)) + i5)] = ((((((A[((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0))] * (1.000000e+00f - ((((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)) - (((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024)) / 32))) + (A[(((((((((i0_i1_fused_i2_fused_i3_fused / 125) * 32768) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 1024)) + (max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024) + (((i4 * 32) + i5) / 32)), 31), 0) * 32)) + max(min(((((i0_i1_fused_i2_fused_i3_fused % 125) / 12) * 1024",
        "gemini1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\n#define max(a, b) ((a) > (b) ? (a) : (b))\n#define min(a, b) ((a) < (b) ? (a) : (b))\n\nvoid default_function_kernel(float* A, float* resize) {\n  #pragma omp parallel for\n  for (int i = 0; i < 375; i++) {\n    int tid = i * 8;\n    for (int j = 0; j < 8; j++) {\n      int idx = tid + j;\n      if (idx < 375) {\n        int x = (i * 6 + (j >> 2)) % 250;\n        int y = (i * 12 + (j >> 1)) % 50;\n        int z = (i * 4 + j) % 10;\n\n        float fx = (((float)(x / 25)) + 0.5f) * 3.2f - 0.5f;\n        float fy = (((float)(y / 5)) + 0.5f) * 3.2f - 0.5f;\n        float fz = (((float)(z / 10)) + 0.5f) * 3.2f - 0.5f;\n\n        int ix = max(min((int)floorf(fx), 31), 0);\n        int iy = max(min((int)floorf(fy), 31), 0);\n        int iz = max(min((int)floorf(fz), 31), 0);\n\n        int a_idx = (i * 128 + (j >> 3)) / 125 * 32768 + ix * 1024 + iy * 32 + iz;\n        int resize_idx = i * 1024 + j;\n\n        resize[resize_idx] = A[a_idx] * (1.0f - (fx - (float)ix)) + A[a_idx + 1] * (fx - (float)ix);\n        resize[resize_idx] *= (1.0f - (fy - (float)iy)) + A[a_idx + 32] * (fy - (float)iy);\n        resize[resize_idx] *= (1.0f - (fz - (float)iz)) + A[a_idx + 1024] * (fz - (float)iz);\n      }\n    }\n  }\n}\n",
        "gemini2_c": "void default_function_kernel(float* A, float* resize) {\n  for (int blockIdx_x = 0; blockIdx_x < 3; blockIdx_x++) {\n    for (int threadIdx_x = 0; threadIdx_x < 1024; threadIdx_x++) {\n      if ((((blockIdx_x * 8) + (threadIdx_x >> 7)) < 375)) {\n        int idx = (blockIdx_x * 1024) + threadIdx_x;\n        int A_idx = (((((blockIdx_x * 128) + (threadIdx_x >> 3)) / 125) * 32768) + (max(min((int)floorf((((((float)((((blockIdx_x * 6) + (threadIdx_x >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min((int)floorf((((((float)((((blockIdx_x * 12) + (threadIdx_x >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((int)floorf((((((float)((((blockIdx_x * 4) + (threadIdx_x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0));\n        resize[idx] = (((A[A_idx] * (1.000000e+00f - (((((float)((((blockIdx_x * 4) + (threadIdx_x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((blockIdx_x * 4) + (threadIdx_x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[A_idx + 1] * (((((float)((((blockIdx_x * 4) + (threadIdx_x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((blockIdx_x * 4) + (threadIdx_x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - (((((float)((((blockIdx_x * 12) + (threadIdx_x >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((blockIdx_x * 12) + (threadIdx_x >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[A_idx + 32] * (1.000000e+00f - (((((float)((((blockIdx_x * 4) + (threadIdx_x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((blockIdx_x * 4) + (threadIdx_x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[A_idx + 33] * (((((float)((((blockIdx_x * 4) + (threadIdx_x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((blockIdx_x * 4) + (threadIdx_x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (((((float)((((blockIdx_x * 12) + (threadIdx_x >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((blockIdx_x * 12) + (threadIdx_x >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - (((((float)((((blockIdx_x * 6) + (threadIdx_x >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((blockIdx_x * 6) + (threadIdx_x >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[A_idx + 1024] * (1.000000e+00f - (((((float)((((blockIdx_x * 4) + (threadIdx_x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((blockIdx_x * 4) + (threadIdx_x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[A_idx + 1025] * (((((float)((((blockIdx_x * 4) + (threadIdx_x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((blockIdx_x * 4) + (threadIdx_x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (((((float)((((blockIdx_x * 12) + (threadIdx_x >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((blockIdx_x * 12) + (threadIdx_x >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - (((((float)((((blockIdx_x * 6) + (threadIdx_x >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((blockIdx_x * 6) + (threadIdx_x >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[A_idx + 1056] * (1.000000e+00f - (((((float)((((blockIdx_x * 4) + (threadIdx_x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((blockIdx_x * 4) + (threadIdx_x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[A_idx + 1057] * (((((float)((((blockIdx_x * 4) + (threadIdx_x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((blockIdx_x * 4) + (threadIdx_x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (((((float)((((blockIdx_x * 12) + (threadIdx_x >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((blockIdx_x * 12) + (threadIdx_x >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (((((float)((((blockIdx_x * 6) + (threadIdx_x >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((blockIdx_x * 6) + (threadIdx_x >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f)))))));\n      }\n    }\n  }\n}\n",
        "gpt1_c": "\n    extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ A, float* __restrict__ resize) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7)) < 375) {\n    resize[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((((((A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * ((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((((A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 1024)) + (max(min(((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 1024)) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min(((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[(((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 125) * 32768) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 1024)) + (max(min((((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 32)) + max(min((((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * ((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * ((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f)))))));\n  }\n}\n\n\n    Translate the above CUDA code to CPU C code, the input tensor shape is [[3, 16, 32, 32, 32]] respectively. Print only single C code function implementation, end with comment \"|End-of-Code|\".\n    ",
        "gpt2_c": "#include <omp.h>\n#include <math.h>\n#include <stdio.h>\n#include <stdlib.h>\n\nvoid default_function_kernel(float* A, float* resize, int blocks, int threads) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads; ++threadIdx_x) {\n            if ((blockIdx_x * 8 + (threadIdx_x >> 7)) < 375) {\n                int idx = blockIdx_x * 1024 + threadIdx_x;\n\n                int A_idx1 = blockIdx_x * 128 + (threadIdx_x >> 3);\n                int A_idx2 = A_idx1 / 125 * 32768;\n\n                int offset1 = max(min((int)floorf((((float)((blockIdx_x * 6 + (threadIdx_x >> 2)) % 250) / 25) + 0.5f) * 3.2f - 0.5f), 31), 0) * 1024;\n                int offset2 = max(min((int)floorf((((float)((blockIdx_x * 12 + (threadIdx_x >> 1)) % 50) / 5) + 0.5f) * 3.2f - 0.5f), 31), 0) * 32;\n                int offset3 = max(min((int)floorf((((float)((blockIdx_x * 4 + threadIdx_x) % 10) + 0.5f) * 3.2f - 0.5f), 31), 0);\n\n                float val1 = A[A_idx2 + offset1 + offset2 + offset3] * \n                             (1.0f - (((float)((blockIdx_x * 4 + threadIdx_x) % 10) + 0.5f) * 3.2f - 0.5f - (float)((int)floorf((((float)((blockIdx_x * 4 + threadIdx_x) % 10) + 0.5f) * 3.2f - 0.5f)))));\n\n                float val2 = A[A_idx2 + offset1 + offset2 + max(min((int)floorf((((float)((blockIdx_x * 4 + threadIdx_x) % 10) + 0.5f) * 3.2f - 0.5f) + 1), 31), 0)] * \n                             ((((float)((blockIdx_x * 4 + threadIdx_x) % 10) + 0.5f) * 3.2f - 0.5f - (float)((int)floorf((((float)((blockIdx_x * 4 + threadIdx_x) % 10) + 0.5f) * 3.2f - 0.5f)))) * \n                             (1.0f - ((((float)((blockIdx_x * 12 + (threadIdx_x >> 1)) % 50) / 5) + 0.5f) * 3.2f - 0.5f - (float)((int)floorf((((float)((blockIdx_x * 12 + (threadIdx_x >> 1)) % 50) / 5) + 0.5f) * 3.2f - 0.5f)))));\n\n                float val3 = A[A_idx2 + offset1 + max(min((int)floorf((((float)((blockIdx_x * 12 + (threadIdx_x >> 1)) % 50) / 5) + 0.5f) * 3.2f - 0.5f) + 1), 31), 0) * 32 + offset3] * \n                             (1.0f - ((((float)((blockIdx_x * 4 + threadIdx_x) % 10) + 0.5f) * 3.2f - 0.5f - (float)((int)floorf((((float)((blockIdx_x * 4 + threadIdx_x) % 10) + 0.5f) * 3.2f - 0.5f))))) + \n                             (A[A_idx2 + offset1 + max(min((int)floorf((((float)((blockIdx_x * 6 + (threadIdx_x >> 2)) % 250) / 25) + 0.5f) * 3.2f - 0.5f) + 1), 31), 0) * 1024 + max(min((((int)floorf((((float)((blockIdx_x * 12 + (threadIdx_x >> 1)) % 50) / 5) + 0.5f) * 3.2f - 0.5f))) + 1), 31), 0) * 32 + offset3] * \n                             (((((float)((blockIdx_x * 4 + threadIdx_x) % 10) + 0.5f) * 3.2f - 0.5f) - (float)((int)floorf((((float)((blockIdx_x * 4 + threadIdx_x) % 10) + 0.5f) * 3.2f - 0.5f))) * \n                             (1.0f - ((((float)((blockIdx_x * 12 + (threadIdx_x >> 1)) % 50) / 5) + 0.5f) * 3.2f - 0.5f - (float)((int)floorf((((float)((blockIdx_x * 12 + (threadIdx_x >> 1)) % 50) / 5) + 0.5f) * 3.2f - 0.5f)))));\n\n                resize[idx] = val1 + val2 + val3;\n            }\n        }\n    }\n}\n\nint main() {\n    // Example usage\n    int blocks = 375;           // As per the tensor shape provided\n    int threads = 1024;         // Launch bounds\n\n    int total_elements = blocks * threads;\n    float *A = (float *)malloc(32 * 32 * 32 * sizeof(float)); // Assuming size based on tensor shape, adjust as necessary\n    float *resize = (float *)malloc(total_elements * sizeof(float));\n\n    // Initialize A (example initialization)\n    for (int i = 0; i < 32 * 32 * 32; ++i) {\n        A[i] = (float)i;\n    }\n\n    // Call the function\n    default_function_kernel(A, resize, blocks, threads);\n\n    // Print the result\n    for (int i = 0; i < total_elements; ++i) {\n        printf(\"%f \", resize[i]);\n    }\n    printf(\"\\n\");\n\n    free(A);\n    free(resize);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* A, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused_i4_fused = 0; i0_i1_fused_i2_fused_i3_fused_i4_fused < 1572864; ++i0_i1_fused_i2_fused_i3_fused_i4_fused) {\n    if (((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 8 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 128) < 375) {\n      resize[i0_i1_fused_i2_fused_i3_fused_i4_fused] = (((((A[(((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 128 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 8) / 125) * 32768 + (fmax(fmin((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 6 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 32) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (fmax(fmin((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 12 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 16) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + fmax(fmin((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 4 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 4 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 4 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[(((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 128 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 8) / 125) * 32768 + (fmax(fmin((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 6 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 32) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 1024)) + (fmax(fmin((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 12 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 16) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + fmax(fmin((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 4 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 4 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 4 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 12 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 16) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 12 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 16) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (((A[(((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 128 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 8) / 125) * 32768 + (fmax(fmin((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 6 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 32) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 1024)) + (fmax(fmin((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 12 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 16) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + fmax(fmin((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 4 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0))] * (1.000000e+00f - ((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 4 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 4 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) + (A[(((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 128 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 8) / 125) * 32768 + (fmax(fmin((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 6 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 32) % 250) / 25)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0) * 1024)) + (fmax(fmin((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 12 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 16) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))), 31), 0) * 32)) + fmax(fmin((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 4 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))) + 1), 31), 0))] * ((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 4 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 4 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024)) % 10)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * (1.000000e+00f - ((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 12 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 16) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f) - ((float)((int)floorf((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 12 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 16) % 50) / 5)) + 5.000000e-01f) * 3.200000e+00f) - 5.000000e-01f))))))) * ((((((float)((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 1024) * 6 + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 1024) / 32) % 250) /"
    },
    {
        "op_name": "rsqrt",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        for (int32_t i3 = 0; i3 < 20; ++i3) {\n          compute[((((i0 * 3520) + (i1 * 320)) + (i2 * 20)) + i3)] = (1.000000e+00f / sqrtf(data[((((i0 * 3520) + (i1 * 320)) + (i2 * 20)) + i3)]));\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 11, 16, 20), \"float32\"), compute: T.Buffer((15, 11, 16, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(15):\n            for i1, i2, i3 in T.grid(11, 16, 20):\n                cse_var_1: T.int32 = i0 * 3520 + i1 * 320 + i2 * 20 + i3\n                compute_1 = T.Buffer((52800,), data=compute.data)\n                data_1 = T.Buffer((52800,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])",
        "op_args": [
            15,
            11,
            16,
            20
        ],
        "input_shape": "[[15, 11, 16, 20]]",
        "output_shape": "[[15, 11, 16, 20]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        for (int32_t i3 = 0; i3 < 20; ++i3) {\n          compute[((((i0 * 640) + (i1 * 64)) + (i2 * 40)) + i3)] = (1.000000e+00f / sqrtf(data[((((i0 * 640) + (i1 * 64)) + (i2 * 40)) + i3)]));\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 4800; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = (1.000000e+00f / sqrtf(data[i0_i1_fused_i2_fused_i3_fused]));\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 4800; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = (1.000000e+00f / sqrtf(data[i0_i1_fused_i2_fused_i3_fused]));\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3520; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 20; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 20) + i3_s)] = (1.000000e+00f / sqrtf(data[((i0_i1_fused_i2_fused * 20) + i3_s)]));\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "sinh",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        for (int32_t i3 = 0; i3 < 17; ++i3) {\n          compute[((((i0 * 1666) + (i1 * 238)) + (i2 * 17)) + i3)] = sinhf(data[((((i0 * 1666) + (i1 * 238)) + (i2 * 17)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(34) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 7, 14, 17), \"float32\"), compute: T.Buffer((13, 7, 14, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(13):\n            for i1, i2, i3 in T.grid(7, 14, 17):\n                cse_var_1: T.int32 = i0 * 1666 + i1 * 238 + i2 * 17 + i3\n                compute_1 = T.Buffer((21658,), data=compute.data)\n                data_1 = T.Buffer((21658,), data=data.data)\n                compute_1[cse_var_1] = T.sinh(data_1[cse_var_1])",
        "op_args": [
            13,
            7,
            14,
            17
        ],
        "input_shape": "[[13, 7, 14, 17]]",
        "output_shape": "[[13, 7, 14, 17]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        for (int32_t i3 = 0; i3 < 17; ++i3) {\n          compute[((((i0 * 1666) + (i1 * 238)) + (i2 * 17)) + i3)] = sinhf(data[((((i0 * 1666) + (i1 * 238)) + (i2 * 17)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 15776; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sinhf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1484; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sinhf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3278; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 17; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 17) + i3_s)] = sinhf(data[((i0_i1_fused_i2_fused * 17) + i3_s)]);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "cosh",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 5814; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = coshf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) / 6)) < 969) {\n    compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 6, 17, 19), \"float32\"), compute: T.Buffer((3, 6, 17, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(5814):\n            compute_1 = T.Buffer((5814,), data=compute.data)\n            data_1 = T.Buffer((5814,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.cosh(data_1[i0_i1_fused_i2_fused_i3_fused])",
        "op_args": [
            3,
            6,
            17,
            19
        ],
        "input_shape": "[[3, 6, 17, 19]]",
        "output_shape": "[[3, 6, 17, 19]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        for (int32_t i3 = 0; i3 < 19; ++i3) {\n          compute[((((i0 * 2108) + (i1 * 351)) + (i2 * 19)) + i3)] = coshf(data[((((i0 * 2108) + (i1 * 351)) + (i2 * 19)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 18696; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = coshf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 6552; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = coshf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 969; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 19; ++i3_s) {\n      if (((i0_i1_fused_i2_fused * 19) + i3_s) < 18444) {\n        compute[((i0_i1_fused_i2_fused * 19) + i3_s)] = coshf(data[((i0_i1_fused_i2_fused * 19) + i3_s)]);\n      }\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "grid_sample",
        "c_code": "void default_function_kernel(float* compute, float* data, float* grid) {\n  #pragma omp parallel for\n  for (int32_t n_c_fused = 0; n_c_fused < 16; ++n_c_fused) {\n    for (int32_t h = 0; h < 16; ++h) {\n      for (int32_t w_inner = 0; w_inner < 16; ++w_inner) {\n        compute[(((n_c_fused * 256) + (h * 16)) + w_inner)] = ((((((((((0 <= ((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) ? data[(((n_c_fused * 64) + (((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))] : 0.000000e+00f) * (1.000000e+00f - ((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * (1.000000e+00f - ((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((0 <= ((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[((((n_c_fused * 64) + (((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 1)] : 0.000000e+00f) * (1.000000e+00f - ((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * ((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((-1 <= ((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) ? data[((((n_c_fused * 64) + (((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 8)] : 0.000000e+00f) * ((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))) * (1.000000e+00f - ((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))))) + (((((((-1 <= ((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[((((n_c_fused * 64) + (((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 9)] : 0.000000e+00f) * ((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[(((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))) * ((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((((n_c_fused >> 2) * 512) + (h * 16)) + w_inner)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ grid) {\n  compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((((((((((0 <= ((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) ? data[((((((int)blockIdx.x) * 256) + ((((int)threadIdx.x) >> 8) * 64)) + (((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))] : 0.000000e+00f) * (1.000000e+00f - ((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * (1.000000e+00f - ((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((0 <= ((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[(((((((int)blockIdx.x) * 256) + ((((int)threadIdx.x) >> 8) * 64)) + (((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 1)] : 0.000000e+00f) * (1.000000e+00f - ((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * ((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((-1 <= ((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) ? data[(((((((int)blockIdx.x) * 256) + ((((int)threadIdx.x) >> 8) * 64)) + (((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 8)] : 0.000000e+00f) * ((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))) * (1.000000e+00f - ((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))))) + (((((((-1 <= ((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[(((((((int)blockIdx.x) * 256) + ((((int)threadIdx.x) >> 8) * 64)) + (((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 9)] : 0.000000e+00f) * ((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))) * ((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((int)blockIdx.x) * 512) + (((int)threadIdx.x) & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 4, 8, 8), \"float32\"), grid: T.Buffer((4, 2, 16, 16), \"float32\"), compute: T.Buffer((4, 4, 16, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for n_c_fused in T.parallel(16):\n            for h, w_inner in T.grid(16, 16):\n                cse_var_4: T.int32 = h * 16\n                cse_var_3: T.int32 = n_c_fused * 64\n                cse_var_2: T.int32 = n_c_fused // 4 * 512 + cse_var_4 + w_inner\n                cse_var_1: T.int32 = cse_var_2 + 256\n                compute_1 = T.Buffer((4096,), data=compute.data)\n                grid_1 = T.Buffer((2048,), data=grid.data)\n                data_1 = T.Buffer((1024,), data=data.data)\n                compute_1[n_c_fused * 256 + cse_var_4 + w_inner] = T.if_then_else(0 <= T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) and 0 <= T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) and T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) < 8 and T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) < 8, data_1[cse_var_3 + T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) * 8 + T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5)))], T.float32(0)) * (T.float32(1) - ((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5) - T.Cast(\"float32\", T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5)))))) * (T.float32(1) - ((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5) - T.Cast(\"float32\", T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5)))))) + T.if_then_else(0 <= T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) and -1 <= T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) and T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) < 8 and T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) < 7, data_1[cse_var_3 + T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) * 8 + T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) + 1], T.float32(0)) * (T.float32(1) - ((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5) - T.Cast(\"float32\", T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5)))))) * ((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5) - T.Cast(\"float32\", T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))))) + T.if_then_else(-1 <= T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) and 0 <= T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) and T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) < 7 and T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) < 8, data_1[cse_var_3 + T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) * 8 + T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) + 8], T.float32(0)) * ((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5) - T.Cast(\"float32\", T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))))) * (T.float32(1) - ((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5) - T.Cast(\"float32\", T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5)))))) + T.if_then_else(-1 <= T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) and -1 <= T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) and T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) < 7 and T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) < 7, data_1[cse_var_3 + T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))) * 8 + T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5))) + 9], T.float32(0)) * ((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5) - T.Cast(\"float32\", T.Cast(\"int32\", T.floor((grid_1[cse_var_1] + T.float32(1)) * T.float32(7) * T.float32(0.5))))) * ((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5) - T.Cast(\"float32\", T.Cast(\"int32\", T.floor((grid_1[cse_var_2] + T.float32(1)) * T.float32(7) * T.float32(0.5)))))",
        "op_args": "None",
        "input_shape": "[[4, 4, 8, 8], [4, 2, 16, 16]]",
        "output_shape": "[[4, 4, 16, 16]]",
        "input_name": [
            "data",
            "grid"
        ],
        "output_name": [
            "compute"
        ],
        "deepseek_c": "#include <stdint.h>\n#include <stddef.h>\n#include <stdlib.h>\n#include <string.h>\n\nvoid default_function_kernel(float* compute, float* data, float* grid) {\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1024; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((((((((((0 <= ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) ? data[((((((i0_i1_fused_i2_fused_i3_fused >> 6) * 256) + (((i0_i1_fused_i2_fused_i3_fused & 63) >> 1) * 64)) + (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))] : 0.000000e+00f) * (1.000000e+00f - ((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * (1.000000e+00f - ((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))))) + (((((((0 <= ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[((((((i0_i1_fused_i2_fused_i3_fused >> 6) * 256) + (((i0_i1_fused_i2_fused_i3_fused & 63) >> 1) * 64)) + (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 1)] : 0.000000e+00f) * (1.000000e+00f - ((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * ((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((0 <= ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[((((((i0_i1_fused_i2_fused_i3_fused >> 6) * 256) + (((i0_i1_fused_i2_fused_i3_fused & 63) >> 1) * 64)) + (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 8)] : 0.000000e+00f) * ((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * (1.000000e+00f - ((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))))) + (((((((0 <= ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[((((((i0_i1_fused_i2_fused_i3_fused >> 6) * 256) + (((i0_i1_fused_i2_fused_i3_fused & 63) >> 1) * 64)) + (((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 9)] : 0.000000e+00f) * ((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6) * 128) + ((i0_i1_fused_i2_fused_i3_fused & 63) >> 1))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((((i0_i1_fused_i2_fused_i3_fused >> 6)",
        "starcoder_c": "void default_function_kernel(float* compute, float* data, float* grid) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2048; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((((((((((0 <= ((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8))? data[((((i0_i1_fused_i2_fused_i3_fused >> 8) * 64) + (((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))] : 0.000000e+00f) * (1.000000e+00f - ((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * (1.000000e+00f - ((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((0 <= ((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7))? data[(((((i0_i1_fused_i2_fused_i3_fused >> 8) * 64) + (((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 1)] : 0.000000e+00f) * (1.000000e+00f - ((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * ((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((-1 <= ((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8))? data[(((((i0_i1_fused_i2_fused_i3_fused >> 8) * 64) + (((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 8)] : 0.000000e+00f) * ((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))) * (1.000000e+00f - ((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))))) + (((((((-1 <= ((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7))? data[(((((i0_i1_fused_i2_fused_i3_fused >> 8) * 64) + (((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 9)] : 0.000000e+00f) * ((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[(i0_i1_fused_i2_fused_i3_fused + 2048)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))) * ((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[i0_i1_fused_i2_fused_i3_fused] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))));\n  }\n}",
        "codellama_c": "#pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 256; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((((((((((0 <= ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) ? data[((((((((((i0_i1_fused_i2_fused_i3_fused >> 4) * 128) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3) * 8)) + (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))] : 0.000000e+00f) * (1.000000e+00f - ((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * (1.000000e+00f - ((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((0 <= ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[((((((((((i0_i1_fused_i2_fused_i3_fused >> 4) * 128) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3) * 8)) + (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 1)] : 0.000000e+00f) * (1.000000e+00f - ((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * ((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((-1 <= ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) ? data[((((((((((i0_i1_fused_i2_fused_i3_fused >> 4) * 128) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3) * 8)) + (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 8)] : 0.000000e+00f) * ((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))) * (1.000000e+00f - ((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))))) + (((((((-1 <= ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[((((((((((i0_i1_fused_i2_fused_i3_fused >> 4) * 128) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3) * 8)) + (((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int32_t)floorf((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + 9)] : 0.000000e+00f) * ((((grid[((i0_i1_fused_i2_fused_i3_fused >> 4) * 16) + (((i0_i1_fused_i2_fused_i3_fused & 15) >> 3)) + 256)] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int32_t)floorf((((grid",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data, float* grid) {\n  // Calculate the number of threads per block (1024 in this case)\n  const int threadsPerBlock = 1024;\n\n  // Calculate the number of blocks needed based on the input tensor shape\n  const int numBlocks = 4 * 4 * 8 * 8 / threadsPerBlock; \n\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int index = (blockIdx * threadsPerBlock) + threadIdx;\n      \n      // Calculate the grid indices\n      int grid_index_1 = (blockIdx * 512) + (threadIdx & 255) + 256;\n      int grid_index_2 = (blockIdx * 512) + (threadIdx & 255);\n\n      // Calculate the data index\n      int data_index = (blockIdx * 256) + ((threadIdx >> 8) * 64) + \n                     (int)floorf((grid[grid_index_1] + 1.0f) * 7.0f * 0.5f) * 8 + \n                     (int)floorf((grid[grid_index_2] + 1.0f) * 7.0f * 0.5f);\n\n      // Calculate the interpolation factors\n      float factor_1 = (grid[grid_index_1] + 1.0f) * 7.0f * 0.5f - (float)(int)floorf((grid[grid_index_1] + 1.0f) * 7.0f * 0.5f);\n      float factor_2 = (grid[grid_index_2] + 1.0f) * 7.0f * 0.5f - (float)(int)floorf((grid[grid_index_2] + 1.0f) * 7.0f * 0.5f);\n\n      // Calculate compute value based on interpolation\n      compute[index] = 0.0f;\n      if (0 <= (int)floorf((grid[grid_index_1] + 1.0f) * 7.0f * 0.5f) && \n          (int)floorf((grid[grid_index_1] + 1.0f) * 7.0f * 0.5f) < 8 &&\n          0 <= (int)floorf((grid[grid_index_2] + 1.0f) * 7.0f * 0.5f) && \n          (int)floorf((grid[grid_index_2] + 1.0f) * 7.0f * 0.5f) < 8) {\n        compute[index] = data[data_index] * (1.0f - factor_1) * (1.0f - factor_2);\n      }\n\n      // Similar calculations for other interpolation cases\n      if (0 <= (int)floorf((grid[grid_index_1] + 1.0f) * 7.0f * 0.5f) && \n          (int)floorf((grid[grid_index_1] + 1.0f) * 7.0f * 0.5f) < 8 &&\n          -1 <= (int)floorf((grid[grid_index_2] + 1.0f) * 7.0f * 0.5f) && \n          (int)floorf((grid[grid_index_2] + 1.0f) * 7.0f * 0.5f) < 7) {\n        compute[index] += data[data_index + 1] * (1.0f - factor_1) * factor_2;\n      }\n\n      if (-1 <= (int)floorf((grid[grid_index_1] + 1.0f) * 7.0f * 0.5f) && \n          (int)floorf((grid[grid_index_1] + 1.0f) * 7.0f * 0.5f) < 7 &&\n          0 <= (int)floorf((grid[grid_index_2] + 1.0f) * 7.0f * 0.5f) && \n          (int)floorf((grid[grid_index_2] + 1.0f) * 7.0f * 0.5f) < 8) {\n        compute[index] += data[data_index + 8] * factor_1 * (1.0f - factor_2);\n      }\n\n      if (-1 <= (int)floorf((grid[grid_index_1] + 1.0f) * 7.0f * 0.5f) && \n          (int)floorf((grid[grid_index_1] + 1.0f) * 7.0f * 0.5f) < 7 &&\n          -1 <= (int)floorf((grid[grid_index_2] + 1.0f) * 7.0f * 0.5f) && \n          (int)floorf((grid[grid_index_2] + 1.0f) * 7.0f * 0.5f) < 7) {\n        compute[index] += data[data_index + 9] * factor_1 * factor_2;\n      }\n    }\n  }\n}\n//|End-of-Code|",
        "gemini2_c": "void default_function_kernel(float* compute, float* data, float* grid, int blockIdx_x, int threadIdx_x) {\n  int idx = (blockIdx_x * 1024) + threadIdx_x;\n  int grid_idx_0 = (blockIdx_x * 512) + (threadIdx_x & 255);\n  int grid_idx_1 = grid_idx_0 + 256;\n  float grid_val_0 = grid[grid_idx_0];\n  float grid_val_1 = grid[grid_idx_1];\n  float floor_grid_val_0 = floorf((grid_val_0 + 1.0f) * 7.0f * 0.5f);\n  float floor_grid_val_1 = floorf((grid_val_1 + 1.0f) * 7.0f * 0.5f);\n  int int_floor_grid_val_0 = (int)floor_grid_val_0;\n  int int_floor_grid_val_1 = (int)floor_grid_val_1;\n\n  if (0 <= int_floor_grid_val_1 && 0 <= int_floor_grid_val_0 && int_floor_grid_val_1 < 8 && int_floor_grid_val_0 < 8) {\n    int data_idx = (blockIdx_x * 256) + ((threadIdx_x >> 8) * 64) + (int_floor_grid_val_1 * 8) + int_floor_grid_val_0;\n    compute[idx] = data[data_idx] * (1.0f - ((grid_val_1 + 1.0f) * 7.0f * 0.5f - floor_grid_val_1)) * (1.0f - ((grid_val_0 + 1.0f) * 7.0f * 0.5f - floor_grid_val_0));\n  } else {\n    compute[idx] = 0.0f;\n  }\n\n  if (0 <= int_floor_grid_val_1 && -1 <= int_floor_grid_val_0 && int_floor_grid_val_1 < 8 && int_floor_grid_val_0 < 7) {\n    int data_idx = (blockIdx_x * 256) + ((threadIdx_x >> 8) * 64) + (int_floor_grid_val_1 * 8) + int_floor_grid_val_0 + 1;\n    compute[idx] += data[data_idx] * (1.0f - ((grid_val_1 + 1.0f) * 7.0f * 0.5f - floor_grid_val_1)) * ((grid_val_0 + 1.0f) * 7.0f * 0.5f - floor_grid_val_0);\n  }\n\n  if (-1 <= int_floor_grid_val_1 && 0 <= int_floor_grid_val_0 && int_floor_grid_val_1 < 7 && int_floor_grid_val_0 < 8) {\n    int data_idx = (blockIdx_x * 256) + ((threadIdx_x >> 8) * 64) + (int_floor_grid_val_1 * 8) + int_floor_grid_val_0 + 8;\n    compute[idx] += data[data_idx] * ((grid_val_1 + 1.0f) * 7.0f * 0.5f - floor_grid_val_1) * (1.0f - ((grid_val_0 + 1.0f) * 7.0f * 0.5f - floor_grid_val_0));\n  }\n\n  if (-1 <= int_floor_grid_val_1 && -1 <= int_floor_grid_val_0 && int_floor_grid_val_1 < 7 && int_floor_grid_val_0 < 7) {\n    int data_idx = (blockIdx_x * 256) + ((threadIdx_x >> 8) * 64) + (int_floor_grid_val_1 * 8) + int_floor_grid_val_0 + 9;\n    compute[idx] += data[data_idx] * ((grid_val_1 + 1.0f) * 7.0f * 0.5f - floor_grid_val_1) * ((grid_val_0 + 1.0f) * 7.0f * 0.5f - floor_grid_val_0);\n  }\n}\n",
        "gpt1_c": "#include <math.h>\n#include <omp.h>\n#include <stdio.h>\n#include <stdlib.h>\n\nvoid default_function_kernel(float* compute, float* data, float* grid, int compute_size, int grid_width) {\n    #pragma omp parallel for\n    for (int idx = 0; idx < compute_size; ++idx) {\n        int blockIdx_x = idx / 1024;\n        int threadIdx_x = idx % 1024;\n        \n        int grid_idx1 = blockIdx_x * grid_width + (threadIdx_x & 255) + 256;\n        int grid_idx2 = blockIdx_x * grid_width + (threadIdx_x & 255);\n\n        float grid_val1 = grid[grid_idx1] + 1.0f;\n        float grid_val2 = grid[grid_idx2] + 1.0f;\n        \n        float grid_val1_scaled = grid_val1 * 7.0f * 0.5f;\n        float grid_val2_scaled = grid_val2 * 7.0f * 0.5f;\n\n        int floor_grid1 = (int)floorf(grid_val1_scaled);\n        int floor_grid2 = (int)floorf(grid_val2_scaled);\n\n        int compute_idx = blockIdx_x * 1024 + threadIdx_x;\n\n        if (0 <= floor_grid1 && 0 <= floor_grid2 && floor_grid1 < 8 && floor_grid2 < 8) {\n            int data_idx = blockIdx_x * 256 + (threadIdx_x >> 8) * 64 + floor_grid1 * 8 + floor_grid2;\n            compute[compute_idx] = data[data_idx] * (1.0f - (grid_val1_scaled - floorf(grid_val1_scaled))) * (1.0f - (grid_val2_scaled - floorf(grid_val2_scaled)));\n        } else {\n            compute[compute_idx] = 0.0f;\n        }\n\n        if (0 <= floor_grid1 && -1 <= floor_grid2 && floor_grid1 < 8 && floor_grid2 < 7) {\n            int data_idx = blockIdx_x * 256 + (threadIdx_x >> 8) * 64 + floor_grid1 * 8 + floor_grid2 + 1;\n            compute[compute_idx] += data[data_idx] * (1.0f - (grid_val1_scaled - floorf(grid_val1_scaled))) * (grid_val2_scaled - floorf(grid_val2_scaled));\n        }\n\n        if (-1 <= floor_grid1 && 0 <= floor_grid2 && floor_grid1 < 7 && floor_grid2 < 8) {\n            int data_idx = blockIdx_x * 256 + (threadIdx_x >> 8) * 64 + floor_grid1 * 8 + floor_grid2 + 8;\n            compute[compute_idx] += data[data_idx] * (grid_val1_scaled - floorf(grid_val1_scaled)) * (1.0f - (grid_val2_scaled - floorf(grid_val2_scaled)));\n        }\n\n        if (-1 <= floor_grid1 && -1 <= floor_grid2 && floor_grid1 < 7 && floor_grid2 < 7) {\n            int data_idx = blockIdx_x * 256 + (threadIdx_x >> 8) * 64 + floor_grid1 * 8 + floor_grid2 + 9;\n            compute[compute_idx] += data[data_idx] * (grid_val1_scaled - floorf(grid_val1_scaled)) * (grid_val2_scaled - floorf(grid_val2_scaled));\n        }\n    }\n}\n\nint main() {\n    int data_size = 4 * 4 * 8 * 8;\n    int grid_size = 4 * 2 * 16 * 16;\n    int compute_size = 4 * 1024;\n    int grid_width = 512;\n\n    float* data = (float*)malloc(data_size * sizeof(float));\n    float* grid = (float*)malloc(grid_size * sizeof(float));\n    float* compute = (float*)malloc(compute_size * sizeof(float));\n\n    // Initialize data and grid (for example purposes)\n    for (int i = 0; i < data_size; ++i) {\n        data[i] = (float)i;\n    }\n    for (int i = 0; i < grid_size; ++i) {\n        grid[i] = (float)i / grid_size;\n    }\n\n    // Call the function\n    default_function_kernel(compute, data, grid, compute_size, grid_width);\n\n    // Print some results (for verification purposes)\n    for (int i = 0; i < 10; ++i) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    // Free the allocated memory\n    free(data);\n    free(grid);\n    free(compute);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <math.h>\n#include <stdio.h>\n#include <stdlib.h>\n\nvoid default_function_kernel(float* compute, float* data, float* grid, int num_blocks, int threads_per_block) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < num_blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads_per_block; ++threadIdx_x) {\n            int idx = blockIdx_x * threads_per_block + threadIdx_x;\n            int grid_idx1 = blockIdx_x * 512 + (threadIdx_x & 255);\n            int grid_idx2 = grid_idx1 + 256;\n\n            float grid_val1 = grid[grid_idx1];\n            float grid_val2 = grid[grid_idx2];\n\n            int floor1 = (int)floorf((grid_val1 + 1.0f) * 3.5f);\n            int floor2 = (int)floorf((grid_val2 + 1.0f) * 3.5f);\n\n            float data_val = (floor1 >= 0 && floor1 < 8 && floor2 >= 0 && floor2 < 8)\n                                ? data[blockIdx_x * 256 + (threadIdx_x >> 8) * 64 + floor2 * 8 + floor1]\n                                : 0.0f;\n\n            float factor1 = 1.0f - ((grid_val2 + 1.0f) * 3.5f - floor2);\n            float factor2 = 1.0f - ((grid_val1 + 1.0f) * 3.5f - floor1);\n\n            compute[idx] = data_val * factor1 * factor2;\n        }\n    }\n}\n\nint main() {\n    int blocks = 4 * 4 * 8 * 8;   // As per the tensor shape provided\n    int threads_per_block = 1024; // Launch bounds\n\n    int total_elements = blocks * threads_per_block;\n    float *compute = (float *)malloc(total_elements * sizeof(float));\n    float *data = (float *)malloc(4 * 2 * 16 * 16 * sizeof(float)); // Adjust size based on tensor shape\n    float *grid = (float *)malloc(4 * 2 * 16 * 16 * sizeof(float)); // Adjust size based on tensor shape\n\n    // Initialize data and grid (example initialization)\n    for (int i = 0; i < 4 * 2 * 16 * 16; ++i) {\n        data[i] = (float)i;\n        grid[i] = (float)i * 0.01f;\n    }\n\n    // Call the function\n    default_function_kernel(compute, data, grid, blocks, threads_per_block);\n\n    // Print the result\n    for (int i = 0; i < total_elements; ++i) {\n        printf(\"%f \", compute[i]);\n    }\n    printf(\"\\n\");\n\n    free(compute);\n    free(data);\n    free(grid);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* compute, float* data, float* grid) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 32768; ++i0_i1_fused_i2_fused_i3_fused) {\n    int32_t i3_s = i0_i1_fused_i2_fused_i3_fused % 1024;\n    int32_t i0_i1_fused_i2_fused = i0_i1_fused_i2_fused_i3_fused / 1024;\n    int32_t i2_s = i0_i1_fused_i2_fused % 256;\n    int32_t i0_i1_fused = i0_i1_fused_i2_fused / 256;\n    int32_t i1_s = i0_i1_fused % 4;\n    int32_t i0_s = i0_i1_fused / 4;\n    compute[((i0_i1_fused_i2_fused * 1024) + i3_s)] = ((((((((((0 <= ((int)floorf((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int)floorf((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) ? data[((((((i0_s * 256) + (((i3_s >> 8) * 64)) + (((int)floorf((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + (i1_s * 16) + i2_s)] : 0.000000e+00f) * (1.000000e+00f - ((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * (1.000000e+00f - ((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((0 <= ((int)floorf((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int)floorf((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) && (((int)floorf((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[((((((i0_s * 256) + (((i3_s >> 8) * 64)) + (((int)floorf((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + (i1_s * 16) + i2_s + 1)] : 0.000000e+00f) * (1.000000e+00f - ((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) * ((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((-1 <= ((int)floorf((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (0 <= ((int)floorf((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int)floorf((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 8)) ? data[((((((i0_s * 256) + (((i3_s >> 8) * 64)) + (((int)floorf((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + (i1_s * 16) + i2_s + 8)] : 0.000000e+00f) * ((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))) * (1.000000e+00f - ((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))))) + (((((((-1 <= ((int)floorf((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) && (-1 <= ((int)floorf((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))))) && (((int)floorf((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) && (((int)floorf((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) < 7)) ? data[((((((i0_s * 256) + (((i3_s >> 8) * 64)) + (((int)floorf((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f))) * 8)) + ((int)floorf((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))) + (i1_s * 16) + i2_s + 9)] : 0.000000e+00f) * ((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((i0_s * 512) + ((i3_s & 255) + 256))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))) * ((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f) - ((float)((int)floorf((((grid[((i0_s * 512) + (i3_s & 255))] + 1.000000e+00f) * 7.000000e+00f) * 5.000000e-01f)))))));\n  }\n}"
    },
    {
        "op_name": "argsort",
        "c_code": "void default_function_kernel(float* data, int32_t* extern_1) {\n  float temp_data[1];\n  int32_t temp_index[1];\n  #pragma omp parallel for\n  for (int32_t i = 0; i < 64; ++i) {\n    for (int32_t j = 0; j < 256; ++j) {\n      extern_1[((i * 512) + (j * 2))] = (j * 2);\n    }\n    for (int32_t k = 0; k < 256; ++k) {\n      extern_1[(((i * 512) + (k * 2)) + 1)] = ((k * 2) + 1);\n    }\n    for (int32_t i_0 = 0; i_0 < 512; ++i_0) {\n      for (int32_t i_1 = 0; i_1 < 256; ++i_1) {\n        if (((((i * 512) + (i_1 * 2)) + (i_0 & 1)) < 511) && (data[(((i * 512) + (i_1 * 2)) + (i_0 & 1))] < data[((((i * 512) + (i_1 * 2)) + (i_0 & 1)) + 1)])) {\n          temp_data[0] = data[(((i * 512) + (i_1 * 2)) + (i_0 & 1))];\n          data[(((i * 512) + (i_1 * 2)) + (i_0 & 1))] = data[((((i * 512) + (i_1 * 2)) + (i_0 & 1)) + 1)];\n          data[((((i * 512) + (i_1 * 2)) + (i_0 & 1)) + 1)] = temp_data[0];\n          temp_index[0] = extern_1[(((i * 512) + (i_1 * 2)) + (i_0 & 1))];\n          extern_1[(((i * 512) + (i_1 * 2)) + (i_0 & 1))] = extern_1[((((i * 512) + (i_1 * 2)) + (i_0 & 1)) + 1)];\n          extern_1[((((i * 512) + (i_1 * 2)) + (i_0 & 1)) + 1)] = temp_index[0];\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3, int64_t cse_var_1, int64_t i_0) {\n  int64_t first[1];\n  int64_t last[1];\n  int64_t first_1[1];\n  int64_t last_1[1];\n  if ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) < (int64_t)512) {\n    if (i_0 == (int64_t)0) {\n      first[0] = max((int64_t)0, ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1))));\n      last[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), (((int64_t)2 << cse_var_1) >> (int64_t)1));\n      while ((first[0] < last[0])) {\n        if (argsort_gpu_v0[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first[0] + last[0]) >> (int64_t)1))] <= argsort_gpu_v0[((((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) >> (int64_t)1)) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first[0] + last[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first[0] = (((first[0] + last[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last[0] = ((first[0] + last[0]) >> (int64_t)1);\n        }\n      }\n      first[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first[0]);\n      last[0] = ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last[0]);\n      for (int i_1 = 0; i_1 < ((int)min((((int64_t)2 << cse_var_1) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_1) {\n        if ((((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) && (last[0] < (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1)))) && (last[0] < (int64_t)512)) {\n          if (argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])] <= argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])]) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        } else {\n          if ((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        }\n      }\n    } else {\n      first_1[0] = max((int64_t)0, ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (int64_t)512));\n      last_1[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), min((((int64_t)2 << cse_var_1) >> (int64_t)1), ((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))));\n      while ((first_1[0] < last_1[0])) {\n        if (argsort_gpu_v2[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first_1[0] + last_1[0]) >> (int64_t)1))] <= argsort_gpu_v2[(((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512)) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first_1[0] + last_1[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first_1[0] = (((first_1[0] + last_1[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last_1[0] = ((first_1[0] + last_1[0]) >> (int64_t)1);\n        }\n      }\n      first_1[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first_1[0]);\n      last_1[0] = ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last_1[0]);\n      for (int i_2 = 0; i_2 < ((int)min((((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_2) {\n        if (((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) && (last_1[0] < (int64_t)512)) {\n          if (argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])] <= argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])]) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        } else {\n          if ((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        }\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ data) {\n  if (((int)threadIdx.x) < 512) {\n    argsort_gpu_v0[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))] = data[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))];\n    argsort_gpu[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))] = ((float)((int)threadIdx.x));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3) {\n  __shared__ float temp_keys_swap[128];\n  __shared__ float temp_values_swap[128];\n  float temp_cond1[1];\n  float temp_cond2[1];\n  float temp_keys[1];\n  float temp_values[1];\n  for (int i = 0; i < 2; ++i) {\n    temp_keys_swap[((((int)threadIdx.x) * 2) + i)] = argsort_gpu_v0[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + i)];\n    temp_values_swap[((((int)threadIdx.x) * 2) + i)] = argsort_gpu[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + i)];\n  }\n  __syncthreads();\n  for (int j = 0; j < 128; ++j) {\n    if (((((int)threadIdx.x) * 2) + (j & 1)) < 127) {\n      temp_cond1[0] = temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))];\n      temp_cond2[0] = temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];\n      if (temp_cond2[0] < temp_cond1[0]) {\n        temp_keys[0] = temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))];\n        temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))] = temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];\n        temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)] = temp_keys[0];\n        temp_values[0] = temp_values_swap[((((int)threadIdx.x) * 2) + (j & 1))];\n        temp_values_swap[((((int)threadIdx.x) * 2) + (j & 1))] = temp_values_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];\n        temp_values_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)] = temp_values[0];\n      }\n    }\n    __syncthreads();\n  }\n  for (int k = 0; k < 2; ++k) {\n    argsort_gpu_v0[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_keys_swap[((((int)threadIdx.x) * 2) + k)];\n    argsort_gpu_v2[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_keys_swap[((((int)threadIdx.x) * 2) + k)];\n    argsort_gpu[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_values_swap[((((int)threadIdx.x) * 2) + k)];\n    argsort_gpu_v3[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_values_swap[((((int)threadIdx.x) * 2) + k)];\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((64, 512), \"float32\"), extern: T.Buffer((64, 512), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        temp_data = T.allocate([1], \"float32\", \"global\")\n        temp_index = T.allocate([1], \"int32\", \"global\")\n        T.attr(0, \"extern_scope\", 0)\n        for i in T.parallel(64):\n            extern_1 = T.Buffer((32768,), \"int32\", data=extern.data)\n            for j in range(256):\n                cse_var_1: T.int32 = j * 2\n                extern_1[i * 512 + cse_var_1] = cse_var_1\n            for k in range(256):\n                cse_var_2: T.int32 = k * 2\n                extern_1[i * 512 + cse_var_2 + 1] = cse_var_2 + 1\n            for i_0, i_1 in T.grid(512, 256):\n                cse_var_4: T.int32 = i * 512 + i_1 * 2 + i_0 % 2\n                cse_var_3: T.int32 = cse_var_4 + 1\n                data_1 = T.Buffer((32768,), data=data.data)\n                if cse_var_4 < 511 and data_1[cse_var_4] < data_1[cse_var_3]:\n                    temp_data_1 = T.Buffer((1,), data=temp_data)\n                    temp_data_1[0] = data_1[cse_var_4]\n                    data_1[cse_var_4] = data_1[cse_var_3]\n                    data_1[cse_var_3] = temp_data_1[0]\n                    temp_index_1 = T.Buffer((1,), \"int32\", data=temp_index)\n                    temp_index_1[0] = extern_1[cse_var_4]\n                    extern_1[cse_var_4] = extern_1[cse_var_3]\n                    extern_1[cse_var_3] = temp_index_1[0]",
        "op_args": "None",
        "input_shape": "[[64, 512]]",
        "output_shape": "[[64, 512]]",
        "input_name": [
            "data"
        ],
        "output_name": [
            "extern"
        ],
        "deepseek_c": "void default_function_kernel_2(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3, int64_t cse_var_1, int64_t i_0) {\n  int64_t first[1];\n  int64_t last[1];\n  int64_t first_1[1];\n  int64_t last_1[1];\n  if ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) < (int64_t)512) {\n    if (i_0 == (int64_t)0) {\n      first[0] = max((int64_t)0, ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1))));\n      last[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), (((int64_t)2 << cse_var_1) >> (int64_t)1));\n      while ((first[0] < last[0])) {\n        if (argsort_gpu_v0[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first[0] + last[0]) >> (int64_t)1))] <= argsort_gpu_v0[((((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) >> (int64_t)1)) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first[0] + last[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first[0] = (((first[0] + last[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last[0] = ((first[0] + last[0]) >> (int64_t)1);\n        }\n      }\n      first[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first[0]);\n      last[0] = ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last[0]);\n      for (int i_1 = 0; i_1 < ((int)min((((int64_t)2 << cse_var_1) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_1) {\n        if ((((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) && (last[0] < (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1)))) && (last[0] < (int64_t)512)) {\n          if (argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])] <= argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])]) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        } else {\n          if ((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        }\n      }\n    } else {\n      first_1[0] = max((int64_t)0, ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (int64_t)512));\n      last_1[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), min((((int64_t)2 << cse_var_1) >> (int64_t)1), ((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))));\n      while ((first_1[0] < last_1[0])) {\n        if (argsort_gpu_v2[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first_1[0] + last_1[0]) >> (int64_t)1))] <= argsort_gpu_v2[(((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512)) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first_1[0] + last_1[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first_1[0] = (((first_1[0] + last_1[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last_1[0] = ((first_1[0] + last_1[0]) >> (int64_t)1);\n        }\n      }\n      first_1[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first_1[0]);\n      last_1[0] = ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last_1[0]);\n      for (int i_2 = 0; i_2 < ((int)min((((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_2) {\n        if (((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) && (last_1[0] < (int64_t)512)) {\n          if (argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])] <= argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])]) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        } else {\n          if ((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel_2(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3, int64_t cse_var_1, int64_t i_0) {\n  int64_t first[1];\n  int64_t last[1];\n  int64_t first_1[1];\n  int64_t last_1[1];\n  if ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) < (int64_t)512) {\n    if (i_0 == (int64_t)0) {\n      first[0] = max((int64_t)0, ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1))));\n      last[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), (((int64_t)2 << cse_var_1) >> (int64_t)1));\n      while ((first[0] < last[0])) {\n        if (argsort_gpu_v0[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first[0] + last[0]) >> (int64_t)1))] <= argsort_gpu_v0[((((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) >> (int64_t)1)) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first[0] + last[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first[0] = (((first[0] + last[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last[0] = ((first[0] + last[0]) >> (int64_t)1);\n        }\n      }\n      first[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first[0]);\n      last[0] = ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last[0]);\n      for (int i_1 = 0; i_1 < ((int)min((((int64_t)2 << cse_var_1) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_1) {\n        if ((((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) && (last[0] < (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1)))) && (last[0] < (int64_t)512)) {\n          if (argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])] <= argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])]) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        } else {\n          if ((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        }\n      }\n    } else {\n      first_1[0] = max((int64_t)0, ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (int64_t)512));\n      last_1[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), min((((int64_t)2 << cse_var_1) >> (int64_t)1), ((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))));\n      while ((first_1[0] < last_1[0])) {\n        if (argsort_gpu_v2[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first_1[0] + last_1[0]) >> (int64_t)1))] <= argsort_gpu_v2[(((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512)) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first_1[0] + last_1[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first_1[0] = (((first_1[0] + last_1[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last_1[0] = ((first_1[0] + last_1[0]) >> (int64_t)1);\n        }\n      }\n      first_1[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first_1[0]);\n      last_1[0] = ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last_1[0]);\n      for (int i_2 = 0; i_2 < ((int)min((((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_2) {\n        if (((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) && (last_1[0] < (int64_t)512)) {\n          if (argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])] <= argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])]) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        } else {\n          if ((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0)))? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        }\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel_2(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3, int64_t cse_var_1, int64_t i_0) {\n  int64_t first[1];\n  int64_t last[1];\n  int64_t first_1[1];\n  int64_t last_1[1];\n  if ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) < (int64_t)512) {\n    if (i_0 == (int64_t)0) {\n      first[0] = max((int64_t)0, ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1))));\n      last[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), (((int64_t)2 << cse_var_1) >> (int64_t)1));\n      while ((first[0] < last[0])) {\n        if (argsort_gpu_v0[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first[0] + last[0]) >> (int64_t)1))] <= argsort_gpu_v0[((((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) >> (int64_t)1)) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first[0] + last[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first[0] = (((first[0] + last[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last[0] = ((first[0] + last[0]) >> (int64_t)1);\n        }\n      }\n      first[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first[0]);\n      last[0] = ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last[0]);\n      for (int i_1 = 0; i_1 < ((int)min((((int64_t)2 << cse_var_1) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_1) {\n        if ((((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) && (last[0] < (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1)))) && (last[0] < (int64_t)512)) {\n          if (argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])] <= argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])]) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        } else {\n          if ((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        }\n      }\n    } else {\n      first_1[0] = max((int64_t)0, ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (int64_t)512));\n      last_1[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), min((((int64_t)2 << cse_var_1) >> (int64_t)1), ((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))));\n      while ((first_1[0] < last_1[0])) {\n        if (argsort_gpu_v2[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first_1[0] + last_1[0]) >> (int64_t)1))] <= argsort_gpu_v2[(((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512)) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first_1[0] + last_1[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first_1[0] = (((first_1[0] + last_1[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last_1[0] = ((first_1[0] + last_1[0]) >> (int64_t)1);\n        }\n      }\n      first_1[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first_1[0]);\n      last_1[0] = ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last_1[0]);\n      for (int i_2 = 0; i_2 < ((int)min((((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_2) {\n        if (((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) && (last_1[0] < (int64_t)512)) {\n          if (argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])] <= argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])]) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        } else {\n          if ((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        }\n      }\n    }\n  }\n}",
        "gemini1_c": "too big\n    extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3, int64_t cse_var_1, int64_t i_0) {\n  int64_t first[1];\n  int64_t last[1];\n  int64_t first_1[1];\n  int64_t last_1[1];\n  if ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) < (int64_t)512) {\n    if (i_0 == (int64_t)0) {\n      first[0] = max((int64_t)0, ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1))));\n      last[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), (((int64_t)2 << cse_var_1) >> (int64_t)1));\n      while ((first[0] < last[0])) {\n        if (argsort_gpu_v0[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first[0] + last[0]) >> (int64_t)1))] <= argsort_gpu_v0[((((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) >> (int64_t)1)) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first[0] + last[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first[0] = (((first[0] + last[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last[0] = ((first[0] + last[0]) >> (int64_t)1);\n        }\n      }\n      first[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first[0]);\n      last[0] = ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last[0]);\n      for (int i_1 = 0; i_1 < ((int)min((((int64_t)2 << cse_var_1) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_1) {\n        if ((((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) && (last[0] < (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1)))) && (last[0] < (int64_t)512)) {\n          if (argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])] <= argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])]) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        } else {\n          if ((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        }\n      }\n    } else {\n      first_1[0] = max((int64_t)0, ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (int64_t)512));\n      last_1[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), min((((int64_t)2 << cse_var_1) >> (int64_t)1), ((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))));\n      while ((first_1[0] < last_1[0])) {\n        if (argsort_gpu_v2[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first_1[0] + last_1[0]) >> (int64_t)1))] <= argsort_gpu_v2[(((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512)) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first_1[0] + last_1[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first_1[0] = (((first_1[0] + last_1[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last_1[0] = ((first_1[0] + last_1[0]) >> (int64_t)1);\n        }\n      }\n      first_1[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first_1[0]);\n      last_1[0] = ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last_1[0]);\n      for (int i_2 = 0; i_2 < ((int)min((((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_2) {\n        if (((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) && (last_1[0] < (int64_t)512)) {\n          if (argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])] <= argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])]) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        } else {\n          if ((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        }\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ data) {\n  if (((int)threadIdx.x) < 512) {\n    argsort_gpu_v0[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))] = data[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))];\n    argsort_gpu[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))] = ((float)((int)threadIdx.x));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3) {\n  __shared__ float temp_keys_swap[128];\n  __shared__ float temp_values_swap[128];\n  float temp_cond1[1];\n  float temp_cond2[1];\n  float temp_keys[1];\n  float temp_values[1];\n  for (int i = 0; i < 2; ++i) {\n    temp_keys_swap[((((int)threadIdx.x) * 2) + i)] = argsort_gpu_v0[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + i)];\n    temp_values_swap[((((int)threadIdx.x) * 2) + i)] = argsort_gpu[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + i)];\n  }\n  __syncthreads();\n  for (int j = 0; j < 128; ++j) {\n    if (((((int)threadIdx.x) * 2) + (j & 1)) < 127) {\n      temp_cond1[0] = temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))];\n      temp_cond2[0] = temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];\n      if (temp_cond2[0] < temp_cond1[0]) {\n        temp_keys[0] = temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))];\n        temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))] = temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];\n        temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)] = temp_keys[0];\n        temp_values[0] = temp_values_swap[((((int)threadIdx.x) * 2) + (j & 1))];\n        temp_values_swap[((((int)threadIdx.x) * 2) + (j & 1))] = temp_values_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];\n        temp_values_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)] = temp_values[0];\n      }\n    }\n    __syncthreads();\n  }\n  for (int k = 0; k < 2; ++k) {\n    argsort_gpu_v0[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_keys_swap[((((int)threadIdx.x) * 2) + k)];\n    argsort_gpu_v2[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_keys_swap[((((int)threadIdx.x) * 2) + k)];\n    argsort_gpu[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_values_swap[((((int)threadIdx.x) * 2) + k)];\n    argsort_gpu_v3[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_values_swap[((((int)threadIdx.x) * 2) + k)];\n  }\n}\n\n\n    Translate the above CUDA code to CPU C code, the input tensor shape is [[64, 512]] respectively. Print only single C code function implementation, end with comment \"|End-of-Code|\".\n    ",
        "gemini2_c": "too big\n    extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3, int64_t cse_var_1, int64_t i_0) {\n  int64_t first[1];\n  int64_t last[1];\n  int64_t first_1[1];\n  int64_t last_1[1];\n  if ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) < (int64_t)512) {\n    if (i_0 == (int64_t)0) {\n      first[0] = max((int64_t)0, ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1))));\n      last[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), (((int64_t)2 << cse_var_1) >> (int64_t)1));\n      while ((first[0] < last[0])) {\n        if (argsort_gpu_v0[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first[0] + last[0]) >> (int64_t)1))] <= argsort_gpu_v0[((((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) >> (int64_t)1)) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first[0] + last[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first[0] = (((first[0] + last[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last[0] = ((first[0] + last[0]) >> (int64_t)1);\n        }\n      }\n      first[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first[0]);\n      last[0] = ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last[0]);\n      for (int i_1 = 0; i_1 < ((int)min((((int64_t)2 << cse_var_1) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_1) {\n        if ((((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) && (last[0] < (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1)))) && (last[0] < (int64_t)512)) {\n          if (argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])] <= argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])]) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        } else {\n          if ((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        }\n      }\n    } else {\n      first_1[0] = max((int64_t)0, ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (int64_t)512));\n      last_1[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), min((((int64_t)2 << cse_var_1) >> (int64_t)1), ((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))));\n      while ((first_1[0] < last_1[0])) {\n        if (argsort_gpu_v2[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first_1[0] + last_1[0]) >> (int64_t)1))] <= argsort_gpu_v2[(((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512)) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first_1[0] + last_1[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first_1[0] = (((first_1[0] + last_1[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last_1[0] = ((first_1[0] + last_1[0]) >> (int64_t)1);\n        }\n      }\n      first_1[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first_1[0]);\n      last_1[0] = ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last_1[0]);\n      for (int i_2 = 0; i_2 < ((int)min((((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_2) {\n        if (((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) && (last_1[0] < (int64_t)512)) {\n          if (argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])] <= argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])]) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        } else {\n          if ((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        }\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ data) {\n  if (((int)threadIdx.x) < 512) {\n    argsort_gpu_v0[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))] = data[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))];\n    argsort_gpu[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))] = ((float)((int)threadIdx.x));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3) {\n  __shared__ float temp_keys_swap[128];\n  __shared__ float temp_values_swap[128];\n  float temp_cond1[1];\n  float temp_cond2[1];\n  float temp_keys[1];\n  float temp_values[1];\n  for (int i = 0; i < 2; ++i) {\n    temp_keys_swap[((((int)threadIdx.x) * 2) + i)] = argsort_gpu_v0[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + i)];\n    temp_values_swap[((((int)threadIdx.x) * 2) + i)] = argsort_gpu[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + i)];\n  }\n  __syncthreads();\n  for (int j = 0; j < 128; ++j) {\n    if (((((int)threadIdx.x) * 2) + (j & 1)) < 127) {\n      temp_cond1[0] = temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))];\n      temp_cond2[0] = temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];\n      if (temp_cond2[0] < temp_cond1[0]) {\n        temp_keys[0] = temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))];\n        temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))] = temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];\n        temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)] = temp_keys[0];\n        temp_values[0] = temp_values_swap[((((int)threadIdx.x) * 2) + (j & 1))];\n        temp_values_swap[((((int)threadIdx.x) * 2) + (j & 1))] = temp_values_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];\n        temp_values_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)] = temp_values[0];\n      }\n    }\n    __syncthreads();\n  }\n  for (int k = 0; k < 2; ++k) {\n    argsort_gpu_v0[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_keys_swap[((((int)threadIdx.x) * 2) + k)];\n    argsort_gpu_v2[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_keys_swap[((((int)threadIdx.x) * 2) + k)];\n    argsort_gpu[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_values_swap[((((int)threadIdx.x) * 2) + k)];\n    argsort_gpu_v3[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_values_swap[((((int)threadIdx.x) * 2) + k)];\n  }\n}\n\n\n    Translate the above CUDA code to CPU C code, the input tensor shape is [[64, 512]] respectively. Print only single C code function implementation, end with comment \"|End-of-Code|\".\n    ",
        "gpt1_c": "#include <omp.h>\n#include <stdint.h>\n#include <stdio.h>\n#include <algorithm> // For std::max and std::min\n\nvoid default_function_kernel_2(float* argsort_gpu, float* argsort_gpu_v0, float* argsort_gpu_v2, float* argsort_gpu_v3, int64_t cse_var_1, int64_t i_0, int gridDim_x, int gridDim_y, int gridDim_z) {\n    #pragma omp parallel for collapse(3)\n    for (int blockIdx_z = 0; blockIdx_z < gridDim_z; ++blockIdx_z) {\n        for (int blockIdx_y = 0; blockIdx_y < gridDim_y; ++blockIdx_y) {\n            for (int threadIdx_x = 0; threadIdx_x < 1024; ++threadIdx_x) {\n                int64_t first[1];\n                int64_t last[1];\n                int64_t first_1[1];\n                int64_t last_1[1];\n\n                if ((((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z))) < (int64_t)512) {\n                    if (i_0 == (int64_t)0) {\n                        first[0] = std::max((int64_t)0, ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (((int64_t)2 << cse_var_1) * (((int64_t)(blockIdx_z)) + (int64_t)1))));\n                        last[0] = std::min((((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), (((int64_t)2 << cse_var_1) >> (int64_t)1));\n                        while ((first[0] < last[0])) {\n                            if (argsort_gpu_v0[(((((int64_t)(blockIdx_y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + ((first[0] + last[0]) >> (int64_t)1))] <= argsort_gpu_v0[(((((((int64_t)(blockIdx_y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) >> (int64_t)1)) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first[0] + last[0]) >> (int64_t)1)) - (int64_t)1)]) {\n                                first[0] = (((first[0] + last[0]) >> (int64_t)1) + (int64_t)1);\n                            } else {\n                                last[0] = ((first[0] + last[0]) >> (int64_t)1);\n                            }\n                        }\n                        first[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z))) + first[0]);\n                        last[0] = ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last[0]);\n                        for (int i_1 = 0; i_1 < ((int)std::min((((int64_t)2 << cse_var_1) - (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_1) {\n                            if ((((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z))))) && (first[0] < (int64_t)512)) && (last[0] < (((int64_t)2 << cse_var_1) * (((int64_t)(blockIdx_z)) + (int64_t)1)))) && (last[0] < (int64_t)512)) {\n                                if (argsort_gpu_v0[((((int64_t)(blockIdx_y)) * (int64_t)512) + first[0])] <= argsort_gpu_v0[((((int64_t)(blockIdx_y)) * (int64_t)512) + last[0])]) {\n                                    argsort_gpu_v2[((((((int64_t)(blockIdx_y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)(blockIdx_y)) * (int64_t)512) + first[0])];\n                                    argsort_gpu_v3[((((((int64_t)(blockIdx_y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)(blockIdx_y)) * (int64_t)512) + first[0])];\n                                    first[0] = (first[0] + (int64_t)1);\n                                } else {\n                                    argsort_gpu_v2[((((((int64_t)(blockIdx_y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)(blockIdx_y)) * (int64_t)512) + last[0])];\n                                    argsort_gpu_v3[((((((int64_t)(blockIdx_y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)(blockIdx_y)) * (int64_t)512) + last[0])];\n                                    last[0] = (last[0] + (int64_t)1);\n                                }\n                            } else {\n                                if ((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z))))) && (first[0] < (int64_t)512)) {\n                                    argsort_gpu_v2[((((((int64_t)(blockIdx_y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)(blockIdx_y)) * (int64_t)512) + first[0])];\n                                    argsort_gpu_v3[((((((int64_t)(blockIdx_y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int6464)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int6464)2 << cse_var_1) - (int64_t)1) / ((int6464)((int)((int6464)2 << cse_var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_1))] = argsort_gpu[((((int6464)(blockIdx_y)) * (int6464)512) + first[0])];\n                                    first[0] = (first[0] + (int6464)1);\n                                } else {\n                                    argsort_gpu_v2[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse_var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse_var_1))) >= (int6464)0) && (((((int6464)2 << cse_var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse_var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse_var_1))) < (int6464)0) && (((((int6464)2 << cse_var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse_var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse_var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse_var_1)))) : (((((int6464)2 << cse_var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse_var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_1))] = argsort_gpu_v0[((((int6464)(blockIdx_y)) * (int6464)512) + last[0])];\n                                    argsort_gpu_v3[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse_var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse_var_1))) >= (int6464)0) && (((((int6464)2 << cse_var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse_var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse_var_1))) < (int6464)0) && (((((int6464)2 << cse_var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse_var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse_var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse_var_1)))) : (((((int6464)2 << cse_var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse_var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_1))] = argsort_gpu[((((int6464)(blockIdx_y)) * (int6464)512) + last[0])];\n                                    last[0] = (last[0] + (int6464)1);\n                                }\n                            }\n                        }\n                    } else {\n                        first_1[0] = std::max((int6464)0, ((std::min(((((int6464)2 << cse_var_1) >> (int6464)1) + (((int6464)2 << cse_var_1) * ((int6464)(blockIdx_z)))), (int6464)512) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse_var_1))) >= (int6464)0) && (((((int6464)2 << cse_var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse_var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse_var_1))) < (int6464)0) && (((((int6464)2 << cse_var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse_var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) - (int6464)512));\n                        last_1[0] = std::min((((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1)), std::min((((int6464)2 << cse var_1) >> (int6464)1), ((int6464)512 - (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z))))));\n                        while ((first_1[0] < last_1[0])) {\n                            if (argsort_gpu_v2[(((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + ((first_1[0] + last_1[0]) >> (int6464)1))] <= argsort_gpu_v2[(((((((int6464)(blockIdx_y)) * (int6464)512) + std::min(((((int6464)2 << cse var_1) >> (int6464)1) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))), (int6464)512)) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) - ((first_1[0] + last_1[0]) >> (int6464)1)) - (int6464)1)]) {\n                                first_1[0] = (((first_1[0] + last_1[0]) >> (int6464)1) + (int6464)1);\n                            } else {\n                                last_1[0] = ((first_1[0] + last_1[0]) >> (int6464)1);\n                            }\n                        }\n                        first_1[0] = ((((int6464)2 << cse var_1) * ((int6464)(blockIdx_z))) + first_1[0]);\n                        last_1[0] = ((std::min(((((int6464)2 << cse var_1) >> (int6464)1) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))), (int6464)512) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) - last_1[0]);\n                        for (int i_2 = 0; i_2 < ((int)std::min((((int6464)512 - (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) - (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))), (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))); ++i_2) {\n                            if (((first_1[0] < ((((int6464)2 << cse var_1) >> (int6464)1) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z))))) && (first_1[0] < (int6464)512)) && (last_1[0] < (int6464)512)) {\n                                if (argsort_gpu_v2[((((int6464)(blockIdx_y)) * (int6464)512) + first_1[0])] <= argsort_gpu_v2[((((int6464)(blockIdx_y)) * (int6464)512) + last_1[0])]) {\n                                    argsort_gpu_v0[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_2))] = argsort_gpu_v2[((((int6464)(blockIdx_y)) * (int6464)512) + first_1[0])];\n                                    argsort_gpu[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_2))] = argsort_gpu_v3[((((int6464)(blockIdx_y)) * (int6464)512) + first_1[0])];\n                                    first_1[0] = (first_1[0] + (int6464)1);\n                                } else {\n                                    argsort_gpu_v0[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_2))] = argsort_gpu_v2[((((int6464)(blockIdx_y)) * (int6464)512) + last_1[0])];\n                                    argsort_gpu[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_2))] = argsort_gpu_v3[((((int6464)(blockIdx_y)) * (int6464)512) + last_1[0])];\n                                    last_1[0] = (last_1[0] + (int6464)1);\n                                }\n                            } else {\n                                if ((first_1[0] < ((((int6464)2 << cse var_1) >> (int6464)1) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z))))) && (first_1[0] < (int6464)512)) {\n                                    argsort_gpu_v0[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_2))] = argsort_gpu_v2[((((int6464)(blockIdx_y)) * (int6464)512) + first_1[0])];\n                                    argsort_gpu[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_2))] = argsort_gpu_v3[((((int6464)(blockIdx_y)) * (int6464)512) + first_1[0])];\n                                    first_1[0] = (first_1[0] + (int6464)1);\n                                } else {\n                                    argsort_gpu_v0[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_2))] = argsort_gpu_v2[((((int6464)(blockIdx_y)) * (int6464)512) + last_1[0])];\n                                    argsort_gpu[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_2))] = argsort_gpu_v3[((((int6464)(blockIdx_y)) * (int6464)512) + last_1[0])];\n                                    last_1[0] = (last_1[0] + (int6464)1);\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nvoid default_function_kernel(float* argsort_gpu, float* argsort_gpu_v0, float* data, int gridDim_y) {\n    #pragma omp parallel for\n    for (int blockIdx_y = 0; blockIdx_y < gridDim_y; ++blockIdx_y) {\n        #pragma omp parallel for\n        for (int threadIdx_x = 0; threadIdx_x < 512; ++threadIdx_x) {\n            argsort_gpu_v0[((((int)blockIdx_y) * 512) + ((int)threadIdx_x))] = data[((((int)blockIdx_y) * 512) + ((int)threadIdx_x))];\n            argsort_gpu[((((int)blockIdx_y) * 512) + ((int)threadIdx_x))] = ((float)((int)threadIdx_x));\n        }\n    }\n}\n\nvoid default_function_kernel_1(float* argsort_gpu, float* argsort_gpu_v0, float* argsort_gpu_v2, float* argsort_gpu_v3, int gridDim_y, int gridDim_x) {\n    #pragma omp parallel for collapse(2)\n    for (int blockIdx_y = 0; blockIdx_y < gridDim_y; ++blockIdx_y) {\n        for (int blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n            float temp_keys_swap[128];\n            float temp_values_swap[128];\n            float temp_cond1[1];\n            float temp_cond2[1];\n            float temp_keys[1];\n            float temp_values[1];\n            for (int i = 0; i < 2; ++i) {\n                temp_keys_swap[((((int)omp_get_thread_num()) * 2) + i)] = argsort_gpu_v0[((((((int)blockIdx_y) * 512) + (((int)blockIdx_x) * 128)) + (((int)omp_get_thread_num()) * 2)) + i)];\n                temp_values_swap[((((int)omp_get_thread_num()) * 2) + i)] = argsort_gpu[((((((int)blockIdx_y) * 512) + (((int)blockIdx_x) * 128)) + (((int)omp_get_thread_num()) * 2)) + i)];\n            }\n            #pragma omp barrier\n            for (int j = 0; j < 128; ++j) {\n                if (((((int)omp_get_thread_num()) * 2) + (j & 1)) < 127) {\n                    temp_cond1[0] = temp_keys_swap[((((int)omp_get_thread_num()) * 2) + (j & 1))];\n                    temp_cond2[0] = temp_keys_swap[(((((int)omp_get_thread_num()) * 2) + (j & 1)) + 1)];\n                    if (temp_cond2[0] < temp_cond1[0]) {\n                        temp_keys[0] = temp_keys_swap[((((int)omp_get_thread_num()) * 2) + (j & 1))];\n                        temp_keys_swap[((((int)omp_get_thread_num()) * 2) + (j & 1))] = temp_keys_swap[(((((int)omp_get_thread_num()) * 2) + (j & 1)) + 1)];\n                        temp_keys_swap[(((((int)omp_get_thread_num()) * 2) + (j & 1)) + 1)] = temp_keys[0];\n                        temp_values[0] = temp_values_swap[((((int)omp_get_thread_num()) * 2) + (j & 1))];\n                        temp_values_swap[((((int)omp_get_thread_num()) * 2) + (j & 1))] = temp_values_swap[(((((int)omp_get_thread_num()) * 2) + (j & 1)) + 1)];\n                        temp_values_swap[(((((int)omp_get_thread_num()) * 2) + (j & 1)) + 1)] = temp_values[0];\n                    }\n                }\n                #pragma omp barrier\n            }\n            for (int k = 0; k < 2; ++k) {\n                argsort_gpu_v0[((((((int)blockIdx_y) * 512) + (((int)blockIdx_x) * 128)) + (((int)omp_get_thread_num()) * 2)) + k)] = temp_keys_swap[((((int)omp_get_thread_num()) * 2) + k)];\n                argsort_gpu_v2[((((((int)blockIdx_y) * 512) + (((int)blockIdx_x) * 128)) + (((int)omp_get_thread_num()) * 2)) + k)] = temp_keys_swap[((((int)omp_get_thread_num()) * 2) + k)];\n                argsort_gpu[((((((int)blockIdx_y) * 512) + (((int)blockIdx_x) * 128)) + (((int)omp_get_thread_num()) * 2)) + k)] = temp_values_swap[((((int)omp_get_thread_num()) * 2) + k)];\n                argsort_gpu_v3[((((((int)blockIdx_y) * 512) + (((int)blockIdx_x) * 128)) + (((int)omp_get_thread_num()) * 2)) + k)] = temp_values_swap[((((int)omp_get_thread_num()) * 2) + k)];\n            }\n        }\n    }\n}\n\n// End-of-Code\n",
        "gpt2_c": "#include <omp.h>\n#include <stdint.h>\n#include <stdio.h>\n#include <algorithm> // For std::max and std::min\n\nvoid default_function_kernel_2(float* argsort_gpu, float* argsort_gpu_v0, float* argsort_gpu_v2, float* argsort_gpu_v3, int64_t cse_var_1, int64_t i_0, int gridDim_x, int gridDim_y, int gridDim_z) {\n    #pragma omp parallel for collapse(3)\n    for (int blockIdx_z = 0; blockIdx_z < gridDim_z; ++blockIdx_z) {\n        for (int blockIdx_y = 0; blockIdx_y < gridDim_y; ++blockIdx_y) {\n            for (int threadIdx_x = 0; threadIdx_x < 1024; ++threadIdx_x) {\n                int64_t first[1];\n                int64_t last[1];\n                int64_t first_1[1];\n                int64_t last_1[1];\n\n                if ((((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z))) < (int64_t)512) {\n                    if (i_0 == (int64_t)0) {\n                        first[0] = std::max((int64_t)0, ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (((int64_t)2 << cse_var_1) * (((int64_t)(blockIdx_z)) + (int64_t)1))));\n                        last[0] = std::min((((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), (((int64_t)2 << cse_var_1) >> (int64_t)1));\n                        while ((first[0] < last[0])) {\n                            if (argsort_gpu_v0[(((((int64_t)(blockIdx_y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + ((first[0] + last[0]) >> (int64_t)1))] <= argsort_gpu_v0[(((((((int64_t)(blockIdx_y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) >> (int64_t)1)) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first[0] + last[0]) >> (int64_t)1)) - (int64_t)1)]) {\n                                first[0] = (((first[0] + last[0]) >> (int64_t)1) + (int64_t)1);\n                            } else {\n                                last[0] = ((first[0] + last[0]) >> (int64_t)1);\n                            }\n                        }\n                        first[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z))) + first[0]);\n                        last[0] = ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last[0]);\n                        for (int i_1 = 0; i_1 < ((int)std::min((((int64_t)2 << cse_var_1) - (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_1) {\n                            if ((((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z))))) && (first[0] < (int64_t)512)) && (last[0] < (((int64_t)2 << cse_var_1) * (((int64_t)(blockIdx_z)) + (int64_t)1)))) && (last[0] < (int64_t)512)) {\n                                if (argsort_gpu_v0[((((int64_t)(blockIdx_y)) * (int64_t)512) + first[0])] <= argsort_gpu_v0[((((int64_t)(blockIdx_y)) * (int64_t)512) + last[0])]) {\n                                    argsort_gpu_v2[((((((int64_t)(blockIdx_y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)(blockIdx_y)) * (int64_t)512) + first[0])];\n                                    argsort_gpu_v3[((((((int64_t)(blockIdx_y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)(blockIdx_y)) * (int64_t)512) + first[0])];\n                                    first[0] = (first[0] + (int64_t)1);\n                                } else {\n                                    argsort_gpu_v2[((((((int64_t)(blockIdx_y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)(blockIdx_y)) * (int64_t)512) + last[0])];\n                                    argsort_gpu_v3[((((((int64_t)(blockIdx_y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)(blockIdx_y)) * (int64_t)512) + last[0])];\n                                    last[0] = (last[0] + (int64_t)1);\n                                }\n                            } else {\n                                if ((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z))))) && (first[0] < (int64_t)512)) {\n                                    argsort_gpu_v2[((((((int64_t)(blockIdx_y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)(blockIdx_y)) * (int64_t)512) + first[0])];\n                                    argsort_gpu_v3[((((((int64_t)(blockIdx_y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)(blockIdx_z)))) + (((int64_t)(threadIdx_x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int6464)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int6464)2 << cse_var_1) - (int64_t)1) / ((int6464)((int)((int6464)2 << cse_var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_1))] = argsort_gpu[((((int6464)(blockIdx_y)) * (int6464)512) + first[0])];\n                                    first[0] = (first[0] + (int6464)1);\n                                } else {\n                                    argsort_gpu_v2[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse_var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse_var_1))) >= (int6464)0) && (((((int6464)2 << cse_var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse_var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse_var_1))) < (int6464)0) && (((((int6464)2 << cse_var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse_var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse_var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse_var_1)))) : (((((int6464)2 << cse_var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse_var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_1))] = argsort_gpu_v0[((((int6464)(blockIdx_y)) * (int6464)512) + last[0])];\n                                    argsort_gpu_v3[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse_var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse_var_1))) >= (int6464)0) && (((((int6464)2 << cse_var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse_var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse_var_1))) < (int6464)0) && (((((int6464)2 << cse_var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse_var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse_var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse_var_1)))) : (((((int6464)2 << cse_var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse_var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_1))] = argsort_gpu[((((int6464)(blockIdx_y)) * (int6464)512) + last[0])];\n                                    last[0] = (last[0] + (int6464)1);\n                                }\n                            }\n                        }\n                    } else {\n                        first_1[0] = std::max((int6464)0, ((std::min(((((int6464)2 << cse_var_1) >> (int6464)1) + (((int6464)2 << cse_var_1) * ((int6464)(blockIdx_z)))), (int6464)512) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse_var_1))) >= (int6464)0) && (((((int6464)2 << cse_var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse_var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse_var_1))) < (int6464)0) && (((((int6464)2 << cse_var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse_var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) - (int6464)512));\n                        last_1[0] = std::min((((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1)), std::min((((int6464)2 << cse var_1) >> (int6464)1), ((int6464)512 - (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z))))));\n                        while ((first_1[0] < last_1[0])) {\n                            if (argsort_gpu_v2[(((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + ((first_1[0] + last_1[0]) >> (int6464)1))] <= argsort_gpu_v2[(((((((int6464)(blockIdx_y)) * (int6464)512) + std::min(((((int6464)2 << cse var_1) >> (int6464)1) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))), (int6464)512)) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) - ((first_1[0] + last_1[0]) >> (int6464)1)) - (int6464)1)]) {\n                                first_1[0] = (((first_1[0] + last_1[0]) >> (int6464)1) + (int6464)1);\n                            } else {\n                                last_1[0] = ((first_1[0] + last_1[0]) >> (int6464)1);\n                            }\n                        }\n                        first_1[0] = ((((int6464)2 << cse var_1) * ((int6464)(blockIdx_z))) + first_1[0]);\n                        last_1[0] = ((std::min(((((int6464)2 << cse var_1) >> (int6464)1) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))), (int6464)512) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) - last_1[0]);\n                        for (int i_2 = 0; i_2 < ((int)std::min((((int6464)512 - (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) - (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))), (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))); ++i_2) {\n                            if (((first_1[0] < ((((int6464)2 << cse var_1) >> (int6464)1) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z))))) && (first_1[0] < (int6464)512)) && (last_1[0] < (int6464)512)) {\n                                if (argsort_gpu_v2[((((int6464)(blockIdx_y)) * (int6464)512) + first_1[0])] <= argsort_gpu_v2[((((int6464)(blockIdx_y)) * (int6464)512) + last_1[0])]) {\n                                    argsort_gpu_v0[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_2))] = argsort_gpu_v2[((((int6464)(blockIdx_y)) * (int6464)512) + first_1[0])];\n                                    argsort_gpu[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_2))] = argsort_gpu_v3[((((int6464)(blockIdx_y)) * (int6464)512) + first_1[0])];\n                                    first_1[0] = (first_1[0] + (int6464)1);\n                                } else {\n                                    argsort_gpu_v0[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_2))] = argsort_gpu_v2[((((int6464)(blockIdx_y)) * (int6464)512) + last_1[0])];\n                                    argsort_gpu[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_2))] = argsort_gpu_v3[((((int6464)(blockIdx_y)) * (int6464)512) + last_1[0])];\n                                    last_1[0] = (last_1[0] + (int6464)1);\n                                }\n                            } else {\n                                if ((first_1[0] < ((((int6464)2 << cse var_1) >> (int6464)1) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z))))) && (first_1[0] < (int6464)512)) {\n                                    argsort_gpu_v0[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_2))] = argsort_gpu_v2[((((int6464)(blockIdx_y)) * (int6464)512) + first_1[0])];\n                                    argsort_gpu[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_2))] = argsort_gpu_v3[((((int6464)(blockIdx_y)) * (int6464)512) + first_1[0])];\n                                    first_1[0] = (first_1[0] + (int6464)1);\n                                } else {\n                                    argsort_gpu_v0[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_2))] = argsort_gpu_v2[((((int6464)(blockIdx_y)) * (int6464)512) + last_1[0])];\n                                    argsort_gpu[((((((int6464)(blockIdx_y)) * (int6464)512) + (((int6464)2 << cse var_1) * ((int6464)(blockIdx_z)))) + (((int6464)(threadIdx_x)) * (((((((int6464)((int)((int6464)2 << cse var_1))) >= (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) >= (int6464)0)) || ((((int6464)((int)((int6464)2 << cse var_1))) < (int6464)0) && (((((int6464)2 << cse var_1) - (int6464)1) % ((int6464)((int)((int6464)2 << cse var_1)))) <= (int6464)0))) ? ((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) : (((((int6464)2 << cse var_1) - (int6464)1) / ((int6464)((int)((int6464)2 << cse var_1)))) - (int6464)1)) + (int6464)1))) + ((int6464)i_2))] = argsort_gpu_v3[((((int6464)(blockIdx_y)) * (int6464)512) + last_1[0])];\n                                    last_1[0] = (last_1[0] + (int6464)1);\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nvoid default_function_kernel(float* argsort_gpu, float* argsort_gpu_v0, float* data, int gridDim_y) {\n    #pragma omp parallel for\n    for (int blockIdx_y = 0; blockIdx_y < gridDim_y; ++blockIdx_y) {\n        #pragma omp parallel for\n        for (int threadIdx_x = 0; threadIdx_x < 512; ++threadIdx_x) {\n            argsort_gpu_v0[((((int)blockIdx_y) * 512) + ((int)threadIdx_x))] = data[((((int)blockIdx_y) * 512) + ((int)threadIdx_x))];\n            argsort_gpu[((((int)blockIdx_y) * 512) + ((int)threadIdx_x))] = ((float)((int)threadIdx_x));\n        }\n    }\n}\n\nvoid default_function_kernel_1(float* argsort_gpu, float* argsort_gpu_v0, float* argsort_gpu_v2, float* argsort_gpu_v3, int gridDim_y, int gridDim_x) {\n    #pragma omp parallel for collapse(2)\n    for (int blockIdx_y = 0; blockIdx_y < gridDim_y; ++blockIdx_y) {\n        for (int blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n            float temp_keys_swap[128];\n            float temp_values_swap[128];\n            float temp_cond1[1];\n            float temp_cond2[1];\n            float temp_keys[1];\n            float temp_values[1];\n            for (int i = 0; i < 2; ++i) {\n                temp_keys_swap[((((int)omp_get_thread_num()) * 2) + i)] = argsort_gpu_v0[((((((int)blockIdx_y) * 512) + (((int)blockIdx_x) * 128)) + (((int)omp_get_thread_num()) * 2)) + i)];\n                temp_values_swap[((((int)omp_get_thread_num()) * 2) + i)] = argsort_gpu[((((((int)blockIdx_y) * 512) + (((int)blockIdx_x) * 128)) + (((int)omp_get_thread_num()) * 2)) + i)];\n            }\n            #pragma omp barrier\n            for (int j = 0; j < 128; ++j) {\n                if (((((int)omp_get_thread_num()) * 2) + (j & 1)) < 127) {\n                    temp_cond1[0] = temp_keys_swap[((((int)omp_get_thread_num()) * 2) + (j & 1))];\n                    temp_cond2[0] = temp_keys_swap[(((((int)omp_get_thread_num()) * 2) + (j & 1)) + 1)];\n                    if (temp_cond2[0] < temp_cond1[0]) {\n                        temp_keys[0] = temp_keys_swap[((((int)omp_get_thread_num()) * 2) + (j & 1))];\n                        temp_keys_swap[((((int)omp_get_thread_num()) * 2) + (j & 1))] = temp_keys_swap[(((((int)omp_get_thread_num()) * 2) + (j & 1)) + 1)];\n                        temp_keys_swap[(((((int)omp_get_thread_num()) * 2) + (j & 1)) + 1)] = temp_keys[0];\n                        temp_values[0] = temp_values_swap[((((int)omp_get_thread_num()) * 2) + (j & 1))];\n                        temp_values_swap[((((int)omp_get_thread_num()) * 2) + (j & 1))] = temp_values_swap[(((((int)omp_get_thread_num()) * 2) + (j & 1)) + 1)];\n                        temp_values_swap[(((((int)omp_get_thread_num()) * 2) + (j & 1)) + 1)] = temp_values[0];\n                    }\n                }\n                #pragma omp barrier\n            }\n            for (int k = 0; k < 2; ++k) {\n                argsort_gpu_v0[((((((int)blockIdx_y) * 512) + (((int)blockIdx_x) * 128)) + (((int)omp_get_thread_num()) * 2)) + k)] = temp_keys_swap[((((int)omp_get_thread_num()) * 2) + k)];\n                argsort_gpu_v2[((((((int)blockIdx_y) * 512) + (((int)blockIdx_x) * 128)) + (((int)omp_get_thread_num()) * 2)) + k)] = temp_keys_swap[((((int)omp_get_thread_num()) * 2) + k)];\n                argsort_gpu[((((((int)blockIdx_y) * 512) + (((int)blockIdx_x) * 128)) + (((int)omp_get_thread_num()) * 2)) + k)] = temp_values_swap[((((int)omp_get_thread_num()) * 2) + k)];\n                argsort_gpu_v3[((((((int)blockIdx_y) * 512) + (((int)blockIdx_x) * 128)) + (((int)omp_get_thread_num()) * 2)) + k)] = temp_values_swap[((((int)omp_get_thread_num()) * 2) + k)];\n            }\n        }\n    }\n}\n\n// End-of-Code\n",
        "gemini3_c": "too big\n    Task: Translate the given CUDA code to its equivalent high-performance CPU C code.\n    Context: You are provided with a CUDA code snippet that needs to be translated into CPU C code. The translation should preserve the same functionality as much as possible. Focus on translating the CUDA-specific parallel constructs into constructs supported by the CPU, such as using OpenMP for parallelism. The resulting CPU C code should be complete and ready to compile.\n\n    Example 1:\n    Input CUDA Code: extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\n\n    Input Tensor Shape: [[17, 12, 7, 9]]\n    Output C Code: void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1428; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 9; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 9) + i3_s)] = atanf(data[((i0_i1_fused_i2_fused * 9) + i3_s)]);\n    }\n  }\n}\n\n\n    //|End-of-Code|\n\n    Example 2:\n    Input CUDA Code: extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_matmul, float* __restrict__ left_matrix, float* __restrict__ right_matrix) {\n  float T_matmul_local[2];\n  __shared__ float left_matrix_shared[4];\n  __shared__ float right_matrix_shared[4];\n  T_matmul_local[0] = 0.000000e+00f;\n  T_matmul_local[1] = 0.000000e+00f;\n  for (int ax0_ax1_fused_outer_outer = 0; ax0_ax1_fused_outer_outer < 2; ++ax0_ax1_fused_outer_outer) {\n    left_matrix_shared[((ax0_ax1_fused_outer_outer * 2) + ((int)threadIdx.x))] = left_matrix[((ax0_ax1_fused_outer_outer * 2) + ((int)threadIdx.x))];\n  }\n  for (int ax0_ax1_fused_outer_outer_1 = 0; ax0_ax1_fused_outer_outer_1 < 2; ++ax0_ax1_fused_outer_outer_1) {\n    right_matrix_shared[((ax0_ax1_fused_outer_outer_1 * 2) + ((int)threadIdx.x))] = right_matrix[((ax0_ax1_fused_outer_outer_1 * 2) + ((int)threadIdx.x))];\n  }\n  __syncthreads();\n  for (int k_inner = 0; k_inner < 2; ++k_inner) {\n    T_matmul_local[0] = (T_matmul_local[0] + (left_matrix_shared[((((int)threadIdx.x) * 2) + k_inner)] * right_matrix_shared[(k_inner * 2)]));\n    T_matmul_local[1] = (T_matmul_local[1] + (left_matrix_shared[((((int)threadIdx.x) * 2) + k_inner)] * right_matrix_shared[((k_inner * 2) + 1)]));\n  }\n  T_matmul[(((int)threadIdx.x) * 2)] = T_matmul_local[0];\n  T_matmul[((((int)threadIdx.x) * 2) + 1)] = T_matmul_local[1];\n}\n\n\n    Input Tensor Shape: [[2, 2], [2, 2]]\n    Output C Code: void default_function_kernel(float* T_matmul, float* left_matrix, float* right_matrix) {\n  for (int32_t ax1_outer_outer_outer = 0; ax1_outer_outer_outer < 2; ++ax1_outer_outer_outer) {\n    for (int32_t ax0_inner_init = 0; ax0_inner_init < 2; ++ax0_inner_init) {\n      T_matmul[((ax0_inner_init * 2) + ax1_outer_outer_outer)] = 0.000000e+00f;\n    }\n    for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n      for (int32_t ax0_inner = 0; ax0_inner < 2; ++ax0_inner) {\n        T_matmul[((ax0_inner * 2) + ax1_outer_outer_outer)] = (T_matmul[((ax0_inner * 2) + ax1_outer_outer_outer)] + (left_matrix[((ax0_inner * 2) + k_inner)] * right_matrix[((k_inner * 2) + ax1_outer_outer_outer)]));\n      }\n    }\n  }\n}\n\n\n    //|End-of-Code|\n\n    Now translate the following CUDA code to its equivalent high-performance CPU C code:\n    Input CUDA Code: extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3, int64_t cse_var_1, int64_t i_0) {\n  int64_t first[1];\n  int64_t last[1];\n  int64_t first_1[1];\n  int64_t last_1[1];\n  if ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) < (int64_t)512) {\n    if (i_0 == (int64_t)0) {\n      first[0] = max((int64_t)0, ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1))));\n      last[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), (((int64_t)2 << cse_var_1) >> (int64_t)1));\n      while ((first[0] < last[0])) {\n        if (argsort_gpu_v0[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first[0] + last[0]) >> (int64_t)1))] <= argsort_gpu_v0[((((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) >> (int64_t)1)) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first[0] + last[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first[0] = (((first[0] + last[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last[0] = ((first[0] + last[0]) >> (int64_t)1);\n        }\n      }\n      first[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first[0]);\n      last[0] = ((((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last[0]);\n      for (int i_1 = 0; i_1 < ((int)min((((int64_t)2 << cse_var_1) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_1) {\n        if ((((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) && (last[0] < (((int64_t)2 << cse_var_1) * (((int64_t)((int)blockIdx.z)) + (int64_t)1)))) && (last[0] < (int64_t)512)) {\n          if (argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])] <= argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])]) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        } else {\n          if ((first[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first[0] < (int64_t)512)) {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first[0])];\n            first[0] = (first[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v2[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu_v0[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            argsort_gpu_v3[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_1))] = argsort_gpu[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last[0])];\n            last[0] = (last[0] + (int64_t)1);\n          }\n        }\n      }\n    } else {\n      first_1[0] = max((int64_t)0, ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - (int64_t)512));\n      last_1[0] = min((((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1)), min((((int64_t)2 << cse_var_1) >> (int64_t)1), ((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))));\n      while ((first_1[0] < last_1[0])) {\n        if (argsort_gpu_v2[(((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + ((first_1[0] + last_1[0]) >> (int64_t)1))] <= argsort_gpu_v2[(((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512)) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - ((first_1[0] + last_1[0]) >> (int64_t)1)) - (int64_t)1)]) {\n          first_1[0] = (((first_1[0] + last_1[0]) >> (int64_t)1) + (int64_t)1);\n        } else {\n          last_1[0] = ((first_1[0] + last_1[0]) >> (int64_t)1);\n        }\n      }\n      first_1[0] = ((((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))) + first_1[0]);\n      last_1[0] = ((min(((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))), (int64_t)512) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) - last_1[0]);\n      for (int i_2 = 0; i_2 < ((int)min((((int64_t)512 - (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) - (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))), (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))); ++i_2) {\n        if (((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) && (last_1[0] < (int64_t)512)) {\n          if (argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])] <= argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])]) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        } else {\n          if ((first_1[0] < ((((int64_t)2 << cse_var_1) >> (int64_t)1) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z))))) && (first_1[0] < (int64_t)512)) {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + first_1[0])];\n            first_1[0] = (first_1[0] + (int64_t)1);\n          } else {\n            argsort_gpu_v0[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v2[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            argsort_gpu[((((((int64_t)((int)blockIdx.y)) * (int64_t)512) + (((int64_t)2 << cse_var_1) * ((int64_t)((int)blockIdx.z)))) + (((int64_t)((int)threadIdx.x)) * (((((((int64_t)((int)((int64_t)2 << cse_var_1))) >= (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) >= (int64_t)0)) || ((((int64_t)((int)((int64_t)2 << cse_var_1))) < (int64_t)0) && (((((int64_t)2 << cse_var_1) - (int64_t)1) % ((int64_t)((int)((int64_t)2 << cse_var_1)))) <= (int64_t)0))) ? ((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) : (((((int64_t)2 << cse_var_1) - (int64_t)1) / ((int64_t)((int)((int64_t)2 << cse_var_1)))) - (int64_t)1)) + (int64_t)1))) + ((int64_t)i_2))] = argsort_gpu_v3[((((int64_t)((int)blockIdx.y)) * (int64_t)512) + last_1[0])];\n            last_1[0] = (last_1[0] + (int64_t)1);\n          }\n        }\n      }\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ data) {\n  if (((int)threadIdx.x) < 512) {\n    argsort_gpu_v0[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))] = data[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))];\n    argsort_gpu[((((int)blockIdx.y) * 512) + ((int)threadIdx.x))] = ((float)((int)threadIdx.x));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ argsort_gpu, float* __restrict__ argsort_gpu_v0, float* __restrict__ argsort_gpu_v2, float* __restrict__ argsort_gpu_v3) {\n  __shared__ float temp_keys_swap[128];\n  __shared__ float temp_values_swap[128];\n  float temp_cond1[1];\n  float temp_cond2[1];\n  float temp_keys[1];\n  float temp_values[1];\n  for (int i = 0; i < 2; ++i) {\n    temp_keys_swap[((((int)threadIdx.x) * 2) + i)] = argsort_gpu_v0[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + i)];\n    temp_values_swap[((((int)threadIdx.x) * 2) + i)] = argsort_gpu[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + i)];\n  }\n  __syncthreads();\n  for (int j = 0; j < 128; ++j) {\n    if (((((int)threadIdx.x) * 2) + (j & 1)) < 127) {\n      temp_cond1[0] = temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))];\n      temp_cond2[0] = temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];\n      if (temp_cond2[0] < temp_cond1[0]) {\n        temp_keys[0] = temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))];\n        temp_keys_swap[((((int)threadIdx.x) * 2) + (j & 1))] = temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];\n        temp_keys_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)] = temp_keys[0];\n        temp_values[0] = temp_values_swap[((((int)threadIdx.x) * 2) + (j & 1))];\n        temp_values_swap[((((int)threadIdx.x) * 2) + (j & 1))] = temp_values_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)];\n        temp_values_swap[(((((int)threadIdx.x) * 2) + (j & 1)) + 1)] = temp_values[0];\n      }\n    }\n    __syncthreads();\n  }\n  for (int k = 0; k < 2; ++k) {\n    argsort_gpu_v0[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_keys_swap[((((int)threadIdx.x) * 2) + k)];\n    argsort_gpu_v2[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_keys_swap[((((int)threadIdx.x) * 2) + k)];\n    argsort_gpu[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_values_swap[((((int)threadIdx.x) * 2) + k)];\n    argsort_gpu_v3[((((((int)blockIdx.y) * 512) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) * 2)) + k)] = temp_values_swap[((((int)threadIdx.x) * 2) + k)];\n  }\n}\n\n\n    Input Tensor Shape: [[64, 512]]\n    Print only a single C function implementation, ending with the comment '|End-of-Code|'.\n    "
    },
    {
        "op_name": "conv2d_opt",
        "c_code": "void default_function_kernel(float* data, float* kernel, float* output_unpack) {\n  float data_vec[4194304];\n  float kernel_vec[18432];\n  #pragma omp parallel for\n  for (int32_t bs_c_fused_h_fused = 0; bs_c_fused_h_fused < 4096; ++bs_c_fused_h_fused) {\n    for (int32_t w = 0; w < 256; ++w) {\n      for (int32_t vc = 0; vc < 4; ++vc) {\n        data_vec[(((bs_c_fused_h_fused * 1024) + (w * 4)) + vc)] = data[(((((bs_c_fused_h_fused >> 8) * 262144) + (vc * 65536)) + ((bs_c_fused_h_fused & 255) * 256)) + w)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t occ_k_h_fused = 0; occ_k_h_fused < 48; ++occ_k_h_fused) {\n    for (int32_t icc = 0; icc < 8; ++icc) {\n      for (int32_t k_w = 0; k_w < 3; ++k_w) {\n        for (int32_t icb = 0; icb < 4; ++icb) {\n          for (int32_t ocb = 0; ocb < 4; ++ocb) {\n            kernel_vec[(((((((occ_k_h_fused / 3) * 1152) + (icc * 144)) + ((occ_k_h_fused % 3) * 48)) + (k_w * 16)) + (icb * 4)) + ocb)] = kernel[(((((((occ_k_h_fused / 3) * 1152) + (ocb * 288)) + (icc * 36)) + (icb * 9)) + ((occ_k_h_fused % 3) * 3)) + k_w)];\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t n_c_outer_fused_h_fused = 0; n_c_outer_fused_h_fused < 8128; ++n_c_outer_fused_h_fused) {\n    float conv2d_NCHWc[1016];\n    float conv2d_NCHWc_global[8];\n    for (int32_t ow_outer = 0; ow_outer < 127; ++ow_outer) {\n      for (int32_t oc_block_c_init = 0; oc_block_c_init < 4; ++oc_block_c_init) {\n        conv2d_NCHWc_global[oc_block_c_init] = 0.000000e+00f;\n      }\n      for (int32_t oc_block_c_init_1 = 0; oc_block_c_init_1 < 4; ++oc_block_c_init_1) {\n        conv2d_NCHWc_global[(oc_block_c_init_1 + 4)] = 0.000000e+00f;\n      }\n      for (int32_t ic_outer = 0; ic_outer < 8; ++ic_outer) {\n        for (int32_t kh = 0; kh < 3; ++kh) {\n          for (int32_t kw = 0; kw < 3; ++kw) {\n            for (int32_t ic_inner = 0; ic_inner < 4; ++ic_inner) {\n              for (int32_t oc_block_c = 0; oc_block_c < 4; ++oc_block_c) {\n                conv2d_NCHWc_global[oc_block_c] = (conv2d_NCHWc_global[oc_block_c] + (data_vec[((((((((n_c_outer_fused_h_fused / 4064) * 2097152) + (ic_outer * 262144)) + (kh * 1024)) + ((n_c_outer_fused_h_fused % 254) * 1024)) + (ow_outer * 8)) + (kw * 4)) + ic_inner)] * kernel_vec[((((((((n_c_outer_fused_h_fused % 4064) / 254) * 1152) + (ic_outer * 144)) + (kh * 48)) + (kw * 16)) + (ic_inner * 4)) + oc_block_c)]));\n              }\n              for (int32_t oc_block_c_1 = 0; oc_block_c_1 < 4; ++oc_block_c_1) {\n                conv2d_NCHWc_global[(oc_block_c_1 + 4)] = (conv2d_NCHWc_global[(oc_block_c_1 + 4)] + (data_vec[(((((((((n_c_outer_fused_h_fused / 4064) * 2097152) + (ic_outer * 262144)) + (kh * 1024)) + ((n_c_outer_fused_h_fused % 254) * 1024)) + (ow_outer * 8)) + (kw * 4)) + ic_inner) + 4)] * kernel_vec[((((((((n_c_outer_fused_h_fused % 4064) / 254) * 1152) + (ic_outer * 144)) + (kh * 48)) + (kw * 16)) + (ic_inner * 4)) + oc_block_c_1)]));\n              }\n            }\n          }\n        }\n      }\n      for (int32_t ow_inner = 0; ow_inner < 2; ++ow_inner) {\n        for (int32_t oc_block = 0; oc_block < 4; ++oc_block) {\n          conv2d_NCHWc[(((ow_outer * 8) + (ow_inner * 4)) + oc_block)] = conv2d_NCHWc_global[((ow_inner * 4) + oc_block)];\n        }\n      }\n    }\n    for (int32_t w_outer = 0; w_outer < 127; ++w_outer) {\n      for (int32_t w_inner = 0; w_inner < 2; ++w_inner) {\n        for (int32_t c_inner = 0; c_inner < 4; ++c_inner) {\n          output_unpack[((((((n_c_outer_fused_h_fused / 254) * 258064) + (c_inner * 64516)) + ((n_c_outer_fused_h_fused % 254) * 254)) + (w_outer * 2)) + w_inner)] = conv2d_NCHWc[(((w_outer * 8) + (w_inner * 4)) + c_inner)];\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[8];\n  __shared__ float pad_temp_shared[32];\n  __shared__ float kernel_shared[576];\n  for (int yy_c_init = 0; yy_c_init < 2; ++yy_c_init) {\n    conv2d_nchw_local[yy_c_init] = 0.000000e+00f;\n    conv2d_nchw_local[(yy_c_init + 2)] = 0.000000e+00f;\n    conv2d_nchw_local[(yy_c_init + 4)] = 0.000000e+00f;\n    conv2d_nchw_local[(yy_c_init + 6)] = 0.000000e+00f;\n  }\n  for (int rc_outer = 0; rc_outer < 16; ++rc_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {\n      pad_temp_shared[(((((int)threadIdx.z) * 4) + (((int)threadIdx.x) * 2)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)] = data[(((((((((((int)blockIdx.z) >> 1) * 2097152) + (rc_outer * 131072)) + ((((int)threadIdx.z) >> 2) * 65536)) + (((int)blockIdx.y) * 512)) + ((((int)threadIdx.z) & 3) * 256)) + (((int)blockIdx.x) * 2)) + (((int)threadIdx.x) * 2)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)];\n    }\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 < 36; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1) {\n      kernel_shared[(((((int)threadIdx.z) * 72) + (((int)threadIdx.x) * 36)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1)] = kernel[(((((((((int)blockIdx.z) & 1) * 9216) + (((int)threadIdx.z) * 1152)) + (((int)threadIdx.x) * 576)) + ((ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 / 18) * 288)) + (rc_outer * 18)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 % 18))];\n    }\n    __syncthreads();\n    for (int rc_inner = 0; rc_inner < 2; ++rc_inner) {\n      for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n        for (int rx_inner = 0; rx_inner < 3; ++rx_inner) {\n          for (int yy_c = 0; yy_c < 2; ++yy_c) {\n            conv2d_nchw_local[yy_c] = (conv2d_nchw_local[yy_c] + (pad_temp_shared[(((((rc_inner * 16) + (yy_c * 4)) + (ry_inner * 4)) + ((int)threadIdx.x)) + rx_inner)] * kernel_shared[((((((int)threadIdx.z) * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner)]));\n            conv2d_nchw_local[(yy_c + 2)] = (conv2d_nchw_local[(yy_c + 2)] + (pad_temp_shared[(((((rc_inner * 16) + (yy_c * 4)) + (ry_inner * 4)) + ((int)threadIdx.x)) + rx_inner)] * kernel_shared[(((((((int)threadIdx.z) * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 144)]));\n            conv2d_nchw_local[(yy_c + 4)] = (conv2d_nchw_local[(yy_c + 4)] + (pad_temp_shared[(((((rc_inner * 16) + (yy_c * 4)) + (ry_inner * 4)) + ((int)threadIdx.x)) + rx_inner)] * kernel_shared[(((((((int)threadIdx.z) * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 288)]));\n            conv2d_nchw_local[(yy_c + 6)] = (conv2d_nchw_local[(yy_c + 6)] + (pad_temp_shared[(((((rc_inner * 16) + (yy_c * 4)) + (ry_inner * 4)) + ((int)threadIdx.x)) + rx_inner)] * kernel_shared[(((((((int)threadIdx.z) * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 432)]));\n          }\n        }\n      }\n    }\n  }\n  for (int yy_inner_inner_inner = 0; yy_inner_inner_inner < 2; ++yy_inner_inner_inner) {\n    conv2d_nchw[((((((((int)blockIdx.z) * 2064512) + (((int)threadIdx.z) * 64516)) + (((int)blockIdx.y) * 508)) + (yy_inner_inner_inner * 254)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.x))] = conv2d_nchw_local[yy_inner_inner_inner];\n    conv2d_nchw[(((((((((int)blockIdx.z) * 2064512) + (((int)threadIdx.z) * 64516)) + (((int)blockIdx.y) * 508)) + (yy_inner_inner_inner * 254)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.x)) + 516128)] = conv2d_nchw_local[(yy_inner_inner_inner + 2)];\n    conv2d_nchw[(((((((((int)blockIdx.z) * 2064512) + (((int)threadIdx.z) * 64516)) + (((int)blockIdx.y) * 508)) + (yy_inner_inner_inner * 254)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.x)) + 1032256)] = conv2d_nchw_local[(yy_inner_inner_inner + 4)];\n    conv2d_nchw[(((((((((int)blockIdx.z) * 2064512) + (((int)threadIdx.z) * 64516)) + (((int)blockIdx.y) * 508)) + (yy_inner_inner_inner * 254)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.x)) + 1548384)] = conv2d_nchw_local[(yy_inner_inner_inner + 6)];\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 32, 256, 256), \"float32\"), kernel: T.Buffer((64, 32, 3, 3), \"float32\"), output_unpack: T.Buffer((2, 64, 254, 254), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_vec = T.allocate([4194304], \"float32\", \"global\")\n        kernel_vec = T.allocate([18432], \"float32\", \"global\")\n        data_vec_1 = T.Buffer((4194304,), data=data_vec)\n        for bs_c_fused_h_fused in T.parallel(4096):\n            for w, vc in T.grid(256, 4):\n                data_1 = T.Buffer((4194304,), data=data.data)\n                data_vec_1[bs_c_fused_h_fused * 1024 + w * 4 + vc] = data_1[bs_c_fused_h_fused // 256 * 262144 + vc * 65536 + bs_c_fused_h_fused % 256 * 256 + w]\n        kernel_vec_1 = T.Buffer((18432,), data=kernel_vec)\n        for occ_k_h_fused in T.parallel(48):\n            for icc, k_w, icb, ocb in T.grid(8, 3, 4, 4):\n                cse_var_2: T.int32 = occ_k_h_fused % 3\n                cse_var_1: T.int32 = occ_k_h_fused // 3 * 1152\n                kernel_1 = T.Buffer((18432,), data=kernel.data)\n                kernel_vec_1[cse_var_1 + icc * 144 + cse_var_2 * 48 + k_w * 16 + icb * 4 + ocb] = kernel_1[cse_var_1 + ocb * 288 + icc * 36 + icb * 9 + cse_var_2 * 3 + k_w]\n        for n_c_outer_fused_h_fused in T.parallel(8128):\n            conv2d_NCHWc = T.allocate([1016], \"float32\", \"global\")\n            conv2d_NCHWc_global = T.allocate([8], \"float32\", \"global\")\n            conv2d_NCHWc_1 = T.Buffer((1016,), data=conv2d_NCHWc)\n            for ow_outer in range(127):\n                conv2d_NCHWc_global_1 = T.Buffer((8,), data=conv2d_NCHWc_global, align=32)\n                for oc_block_c_init in range(4):\n                    conv2d_NCHWc_global_1[oc_block_c_init] = T.float32(0)\n                for oc_block_c_init in range(4):\n                    conv2d_NCHWc_global_1[oc_block_c_init + 4] = T.float32(0)\n                for ic_outer, kh, kw, ic_inner in T.grid(8, 3, 3, 4):\n                    for oc_block_c in range(4):\n                        conv2d_NCHWc_global_1[oc_block_c] = conv2d_NCHWc_global_1[oc_block_c] + data_vec_1[n_c_outer_fused_h_fused // 4064 * 2097152 + ic_outer * 262144 + kh * 1024 + n_c_outer_fused_h_fused % 254 * 1024 + ow_outer * 8 + kw * 4 + ic_inner] * kernel_vec_1[n_c_outer_fused_h_fused % 4064 // 254 * 1152 + ic_outer * 144 + kh * 48 + kw * 16 + ic_inner * 4 + oc_block_c]\n                    for oc_block_c in range(4):\n                        cse_var_3: T.int32 = oc_block_c + 4\n                        conv2d_NCHWc_global_1[cse_var_3] = conv2d_NCHWc_global_1[cse_var_3] + data_vec_1[n_c_outer_fused_h_fused // 4064 * 2097152 + ic_outer * 262144 + kh * 1024 + n_c_outer_fused_h_fused % 254 * 1024 + ow_outer * 8 + kw * 4 + ic_inner + 4] * kernel_vec_1[n_c_outer_fused_h_fused % 4064 // 254 * 1152 + ic_outer * 144 + kh * 48 + kw * 16 + ic_inner * 4 + oc_block_c]\n                for ow_inner, oc_block in T.grid(2, 4):\n                    cse_var_4: T.int32 = ow_inner * 4\n                    conv2d_NCHWc_1[ow_outer * 8 + cse_var_4 + oc_block] = conv2d_NCHWc_global_1[cse_var_4 + oc_block]\n            for w_outer, w_inner, c_inner in T.grid(127, 2, 4):\n                output_unpack_1 = T.Buffer((8258048,), data=output_unpack.data)\n                output_unpack_1[n_c_outer_fused_h_fused // 254 * 258064 + c_inner * 64516 + n_c_outer_fused_h_fused % 254 * 254 + w_outer * 2 + w_inner] = conv2d_NCHWc_1[w_outer * 8 + w_inner * 4 + c_inner]",
        "op_args": "None",
        "input_shape": "[[2, 32, 256, 256], [64, 32, 3, 3]]",
        "output_shape": "[[2, 64, 254, 254]]",
        "input_name": [
            "data",
            "kernel"
        ],
        "output_name": [
            "output_unpack"
        ],
        "deepseek_c": "void default_function_kernel(float* conv2d_nchw, float* data, float* kernel) {\n  float conv2d_nchw_local[8];\n  for (int yy_c_init = 0; yy_c_init < 2; ++yy_c_init) {\n    conv2d_nchw_local[yy_c_init] = 0.000000e+00f;\n    conv2d_nchw_local[(yy_c_init + 2)] = 0.000000e+00f;\n    conv2d_nchw_local[(yy_c_init + 4)] = 0.000000e+00f;\n    conv2d_nchw_local[(yy_c_init + 6)] = 0.000000e+00f;\n  }\n  for (int rc_outer = 0; rc_outer < 16; ++rc_outer) {\n    for (int yy_c_init_1 = 0; yy_c_init_1 < 2; ++yy_c_init_1) {\n      for (int ry_outer = 0; ry_outer < 3; ++ry_outer) {\n        for (int rx_outer = 0; rx_outer < 3; ++rx_outer) {\n          for (int yy_c_inner = 0; yy_c_inner < 2; ++yy_c_inner) {\n            for (int rc_inner = 0; rc_inner < 2; ++rc_inner) {\n              conv2d_nchw_local[yy_c_inner] = (conv2d_nchw_local[yy_c_inner] + (data[(((((((((((int)blockIdx.z) >> 1) * 2097152) + (rc_outer * 131072)) + ((((int)threadIdx.z) >> 2) * 65536)) + (((int)blockIdx.y) * 512)) + ((((int)threadIdx.z) & 3) * 256)) + (((int)blockIdx.x) * 2)) + (((int)threadIdx.x) * 2)) + ((((rc_inner * 16) + (yy_c_inner * 4)) + (ry_outer * 4)) + rx_outer))] * kernel[((((((((((int)blockIdx.z) & 1) * 9216) + (((int)threadIdx.z) * 1152)) + (((int)threadIdx.x) * 576)) + ((rc_inner * 288) + (yy_c_init_1 * 144)) + (ry_outer * 48)) + (rx_outer * 16)) + (rc_inner * 9)) + (yy_c_init_1 * 3)) + rx_outer)]));\n              conv2d_nchw_local[(yy_c_inner + 2)] = (conv2d_nchw_local[(yy_c_inner + 2)] + (data[(((((((((((int)blockIdx.z) >> 1) * 2097152) + (rc_outer * 131072)) + ((((int)threadIdx.z) >> 2) * 65536)) + (((int)blockIdx.y) * 512)) + ((((int)threadIdx.z) & 3) * 256)) + (((int)blockIdx.x) * 2)) + (((int)threadIdx.x) * 2)) + ((((rc_inner * 16) + (yy_c_inner * 4)) + (ry_outer * 4)) + rx_outer))] * kernel[((((((((((int)blockIdx.z) & 1) * 9216) + (((int)threadIdx.z) * 1152)) + (((int)threadIdx.x) * 576)) + ((rc_inner * 288) + (yy_c_init_1 * 144)) + (ry_outer * 48)) + (rx_outer * 16)) + (rc_inner * 9)) + (yy_c_init_1 * 3)) + rx_outer) + 144)]));\n              conv2d_nchw_local[(yy_c_inner + 4)] = (conv2d_nchw_local[(yy_c_inner + 4)] + (data[(((((((((((int)blockIdx.z) >> 1) * 2097152) + (rc_outer * 131072)) + ((((int)threadIdx.z) >> 2) * 65536)) + (((int)blockIdx.y) * 512)) + ((((int)threadIdx.z) & 3) * 256)) + (((int)blockIdx.x) * 2)) + (((int)threadIdx.x) * 2)) + ((((rc_inner * 16) + (yy_c_inner * 4)) + (ry_outer * 4)) + rx_outer))] * kernel[((((((((((int)blockIdx.z) & 1) * 9216) + (((int)threadIdx.z) * 1152)) + (((int)threadIdx.x) * 576)) + ((rc_inner * 288) + (yy_c_init_1 * 144)) + (ry_outer * 48)) + (rx_outer * 16)) + (rc_inner * 9)) + (yy_c_init_1 * 3)) + rx_outer) + 288)]));\n              conv2d_nchw_local[(yy_c_inner + 6)] = (conv2d_nchw_local[(yy_c_inner + 6)] + (data[(((((((((((int)blockIdx.z) >> 1) * 2097152) + (rc_outer * 131072)) + ((((int)threadIdx.z) >> 2) * 65536)) + (((int)blockIdx.y) * 512)) + ((((int)threadIdx.z) & 3) * 256)) + (((int)blockIdx.x) * 2)) + (((int)threadIdx.x) * 2)) + ((((rc_inner * 16) + (yy_c_inner * 4)) + (ry_outer * 4)) + rx_outer))] * kernel[((((((((((int)blockIdx.z) & 1) * 9216) + (((int)threadIdx.z) * 1152)) + (((int)threadIdx.x) * 576)) + ((rc_inner * 288) + (yy_c_init_1 * 144)) + (ry_outer * 48)) + (rx_outer * 16)) + (rc_inner * 9)) + (yy_c_init_1 * 3)) + rx_outer) + 432)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  for (int yy_inner_inner_inner = 0; yy_inner_inner_inner < 2; ++yy_inner_inner_inner) {\n    conv2d_nchw[((((((((int)blockIdx.z) * 2064512) + (((int)threadIdx.z) * 64516)) + (((int)blockIdx.y) * 508)) + (yy_inner_inner_inner * 254)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.x))] = conv2d_nchw_local[yy_inner_inner_inner];\n    conv2d_nchw[(((((((((int)blockIdx.z) * 2064512) + (((int)threadIdx.z) * 64516)) + (((int)blockIdx.y) * 508)) + (yy_inner_inner_inner * 254)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.x)) + 516128)] = conv2d_nchw_local[(yy_inner_inner_inner + 2)];\n    conv2d_nchw[(((((((((int)blockIdx.z) * 2064512) + (((int)threadIdx.z) * 64516)) + (((int)blockIdx.y) * 508)) + (yy_inner_inner_inner * 254)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.x)) + 1032256)] = conv2d_nchw_local[(yy_inner_inner_inner + 4)];\n    conv2d_nchw[(((((((((int)blockIdx.z) * 2064512) + (((int)threadIdx.z) * 64516)) + (((int)blockIdx.y) * 508)) + (yy_inner_inner_inner * 254)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.x)) + 1548384)] = conv2d_nchw_local[(yy_inner_inner_inner + 6)];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* conv2d_nchw, float* data, float* kernel) {\n  #pragma omp parallel for\n  for (int32_t n_c_fused = 0; n_c_fused < 64; ++n_c_fused) {\n    float conv2d_nchw_local[16];\n    float pad_temp[2592];\n    float kernel_local[576];\n    for (int32_t n_c_fused_1 = 0; n_c_fused_1 < 16; ++n_c_fused_1) {\n      conv2d_nchw_local[n_c_fused_1] = 0.000000e+00f;\n    }\n    for (int32_t rc_outer_outer = 0; rc_outer_outer < 16; ++rc_outer_outer) {\n      for (int32_t rc_outer_inner = 0; rc_outer_inner < 2; ++rc_outer_inner) {\n        for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n          for (int32_t rx_outer = 0; rx_outer < 3; ++rx_outer) {\n            for (int32_t rc_inner = 0; rc_inner < 2; ++rc_inner) {\n              for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n                for (int32_t rx_inner = 0; rx_inner < 3; ++rx_inner) {\n                  for (int32_t rc_inner_inner = 0; rc_inner_inner < 2; ++rc_inner_inner) {\n                    conv2d_nchw_local[((rc_inner * 8) + (rc_inner_inner * 4))] = (conv2d_nchw_local[((rc_inner * 8) + (rc_inner_inner * 4))] + (data[((((((n_c_fused * 2097152) + (rc_outer_outer * 131072)) + (rc_inner * 65536)) + (ry_outer * 2048)) + (rc_outer_inner * 1024)) + (ry_inner * 32)) + (rc_inner_inner * 16))] * kernel[((((((n_c_fused_1 * 1152) + (rc_outer_outer * 18)) + (rc_inner * 9)) + (ry_outer * 3)) + (rc_outer_inner * 3)) + (ry_inner * 1)) + rx_inner]);\n                    conv2d_nchw_local[(((rc_inner * 8) + (rc_inner_inner * 4)) + 1)] = (conv2d_nchw_local[(((rc_inner * 8) + (rc_inner_inner * 4)) + 1)] + (data[((((((n_c_fused * 2097152) + (rc_outer_outer * 131072)) + (rc_inner * 65536)) + (ry_outer * 2048)) + (rc_outer_inner * 1024)) + (ry_inner * 32)) + (rc_inner_inner * 16))] * kernel[(((((((n_c_fused_1 * 1152) + (rc_outer_outer * 18)) + (rc_inner * 9)) + (ry_outer * 3)) + (rc_outer_inner * 3)) + (ry_inner * 1)) + rx_inner) + 144]);\n                    conv2d_nchw_local[(((rc_inner * 8) + (rc_inner_inner * 4)) + 2)] = (conv2d_nchw_local[(((rc_inner * 8) + (rc_inner_inner * 4)) + 2)] + (data[((((((n_c_fused * 2097152) + (rc_outer_outer * 131072)) + (rc_inner * 65536)) + (ry_outer * 2048)) + (rc_outer_inner * 1024)) + (ry_inner * 32)) + (rc_inner_inner * 16))] * kernel[(((((((n_c_fused_1 * 1152) + (rc_outer_outer * 18)) + (rc_inner * 9)) + (ry_outer * 3)) + (rc_outer_inner * 3)) + (ry_inner * 1)) + rx_inner) + 288]);\n                    conv2d_nchw_local[(((rc_inner * 8) + (rc_inner_inner * 4)) + 3)] = (conv2d_nchw_local[(((rc_inner * 8) + (rc_inner_inner * 4)) + 3)] + (data[((((((n_c_fused * 2097152) + (rc_outer_outer * 131072)) + (rc_inner * 65536)) + (ry_outer * 2048)) + (rc_outer_inner * 1024)) + (ry_inner * 32)) + (rc_inner_inner * 16))] * kernel[(((((((n_c_fused_1 * 1152) + (rc_outer_outer * 18)) + (rc_inner * 9)) + (ry_outer * 3)) + (rc_outer_inner * 3)) + (ry_inner * 1)) + rx_inner) + 432]);\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    for (int32_t n_inner = 0; n_inner < 2; ++n_inner) {\n      for (int32_t c_inner = 0; c_inner < 8; ++c_inner) {\n        conv2d_nchw[((((n_c_fused * 2064512) + (n_inner * 1032256)) + (c_inner * 254)) + (n_c_fused_1 * 16))] = conv2d_nchw_local[((n_inner * 8) + c_inner)];\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* conv2d_nchw, float* data, float* kernel) {\n  #pragma omp parallel for\n  for (int32_t n_c_fused_ax0_c_fused_ax1_fused = 0; n_c_fused_ax0_c_fused_ax1_fused < 64; ++n_c_fused_ax0_c_fused_ax1_fused) {\n    float conv2d_nchw_local[16];\n    for (int32_t ax1 = 0; ax1 < 32; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 256; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 256; ++ax3) {\n          for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n            conv2d_nchw_local[ax0_ax1_fused_ax2_fused_ax3_fused] = 0.000000e+00f;\n            conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused + 2)] = 0.000000e+00f;\n            conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused + 4)] = 0.000000e+00f;\n            conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused + 6)] = 0.000000e+00f;\n          }\n          for (int32_t rc_outer = 0; rc_outer < 16; ++rc_outer) {\n            for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner) {\n              for (int32_t rc_inner = 0; rc_inner < 2; ++rc_inner) {\n                for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n                  for (int32_t rx_inner = 0; rx_inner < 3; ++rx_inner) {\n                    for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {\n                      conv2d_nchw_local[ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner] = (conv2d_nchw_local[ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner] + (data[((((((((n_c_fused_ax0_c_fused_ax1_fused * 2097152) + (rc_outer * 131072)) + (ax1 * 65536)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner * 32768)) + (ax2 * 2048)) + (ax3 * 1024)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner * 512)) + (rc_inner * 256)) + ((ry_inner * 128) + (rx_inner * 64))) * kernel[((((((n_c_fused_ax0_c_fused_ax1_fused * 1152) + (ax1 * 288)) + (rc_outer * 18)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner * 9)) + (ry_inner * 3)) + rx_inner)]));\n                      conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner + 2)] = (conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner + 2)] + (data[((((((((n_c_fused_ax0_c_fused_ax1_fused * 2097152) + (rc_outer * 131072)) + (ax1 * 65536)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner * 32768)) + (ax2 * 2048)) + (ax3 * 1024)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner * 512)) + (rc_inner * 256)) + ((ry_inner * 128) + (rx_inner * 64))) * kernel[((((((n_c_fused_ax0_c_fused_ax1_fused * 1152) + (ax1 * 288)) + (rc_outer * 18)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner * 9)) + (ry_inner * 3)) + rx_inner) + 144)]);\n                      conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner + 4)] = (conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner + 4)] + (data[((((((((n_c_fused_ax0_c_fused_ax1_fused * 2097152) + (rc_outer * 131072)) + (ax1 * 65536)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner * 32768)) + (ax2 * 2048)) + (ax3 * 1024)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner * 512)) + (rc_inner * 256)) + ((ry_inner * 128) + (rx_inner * 64))) * kernel[((((((n_c_fused_ax0_c_fused_ax1_fused * 1152) + (ax1 * 288)) + (rc_outer * 18)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner * 9)) + (ry_inner * 3)) + rx_inner) + 288)]);\n                      conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner + 6)] = (conv2d_nchw_local[(ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner + 6)] + (data[((((((((n_c_fused_ax0_c_fused_ax1_fused * 2097152) + (rc_outer * 131072)) + (ax1 * 65536)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner * 32768)) + (ax2 * 2048)) + (ax3 * 1024)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner * 512)) + (rc_inner * 256)) + ((ry_inner * 128) + (rx_inner * 64))) * kernel[((((((n_c_fused_ax0_c_fused_ax1_fused * 1152) + (ax1 * 288)) + (rc_outer * 18)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner * 9)) + (ry_inner * 3)) + rx_inner) + 432)]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n          for (int32_t yy_inner_inner_inner = 0; yy_inner_inner_inner < 2; ++yy_inner_inner_inner) {\n            conv2d_nchw[((((((n_c_fused_ax0_c_fused_ax1_fused * 65536) + (ax1 * 32768)) + (ax2 * 2048)) + (ax3 * 1024)) + (yy_inner_inner_inner * 512)) + ((ax0_ax1_fused_ax2_fused_ax3_fused * 256) + ((ry_inner * 128) + (rx_inner * 64))))] = conv2d_nchw_local[yy_inner_inner_inner];\n            conv2d_nchw[((((((n_c_fused_ax0_c_fused_ax1_fused * 65536) + (ax1 * 32768)) + (ax2 * 2048)) + (ax3 * 1024)) + (yy_inner_inner_inner * 512)) + ((ax0_ax1_fused_ax2_fused_ax3_fused * 256) + ((ry_inner * 128) + (rx_inner * 64)))) + 516128] = conv2d_nchw_local[(yy_inner_inner_inner + 2)];\n            conv2d_nchw[((((((n_c_fused_ax0_c_fused_ax1_fused * 65536) + (ax1 * 32768)) + (ax2 * 2048)) + (ax3 * 1024)) + (yy_inner_inner_inner * 512)) + ((ax0_ax1_fused_ax2_fused_ax3_fused * 256) + ((ry_inner * 128) + (rx_inner * 64)))) + 1032256] = conv2d_nchw_local[(yy_inner_inner_inner + 4)];\n            conv2d_nchw[((((((n_c_fused_ax0_c_fused_ax1_fused * 65536) + (ax1 * 32768)) + (ax2 * 2048)) + (ax3 * 1024)) + (yy_inner_inner_inner * 512)) + ((ax0_ax1_fused_ax2_fused_ax3_fused * 256) + ((ry_inner * 128) + (rx_inner * 64)))) + 1548384] = conv2d_nchw_local[(yy_inner_inner_inner + 6)];\n          }\n        }\n      }\n    }\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* conv2d_nchw, float* data, float* kernel) {\n  #pragma omp parallel for\n  for (int block_z = 0; block_z < 2; ++block_z) {\n    for (int block_y = 0; block_y < 32; ++block_y) {\n      for (int block_x = 0; block_x < 32; ++block_x) {\n        for (int thread_z = 0; thread_z < 4; ++thread_z) {\n          for (int thread_x = 0; thread_x < 2; ++thread_x) {\n            float conv2d_nchw_local[8];\n            float pad_temp_shared[32];\n            float kernel_shared[576];\n            for (int yy_c_init = 0; yy_c_init < 2; ++yy_c_init) {\n              conv2d_nchw_local[yy_c_init] = 0.000000e+00f;\n              conv2d_nchw_local[(yy_c_init + 2)] = 0.000000e+00f;\n              conv2d_nchw_local[(yy_c_init + 4)] = 0.000000e+00f;\n              conv2d_nchw_local[(yy_c_init + 6)] = 0.000000e+00f;\n            }\n            for (int rc_outer = 0; rc_outer < 16; ++rc_outer) {\n              #pragma omp simd\n              for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {\n                pad_temp_shared[(((((thread_z * 4) + (thread_x * 2)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)] = data[(((((((((((block_z >> 1) * 2097152) + (rc_outer * 131072)) + (((thread_z >> 2) * 65536)) + (block_y * 512)) + (((thread_z & 3) * 256)) + ((block_x * 2)) + ((thread_x * 2)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)];\n              }\n              #pragma omp simd\n              for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 < 36; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1) {\n                kernel_shared[(((((thread_z * 72) + (thread_x * 36)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1)] = kernel[(((((((((block_z & 1) * 9216) + ((thread_z * 1152)) + ((thread_x * 576)) + ((ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 / 18) * 288)) + (rc_outer * 18)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 % 18))];\n              }\n              for (int rc_inner = 0; rc_inner < 2; ++rc_inner) {\n                for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n                  for (int rx_inner = 0; rx_inner < 3; ++rx_inner) {\n                    for (int yy_c = 0; yy_c < 2; ++yy_c) {\n                      conv2d_nchw_local[yy_c] = (conv2d_nchw_local[yy_c] + (pad_temp_shared[(((((rc_inner * 16) + (yy_c * 4)) + (ry_inner * 4)) + (thread_x)) + rx_inner)] * kernel_shared[(((((((thread_z * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner)]));\n                      conv2d_nchw_local[(yy_c + 2)] = (conv2d_nchw_local[(yy_c + 2)] + (pad_temp_shared[(((((rc_inner * 16) + (yy_c * 4)) + (ry_inner * 4)) + (thread_x)) + rx_inner)] * kernel_shared[(((((((thread_z * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 144)]));\n                      conv2d_nchw_local[(yy_c + 4)] = (conv2d_nchw_local[(yy_c + 4)] + (pad_temp_shared[(((((rc_inner * 16) + (yy_c * 4)) + (ry_inner * 4)) + (thread_x)) + rx_inner)] * kernel_shared[(((((((thread_z * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 288)]));\n                      conv2d_nchw_local[(yy_c + 6)] = (conv2d_nchw_local[(yy_c + 6)] + (pad_temp_shared[(((((rc_inner * 16) + (yy_c * 4)) + (ry_inner * 4)) + (thread_x)) + rx_inner)] * kernel_shared[(((((((thread_z * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 432)]));\n                    }\n                  }\n                }\n              }\n            }\n            for (int yy_inner_inner_inner = 0; yy_inner_inner_inner < 2; ++yy_inner_inner_inner) {\n              conv2d_nchw[((((((((block_z * 2064512) + ((thread_z * 64516)) + (block_y * 508)) + (yy_inner_inner_inner * 254)) + ((block_x * 2)) + (thread_x))] = conv2d_nchw_local[yy_inner_inner_inner];\n              conv2d_nchw[(((((((((block_z * 2064512) + ((thread_z * 64516)) + (block_y * 508)) + (yy_inner_inner_inner * 254)) + ((block_x * 2)) + (thread_x)) + 516128)] = conv2d_nchw_local[(yy_inner_inner_inner + 2)];\n              conv2d_nchw[(((((((((block_z * 2064512) + ((thread_z * 64516)) + (block_y * 508)) + (yy_inner_inner_inner * 254)) + ((block_x * 2)) + (thread_x)) + 1032256)] = conv2d_nchw_local[(yy_inner_inner_inner + 4)];\n              conv2d_nchw[(((((((((block_z * 2064512) + ((thread_z * 64516)) + (block_y * 508)) + (yy_inner_inner_inner * 254)) + ((block_x * 2)) + (thread_x)) + 1548384)] = conv2d_nchw_local[(yy_inner_inner_inner + 6)];\n            }\n          }\n        }\n      }\n    }\n  }\n}\n//End-of-Code|",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel(float* conv2d_nchw, float* data, float* kernel) {\n  #pragma omp parallel for\n  for (int block_z = 0; block_z < 2; ++block_z) {\n    for (int block_y = 0; block_y < 32; ++block_y) {\n      for (int thread_z = 0; thread_z < 16; ++thread_z) {\n        for (int block_x = 0; block_x < 32; ++block_x) {\n          for (int thread_x = 0; thread_x < 2; ++thread_x) {\n            float conv2d_nchw_local[8] = {0.0f};\n            float pad_temp_shared[32];\n            float kernel_shared[576];\n\n            for (int rc_outer = 0; rc_outer < 16; ++rc_outer) {\n              // Load data into shared memory\n              for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0;\n                   ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 2;\n                   ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {\n                pad_temp_shared[((thread_z * 4) + (thread_x * 2)) +\n                               ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner] =\n                    data[(((((((((block_z >> 1) * 2097152) + (rc_outer * 131072) + ((thread_z >> 2) * 65536) + (block_y * 512) +\n                                 ((thread_z & 3) * 256) + (block_x * 2) + (thread_x * 2)) +\n                                ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)];\n\n                kernel_shared[((thread_z * 72) + (thread_x * 36)) +\n                              ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1] =\n                    kernel[(((((((((block_z & 1) * 9216) + (thread_z * 1152) + (thread_x * 576) + ((ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 / 18) * 288)) +\n                                 (rc_outer * 18)) +\n                                (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 % 18))];\n              }\n\n              // Perform convolution\n              for (int rc_inner = 0; rc_inner < 2; ++rc_inner) {\n                for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n                  for (int rx_inner = 0; rx_inner < 3; ++rx_inner) {\n                    for (int yy_c = 0; yy_c < 2; ++yy_c) {\n                      conv2d_nchw_local[yy_c] += pad_temp_shared[((rc_inner * 16) + (yy_c * 4) + (ry_inner * 4) + thread_x) + rx_inner] *\n                                                 kernel_shared[(((thread_z * 18) + (rc_inner * 9) + (ry_inner * 3)) + rx_inner)];\n                      conv2d_nchw_local[yy_c + 2] += pad_temp_shared[((rc_inner * 16) + (yy_c * 4) + (ry_inner * 4) + thread_x) + rx_inner] *\n                                                    kernel_shared[(((thread_z * 18) + (rc_inner * 9) + (ry_inner * 3)) + rx_inner) + 144];\n                      conv2d_nchw_local[yy_c + 4] += pad_temp_shared[((rc_inner * 16) + (yy_c * 4) + (ry_inner * 4) + thread_x) + rx_inner] *\n                                                    kernel_shared[(((thread_z * 18) + (rc_inner * 9) + (ry_inner * 3)) + rx_inner) + 288];\n                      conv2d_nchw_local[yy_c + 6] += pad_temp_shared[((rc_inner * 16) + (yy_c * 4) + (ry_inner * 4) + thread_x) + rx_inner] *\n                                                    kernel_shared[(((thread_z * 18) + (rc_inner * 9) + (ry_inner * 3)) + rx_inner) + 432];\n                    }\n                  }\n                }\n              }\n            }\n\n            // Store results\n            for (int yy_inner_inner_inner = 0; yy_inner_inner_inner < 2; ++yy_inner_inner_inner) {\n              conv2d_nchw[((((((block_z * 2064512) + (thread_z * 64516) + (block_y * 508) + (yy_inner_inner_inner * 254) + (block_x * 2) +\n                               thread_x)) +\n                             516128 * yy_inner_inner_inner)];\n            }\n          }\n        }\n      }\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* conv2d_nchw, float* data, float* kernel) {\n    float conv2d_nchw_local[8];\n    float pad_temp_shared[32];\n    float kernel_shared[576];\n\n    #pragma omp parallel private(conv2d_nchw_local, pad_temp_shared, kernel_shared)\n    {\n        for (int yy_c_init = 0; yy_c_init < 2; ++yy_c_init) {\n            conv2d_nchw_local[yy_c_init] = 0.0f;\n            conv2d_nchw_local[yy_c_init + 2] = 0.0f;\n            conv2d_nchw_local[yy_c_init + 4] = 0.0f;\n            conv2d_nchw_local[yy_c_init + 6] = 0.0f;\n        }\n\n        for (int rc_outer = 0; rc_outer < 16; ++rc_outer) {\n            #pragma omp barrier\n            #pragma omp for collapse(2)\n            for (int thread_z = 0; thread_z < 8; ++thread_z) {\n                for (int thread_x = 0; thread_x < 2; ++thread_x) {\n                    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {\n                        pad_temp_shared[(thread_z * 4) + (thread_x * 2) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner] = data[((blockIdx_z >> 1) * 2097152 + rc_outer * 131072 + (thread_z >> 2) * 65536 + blockIdx_y * 512 + (thread_z & 3) * 256 + blockIdx_x * 2 + thread_x * 2 + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)];\n                    }\n                    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 < 36; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1) {\n                        kernel_shared[thread_z * 72 + thread_x * 36 + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1] = kernel[((blockIdx_z & 1) * 9216 + thread_z * 1152 + thread_x * 576 + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 / 18) * 288 + rc_outer * 18 + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 % 18)];\n                    }\n                }\n            }\n            #pragma omp barrier\n\n            for (int rc_inner = 0; rc_inner < 2; ++rc_inner) {\n                for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n                    for (int rx_inner = 0; rx_inner < 3; ++rx_inner) {\n                        for (int yy_c = 0; yy_c < 2; ++yy_c) {\n                            conv2d_nchw_local[yy_c] += pad_temp_shared[(rc_inner * 16 + yy_c * 4 + ry_inner * 4 + thread_x + rx_inner)] * kernel_shared[(thread_z * 18 + rc_inner * 9 + ry_inner * 3 + rx_inner)];\n                            conv2d_nchw_local[yy_c + 2] += pad_temp_shared[(rc_inner * 16 + yy_c * 4 + ry_inner * 4 + thread_x + rx_inner)] * kernel_shared[(thread_z * 18 + rc_inner * 9 + ry_inner * 3 + rx_inner + 144)];\n                            conv2d_nchw_local[yy_c + 4] += pad_temp_shared[(rc_inner * 16 + yy_c * 4 + ry_inner * 4 + thread_x + rx_inner)] * kernel_shared[(thread_z * 18 + rc_inner * 9 + ry_inner * 3 + rx_inner + 288)];\n                            conv2d_nchw_local[yy_c + 6] += pad_temp_shared[(rc_inner * 16 + yy_c * 4 + ry_inner * 4 + thread_x + rx_inner)] * kernel_shared[(thread_z * 18 + rc_inner * 9 + ry_inner * 3 + rx_inner + 432)];\n                        }\n                    }\n                }\n            }\n        }\n\n        #pragma omp for collapse(2)\n        for (int yy_inner_inner_inner = 0; yy_inner_inner_inner < 2; ++yy_inner_inner_inner) {\n            for (int thread_z = 0; thread_z < 8; ++thread_z) {\n                for (int thread_x = 0; thread_x < 2; ++thread_x) {\n                    conv2d_nchw[((blockIdx_z * 2064512 + thread_z * 64516 + blockIdx_y * 508 + yy_inner_inner_inner * 254 + blockIdx_x * 2 + thread_x))] = conv2d_nchw_local[yy_inner_inner_inner];\n                    conv2d_nchw[((blockIdx_z * 2064512 + thread_z * 64516 + blockIdx_y * 508 + yy_inner_inner_inner * 254 + blockIdx_x * 2 + thread_x) + 516128)] = conv2d_nchw_local[yy_inner_inner_inner + 2];\n                    conv2d_nchw[((blockIdx_z * 2064512 + thread_z * 64516 + blockIdx_y * 508 + yy_inner_inner_inner * 254 + blockIdx_x * 2 + thread_x) + 1032256)] = conv2d_nchw_local[yy_inner_inner_inner + 4];\n                    conv2d_nchw[((blockIdx_z * 2064512 + thread_z * 64516 + blockIdx_y * 508 + yy_inner_inner_inner * 254 + blockIdx_x * 2 + thread_x) + 1548384)] = conv2d_nchw_local[yy_inner_inner_inner + 6];\n                }\n            }\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* conv2d_nchw, float* data, float* kernel) {\n    int blockIdx_z, blockIdx_y, blockIdx_x, threadIdx_z, threadIdx_x;\n    int gridDim_z = 2, gridDim_y = 256, gridDim_x = 128;\n    int blockDim_z = 4, blockDim_x = 4;\n\n    #pragma omp parallel for collapse(3) private(blockIdx_z, blockIdx_y, blockIdx_x, threadIdx_z, threadIdx_x)\n    for (blockIdx_z = 0; blockIdx_z < gridDim_z; ++blockIdx_z) {\n        for (blockIdx_y = 0; blockIdx_y < gridDim_y; ++blockIdx_y) {\n            for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n                for (threadIdx_z = 0; threadIdx_z < blockDim_z; ++threadIdx_z) {\n                    for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n                        float conv2d_nchw_local[8] = {0.0f};\n                        float pad_temp_shared[32];\n                        float kernel_shared[576];\n\n                        for (int rc_outer = 0; rc_outer < 16; ++rc_outer) {\n                            for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {\n                                pad_temp_shared[(threadIdx_z * 4) + (threadIdx_x * 2) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner] = data[((((((blockIdx_z >> 1) * 2097152) + (rc_outer * 131072)) + ((threadIdx_z >> 2) * 65536)) + (blockIdx_y * 512)) + ((threadIdx_z & 3) * 256) + (blockIdx_x * 2) + (threadIdx_x * 2) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)];\n                            }\n\n                            for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 < 36; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1) {\n                                kernel_shared[(threadIdx_z * 72) + (threadIdx_x * 36) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1] = kernel[((((((blockIdx_z & 1) * 9216) + (threadIdx_z * 1152)) + (threadIdx_x * 576)) + ((ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 / 18) * 288)) + (rc_outer * 18)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 % 18)];\n                            }\n\n                            for (int rc_inner = 0; rc_inner < 2; ++rc_inner) {\n                                for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n                                    for (int rx_inner = 0; rx_inner < 3; ++rx_inner) {\n                                        for (int yy_c = 0; yy_c < 2; ++yy_c) {\n                                            conv2d_nchw_local[yy_c] += pad_temp_shared[(rc_inner * 16) + (yy_c * 4) + (ry_inner * 4) + threadIdx_x + rx_inner] * kernel_shared[(threadIdx_z * 18) + (rc_inner * 9) + (ry_inner * 3) + rx_inner];\n                                            conv2d_nchw_local[yy_c + 2] += pad_temp_shared[(rc_inner * 16) + (yy_c * 4) + (ry_inner * 4) + threadIdx_x + rx_inner] * kernel_shared[(threadIdx_z * 18) + (rc_inner * 9) + (ry_inner * 3) + rx_inner + 144];\n                                            conv2d_nchw_local[yy_c + 4] += pad_temp_shared[(rc_inner * 16) + (yy_c * 4) + (ry_inner * 4) + threadIdx_x + rx_inner] * kernel_shared[(threadIdx_z * 18) + (rc_inner * 9) + (ry_inner * 3) + rx_inner + 288];\n                                            conv2d_nchw_local[yy_c + 6] += pad_temp_shared[(rc_inner * 16) + (yy_c * 4) + (ry_inner * 4) + threadIdx_x + rx_inner] * kernel_shared[(threadIdx_z * 18) + (rc_inner * 9) + (ry_inner * 3) + rx_inner + 432];\n                                        }\n                                    }\n                                }\n                            }\n                        }\n\n                        for (int yy_inner_inner_inner = 0; yy_inner_inner_inner < 2; ++yy_inner_inner_inner) {\n                            conv2d_nchw[((((blockIdx_z * 2064512) + (threadIdx_z * 64516)) + (blockIdx_y * 508)) + (yy_inner_inner_inner * 254) + (blockIdx_x * 2) + threadIdx_x)] = conv2d_nchw_local[yy_inner_inner_inner];\n                            conv2d_nchw[((((blockIdx_z * 2064512) + (threadIdx_z * 64516)) + (blockIdx_y * 508)) + (yy_inner_inner_inner * 254) + (blockIdx_x * 2) + threadIdx_x + 516128)] = conv2d_nchw_local[yy_inner_inner_inner + 2];\n                            conv2d_nchw[((((blockIdx_z * 2064512) + (threadIdx_z * 64516)) + (blockIdx_y * 508)) + (yy_inner_inner_inner * 254) + (blockIdx_x * 2) + threadIdx_x + 1032256)] = conv2d_nchw_local[yy_inner_inner_inner + 4];\n                            conv2d_nchw[((((blockIdx_z * 2064512) + (threadIdx_z * 64516)) + (blockIdx_y * 508)) + (yy_inner_inner_inner * 254) + (blockIdx_x * 2) + threadIdx_x + 1548384)] = conv2d_nchw_local[yy_inner_inner_inner + 6];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* conv2d_nchw, float* data, float* kernel) {\n  #pragma omp parallel for\n  for (int32_t blockIdx_z = 0; blockIdx_z < 64; ++blockIdx_z) {\n    for (int32_t threadIdx_z = 0; threadIdx_z < 16; ++threadIdx_z) {\n      for (int32_t blockIdx_y = 0; blockIdx_y < 8; ++blockIdx_y) {\n        for (int32_t blockIdx_x = 0; blockIdx_x < 8; ++blockIdx_x) {\n          for (int32_t threadIdx_x = 0; threadIdx_x < 2; ++threadIdx_x) {\n            float conv2d_nchw_local[8];\n            for (int32_t yy_c_init = 0; yy_c_init < 2; ++yy_c_init) {\n              conv2d_nchw_local[yy_c_init] = 0.000000e+00f;\n              conv2d_nchw_local[(yy_c_init + 2)] = 0.000000e+00f;\n              conv2d_nchw_local[(yy_c_init + 4)] = 0.000000e+00f;\n              conv2d_nchw_local[(yy_c_init + 6)] = 0.000000e+00f;\n            }\n            for (int32_t rc_outer = 0; rc_outer < 16; ++rc_outer) {\n              float pad_temp_shared[32];\n              float kernel_shared[576];\n              for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {\n                pad_temp_shared[(((((threadIdx_z * 4) + (threadIdx_x * 2)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)))] = data[(((((((((((blockIdx_z >> 1) * 2097152) + (rc_outer * 131072)) + (((threadIdx_z >> 2) * 65536)) + (blockIdx_y * 512)) + (((threadIdx_z & 3) * 256)) + ((blockIdx_x * 2)) + (threadIdx_x * 2)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)))] ;\n              }\n              for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 < 36; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1) {\n                kernel_shared[(((((threadIdx_z * 72) + (threadIdx_x * 36)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1)))] = kernel[(((((((((blockIdx_z & 1) * 9216) + (threadIdx_z * 1152)) + (threadIdx_x * 576)) + ((ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 / 18) * 288)) + (rc_outer * 18)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner_1 % 18)))] ;\n              }\n              for (int32_t rc_inner = 0; rc_inner < 2; ++rc_inner) {\n                for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n                  for (int32_t rx_inner = 0; rx_inner < 3; ++rx_inner) {\n                    for (int32_t yy_c = 0; yy_c < 2; ++yy_c) {\n                      conv2d_nchw_local[yy_c] = (conv2d_nchw_local[yy_c] + (pad_temp_shared[(((((rc_inner * 16) + (yy_c * 4)) + (ry_inner * 4)) + threadIdx_x) + rx_inner))] * kernel_shared[((((((threadIdx_z * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner)))]));\n                      conv2d_nchw_local[(yy_c + 2)] = (conv2d_nchw_local[(yy_c + 2)] + (pad_temp_shared[(((((rc_inner * 16) + (yy_c * 4)) + (ry_inner * 4)) + threadIdx_x) + rx_inner))] * kernel_shared[((((((threadIdx_z * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 144))]));\n                      conv2d_nchw_local[(yy_c + 4)] = (conv2d_nchw_local[(yy_c + 4)] + (pad_temp_shared[(((((rc_inner * 16) + (yy_c * 4)) + (ry_inner * 4)) + threadIdx_x) + rx_inner))] * kernel_shared[((((((threadIdx_z * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 288))]));\n                      conv2d_nchw_local[(yy_c + 6)] = (conv2d_nchw_local[(yy_c + 6)] + (pad_temp_shared[(((((rc_inner * 16) + (yy_c * 4)) + (ry_inner * 4)) + threadIdx_x) + rx_inner))] * kernel_shared[((((((threadIdx_z * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 432))]));\n                    }\n                  }\n                }\n              }\n            }\n            for (int32_t yy_inner_inner_inner = 0; yy_inner_inner_inner < 2; ++yy_inner_inner_inner) {\n              conv2d_nchw[((((((((blockIdx_z * 2064512) + (threadIdx_z * 64516)) + (blockIdx_y * 508)) + (yy_inner_inner_inner * 254)) + (blockIdx_x * 2)) + threadIdx_x)))] = conv2d_nchw_local[yy_inner_inner_inner];\n              conv2d_nchw[(((((((((blockIdx_z * 2064512) + (threadIdx_z * 64516)) + (blockIdx_y * 508)) + (yy_inner_inner_inner * 254)) + (blockIdx_x * 2)) + threadIdx_x) + 516128)))] = conv2d_nchw_local[(yy_inner_inner_inner + 2)];\n              conv2d_nchw[(((((((((blockIdx_z * 2064512) + (threadIdx_z * 64516)) + (blockIdx_y * 508)) + (yy_inner_inner_inner * 254)) + (blockIdx_x * 2)) + threadIdx_x) + 1032256)))] = conv2d_nchw_local[(yy_inner_inner_inner + 4)];\n              conv2d_nchw[(((((((((blockIdx_z * 2064512) + (threadIdx_z * 64516)) + (blockIdx_y * 508)) + (yy_inner_inner_inner * 254)) + (blockIdx_x * 2)) + threadIdx_x) + 1548384)))] = conv2d_nchw_local[(yy_inner_inner_inner + 6)];\n            }\n          }\n        }\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "dft",
        "c_code": "void default_function_kernel(float* Im, float* Re, float* dft_cpu, float* dft_cpu_1) {\n  #pragma omp parallel for\n  for (int32_t i = 0; i < 1200; ++i) {\n    for (int32_t j = 0; j < 50; ++j) {\n      dft_cpu[((i * 50) + j)] = 0.000000e+00f;\n      dft_cpu_1[((i * 50) + j)] = 0.000000e+00f;\n      for (int32_t k = 0; k < 50; ++k) {\n        float cse_var_2 = (((-6.283185e+00f * ((float)j)) * 2.000000e-02f) * ((float)k));\n        dft_cpu[((i * 50) + j)] = (dft_cpu[((i * 50) + j)] + ((Re[((i * 50) + k)] * cosf(cse_var_2)) - (Im[((i * 50) + k)] * sinf(cse_var_2))));\n        dft_cpu_1[((i * 50) + j)] = (dft_cpu_1[((i * 50) + j)] + ((Re[((i * 50) + k)] * sinf(cse_var_2)) + (Im[((i * 50) + k)] * cosf(cse_var_2))));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ Im, float* __restrict__ Re, float* __restrict__ dft_cuda, float* __restrict__ dft_cuda_1) {\n  if (((((int)blockIdx.x) * 64) + (((int)threadIdx.x) >> 4)) < 75) {\n    for (int i = 0; i < 50; ++i) {\n      dft_cuda[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + i)] = 0.000000e+00f;\n      dft_cuda_1[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + i)] = 0.000000e+00f;\n      for (int j = 0; j < 50; ++j) {\n        float cse_var_1 = (((-6.283185e+00f * ((float)i)) * 2.000000e-02f) * ((float)j));\n        dft_cuda[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + i)] = (dft_cuda[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + i)] + ((Re[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + j)] * __cosf(cse_var_1)) - (Im[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + j)] * __sinf(cse_var_1))));\n        dft_cuda_1[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + i)] = (dft_cuda_1[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + i)] + ((Re[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + j)] * __sinf(cse_var_1)) + (Im[(((((int)blockIdx.x) * 51200) + (((int)threadIdx.x) * 50)) + j)] * __cosf(cse_var_1))));\n      }\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(Re: T.Buffer((30, 40, 50), \"float32\"), Im: T.Buffer((30, 40, 50), \"float32\"), dft_cpu: T.Buffer((30, 40, 50), \"float32\"), dft_cpu_1: T.Buffer((30, 40, 50), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T.attr(0, \"extern_scope\", 0)\n        for i in T.parallel(1200):\n            for j in range(50):\n                cse_var_1: T.int32 = i * 50 + j\n                dft_cpu_2 = T.Buffer((60000,), data=dft_cpu.data)\n                dft_cpu_2[cse_var_1] = T.float32(0)\n                dft_cpu_3 = T.Buffer((60000,), data=dft_cpu_1.data)\n                dft_cpu_3[cse_var_1] = T.float32(0)\n                for k in range(50):\n                    cse_var_3: T.int32 = i * 50 + k\n                    cse_var_2: T.float32 = T.float32(-6.2831853071795862) * T.Cast(\"float32\", j) * T.float32(0.02) * T.Cast(\"float32\", k)\n                    Re_1 = T.Buffer((60000,), data=Re.data)\n                    Im_1 = T.Buffer((60000,), data=Im.data)\n                    dft_cpu_2[cse_var_1] = dft_cpu_2[cse_var_1] + (Re_1[cse_var_3] * T.cos(cse_var_2) - Im_1[cse_var_3] * T.sin(cse_var_2))\n                    dft_cpu_3[cse_var_1] = dft_cpu_3[cse_var_1] + (Re_1[cse_var_3] * T.sin(cse_var_2) + Im_1[cse_var_3] * T.cos(cse_var_2))",
        "op_args": "None",
        "input_shape": "[[30, 40, 50], [30, 40, 50]]",
        "output_shape": "[[30, 40, 50], [30, 40, 50]]",
        "input_name": [
            "Re",
            "Im"
        ],
        "output_name": [
            "dft_cpu.v0",
            "dft_cpu.v1"
        ],
        "deepseek_c": "void default_function_kernel(float* dft_cuda, float* dft_cuda_1, float* Re, float* Im) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 60000; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 50; ++ax3) {\n      float cse_var_1 = (-6.283185e+00f * ((float)(ax3)));\n      dft_cuda[((ax0_ax1_fused_ax2_fused * 50) + ax3)] = 0.000000e+00f;\n      dft_cuda_1[((ax0_ax1_fused_ax2_fused * 50) + ax3)] = 0.000000e+00f;\n      for (int32_t j = 0; j < 50; ++j) {\n        dft_cuda[((ax0_ax1_fused_ax2_fused * 50) + ax3)] = (dft_cuda[((ax0_ax1_fused_ax2_fused * 50) + ax3)] + ((Re[((ax0_ax1_fused_ax2_fused * 50) + j)] * cosf(cse_var_1)) - (Im[((ax0_ax1_fused_ax2_fused * 50) + j)] * sinf(cse_var_1))));\n        dft_cuda_1[((ax0_ax1_fused_ax2_fused * 50) + ax3)] = (dft_cuda_1[((ax0_ax1_fused_ax2_fused * 50) + ax3)] + ((Re[((ax0_ax1_fused_ax2_fused * 50) + j)] * sinf(cse_var_1)) + (Im[((ax0_ax1_fused_ax2_fused * 50) + j)] * cosf(cse_var_1))));\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* Im, float* Re, float* dft_cuda, float* dft_cuda_1) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 60000; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 50; ++i3) {\n      dft_cuda[i0_i1_fused_i2_fused] = 0.000000e+00f;\n      dft_cuda_1[i0_i1_fused_i2_fused] = 0.000000e+00f;\n      for (int32_t j = 0; j < 50; ++j) {\n        float cse_var_1 = (((-6.283185e+00f * ((float)i3)) * 2.000000e-02f) * ((float)j));\n        dft_cuda[i0_i1_fused_i2_fused] = (dft_cuda[i0_i1_fused_i2_fused] + ((Re[((i0_i1_fused_i2_fused / 50) * 2000 + (i3 * 50) + j)] * cosf(cse_var_1)) - (Im[((i0_i1_fused_i2_fused / 50) * 2000 + (i3 * 50) + j)] * sinf(cse_var_1))));\n        dft_cuda_1[i0_i1_fused_i2_fused] = (dft_cuda_1[i0_i1_fused_i2_fused] + ((Re[((i0_i1_fused_i2_fused / 50) * 2000 + (i3 * 50) + j)] * sinf(cse_var_1)) + (Im[((i0_i1_fused_i2_fused / 50) * 2000 + (i3 * 50) + j)] * cosf(cse_var_1))));\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* Im, float* Re, float* dft_cuda, float* dft_cuda_1) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 7500; ++i0_i1_fused_i2_fused) {\n    for (int32_t i0_i1_fused = 0; i0_i1_fused < 1200; ++i0_i1_fused) {\n      dft_cuda[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] = 0.000000e+00f;\n      dft_cuda_1[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] = 0.000000e+00f;\n      for (int32_t j = 0; j < 50; ++j) {\n        float cse_var_1 = (((-6.283185e+00f * ((float)i0_i1_fused_i2_fused)) * 2.000000e-02f) * ((float)j));\n        dft_cuda[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] = (dft_cuda[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] + ((Re[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] * __cosf(cse_var_1)) - (Im[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] * __sinf(cse_var_1))));\n        dft_cuda_1[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] = (dft_cuda_1[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] + ((Re[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] * __sinf(cse_var_1)) + (Im[((i0_i1_fused * 7500) + i0_i1_fused_i2_fused)] * __cosf(cse_var_1))));\n      }\n    }\n  }\n}",
        "gemini1_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* Im, float* Re, float* dft_cuda, float* dft_cuda_1) {\n  #pragma omp parallel for\n  for (int block_x = 0; block_x < 30; ++block_x) {\n    for (int thread_x = 0; thread_x < 1024; ++thread_x) {\n      if (((block_x * 64) + (thread_x >> 4)) < 75) {\n        for (int i = 0; i < 50; ++i) {\n          dft_cuda[((block_x * 51200) + (thread_x * 50) + i)] = 0.000000e+00f;\n          dft_cuda_1[((block_x * 51200) + (thread_x * 50) + i)] = 0.000000e+00f;\n          for (int j = 0; j < 50; ++j) {\n            float cse_var_1 = (((-6.283185e+00f * ((float)i)) * 2.000000e-02f) * ((float)j));\n            dft_cuda[((block_x * 51200) + (thread_x * 50) + i)] = (dft_cuda[((block_x * 51200) + (thread_x * 50) + i)] + ((Re[((block_x * 51200) + (thread_x * 50) + j)] * cosf(cse_var_1)) - (Im[((block_x * 51200) + (thread_x * 50) + j)] * sinf(cse_var_1))));\n            dft_cuda_1[((block_x * 51200) + (thread_x * 50) + i)] = (dft_cuda_1[((block_x * 51200) + (thread_x * 50) + i)] + ((Re[((block_x * 51200) + (thread_x * 50) + j)] * sinf(cse_var_1)) + (Im[((block_x * 51200) + (thread_x * 50) + j)] * cosf(cse_var_1))));\n          }\n        }\n      }\n    }\n  }\n}\n//End-of-Code|",
        "gemini2_c": "void default_function_kernel(float* Im, float* Re, float* dft_cuda, float* dft_cuda_1) {\n  for (int blockIdx_x = 0; blockIdx_x < 1; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 1024; ++threadIdx_x) {\n      if (((((int)blockIdx_x) * 64) + (((int)threadIdx_x) >> 4)) < 75) {\n        for (int i = 0; i < 50; ++i) {\n          dft_cuda[(((((int)blockIdx_x) * 51200) + (((int)threadIdx_x) * 50)) + i)] = 0.000000e+00f;\n          dft_cuda_1[(((((int)blockIdx_x) * 51200) + (((int)threadIdx_x) * 50)) + i)] = 0.000000e+00f;\n          for (int j = 0; j < 50; ++j) {\n            float cse_var_1 = (((-6.283185e+00f * ((float)i)) * 2.000000e-02f) * ((float)j));\n            dft_cuda[(((((int)blockIdx_x) * 51200) + (((int)threadIdx_x) * 50)) + i)] = (dft_cuda[(((((int)blockIdx_x) * 51200) + (((int)threadIdx_x) * 50)) + i)] + ((Re[(((((int)blockIdx_x) * 51200) + (((int)threadIdx_x) * 50)) + j)] * cosf(cse_var_1)) - (Im[(((((int)blockIdx_x) * 51200) + (((int)threadIdx_x) * 50)) + j)] * sinf(cse_var_1))));\n            dft_cuda_1[(((((int)blockIdx_x) * 51200) + (((int)threadIdx_x) * 50)) + i)] = (dft_cuda_1[(((((int)blockIdx_x) * 51200) + (((int)threadIdx_x) * 50)) + i)] + ((Re[(((((int)blockIdx_x) * 51200) + (((int)threadIdx_x) * 50)) + j)] * sinf(cse_var_1)) + (Im[(((((int)blockIdx_x) * 51200) + (((int)threadIdx_x) * 50)) + j)] * cosf(cse_var_1))));\n          }\n        }\n      }\n    }\n  }\n}\n// |End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* Im, float* Re, float* dft_cuda, float* dft_cuda_1) {\n    int num_blocks = 30; // Assume 30 blocks based on input tensor shape\n    int threads_per_block = 1024;\n    int elements_per_thread = 50;\n    int block_size = 51200;\n\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < num_blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads_per_block; ++threadIdx_x) {\n            if (((blockIdx_x * 64) + (threadIdx_x >> 4)) < 75) {\n                for (int i = 0; i < elements_per_thread; ++i) {\n                    int index = (blockIdx_x * block_size) + (threadIdx_x * elements_per_thread) + i;\n                    dft_cuda[index] = 0.0f;\n                    dft_cuda_1[index] = 0.0f;\n                    for (int j = 0; j < elements_per_thread; ++j) {\n                        float cse_var_1 = ((-6.283185f * ((float)i)) * 0.02f) * ((float)j);\n                        dft_cuda[index] += (Re[(blockIdx_x * block_size) + (threadIdx_x * elements_per_thread) + j] * cosf(cse_var_1)) -\n                                           (Im[(blockIdx_x * block_size) + (threadIdx_x * elements_per_thread) + j] * sinf(cse_var_1));\n                        dft_cuda_1[index] += (Re[(blockIdx_x * block_size) + (threadIdx_x * elements_per_thread) + j] * sinf(cse_var_1)) +\n                                             (Im[(blockIdx_x * block_size) + (threadIdx_x * elements_per_thread) + j] * cosf(cse_var_1));\n                    }\n                }\n            }\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* Im, float* Re, float* dft_cuda, float* dft_cuda_1) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 30;\n    int blockDim_x = 1024;\n\n    #pragma omp parallel for collapse(2) private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int global_idx = (blockIdx_x * 64) + (threadIdx_x >> 4);\n            if (global_idx < 75) {\n                for (int i = 0; i < 50; ++i) {\n                    int idx = (blockIdx_x * 51200) + (threadIdx_x * 50) + i;\n                    dft_cuda[idx] = 0.0f;\n                    dft_cuda_1[idx] = 0.0f;\n                    for (int j = 0; j < 50; ++j) {\n                        float cse_var_1 = (-6.283185f * (float)i * 0.02f * (float)j);\n                        dft_cuda[idx] += (Re[(blockIdx_x * 51200) + (threadIdx_x * 50) + j] * cosf(cse_var_1)) - (Im[(blockIdx_x * 51200) + (threadIdx_x * 50) + j] * sinf(cse_var_1));\n                        dft_cuda_1[idx] += (Re[(blockIdx_x * 51200) + (threadIdx_x * 50) + j] * sinf(cse_var_1)) + (Im[(blockIdx_x * 51200) + (threadIdx_x * 50) + j] * cosf(cse_var_1));\n                    }\n                }\n            }\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* Im, float* Re, float* dft_cuda, float* dft_cuda_1) {\n  #pragma omp parallel for\n  for (int32_t blockIdx_x = 0; blockIdx_x < 30; ++blockIdx_x) {\n    for (int32_t threadIdx_x = 0; threadIdx_x < 1024; ++threadIdx_x) {\n      if (((blockIdx_x * 64) + (threadIdx_x >> 4)) < 75) {\n        for (int32_t i = 0; i < 50; ++i) {\n          dft_cuda[((blockIdx_x * 51200) + (threadIdx_x * 50) + i)] = 0.000000e+00f;\n          dft_cuda_1[((blockIdx_x * 51200) + (threadIdx_x * 50) + i)] = 0.000000e+00f;\n          for (int32_t j = 0; j < 50; ++j) {\n            float cse_var_1 = (((-6.283185e+00f * ((float)i)) * 2.000000e-02f) * ((float)j));\n            dft_cuda[((blockIdx_x * 51200) + (threadIdx_x * 50) + i)] = (dft_cuda[((blockIdx_x * 51200) + (threadIdx_x * 50) + i)] + ((Re[((blockIdx_x * 51200) + (threadIdx_x * 50) + j)] * cosf(cse_var_1)) - (Im[((blockIdx_x * 51200) + (threadIdx_x * 50) + j)] * sinf(cse_var_1))));\n            dft_cuda_1[((blockIdx_x * 51200) + (threadIdx_x * 50) + i)] = (dft_cuda_1[((blockIdx_x * 51200) + (threadIdx_x * 50) + i)] + ((Re[((blockIdx_x * 51200) + (threadIdx_x * 50) + j)] * sinf(cse_var_1)) + (Im[((blockIdx_x * 51200) + (threadIdx_x * 50) + j)] * cosf(cse_var_1))));\n          }\n        }\n      }\n    }\n  }\n}\n"
    },
    {
        "op_name": "group_conv2d_opt",
        "c_code": "void default_function_kernel(float* A, float* W, float* output_unpack) {\n  float data_vec[8388608];\n  float kernel_vec[4608];\n  float conv_global[16516096];\n  for (int32_t g = 0; g < 4; ++g) {\n    #pragma omp parallel for\n    for (int32_t n_C_fused_h_fused = 0; n_C_fused_h_fused < 2048; ++n_C_fused_h_fused) {\n      for (int32_t c = 0; c < 4; ++c) {\n        for (int32_t w = 0; w < 256; ++w) {\n          data_vec[((((g * 2097152) + (n_C_fused_h_fused * 1024)) + (c * 256)) + w)] = A[(((((((n_C_fused_h_fused >> 9) * 2097152) + (g * 524288)) + (((n_C_fused_h_fused & 511) >> 8) * 262144)) + (c * 65536)) + ((n_C_fused_h_fused & 255) * 256)) + w)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t g_out_channel_fused_h_fused = 0; g_out_channel_fused_h_fused < 48; ++g_out_channel_fused_h_fused) {\n    for (int32_t in_channel = 0; in_channel < 2; ++in_channel) {\n      for (int32_t w_1 = 0; w_1 < 3; ++w_1) {\n        for (int32_t ci = 0; ci < 4; ++ci) {\n          for (int32_t co = 0; co < 4; ++co) {\n            kernel_vec[(((((((g_out_channel_fused_h_fused / 3) * 288) + (in_channel * 144)) + ((g_out_channel_fused_h_fused % 3) * 48)) + (w_1 * 16)) + (ci * 4)) + co)] = W[(((((((g_out_channel_fused_h_fused / 3) * 288) + (co * 72)) + (in_channel * 36)) + (ci * 9)) + ((g_out_channel_fused_h_fused % 3) * 3)) + w_1)];\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused = 0; g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused < 16256; ++g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused) {\n    for (int32_t ow_c_outer = 0; ow_c_outer < 127; ++ow_c_outer) {\n      for (int32_t oc_block_c_init = 0; oc_block_c_init < 4; ++oc_block_c_init) {\n        conv_global[(((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016) + (ow_c_outer * 8)) + oc_block_c_init)] = 0.000000e+00f;\n      }\n      for (int32_t oc_block_c_init_1 = 0; oc_block_c_init_1 < 4; ++oc_block_c_init_1) {\n        conv_global[((((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016) + (ow_c_outer * 8)) + oc_block_c_init_1) + 4)] = 0.000000e+00f;\n      }\n      for (int32_t ic_outer = 0; ic_outer < 2; ++ic_outer) {\n        for (int32_t kh = 0; kh < 3; ++kh) {\n          for (int32_t kw = 0; kw < 3; ++kw) {\n            for (int32_t ic_inner = 0; ic_inner < 4; ++ic_inner) {\n              for (int32_t oc_block_c = 0; oc_block_c < 4; ++oc_block_c) {\n                conv_global[(((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016) + (ow_c_outer * 8)) + oc_block_c)] = (conv_global[(((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016) + (ow_c_outer * 8)) + oc_block_c)] + (data_vec[((((((((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused / 1016) * 524288) + (ic_outer * 262144)) + (kh * 1024)) + ((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused % 254) * 1024)) + (ic_inner * 256)) + (ow_c_outer * 2)) + kw)] * kernel_vec[((((((((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused / 4064) * 1152) + (((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused % 1016) / 254) * 288)) + (ic_outer * 144)) + (kh * 48)) + (kw * 16)) + (ic_inner * 4)) + oc_block_c)]));\n              }\n              for (int32_t oc_block_c_1 = 0; oc_block_c_1 < 4; ++oc_block_c_1) {\n                conv_global[((((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016) + (ow_c_outer * 8)) + oc_block_c_1) + 4)] = (conv_global[((((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016) + (ow_c_outer * 8)) + oc_block_c_1) + 4)] + (data_vec[(((((((((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused / 1016) * 524288) + (ic_outer * 262144)) + (kh * 1024)) + ((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused % 254) * 1024)) + (ic_inner * 256)) + (ow_c_outer * 2)) + kw) + 1)] * kernel_vec[((((((((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused / 4064) * 1152) + (((g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused % 1016) / 254) * 288)) + (ic_outer * 144)) + (kh * 48)) + (kw * 16)) + (ic_inner * 4)) + oc_block_c_1)]));\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n  for (int32_t n = 0; n < 4; ++n) {\n    #pragma omp parallel for\n    for (int32_t c_outer_h_fused = 0; c_outer_h_fused < 4064; ++c_outer_h_fused) {\n      for (int32_t w_outer = 0; w_outer < 127; ++w_outer) {\n        for (int32_t w_inner = 0; w_inner < 2; ++w_inner) {\n          for (int32_t c_inner = 0; c_inner < 4; ++c_inner) {\n            output_unpack[((((((n * 4129024) + ((c_outer_h_fused / 254) * 258064)) + (c_inner * 64516)) + ((c_outer_h_fused % 254) * 254)) + (w_outer * 2)) + w_inner)] = conv_global[(((((((c_outer_h_fused / 1016) * 4129024) + (n * 1032256)) + ((c_outer_h_fused % 1016) * 1016)) + (w_outer * 8)) + (w_inner * 4)) + c_inner)];\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ A, float* __restrict__ W, float* __restrict__ group_conv2d_nchw) {\n  float group_conv2d_nchw_local[1];\n  __shared__ float pad_temp_shared[1];\n  __shared__ float W_shared[1];\n  group_conv2d_nchw_local[0] = 0.000000e+00f;\n  for (int rc_outer = 0; rc_outer < 8; ++rc_outer) {\n    for (int ry_outer = 0; ry_outer < 3; ++ry_outer) {\n      for (int rx_outer = 0; rx_outer < 3; ++rx_outer) {\n        __syncthreads();\n        pad_temp_shared[0] = A[(((((((((int)blockIdx.z) * 2097152) + ((((int)blockIdx.y) >> 4) * 524288)) + (rc_outer * 65536)) + ((((int)blockIdx.x) / 254) * 256)) + (ry_outer * 256)) + rx_outer) + (((int)blockIdx.x) % 254))];\n        W_shared[0] = W[((((((int)blockIdx.y) * 72) + (rc_outer * 9)) + (ry_outer * 3)) + rx_outer)];\n        __syncthreads();\n        group_conv2d_nchw_local[0] = (group_conv2d_nchw_local[0] + (pad_temp_shared[0] * W_shared[0]));\n      }\n    }\n  }\n  group_conv2d_nchw[(((((int)blockIdx.z) * 4129024) + (((int)blockIdx.y) * 64516)) + ((int)blockIdx.x))] = group_conv2d_nchw_local[0];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(A: T.Buffer((4, 32, 256, 256), \"float32\"), W: T.Buffer((64, 8, 3, 3), \"float32\"), output_unpack: T.Buffer((4, 64, 254, 254), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_vec = T.allocate([8388608], \"float32\", \"global\")\n        kernel_vec = T.allocate([4608], \"float32\", \"global\")\n        conv_global = T.allocate([16516096], \"float32\", \"global\")\n        data_vec_1 = T.Buffer((8388608,), data=data_vec)\n        for g in range(4):\n            for n_C_fused_h_fused in T.parallel(2048):\n                for c, w in T.grid(4, 256):\n                    A_1 = T.Buffer((8388608,), data=A.data)\n                    data_vec_1[g * 2097152 + n_C_fused_h_fused * 1024 + c * 256 + w] = A_1[n_C_fused_h_fused // 512 * 2097152 + g * 524288 + n_C_fused_h_fused % 512 // 256 * 262144 + c * 65536 + n_C_fused_h_fused % 256 * 256 + w]\n        kernel_vec_1 = T.Buffer((4608,), data=kernel_vec)\n        for g_out_channel_fused_h_fused in T.parallel(48):\n            for in_channel, w, ci, co in T.grid(2, 3, 4, 4):\n                cse_var_2: T.int32 = g_out_channel_fused_h_fused % 3\n                cse_var_1: T.int32 = g_out_channel_fused_h_fused // 3 * 288\n                W_1 = T.Buffer((4608,), data=W.data)\n                kernel_vec_1[cse_var_1 + in_channel * 144 + cse_var_2 * 48 + w * 16 + ci * 4 + co] = W_1[cse_var_1 + co * 72 + in_channel * 36 + ci * 9 + cse_var_2 * 3 + w]\n        for g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused in T.parallel(16256):\n            for ow_c_outer in range(127):\n                conv_global_1 = T.Buffer((16516096,), data=conv_global)\n                for oc_block_c_init in range(4):\n                    conv_global_1[g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016 + ow_c_outer * 8 + oc_block_c_init] = T.float32(0)\n                for oc_block_c_init in range(4):\n                    conv_global_1[g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016 + ow_c_outer * 8 + oc_block_c_init + 4] = T.float32(0)\n                for ic_outer, kh, kw, ic_inner in T.grid(2, 3, 3, 4):\n                    for oc_block_c in range(4):\n                        cse_var_3: T.int32 = g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016 + ow_c_outer * 8 + oc_block_c\n                        conv_global_1[cse_var_3] = conv_global_1[cse_var_3] + data_vec_1[g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused // 1016 * 524288 + ic_outer * 262144 + kh * 1024 + g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused % 254 * 1024 + ic_inner * 256 + ow_c_outer * 2 + kw] * kernel_vec_1[g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused // 4064 * 1152 + g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused % 1016 // 254 * 288 + ic_outer * 144 + kh * 48 + kw * 16 + ic_inner * 4 + oc_block_c]\n                    for oc_block_c in range(4):\n                        cse_var_4: T.int32 = g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused * 1016 + ow_c_outer * 8 + oc_block_c + 4\n                        conv_global_1[cse_var_4] = conv_global_1[cse_var_4] + data_vec_1[g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused // 1016 * 524288 + ic_outer * 262144 + kh * 1024 + g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused % 254 * 1024 + ic_inner * 256 + ow_c_outer * 2 + kw + 1] * kernel_vec_1[g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused // 4064 * 1152 + g_c_n_c_fused_oc_chunk_c_fused_oh_c_fused % 1016 // 254 * 288 + ic_outer * 144 + kh * 48 + kw * 16 + ic_inner * 4 + oc_block_c]\n        for n in range(4):\n            for c_outer_h_fused in T.parallel(4064):\n                for w_outer, w_inner, c_inner in T.grid(127, 2, 4):\n                    output_unpack_1 = T.Buffer((16516096,), data=output_unpack.data)\n                    conv_global_1 = T.Buffer((16516096,), data=conv_global)\n                    output_unpack_1[n * 4129024 + c_outer_h_fused // 254 * 258064 + c_inner * 64516 + c_outer_h_fused % 254 * 254 + w_outer * 2 + w_inner] = conv_global_1[c_outer_h_fused // 1016 * 4129024 + n * 1032256 + c_outer_h_fused % 1016 * 1016 + w_outer * 8 + w_inner * 4 + c_inner]",
        "op_args": "None",
        "input_shape": "[[4, 32, 256, 256], [64, 8, 3, 3]]",
        "output_shape": "[[4, 64, 254, 254]]",
        "input_name": [
            "A",
            "W"
        ],
        "output_name": [
            "output_unpack"
        ],
        "deepseek_c": "void default_function_kernel(float* __restrict__ A, float* __restrict__ W, float* __restrict__ group_conv2d_nchw) {\n  float group_conv2d_nchw_local[1];\n  __shared__ float pad_temp_shared[1];\n  __shared__ float W_shared[1];\n  group_conv2d_nchw_local[0] = 0.000000e+00f;\n  for (int rc_outer = 0; rc_outer < 8; ++rc_outer) {\n    for (int ry_outer = 0; ry_outer < 3; ++ry_outer) {\n      for (int rx_outer = 0; rx_outer < 3; ++rx_outer) {\n        __syncthreads();\n        pad_temp_shared[0] = A[(((((((((int)blockIdx.z) * 2097152) + ((((int)blockIdx.y) >> 4) * 524288)) + (rc_outer * 65536)) + ((((int)blockIdx.x) / 254) * 256)) + (ry_outer * 256)) + rx_outer) + (((int)blockIdx.x) % 254))];\n        W_shared[0] = W[((((((int)blockIdx.y) * 72) + (rc_outer * 9)) + (ry_outer * 3)) + rx_outer)];\n        __syncthreads();\n        group_conv2d_nchw_local[0] = (group_conv2d_nchw_local[0] + (pad_temp_shared[0] * W_shared[0]));\n      }\n    }\n  }\n  group_conv2d_nchw[(((((int)blockIdx.z) * 4129024) + (((int)blockIdx.y) * 64516)) + ((int)blockIdx.x))] = group_conv2d_nchw_local[0];\n}",
        "starcoder_c": "void default_function_kernel(float* __restrict__ A, float* __restrict__ W, float* __restrict__ group_conv2d_nchw) {\n  float group_conv2d_nchw_local[1];\n  __shared__ float pad_temp_shared[1];\n  __shared__ float W_shared[1];\n  group_conv2d_nchw_local[0] = 0.000000e+00f;\n  for (int rc_outer = 0; rc_outer < 8; ++rc_outer) {\n    for (int ry_outer = 0; ry_outer < 3; ++ry_outer) {\n      for (int rx_outer = 0; rx_outer < 3; ++rx_outer) {\n        __syncthreads();\n        pad_temp_shared[0] = A[(((((((((int)blockIdx.z) * 2097152) + ((((int)blockIdx.y) >> 4) * 524288)) + (rc_outer * 65536)) + ((((int)blockIdx.x) / 254) * 256)) + (ry_outer * 256)) + rx_outer) + (((int)blockIdx.x) % 254))];\n        W_shared[0] = W[((((((int)blockIdx.y) * 72) + (rc_outer * 9)) + (ry_outer * 3)) + rx_outer)];\n        __syncthreads();\n        group_conv2d_nchw_local[0] = (group_conv2d_nchw_local[0] + (pad_temp_shared[0] * W_shared[0]));\n      }\n    }\n  }\n  group_conv2d_nchw[(((((int)blockIdx.z) * 4129024) + (((int)blockIdx.y) * 64516)) + ((int)blockIdx.x))] = group_conv2d_nchw_local[0];\n}",
        "codellama_c": "void default_function_kernel(float* __restrict__ A, float* __restrict__ W, float* __restrict__ group_conv2d_nchw) {\n  float group_conv2d_nchw_local[1];\n  __shared__ float pad_temp_shared[1];\n  __shared__ float W_shared[1];\n  group_conv2d_nchw_local[0] = 0.000000e+00f;\n  for (int rc_outer = 0; rc_outer < 8; ++rc_outer) {\n    for (int ry_outer = 0; ry_outer < 3; ++ry_outer) {\n      for (int rx_outer = 0; rx_outer < 3; ++rx_outer) {\n        __syncthreads();\n        pad_temp_shared[0] = A[(((((((((int)blockIdx.z) * 2097152) + ((((int)blockIdx.y) >> 4) * 524288)) + (rc_outer * 65536)) + ((((int)blockIdx.x) / 254) * 256)) + (ry_outer * 256)) + rx_outer) + (((int)blockIdx.x) % 254))];\n        W_shared[0] = W[((((((int)blockIdx.y) * 72) + (rc_outer * 9)) + (ry_outer * 3)) + rx_outer)];\n        __syncthreads();\n        group_conv2d_nchw_local[0] = (group_conv2d_nchw_local[0] + (pad_temp_shared[0] * W_shared[0]));\n      }\n    }\n  }\n  group_conv2d_nchw[(((((int)blockIdx.z) * 4129024) + (((int)blockIdx.y) * 64516)) + ((int)blockIdx.x))] = group_conv2d_nchw_local[0];\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* A, float* W, float* group_conv2d_nchw) {\n  #pragma omp parallel for\n  for (int block_z = 0; block_z < 4; ++block_z) {\n    for (int block_y = 0; block_y < 32; ++block_y) {\n      for (int block_x = 0; block_x < 256; ++block_x) {\n        float group_conv2d_nchw_local = 0.0f;\n        for (int rc_outer = 0; rc_outer < 8; ++rc_outer) {\n          for (int ry_outer = 0; ry_outer < 3; ++ry_outer) {\n            for (int rx_outer = 0; rx_outer < 3; ++rx_outer) {\n              int A_index = (((((block_z * 2097152) + ((block_y >> 4) * 524288)) + (rc_outer * 65536)) + (((block_x / 254) * 256)) + (ry_outer * 256)) + rx_outer) + (block_x % 254);\n              int W_index = (((block_y * 72) + (rc_outer * 9)) + (ry_outer * 3)) + rx_outer;\n              group_conv2d_nchw_local += A[A_index] * W[W_index];\n            }\n          }\n        }\n        group_conv2d_nchw[((block_z * 4129024) + (block_y * 64516) + block_x)] = group_conv2d_nchw_local;\n      }\n    }\n  }\n}\n//End-of-Code|",
        "gemini2_c": "void default_function_kernel(float* A, float* W, float* group_conv2d_nchw) {\n  for (int blockIdx_z = 0; blockIdx_z < 4; ++blockIdx_z) {\n    for (int blockIdx_y = 0; blockIdx_y < 32; ++blockIdx_y) {\n      for (int blockIdx_x = 0; blockIdx_x < 256; ++blockIdx_x) {\n        float group_conv2d_nchw_local[1];\n        group_conv2d_nchw_local[0] = 0.000000e+00f;\n        for (int rc_outer = 0; rc_outer < 8; ++rc_outer) {\n          for (int ry_outer = 0; ry_outer < 3; ++ry_outer) {\n            for (int rx_outer = 0; rx_outer < 3; ++rx_outer) {\n              float pad_temp_shared[1];\n              float W_shared[1];\n              pad_temp_shared[0] = A[(((((((((int)blockIdx_z) * 2097152) + ((((int)blockIdx_y) >> 4) * 524288)) + (rc_outer * 65536)) + ((((int)blockIdx_x) / 254) * 256)) + (ry_outer * 256)) + rx_outer) + (((int)blockIdx_x) % 254))];\n              W_shared[0] = W[((((((int)blockIdx_y) * 72) + (rc_outer * 9)) + (ry_outer * 3)) + rx_outer)];\n              group_conv2d_nchw_local[0] = (group_conv2d_nchw_local[0] + (pad_temp_shared[0] * W_shared[0]));\n            }\n          }\n        }\n        group_conv2d_nchw[(((((int)blockIdx_z) * 4129024) + (((int)blockIdx_y) * 64516)) + ((int)blockIdx_x))] = group_conv2d_nchw_local[0];\n      }\n    }\n  }\n}\n",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* A, float* W, float* group_conv2d_nchw) {\n    int blockIdx_z, blockIdx_y, blockIdx_x;\n    int gridDim_z = 4, gridDim_y = 256, gridDim_x = 256;\n\n    #pragma omp parallel for collapse(3) private(blockIdx_z, blockIdx_y, blockIdx_x)\n    for (blockIdx_z = 0; blockIdx_z < gridDim_z; ++blockIdx_z) {\n        for (blockIdx_y = 0; blockIdx_y < gridDim_y; ++blockIdx_y) {\n            for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n                float group_conv2d_nchw_local = 0.0f;\n\n                for (int rc_outer = 0; rc_outer < 8; ++rc_outer) {\n                    for (int ry_outer = 0; ry_outer < 3; ++ry_outer) {\n                        for (int rx_outer = 0; rx_outer < 3; ++rx_outer) {\n                            float pad_temp_shared = A[((((blockIdx_z * 2097152) + ((blockIdx_y >> 4) * 524288)) + (rc_outer * 65536) + ((blockIdx_x / 254) * 256) + (ry_outer * 256) + rx_outer + (blockIdx_x % 254)))];\n                            float W_shared = W[(((blockIdx_y * 72) + (rc_outer * 9) + (ry_outer * 3) + rx_outer))];\n\n                            group_conv2d_nchw_local += pad_temp_shared * W_shared;\n                        }\n                    }\n                }\n\n                group_conv2d_nchw[((blockIdx_z * 4129024) + (blockIdx_y * 64516) + blockIdx_x)] = group_conv2d_nchw_local;\n            }\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* A, float* W, float* group_conv2d_nchw) {\n    int blockIdx_z, blockIdx_y, blockIdx_x;\n    int gridDim_z = 4, gridDim_y = 32, gridDim_x = 256;\n\n    #pragma omp parallel for collapse(3) private(blockIdx_z, blockIdx_y, blockIdx_x)\n    for (blockIdx_z = 0; blockIdx_z < gridDim_z; ++blockIdx_z) {\n        for (blockIdx_y = 0; blockIdx_y < gridDim_y; ++blockIdx_y) {\n            for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n                float group_conv2d_nchw_local = 0.0f;\n\n                for (int rc_outer = 0; rc_outer < 8; ++rc_outer) {\n                    for (int ry_outer = 0; ry_outer < 3; ++ry_outer) {\n                        for (int rx_outer = 0; rx_outer < 3; ++rx_outer) {\n                            float pad_temp_shared = A[((blockIdx_z * 2097152) + ((blockIdx_y >> 4) * 524288) + (rc_outer * 65536) + ((blockIdx_x / 254) * 256) + (ry_outer * 256) + rx_outer + (blockIdx_x % 254))];\n                            float W_shared = W[(blockIdx_y * 72) + (rc_outer * 9) + (ry_outer * 3) + rx_outer];\n\n                            group_conv2d_nchw_local += pad_temp_shared * W_shared;\n                        }\n                    }\n                }\n\n                group_conv2d_nchw[(blockIdx_z * 4129024) + (blockIdx_y * 64516) + blockIdx_x] = group_conv2d_nchw_local;\n            }\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* A, float* W, float* group_conv2d_nchw) {\n  #pragma omp parallel for\n  for (int32_t blockIdx_z = 0; blockIdx_z < 4; ++blockIdx_z) {\n    for (int32_t blockIdx_y = 0; blockIdx_y < 32; ++blockIdx_y) {\n      for (int32_t blockIdx_x = 0; blockIdx_x < 256; ++blockIdx_x) {\n        float group_conv2d_nchw_local = 0.000000e+00f;\n        for (int32_t rc_outer = 0; rc_outer < 8; ++rc_outer) {\n          for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n            for (int32_t rx_outer = 0; rx_outer < 3; ++rx_outer) {\n              float pad_temp_shared = A[(((((((((blockIdx_z * 2097152) + ((blockIdx_y >> 4) * 524288)) + (rc_outer * 65536)) + ((blockIdx_x / 254) * 256)) + (ry_outer * 256)) + rx_outer) + (blockIdx_x % 254))];\n              float W_shared = W[((((blockIdx_y * 72) + (rc_outer * 9)) + (ry_outer * 3)) + rx_outer)];\n              group_conv2d_nchw_local = (group_conv2d_nchw_local + (pad_temp_shared * W_shared));\n            }\n          }\n        }\n        group_conv2d_nchw[((blockIdx_z * 4129024) + (blockIdx_y * 64516) + blockIdx_x)] = group_conv2d_nchw_local;\n      }\n    }\n  }\n}\n"
    },
    {
        "op_name": "batch_matmul_opt",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NT, float* x, float* y) {\n  #pragma omp parallel for\n  for (int32_t b_i_outer_fused_j_outer_fused = 0; b_i_outer_fused_j_outer_fused < 1048576; ++b_i_outer_fused_j_outer_fused) {\n    float T_batch_matmul_NT_global_rf[1024];\n    float T_batch_matmul_NT_global[64];\n    for (int32_t k_inner = 0; k_inner < 16; ++k_inner) {\n      for (int32_t i_c_j_c_fused = 0; i_c_j_c_fused < 64; ++i_c_j_c_fused) {\n        T_batch_matmul_NT_global_rf[((k_inner * 64) + i_c_j_c_fused)] = 0.000000e+00f;\n        for (int32_t k_outer = 0; k_outer < 128; ++k_outer) {\n          T_batch_matmul_NT_global_rf[((k_inner * 64) + i_c_j_c_fused)] = (T_batch_matmul_NT_global_rf[((k_inner * 64) + i_c_j_c_fused)] + (x[(((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + ((i_c_j_c_fused >> 3) * 2048)) + (k_outer * 16)) + k_inner)] * y[((((((b_i_outer_fused_j_outer_fused >> 16) * 4194304) + ((b_i_outer_fused_j_outer_fused & 255) * 16384)) + ((i_c_j_c_fused & 7) * 2048)) + (k_outer * 16)) + k_inner)]));\n        }\n      }\n    }\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = 0.000000e+00f;\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[((ax1 * 8) + ax2)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 64)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 128)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 192)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 256)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 320)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 384)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 448)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 512)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 576)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 640)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 704)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 768)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 832)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 896)]);\n        T_batch_matmul_NT_global[((ax1 * 8) + ax2)] = (T_batch_matmul_NT_global[((ax1 * 8) + ax2)] + T_batch_matmul_NT_global_rf[(((ax1 * 8) + ax2) + 960)]);\n      }\n    }\n    for (int32_t i_inner = 0; i_inner < 8; ++i_inner) {\n      T_batch_matmul_NT[((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + (i_inner * 2048)) + ((b_i_outer_fused_j_outer_fused & 255) * 8))] = T_batch_matmul_NT_global[(i_inner * 8)];\n      T_batch_matmul_NT[(((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + (i_inner * 2048)) + ((b_i_outer_fused_j_outer_fused & 255) * 8)) + 1)] = T_batch_matmul_NT_global[((i_inner * 8) + 1)];\n      T_batch_matmul_NT[(((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + (i_inner * 2048)) + ((b_i_outer_fused_j_outer_fused & 255) * 8)) + 2)] = T_batch_matmul_NT_global[((i_inner * 8) + 2)];\n      T_batch_matmul_NT[(((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + (i_inner * 2048)) + ((b_i_outer_fused_j_outer_fused & 255) * 8)) + 3)] = T_batch_matmul_NT_global[((i_inner * 8) + 3)];\n      T_batch_matmul_NT[(((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + (i_inner * 2048)) + ((b_i_outer_fused_j_outer_fused & 255) * 8)) + 4)] = T_batch_matmul_NT_global[((i_inner * 8) + 4)];\n      T_batch_matmul_NT[(((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + (i_inner * 2048)) + ((b_i_outer_fused_j_outer_fused & 255) * 8)) + 5)] = T_batch_matmul_NT_global[((i_inner * 8) + 5)];\n      T_batch_matmul_NT[(((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + (i_inner * 2048)) + ((b_i_outer_fused_j_outer_fused & 255) * 8)) + 6)] = T_batch_matmul_NT_global[((i_inner * 8) + 6)];\n      T_batch_matmul_NT[(((((b_i_outer_fused_j_outer_fused >> 8) * 16384) + (i_inner * 2048)) + ((b_i_outer_fused_j_outer_fused & 255) * 8)) + 7)] = T_batch_matmul_NT_global[((i_inner * 8) + 7)];\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_batch_matmul_NT, float* __restrict__ x, float* __restrict__ y) {\n  float T_batch_matmul_NT_local[64];\n  __shared__ float x_shared[512];\n  __shared__ float y_shared[512];\n  float x_shared_local[8];\n  float y_shared_local[8];\n  for (int i_c_init = 0; i_c_init < 8; ++i_c_init) {\n    for (int j_c_init = 0; j_c_init < 8; ++j_c_init) {\n      T_batch_matmul_NT_local[((i_c_init * 8) + j_c_init)] = 0.000000e+00f;\n    }\n  }\n  for (int k_outer = 0; k_outer < 256; ++k_outer) {\n    __syncthreads();\n    #pragma unroll\n    for (int ax1_inner = 0; ax1_inner < 8; ++ax1_inner) {\n      x_shared[(((((int)threadIdx.y) * 64) + (ax1_inner * 8)) + ((int)threadIdx.x))] = x[((((((((int)blockIdx.z) * 4194304) + (((int)blockIdx.y) * 131072)) + (((int)threadIdx.y) * 16384)) + (ax1_inner * 2048)) + (k_outer * 8)) + ((int)threadIdx.x))];\n    }\n    #pragma unroll\n    for (int ax1_inner_1 = 0; ax1_inner_1 < 8; ++ax1_inner_1) {\n      y_shared[(((((int)threadIdx.y) * 64) + (ax1_inner_1 * 8)) + ((int)threadIdx.x))] = y[((((((((int)blockIdx.z) * 4194304) + (((int)blockIdx.x) * 131072)) + (((int)threadIdx.y) * 16384)) + (ax1_inner_1 * 2048)) + (k_outer * 8)) + ((int)threadIdx.x))];\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 8; ++k_inner) {\n      #pragma unroll\n      for (int ax1 = 0; ax1 < 8; ++ax1) {\n        x_shared_local[ax1] = x_shared[(((((int)threadIdx.y) * 64) + (ax1 * 8)) + k_inner)];\n      }\n      #pragma unroll\n      for (int ax1_1 = 0; ax1_1 < 8; ++ax1_1) {\n        y_shared_local[ax1_1] = y_shared[(((((int)threadIdx.x) * 64) + (ax1_1 * 8)) + k_inner)];\n      }\n      for (int i_c = 0; i_c < 8; ++i_c) {\n        #pragma unroll\n        for (int j_c = 0; j_c < 8; ++j_c) {\n          T_batch_matmul_NT_local[((i_c * 8) + j_c)] = (T_batch_matmul_NT_local[((i_c * 8) + j_c)] + (x_shared_local[i_c] * y_shared_local[j_c]));\n        }\n      }\n    }\n  }\n  for (int i_inner_inner = 0; i_inner_inner < 8; ++i_inner_inner) {\n    #pragma unroll\n    for (int j_inner_inner = 0; j_inner_inner < 8; ++j_inner_inner) {\n      T_batch_matmul_NT[(((((((((int)blockIdx.z) * 4194304) + (((int)blockIdx.y) * 131072)) + (((int)threadIdx.y) * 16384)) + (i_inner_inner * 2048)) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.x) * 8)) + j_inner_inner)] = T_batch_matmul_NT_local[((i_inner_inner * 8) + j_inner_inner)];\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(x: T.Buffer((16, 2048, 2048), \"float32\"), y: T.Buffer((16, 2048, 2048), \"float32\"), T_batch_matmul_NT: T.Buffer((16, 2048, 2048), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_i_outer_fused_j_outer_fused in T.parallel(1048576):\n            T_batch_matmul_NT_global_rf = T.allocate([1024], \"float32\", \"global\")\n            T_batch_matmul_NT_global = T.allocate([64], \"float32\", \"global\")\n            T_batch_matmul_NT_global_rf_1 = T.Buffer((1024,), data=T_batch_matmul_NT_global_rf)\n            for k_inner, i_c_j_c_fused in T.grid(16, 64):\n                T_batch_matmul_NT_global_rf_1[k_inner * 64 + i_c_j_c_fused] = T.float32(0)\n                for k_outer in range(128):\n                    cse_var_2: T.int32 = k_outer * 16\n                    cse_var_1: T.int32 = k_inner * 64 + i_c_j_c_fused\n                    x_1 = T.Buffer((67108864,), data=x.data)\n                    y_1 = T.Buffer((67108864,), data=y.data)\n                    T_batch_matmul_NT_global_rf_1[cse_var_1] = T_batch_matmul_NT_global_rf_1[cse_var_1] + x_1[b_i_outer_fused_j_outer_fused // 256 * 16384 + i_c_j_c_fused // 8 * 2048 + cse_var_2 + k_inner] * y_1[b_i_outer_fused_j_outer_fused // 65536 * 4194304 + b_i_outer_fused_j_outer_fused % 256 * 16384 + i_c_j_c_fused % 8 * 2048 + cse_var_2 + k_inner]\n            T_batch_matmul_NT_global_1 = T.Buffer((64,), data=T_batch_matmul_NT_global)\n            for ax1, ax2 in T.grid(8, 8):\n                cse_var_3: T.int32 = ax1 * 8 + ax2\n                T_batch_matmul_NT_global_1[cse_var_3] = T.float32(0)\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 64]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 128]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 192]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 256]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 320]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 384]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 448]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 512]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 576]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 640]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 704]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 768]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 832]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 896]\n                T_batch_matmul_NT_global_1[cse_var_3] = T_batch_matmul_NT_global_1[cse_var_3] + T_batch_matmul_NT_global_rf_1[cse_var_3 + 960]\n            for i_inner in range(8):\n                cse_var_5: T.int32 = i_inner * 8\n                cse_var_4: T.int32 = b_i_outer_fused_j_outer_fused // 256 * 16384 + i_inner * 2048 + b_i_outer_fused_j_outer_fused % 256 * 8\n                T_batch_matmul_NT_1 = T.Buffer((67108864,), data=T_batch_matmul_NT.data)\n                T_batch_matmul_NT_1[cse_var_4] = T_batch_matmul_NT_global_1[cse_var_5]\n                T_batch_matmul_NT_1[cse_var_4 + 1] = T_batch_matmul_NT_global_1[cse_var_5 + 1]\n                T_batch_matmul_NT_1[cse_var_4 + 2] = T_batch_matmul_NT_global_1[cse_var_5 + 2]\n                T_batch_matmul_NT_1[cse_var_4 + 3] = T_batch_matmul_NT_global_1[cse_var_5 + 3]\n                T_batch_matmul_NT_1[cse_var_4 + 4] = T_batch_matmul_NT_global_1[cse_var_5 + 4]\n                T_batch_matmul_NT_1[cse_var_4 + 5] = T_batch_matmul_NT_global_1[cse_var_5 + 5]\n                T_batch_matmul_NT_1[cse_var_4 + 6] = T_batch_matmul_NT_global_1[cse_var_5 + 6]\n                T_batch_matmul_NT_1[cse_var_4 + 7] = T_batch_matmul_NT_global_1[cse_var_5 + 7]",
        "op_args": "None",
        "input_shape": "[[16, 2048, 2048], [16, 2048, 2048]]",
        "output_shape": "[[16, 2048, 2048]]",
        "input_name": [
            "x",
            "y"
        ],
        "output_name": [
            "T_batch_matmul_NT"
        ],
        "deepseek_c": "void default_function_kernel(float* T_batch_matmul_NT, float* x, float* y) {\n  float T_batch_matmul_NT_local[64];\n  for (int i_c_init = 0; i_c_init < 8; ++i_c_init) {\n    for (int j_c_init = 0; j_c_init < 8; ++j_c_init) {\n      T_batch_matmul_NT_local[((i_c_init * 8) + j_c_init)] = 0.000000e+00f;\n    }\n  }\n  for (int k_outer = 0; k_outer < 256; ++k_outer) {\n    for (int i_c_init_1 = 0; i_c_init_1 < 8; ++i_c_init_1) {\n      for (int j_c_init_1 = 0; j_c_init_1 < 8; ++j_c_init_1) {\n        for (int i_inner_inner = 0; i_inner_inner < 8; ++i_inner_inner) {\n          for (int j_inner_inner = 0; j_inner_inner < 8; ++j_inner_inner) {\n            T_batch_matmul_NT_local[((i_inner_inner * 8) + j_inner_inner)] = (T_batch_matmul_NT_local[((i_inner_inner * 8) + j_inner_inner)] + (x[((((((i_c_init_1 * 2048) + (i_inner_inner * 256)) + k_outer) * 8) + j_c_init_1)] * y[((((((j_c_init_1 * 2048) + (j_inner_inner * 256)) + k_outer) * 8) + i_c_init_1)]));\n          }\n        }\n      }\n    }\n  }\n  for (int i_inner_inner_outer = 0; i_inner_inner_outer < 8; ++i_inner_inner_outer) {\n    for (int j_inner_inner_outer = 0; j_inner_inner_outer < 8; ++j_inner_inner_outer) {\n      T_batch_matmul_NT[((((((i_inner_inner_outer * 64) + (i_inner_inner_outer * 64)) + (i_inner_inner_outer * 8)) + (i_inner_inner_outer * 8)) + j_inner_inner_outer) + (i_inner_inner_outer * 64))] = T_batch_matmul_NT_local[((i_inner_inner_outer * 8) + j_inner_inner_outer)];\n    }\n  }\n}",
        "starcoder_c": "",
        "codellama_c": "void default_function_kernel(float* T_batch_matmul_NT, float* x, float* y) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 32768; ++ax0_ax1_fused_ax2_fused) {\n    float T_batch_matmul_NT_local[64];\n    for (int32_t i_c_init = 0; i_c_init < 8; ++i_c_init) {\n      for (int32_t j_c_init = 0; j_c_init < 8; ++j_c_init) {\n        T_batch_matmul_NT_local[((i_c_init * 8) + j_c_init)] = 0.000000e+00f;\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 256; ++k_outer) {\n      for (int32_t k_inner = 0; k_inner < 8; ++k_inner) {\n        float x_shared_local[8];\n        float y_shared_local[8];\n        for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n          x_shared_local[ax1] = x[((((ax0_ax1_fused_ax2_fused * 16384) + (ax1 * 2048)) + (k_outer * 8)) + k_inner)];\n        }\n        for (int32_t ax1_1 = 0; ax1_1 < 8; ++ax1_1) {\n          y_shared_local[ax1_1] = y[((((ax0_ax1_fused_ax2_fused * 16384) + (ax1_1 * 2048)) + (k_outer * 8)) + k_inner)];\n        }\n        for (int32_t i_c = 0; i_c < 8; ++i_c) {\n          for (int32_t j_c = 0; j_c < 8; ++j_c) {\n            T_batch_matmul_NT_local[((i_c * 8) + j_c)] = (T_batch_matmul_NT_local[((i_c * 8) + j_c)] + (x_shared_local[i_c] * y_shared_local[j_c]));\n          }\n        }\n      }\n    }\n    for (int32_t i_inner_inner = 0; i_inner_inner < 8; ++i_inner_inner) {\n      for (int32_t j_inner_inner = 0; j_inner_inner < 8; ++j_inner_inner) {\n        T_batch_matmul_NT[((((ax0_ax1_fused_ax2_fused * 64) + (i_inner_inner * 8)) + (j_inner_inner * 8)) + k_inner)] = T_batch_matmul_NT_local[((i_inner_inner * 8) + j_inner_inner)];\n      }\n    }\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* T_batch_matmul_NT, float* x, float* y) {\n  #pragma omp parallel for\n  for (int block_z = 0; block_z < 16; ++block_z) {\n    for (int block_y = 0; block_y < 16; ++block_y) {\n      for (int block_x = 0; block_x < 16; ++block_x) {\n        for (int thread_y = 0; thread_y < 8; ++thread_y) {\n          for (int thread_x = 0; thread_x < 8; ++thread_x) {\n            float T_batch_matmul_NT_local[64];\n            float x_shared_local[8];\n            float y_shared_local[8];\n            for (int i_c_init = 0; i_c_init < 8; ++i_c_init) {\n              for (int j_c_init = 0; j_c_init < 8; ++j_c_init) {\n                T_batch_matmul_NT_local[((i_c_init * 8) + j_c_init)] = 0.000000e+00f;\n              }\n            }\n            for (int k_outer = 0; k_outer < 256; ++k_outer) {\n              #pragma omp simd\n              for (int ax1_inner = 0; ax1_inner < 8; ++ax1_inner) {\n                x_shared_local[ax1_inner] = x[((((((block_z * 4194304) + (block_y * 131072)) + (thread_y * 16384)) + (ax1_inner * 2048)) + (k_outer * 8)) + thread_x)];\n              }\n              #pragma omp simd\n              for (int ax1_inner_1 = 0; ax1_inner_1 < 8; ++ax1_inner_1) {\n                y_shared_local[ax1_inner_1] = y[((((((block_z * 4194304) + (block_x * 131072)) + (thread_y * 16384)) + (ax1_inner_1 * 2048)) + (k_outer * 8)) + thread_x)];\n              }\n              for (int k_inner = 0; k_inner < 8; ++k_inner) {\n                #pragma omp simd\n                for (int ax1 = 0; ax1 < 8; ++ax1) {\n                  x_shared_local[ax1] = x_shared_local[ax1];\n                }\n                #pragma omp simd\n                for (int ax1_1 = 0; ax1_1 < 8; ++ax1_1) {\n                  y_shared_local[ax1_1] = y_shared_local[ax1_1];\n                }\n                for (int i_c = 0; i_c < 8; ++i_c) {\n                  #pragma omp simd\n                  for (int j_c = 0; j_c < 8; ++j_c) {\n                    T_batch_matmul_NT_local[((i_c * 8) + j_c)] = (T_batch_matmul_NT_local[((i_c * 8) + j_c)] + (x_shared_local[i_c] * y_shared_local[j_c]));\n                  }\n                }\n              }\n            }\n            for (int i_inner_inner = 0; i_inner_inner < 8; ++i_inner_inner) {\n              #pragma omp simd\n              for (int j_inner_inner = 0; j_inner_inner < 8; ++j_inner_inner) {\n                T_batch_matmul_NT[(((((((((block_z * 4194304) + (block_y * 131072)) + (thread_y * 16384)) + (i_inner_inner * 2048)) + (block_x * 64)) + (thread_x * 8)) + j_inner_inner)] = T_batch_matmul_NT_local[((i_inner_inner * 8) + j_inner_inner)];\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n//End-of-Code|",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel(float* T_batch_matmul_NT, float* x, float* y) {\n  #pragma omp parallel for\n  for (int block_z = 0; block_z < 16; ++block_z) {\n    for (int block_y = 0; block_y < 16; ++block_y) {\n      for (int thread_y = 0; thread_y < 64; ++thread_y) {\n        for (int block_x = 0; block_x < 16; ++block_x) {\n          for (int thread_x = 0; thread_x < 8; ++thread_x) {\n            float T_batch_matmul_NT_local[64] = {0.0f};\n            float x_shared[512];\n            float y_shared[512];\n            float x_shared_local[8];\n            float y_shared_local[8];\n\n            for (int k_outer = 0; k_outer < 256; ++k_outer) {\n              // Load data into shared memory\n              for (int ax1_inner = 0; ax1_inner < 8; ++ax1_inner) {\n                x_shared[((thread_y * 64) + (ax1_inner * 8)) + thread_x] =\n                    x[((((block_z * 4194304) + (block_y * 131072) + (thread_y * 16384) + (ax1_inner * 2048) + (k_outer * 8)) + thread_x)];\n                y_shared[((thread_y * 64) + (ax1_inner * 8)) + thread_x] =\n                    y[((((block_z * 4194304) + (block_x * 131072) + (thread_y * 16384) + (ax1_inner * 2048) + (k_outer * 8)) + thread_x)];\n              }\n\n              // Perform matrix multiplication\n              for (int k_inner = 0; k_inner < 8; ++k_inner) {\n                for (int ax1 = 0; ax1 < 8; ++ax1) {\n                  x_shared_local[ax1] = x_shared[((thread_y * 64) + (ax1 * 8)) + k_inner];\n                }\n                for (int ax1_1 = 0; ax1_1 < 8; ++ax1_1) {\n                  y_shared_local[ax1_1] = y_shared[((thread_x * 64) + (ax1_1 * 8)) + k_inner];\n                }\n                for (int i_c = 0; i_c < 8; ++i_c) {\n                  for (int j_c = 0; j_c < 8; ++j_c) {\n                    T_batch_matmul_NT_local[(i_c * 8) + j_c] += x_shared_local[i_c] * y_shared_local[j_c];\n                  }\n                }\n              }\n            }\n\n            // Store results\n            for (int i_inner_inner = 0; i_inner_inner < 8; ++i_inner_inner) {\n              for (int j_inner_inner = 0; j_inner_inner < 8; ++j_inner_inner) {\n                T_batch_matmul_NT[((((block_z * 4194304) + (block_y * 131072) + (thread_y * 16384) + (i_inner_inner * 2048) + (block_x * 64) + (thread_x * 8)) +\n                                  j_inner_inner)] =\n                    T_batch_matmul_NT_local[(i_inner_inner * 8) + j_inner_inner];\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* T_batch_matmul_NT, float* x, float* y) {\n    float T_batch_matmul_NT_local[64];\n    float x_shared[512];\n    float y_shared[512];\n    float x_shared_local[8];\n    float y_shared_local[8];\n    \n    #pragma omp parallel\n    {\n        for (int i_c_init = 0; i_c_init < 8; ++i_c_init) {\n            for (int j_c_init = 0; j_c_init < 8; ++j_c_init) {\n                T_batch_matmul_NT_local[((i_c_init * 8) + j_c_init)] = 0.0f;\n            }\n        }\n        for (int k_outer = 0; k_outer < 256; ++k_outer) {\n            #pragma omp barrier\n            #pragma omp for\n            for (int ax1_inner = 0; ax1_inner < 8; ++ax1_inner) {\n                int tid = omp_get_thread_num();\n                int num_threads = omp_get_num_threads();\n                for (int i = tid; i < 64; i += num_threads) {\n                    x_shared[((i / 8) * 64) + (ax1_inner * 8) + (i % 8)] = x[(((i / 8) * 4194304) + ((i % 8) * 131072) + ((i / 8) * 16384) + (ax1_inner * 2048) + (k_outer * 8) + (i % 8))];\n                    y_shared[((i / 8) * 64) + (ax1_inner * 8) + (i % 8)] = y[(((i / 8) * 4194304) + ((i % 8) * 131072) + ((i / 8) * 16384) + (ax1_inner * 2048) + (k_outer * 8) + (i % 8))];\n                }\n            }\n            #pragma omp barrier\n            for (int k_inner = 0; k_inner < 8; ++k_inner) {\n                #pragma omp for\n                for (int ax1 = 0; ax1 < 8; ++ax1) {\n                    x_shared_local[ax1] = x_shared[((ax1 * 8) + k_inner)];\n                    y_shared_local[ax1] = y_shared[((ax1 * 8) + k_inner)];\n                }\n                #pragma omp for collapse(2)\n                for (int i_c = 0; i_c < 8; ++i_c) {\n                    for (int j_c = 0; j_c < 8; ++j_c) {\n                        T_batch_matmul_NT_local[((i_c * 8) + j_c)] += x_shared_local[i_c] * y_shared_local[j_c];\n                    }\n                }\n            }\n        }\n        #pragma omp for collapse(2)\n        for (int i_inner_inner = 0; i_inner_inner < 8; ++i_inner_inner) {\n            for (int j_inner_inner = 0; j_inner_inner < 8; ++j_inner_inner) {\n                int tid = omp_get_thread_num();\n                int num_threads = omp_get_num_threads();\n                for (int i = tid; i < 64; i += num_threads) {\n                    T_batch_matmul_NT[((((i / 8) * 4194304) + ((i % 8) * 131072) + ((i / 8) * 16384) + (i_inner_inner * 2048) + ((i % 8) * 64) + (i_inner_inner * 8)) + j_inner_inner)] = T_batch_matmul_NT_local[((i_inner_inner * 8) + j_inner_inner)];\n                }\n            }\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* T_batch_matmul_NT, float* x, float* y) {\n    int blockIdx_z, blockIdx_y, blockIdx_x;\n    int gridDim_z = 16, gridDim_y = 16, gridDim_x = 16;\n    int threadIdx_y, threadIdx_x;\n    int blockDim_y = 8, blockDim_x = 8;\n\n    #pragma omp parallel for collapse(3) private(blockIdx_z, blockIdx_y, blockIdx_x, threadIdx_y, threadIdx_x)\n    for (blockIdx_z = 0; blockIdx_z < gridDim_z; ++blockIdx_z) {\n        for (blockIdx_y = 0; blockIdx_y < gridDim_y; ++blockIdx_y) {\n            for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n                for (threadIdx_y = 0; threadIdx_y < blockDim_y; ++threadIdx_y) {\n                    for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n                        float T_batch_matmul_NT_local[64];\n                        float x_shared[512];\n                        float y_shared[512];\n                        float x_shared_local[8];\n                        float y_shared_local[8];\n\n                        // Initialize local output array\n                        for (int i_c_init = 0; i_c_init < 8; ++i_c_init) {\n                            for (int j_c_init = 0; j_c_init < 8; ++j_c_init) {\n                                T_batch_matmul_NT_local[(i_c_init * 8) + j_c_init] = 0.0f;\n                            }\n                        }\n\n                        // Loop over k_outer\n                        for (int k_outer = 0; k_outer < 256; ++k_outer) {\n                            // Load x_shared and y_shared\n                            for (int ax1_inner = 0; ax1_inner < 8; ++ax1_inner) {\n                                x_shared[(threadIdx_y * 64) + (ax1_inner * 8) + threadIdx_x] = x[((((blockIdx_z * 4194304) + (blockIdx_y * 131072) + (threadIdx_y * 16384) + (ax1_inner * 2048) + (k_outer * 8) + threadIdx_x))];\n                                y_shared[(threadIdx_y * 64) + (ax1_inner * 8) + threadIdx_x] = y[((((blockIdx_z * 4194304) + (blockIdx_x * 131072) + (threadIdx_y * 16384) + (ax1_inner * 2048) + (k_outer * 8) + threadIdx_x))];\n                            }\n\n                            // Compute local output\n                            for (int k_inner = 0; k_inner < 8; ++k_inner) {\n                                for (int ax1 = 0; ax1 < 8; ++ax1) {\n                                    x_shared_local[ax1] = x_shared[(threadIdx_y * 64) + (ax1 * 8) + k_inner];\n                                    y_shared_local[ax1] = y_shared[(threadIdx_x * 64) + (ax1 * 8) + k_inner];\n                                }\n                                for (int i_c = 0; i_c < 8; ++i_c) {\n                                    for (int j_c = 0; j_c < 8; ++j_c) {\n                                        T_batch_matmul_NT_local[(i_c * 8) + j_c] += x_shared_local[i_c] * y_shared_local[j_c];\n                                    }\n                                }\n                            }\n                        }\n\n                        // Store the result back to global memory\n                        for (int i_inner_inner = 0; i_inner_inner < 8; ++i_inner_inner) {\n                            for (int j_inner_inner = 0; j_inner_inner < 8; ++j_inner_inner) {\n                                T_batch_matmul_NT[((((blockIdx_z * 4194304) + (blockIdx_y * 131072) + (threadIdx_y * 16384) + (i_inner_inner * 2048) + (blockIdx_x * 64) + (threadIdx_x * 8) + j_inner_inner))] = T_batch_matmul_NT_local[(i_inner_inner * 8) + j_inner_inner];\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* T_batch_matmul_NT, float* x, float* y) {\n  #pragma omp parallel for\n  for (int32_t i0_outer_outer_outer = 0; i0_outer_outer_outer < 16; ++i0_outer_outer_outer) {\n    for (int32_t i1_outer_outer = 0; i1_outer_outer < 2048; ++i1_outer_outer) {\n      for (int32_t i2_outer = 0; i2_outer < 2048; ++i2_outer) {\n        for (int32_t i3_inner = 0; i3_inner < 2048; ++i3_inner) {\n          T_batch_matmul_NT[((((((i0_outer_outer_outer * 4194304) + (i1_outer_outer * 131072)) + (i2_outer * 16384)) + (i3_inner * 8)) + (i1_outer_outer * 64)) + (i2_outer * 8))] = 0.000000e+00f;\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_outer_outer_outer = 0; i0_outer_outer_outer < 16; ++i0_outer_outer_outer) {\n    for (int32_t i1_outer_outer = 0; i1_outer_outer < 2048; ++i1_outer_outer) {\n      for (int32_t i2_outer = 0; i2_outer < 2048; ++i2_outer) {\n        for (int32_t i3_inner = 0; i3_inner < 2048; ++i3_inner) {\n          for (int32_t i4_inner = 0; i4_inner < 8; ++i4_inner) {\n            T_batch_matmul_NT[((((((i0_outer_outer_outer * 4194304) + (i1_outer_outer * 131072)) + (i2_outer * 16384)) + (i3_inner * 8)) + (i1_outer_outer * 64)) + (i2_outer * 8))] = (T_batch_matmul_NT[((((((i0_outer_outer_outer * 4194304) + (i1_outer_outer * 131072)) + (i2_outer * 16384)) + (i3_inner * 8)) + (i1_outer_outer * 64)) + (i2_outer * 8))] + (x[((((((i0_outer_outer_outer * 4194304) + (i1_outer_outer * 131072)) + (i2_outer * 16384)) + (i3_inner * 8)) + (i1_outer_outer * 64)) + (i4_inner))] * y[((((((i0_outer_outer_outer * 4194304) + (i1_outer_outer * 131072)) + (i2_outer * 16384)) + (i4_inner * 2048)) + (i3_inner * 8)) + (i2_outer * 8))]));\n          }\n        }\n      }\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "sin",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2048; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sinf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 2, 16, 4), \"float32\"), compute: T.Buffer((16, 2, 16, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(2048):\n            compute_1 = T.Buffer((2048,), data=compute.data)\n            data_1 = T.Buffer((2048,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sin(data_1[i0_i1_fused_i2_fused_i3_fused])",
        "op_args": [
            16,
            2,
            16,
            4
        ],
        "input_shape": "[[16, 2, 16, 4]]",
        "output_shape": "[[16, 2, 16, 4]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        for (int32_t i3 = 0; i3 < 4; ++i3) {\n          compute[((((i0 * 128) + (i1 * 64)) + (i2 * 4)) + i3)] = sinf(data[((((i0 * 128) + (i1 * 64)) + (i2 * 4)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 256; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sinf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2048; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sinf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 512; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 4; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 4) + i3_s)] = sinf(data[((i0_i1_fused_i2_fused * 4) + i3_s)]);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "adaptive_pool_max",
        "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2040; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      adaptive_pool_max[((ax0_ax1_fused_ax2_fused * 8) + ax3)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < (((((((ax0_ax1_fused_ax2_fused & 7) * 5) + 5) % 8) == 0) ? ((((ax0_ax1_fused_ax2_fused & 7) * 5) + 5) >> 3) : (((((ax0_ax1_fused_ax2_fused & 7) * 5) + 5) >> 3) + 1)) - (((ax0_ax1_fused_ax2_fused & 7) * 5) >> 3)); ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 2; ++rv1) {\n          adaptive_pool_max[((ax0_ax1_fused_ax2_fused * 8) + ax3)] = max(adaptive_pool_max[((ax0_ax1_fused_ax2_fused * 8) + ax3)], data[((((((ax0_ax1_fused_ax2_fused >> 3) * 80) + ((((ax0_ax1_fused_ax2_fused & 7) * 5) >> 3) * 16)) + (rv0 * 16)) + (ax3 * 2)) + rv1)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < ((((((((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 2)) & 15) >> 1) * 5) + 5) % 8) == 0) ? (((((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 2)) & 15) >> 1) * 5) + 5) >> 3) : ((((((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 2)) & 15) >> 1) * 5) + 5) >> 3) + 1)) - ((((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 2)) & 15) >> 1) * 5) >> 3)); ++rv0) {\n    for (int rv1 = 0; rv1 < 2; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], data[((((((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 2)) >> 4) * 80) + (((((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 2)) & 15) >> 1) * 5) >> 3) * 16)) + (rv0 * 16)) + ((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) & 7) * 2)) + rv1)]);\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 17, 5, 16), \"float32\"), adaptive_pool_max: T.Buffer((15, 17, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(2040):\n            for ax3 in range(8):\n                adaptive_pool_max_1 = T.Buffer((16320,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(T.Let(T.Let(T.Let(T.Select(cse_var_3 % 8 == 0, cse_var_1, cse_var_1 + 1) - cse_var_2 // 8, where={cse_var_1: cse_var_3 // 8}), where={cse_var_3: cse_var_2 + 5}), where={cse_var_2: ax0_ax1_fused_ax2_fused % 8 * 5}), 2):\n                    cse_var_2 = T.int32()\n                    cse_var_3 = T.int32()\n                    cse_var_1 = T.int32()\n                    cse_var_4: T.int32 = ax0_ax1_fused_ax2_fused * 8 + ax3\n                    data_1 = T.Buffer((20400,), data=data.data)\n                    adaptive_pool_max_1[cse_var_4] = T.max(adaptive_pool_max_1[cse_var_4], data_1[ax0_ax1_fused_ax2_fused // 8 * 80 + ax0_ax1_fused_ax2_fused % 8 * 5 // 8 * 16 + rv0 * 16 + ax3 * 2 + rv1])",
        "op_args": [
            15,
            17,
            5,
            16
        ],
        "input_shape": "[[15, 17, 5, 16]]",
        "output_shape": "[[15, 17, 8, 8]]",
        "deepseek_c": "void default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  for (int32_t _ind = 0; _ind < 255; ++_ind) {\n    adaptive_pool_max[_ind] = -3.402823e+38f;\n    for (int32_t _ind1 = 0; _ind1 < 10; ++_ind1) {\n      for (int32_t _ind2 = 0; _ind2 < 2; ++_ind2) {\n        adaptive_pool_max[_ind] = max(adaptive_pool_max[_ind], data[((((((((((_ind / 5) >> 1) * 80) + ((((_ind / 5) & 1) * 5) + (_ind % 5))) * 2) + (_ind2 * 1)) + (_ind1 * 16)) + (_ind / 5)) + (_ind1 * 16)) + (_ind2 * 8)) + (_ind1 * 1))]);\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 255; ++ax0_ax1_fused) {\n    adaptive_pool_max[ax0_ax1_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < (((((ax0_ax1_fused & 15) >> 1) * 5) + 5) >> 3); ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 2; ++rv1) {\n        adaptive_pool_max[ax0_ax1_fused] = max(adaptive_pool_max[ax0_ax1_fused], data[((((ax0_ax1_fused * 80) + (rv0 * 16)) + ((ax0_ax1_fused & 7) * 2)) + rv1)]);\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 175; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n      adaptive_pool_max[((ax0_ax1_fused_ax2_fused * 16) + ax3)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < (((((ax0_ax1_fused_ax2_fused % 8) == 0) ? ((ax0_ax1_fused_ax2_fused * 5) + 5) : ((ax0_ax1_fused_ax2_fused * 5) + 5)) % 8) == 0 ? (((((ax0_ax1_fused_ax2_fused % 8) == 0) ? ((ax0_ax1_fused_ax2_fused * 5) + 5) : ((ax0_ax1_fused_ax2_fused * 5) + 5)) % 8) >> 3) : (((((ax0_ax1_fused_ax2_fused % 8) == 0) ? ((ax0_ax1_fused_ax2_fused * 5) + 5) : ((ax0_ax1_fused_ax2_fused * 5) + 5)) % 8) >> 3) + 1)); ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 2; ++rv1) {\n          adaptive_pool_max[((ax0_ax1_fused_ax2_fused * 16) + ax3)] = max(adaptive_pool_max[((ax0_ax1_fused_ax2_fused * 16) + ax3)], data[(((((ax0_ax1_fused_ax2_fused / 8) * 80) + (rv0 * 16)) + (ax3 * 2)) + rv1)]);\n        }\n      }\n    }\n  }\n}",
        "gemini1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n#include <math.h> // for fmax\n\n#define BLOCK_SIZE 60\n\nfloat max(float a, float b) {\n  return (a > b) ? a : b;\n}\n\nvoid default_function_kernel(float* adaptive_pool_max, float* data, int N) {\n  #pragma omp parallel for\n  for (int i = 0; i < N; i++) {\n    int blockIdx_x = i / BLOCK_SIZE;\n    int threadIdx_x = i % BLOCK_SIZE;\n\n    adaptive_pool_max[i] = -3.402823e+38f; // Initialize with negative infinity\n\n    for (int rv0 = 0; rv0 < ((((((((((((blockIdx_x) * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5) + 5) % 8) == 0) ? (((((((((blockIdx_x) * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5) + 5) >> 3) : ((((((((((blockIdx_x) * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5) + 5) >> 3) + 1)) - ((((((((blockIdx_x) * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5) >> 3)); ++rv0) {\n      for (int rv1 = 0; rv1 < 2; ++rv1) {\n        adaptive_pool_max[i] = max(adaptive_pool_max[i], data[((((((((((blockIdx_x) * 15) + (threadIdx_x >> 2)) >> 4) * 80) + (((((((((blockIdx_x) * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5) >> 3) * 16)) + (rv0 * 16)) + ((((((blockIdx_x) * 4) + (threadIdx_x)) & 7) * 2)) + rv1)]);\n      }\n    }\n  }\n}\n\nint main() {\n  int N = 15 * 17 * 5 * 16; // Input tensor shape: [15, 17, 5, 16]\n  float* data = (float*)malloc(N * sizeof(float));\n  float* adaptive_pool_max = (float*)malloc(N * sizeof(float));\n\n  // Initialize data (replace with your actual data initialization)\n  for (int i = 0; i < N; i++) {\n    data[i] = (float)i;\n  }\n\n  default_function_kernel(adaptive_pool_max, data, N);\n\n  // Print or use the results (adaptive_pool_max)\n  // ...\n\n  free(data);\n  free(adaptive_pool_max);\n\n  return 0;\n}\n\n    ",
        "gemini2_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n#include <math.h> // for the max function\n\n#define max(a, b) ((a) > (b) ? (a) : (b))\n\nvoid default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int blockIdx_x = 0; blockIdx_x < 15; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 60; ++threadIdx_x) {\n      adaptive_pool_max[((blockIdx_x * 60) + threadIdx_x)] = -3.402823e+38f;\n      for (int rv0 = 0; rv0 < ((((((((((((blockIdx_x * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5) + 5) % 8) == 0) ? (((((((((blockIdx_x * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5) + 5) >> 3) : (((((((((blockIdx_x * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5) + 5) >> 3) + 1)) - ((((((((blockIdx_x * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5) >> 3)); ++rv0) {\n        for (int rv1 = 0; rv1 < 2; ++rv1) {\n          adaptive_pool_max[((blockIdx_x * 60) + threadIdx_x)] = max(adaptive_pool_max[((blockIdx_x * 60) + threadIdx_x)], data[((((((((((blockIdx_x * 15) + (threadIdx_x >> 2)) >> 4) * 80) + (((((((((blockIdx_x * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5) >> 3) * 16)) + (rv0 * 16)) + (((((blockIdx_x * 4) + threadIdx_x) & 7) * 2)) + rv1)]);\n        }\n      }\n    }\n  }\n}\n\n    ",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <float.h> // For FLT_MIN\n\nvoid default_function_kernel(float* adaptive_pool_max, float* data, int num_blocks, int threads_per_block) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < num_blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads_per_block; ++threadIdx_x) {\n            int idx = blockIdx_x * threads_per_block + threadIdx_x;\n            adaptive_pool_max[idx] = -FLT_MAX;\n\n            int rv0_limit = ((((((((((blockIdx_x * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5) + 5) % 8) == 0) ?\n                             (((((((blockIdx_x * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5) + 5) >> 3) :\n                             ((((((((blockIdx_x * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5) + 5) >> 3) + 1)) -\n                             ((((((blockIdx_x * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5) >> 3);\n\n            for (int rv0 = 0; rv0 < rv0_limit; ++rv0) {\n                for (int rv1 = 0; rv1 < 2; ++rv1) {\n                    int data_idx = (((((blockIdx_x * 15) + (threadIdx_x >> 2)) >> 4) * 80) +\n                                    (((((blockIdx_x * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5 >> 3) * 16 +\n                                    rv0 * 16 + (((blockIdx_x * 4) + (threadIdx_x & 7)) * 2) + rv1);\n\n                    adaptive_pool_max[idx] = fmax(adaptive_pool_max[idx], data[data_idx]);\n                }\n            }\n        }\n    }\n}\n\nint main() {\n    int blocks = 15 * 17 * 5 * 16;  // As per the tensor shape provided\n    int threads_per_block = 60;     // Launch bounds\n\n    int total_elements = blocks * threads_per_block;\n    float *adaptive_pool_max = (float *)malloc(total_elements * sizeof(float));\n    float *data = (float *)malloc(total_elements * sizeof(float));\n\n    // Initialize data (example initialization)\n    for (int i = 0; i < total_elements; ++i) {\n        data[i] = (float)i * 0.01f;\n    }\n\n    // Call the function\n    default_function_kernel(adaptive_pool_max, data, blocks, threads_per_block);\n\n    // Print the result\n    for (int i = 0; i < total_elements; ++i) {\n        printf(\"%f \", adaptive_pool_max[i]);\n    }\n    printf(\"\\n\");\n\n    free(adaptive_pool_max);\n    free(data);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n#include <float.h> // For FLT_MIN\n\n// Function to compute the maximum value in the adaptive pooling operation\nvoid default_function_kernel(const float* data, float* adaptive_pool_max, int num_blocks) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < num_blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < 60; ++threadIdx_x) {\n            int index = (blockIdx_x * 60) + threadIdx_x;\n            adaptive_pool_max[index] = -FLT_MAX; // Initialize to -infinity\n\n            int limit_rv0 = (((((blockIdx_x * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5 + 5) % 8 == 0 ?\n                            (((((blockIdx_x * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5 + 5) >> 3 :\n                            (((((blockIdx_x * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5 + 5) >> 3 + 1;\n            limit_rv0 -= ((((blockIdx_x * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5 >> 3;\n\n            for (int rv0 = 0; rv0 < limit_rv0; ++rv0) {\n                for (int rv1 = 0; rv1 < 2; ++rv1) {\n                    int data_index = (((((blockIdx_x * 15) + (threadIdx_x >> 2)) >> 4) * 80) +\n                                      (((((blockIdx_x * 15) + (threadIdx_x >> 2)) & 15) >> 1) * 5 >> 3) * 16) +\n                                      (rv0 * 16) + (((blockIdx_x * 4) + threadIdx_x) & 7) * 2 + rv1;\n                    if (data_index < 15 * 17 * 5 * 16) { // Check boundary\n                        adaptive_pool_max[index] = fmaxf(adaptive_pool_max[index], data[data_index]);\n                    }\n                }\n            }\n        }\n    }\n}\n\n// Example usage\nint main() {\n    int num_blocks = 15; // Number of blocks corresponding to the first dimension\n    int total_size_max = num_blocks * 60;\n    int total_size_data = 15 * 17 * 5 * 16; // Based on input shape\n\n    float* adaptive_pool_max = (float*)malloc(total_size_max * sizeof(float));\n    float* data = (float*)malloc(total_size_data * sizeof(float));\n\n    // Fill 'data' with values...\n\n    default_function_kernel(data, adaptive_pool_max, num_blocks);\n\n    // Use 'adaptive_pool_max' as needed...\n\n    free(data);\n    free(adaptive_pool_max);\n\n    return 0;\n}\n",
        "gemini3_c": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1275; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 60; ++i3_s) {\n      adaptive_pool_max[((i0_i1_fused_i2_fused * 60) + i3_s)] = -3.402823e+38f;\n      for (int rv0 = 0; rv0 < ((((((((((((i0_i1_fused_i2_fused * 15) + (i3_s >> 2)) & 15) >> 1) * 5) + 5) % 8) == 0) ? (((((((((i0_i1_fused_i2_fused * 15) + (i3_s >> 2)) & 15) >> 1) * 5) + 5) >> 3) : ((((((((((i0_i1_fused_i2_fused * 15) + (i3_s >> 2)) & 15) >> 1) * 5) + 5) >> 3) + 1)) - ((((((((i0_i1_fused_i2_fused * 15) + (i3_s >> 2)) & 15) >> 1) * 5) >> 3)); ++rv0) {\n        for (int rv1 = 0; rv1 < 2; ++rv1) {\n          adaptive_pool_max[((i0_i1_fused_i2_fused * 60) + i3_s)] = max(adaptive_pool_max[((i0_i1_fused_i2_fused * 60) + i3_s)], data[((((((((((i0_i1_fused_i2_fused * 15) + (i3_s >> 2)) >> 4) * 80) + (((((((((i0_i1_fused_i2_fused * 15) + (i3_s >> 2)) & 15) >> 1) * 5) >> 3) * 16)) + (rv0 * 16)) + ((((((i0_i1_fused_i2_fused * 4) + i3_s) & 7) * 2)) + rv1)]);\n        }\n      }\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "abs",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 12; ++i3) {\n        compute[(((i0_i1_fused * 48) + (i2 * 12)) + i3)] = fabsf(data[(((i0_i1_fused * 48) + (i2 * 12)) + i3)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 3, 4, 12), \"float32\"), compute: T.Buffer((16, 3, 4, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(48):\n            for i2, i3 in T.grid(4, 12):\n                cse_var_1: T.int32 = i0_i1_fused * 48 + i2 * 12 + i3\n                compute_1 = T.Buffer((2304,), data=compute.data)\n                data_1 = T.Buffer((2304,), data=data.data)\n                compute_1[cse_var_1] = T.fabs(data_1[cse_var_1])",
        "op_args": [
            16,
            3,
            4,
            12
        ],
        "input_shape": "[[16, 3, 4, 12]]",
        "output_shape": "[[16, 3, 4, 12]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        for (int32_t i3 = 0; i3 < 12; ++i3) {\n          compute[((((i0 * 144) + (i1 * 48)) + (i2 * 12)) + i3)] = fabsf(data[((((i0 * 144) + (i1 * 48)) + (i2 * 12)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 768; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = fabsf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1344; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = fabsf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 192; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 12; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 12) + i3_s)] = fabsf(data[((i0_i1_fused_i2_fused * 12) + i3_s)]);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "cos",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3094; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = cosf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 2, 13, 7), \"float32\"), compute: T.Buffer((17, 2, 13, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3094):\n            compute_1 = T.Buffer((3094,), data=compute.data)\n            data_1 = T.Buffer((3094,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.cos(data_1[i0_i1_fused_i2_fused_i3_fused])",
        "op_args": [
            17,
            2,
            13,
            7
        ],
        "input_shape": "[[17, 2, 13, 7]]",
        "output_shape": "[[17, 2, 13, 7]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        for (int32_t i3 = 0; i3 < 7; ++i3) {\n          compute[((((i0 * 182) + (i1 * 91)) + (i2 * 7)) + i3)] = cosf(data[((((i0 * 182) + (i1 * 91)) + (i2 * 7)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "# C code\ndefault_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3094; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = cosf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 5482; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = cosf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 442; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 7; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 7) + i3_s)] = __cosf(data[((i0_i1_fused_i2_fused * 7) + i3_s)]);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "atan",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1428; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 9; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 9) + i3_s)] = atanf(data[((i0_i1_fused_i2_fused * 9) + i3_s)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 12, 7, 9), \"float32\"), compute: T.Buffer((17, 12, 7, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1428):\n            for i3_s in range(9):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 9 + i3_s\n                compute_1 = T.Buffer((12852,), data=compute.data)\n                data_1 = T.Buffer((12852,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])",
        "op_args": [
            17,
            12,
            7,
            9
        ],
        "input_shape": "[[17, 12, 7, 9]]",
        "output_shape": "[[17, 12, 7, 9]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        for (int32_t i3 = 0; i3 < 9; ++i3) {\n          compute[((((i0 * 756) + (i1 * 63)) + (i2 * 9)) + i3)] = atanf(data[((((i0 * 756) + (i1 * 63)) + (i2 * 9)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "# C code\ndefault_function_kernel(compute, data)\n\n# C code\ndef default_function_kernel(compute, data):\n    # compute_1 = atan(data)\n    compute_1 = np.arctan(data)\n    # compute = data + compute_1\n    compute[:] = data + compute_1\n\n\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1684; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = atanf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1428; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 9; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 9) + i3_s)] = atanf(data[((i0_i1_fused_i2_fused * 9) + i3_s)]);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "add",
        "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 18; ++ax3) {\n          T_add[((((ax0 * 1620) + (ax1 * 162)) + (ax2 * 18)) + ax3)] = (data[((((ax0 * 1620) + (ax1 * 162)) + (ax2 * 18)) + ax3)] + data_1[((((ax0 * 1620) + (ax1 * 162)) + (ax2 * 18)) + ax3)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 10, 9, 18), \"float32\"), data_1: T.Buffer((20, 10, 9, 18), \"float32\"), T_add: T.Buffer((20, 10, 9, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(20):\n            for ax1, ax2, ax3 in T.grid(10, 9, 18):\n                cse_var_1: T.int32 = ax0 * 1620 + ax1 * 162 + ax2 * 18 + ax3\n                T_add_1 = T.Buffer((32400,), data=T_add.data)\n                data_2 = T.Buffer((32400,), data=data.data)\n                data_3 = T.Buffer((32400,), data=data_1.data)\n                T_add_1[cse_var_1] = data_2[cse_var_1] + data_3[cse_var_1]",
        "op_args": [
            20,
            10,
            9,
            18
        ],
        "input_shape": "[[20, 10, 9, 18], [20, 10, 9, 18]]",
        "output_shape": "[[20, 10, 9, 18]]",
        "deepseek_c": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  for (int k0 = 0; k0 < 20; ++k0) {\n    for (int k1 = 0; k1 < 10; ++k1) {\n      for (int k2 = 0; k2 < 9; ++k2) {\n        for (int k3 = 0; k3 < 18; ++k3) {\n          T_add[((((k0 * 1620) + (k1 * 162)) + (k2 * 18)) + k3)] = (data[((((k0 * 1620) + (k1 * 162)) + (k2 * 18)) + k3)] + data_1[((((k0 * 1620) + (k1 * 162)) + (k2 * 18)) + k3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1800; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 18; ++ax3) {\n      T_add[((ax0_ax1_fused_ax2_fused * 18) + ax3)] = (data[((ax0_ax1_fused_ax2_fused * 18) + ax3)] + data_1[((ax0_ax1_fused_ax2_fused * 18) + ax3)]);\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1800; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 18; ++ax3) {\n      T_add[((ax0_ax1_fused_ax2_fused * 18) + ax3)] = (data[((ax0_ax1_fused_ax2_fused * 18) + ax3)] + data_1[((ax0_ax1_fused_ax2_fused * 18) + ax3)]);\n    }\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1800; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 18; ++i3_s) {\n      T_add[((i0_i1_fused_i2_fused * 18) + i3_s)] = (data[((i0_i1_fused_i2_fused * 18) + i3_s)] + data_1[((i0_i1_fused_i2_fused * 18) + i3_s)]);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "adaptive_pool_avg",
        "c_code": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 195; ++ax0_ax1_fused) {\n    float adaptive_pool_sum[8];\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        adaptive_pool_sum[ax3] = 0.000000e+00f;\n        for (int32_t rv0 = 0; rv0 < ((((ax2 + 1) % 8) == 0) ? ((ax2 + 1) >> 3) : (((ax2 + 1) >> 3) + 1)); ++rv0) {\n          for (int32_t rv1 = 0; rv1 < ((((((ax3 * 2) + 2) % 8) == 0) ? ((ax3 + 1) >> 2) : (((ax3 + 1) >> 2) + 1)) - (ax3 >> 2)); ++rv1) {\n            adaptive_pool_sum[ax3] = (adaptive_pool_sum[ax3] + data[((((ax0_ax1_fused * 2) + (rv0 * 2)) + (ax3 >> 2)) + rv1)]);\n          }\n        }\n      }\n      for (int32_t ax3_1 = 0; ax3_1 < 8; ++ax3_1) {\n        adaptive_pool_avg[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3_1)] = (adaptive_pool_sum[ax3_1] / (((float)((((ax2 + 1) % 8) == 0) ? ((ax2 + 1) >> 3) : (((ax2 + 1) >> 3) + 1))) * ((float)((((((ax3_1 * 2) + 2) % 8) == 0) ? ((ax3_1 + 1) >> 2) : (((ax3_1 + 1) >> 2) + 1)) - (ax3_1 >> 2)))));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] / (((float)(((((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 3)) & 7) + 1) % 8) == 0) ? (((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 3)) & 7) + 1) >> 3) : ((((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 3)) & 7) + 1) >> 3) + 1))) * ((float)(((((((((int)threadIdx.x) & 7) * 2) + 2) % 8) == 0) ? (((((int)threadIdx.x) & 7) + 1) >> 2) : ((((((int)threadIdx.x) & 7) + 1) >> 2) + 1)) - ((((int)threadIdx.x) & 7) >> 2)))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < (((((((((int)blockIdx.x) & 1) * 4) + (((int)threadIdx.x) >> 3)) + 1) % 8) == 0) ? ((((((int)threadIdx.x) + 8) >> 5) + (((int)blockIdx.x) & 1)) >> 1) : (((((((int)threadIdx.x) + 8) >> 5) + (((int)blockIdx.x) & 1)) >> 1) + 1)); ++rv0) {\n    for (int rv1 = 0; rv1 < (((((((((int)threadIdx.x) & 7) * 2) + 2) % 8) == 0) ? (((((int)threadIdx.x) & 7) + 1) >> 2) : ((((((int)threadIdx.x) & 7) + 1) >> 2) + 1)) - ((((int)threadIdx.x) & 7) >> 2)); ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + data[(((((((int)blockIdx.x) >> 1) * 2) + (rv0 * 2)) + ((((int)threadIdx.x) & 7) >> 2)) + rv1)]);\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 13, 1, 2), \"float32\"), adaptive_pool_avg: T.Buffer((15, 13, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(195):\n            adaptive_pool_sum = T.allocate([8], \"float32\", \"global\")\n            for ax2 in range(8):\n                adaptive_pool_sum_1 = T.Buffer((8,), data=adaptive_pool_sum, align=32)\n                for ax3 in range(8):\n                    adaptive_pool_sum_1[ax3] = T.float32(0)\n                    for rv0, rv1 in T.grid(T.Let(T.Select((ax2 + 1) % 8 == 0, cse_var_1, cse_var_1 + 1), where={cse_var_1: (ax2 + 1) // 8}), T.Let(T.Select((ax3 * 2 + 2) % 8 == 0, cse_var_2, cse_var_2 + 1) - ax3 // 4, where={cse_var_2: (ax3 + 1) // 4})):\n                        cse_var_1 = T.int32()\n                        cse_var_2 = T.int32()\n                        data_1 = T.Buffer((390,), data=data.data)\n                        adaptive_pool_sum_1[ax3] = adaptive_pool_sum_1[ax3] + data_1[ax0_ax1_fused * 2 + rv0 * 2 + ax3 // 4 + rv1]\n                for ax3 in range(8):\n                    cse_var_5: T.int32 = ax2 + 1\n                    cse_var_4: T.int32 = cse_var_5 // 8\n                    cse_var_3: T.int32 = (ax3 + 1) // 4\n                    adaptive_pool_avg_1 = T.Buffer((12480,), data=adaptive_pool_avg.data)\n                    adaptive_pool_avg_1[ax0_ax1_fused * 64 + ax2 * 8 + ax3] = adaptive_pool_sum_1[ax3] / (T.Cast(\"float32\", T.Select(cse_var_5 % 8 == 0, cse_var_4, cse_var_4 + 1)) * T.Cast(\"float32\", T.Select((ax3 * 2 + 2) % 8 == 0, cse_var_3, cse_var_3 + 1) - ax3 // 4))",
        "op_args": [
            15,
            13,
            1,
            2
        ],
        "input_shape": "[[15, 13, 1, 2]]",
        "output_shape": "[[15, 13, 8, 8]]",
        "deepseek_c": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 15; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 1; ++ax2) {\n      float adaptive_pool_sum[1];\n      adaptive_pool_sum[0] = 0.000000e+00f;\n      for (int32_t rv0 = 0; rv0 < (((((((((int)ax0_ax1_fused) & 1) * 4) + (((int)ax2) >> 3)) + 1) % 8) == 0) ? ((((((int)ax2) + 8) >> 5) + (((int)ax0_ax1_fused) & 1)) >> 1) : (((((((int)ax2) + 8) >> 5) + (((int)ax0_ax1_fused) & 1)) >> 1) + 1)); ++rv0) {\n        for (int32_t rv1 = 0; rv1 < (((((((((int)ax2) & 7) * 2) + 2) % 8) == 0) ? (((((int)ax2) & 7) + 1) >> 2) : ((((((int)ax2) & 7) + 1) >> 2) + 1)) - ((((int)ax2) & 7) >> 2)); ++rv1) {\n          adaptive_pool_sum[0] = (adaptive_pool_sum[0] + data[(((((((int)ax0_ax1_fused) >> 1) * 2) + (rv0 * 2)) + ((((int)ax2) & 7) >> 2)) + rv1)]);\n        }\n      }\n      adaptive_pool_avg[((ax0_ax1_fused * 2) + ax2)] = (adaptive_pool_sum[0] / (((float)(((((((((int)ax0_ax1_fused) & 1) * 4) + (((int)ax2) >> 3)) + 1) % 8) == 0) ? ((((((int)ax0_ax1_fused) & 1) * 4) + (((int)ax2) >> 3)) + 1) >> 3) : ((((((((int)ax0_ax1_fused) & 1) * 4) + (((int)ax2) >> 3)) + 1) >> 3) + 1))) * ((float)(((((((((int)ax2) & 7) * 2) + 2) % 8) == 0) ? (((((int)ax2) & 7) + 1) >> 2) : ((((((int)ax2) & 7) + 1) >> 2) + 1)) - ((((int)ax2) & 7) >> 2)))));\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 15; ++ax0_ax1_fused) {\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < (((((ax0_ax1_fused & 1) * 4) + 1) % 8) == 0? (((ax0_ax1_fused + 1) >> 1) >> 1) : ((((ax0_ax1_fused + 1) >> 1) >> 1) + 1)); ++rv0) {\n      for (int32_t rv1 = 0; rv1 < (((((ax0_ax1_fused & 7) + 1) % 8) == 0? (((ax0_ax1_fused & 7) + 1) >> 2) : ((((ax0_ax1_fused & 7) + 1) >> 2) + 1)) - (ax0_ax1_fused >> 2)); ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + data[((ax0_ax1_fused * 2) + (rv0 * 2) + (ax0_ax1_fused >> 2)) + rv1]);\n      }\n    }\n    adaptive_pool_avg[ax0_ax1_fused] = (adaptive_pool_sum[0] / (((float)(((((ax0_ax1_fused & 7) + 1) % 8) == 0? (((ax0_ax1_fused & 7) + 1) >> 3) : ((((ax0_ax1_fused & 7) + 1) >> 3) + 1))) * ((float)(((((ax0_ax1_fused & 7) + 1) % 8) == 0? (((ax0_ax1_fused & 7) + 1) >> 2) : ((((ax0_ax1_fused & 7) + 1) >> 2) + 1)) - (ax0_ax1_fused >> 2))))));\n  }\n}",
        "codellama_c": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 195; ++ax0_ax1_fused_ax2_fused) {\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < (((((ax0_ax1_fused_ax2_fused & 7) + 1) % 8) == 0) ? ((ax0_ax1_fused_ax2_fused & 7) >> 3) : ((ax0_ax1_fused_ax2_fused & 7) >> 3) + 1); ++rv0) {\n      for (int32_t rv1 = 0; rv1 < ((((((ax0_ax1_fused_ax2_fused & 7) * 2) + 2) % 8) == 0) ? (((ax0_ax1_fused_ax2_fused & 7) + 1) >> 2) : ((ax0_ax1_fused_ax2_fused & 7) + 1) >> 2); ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + data[((((rv0 * 2) + ((ax0_ax1_fused_ax2_fused & 7) >> 3)) + (((ax0_ax1_fused_ax2_fused & 7) + 1) >> 2)) + rv1)]);\n      }\n    }\n    adaptive_pool_avg[ax0_ax1_fused_ax2_fused] = (adaptive_pool_sum[0] / (((float)((((ax0_ax1_fused_ax2_fused & 7) + 1) % 8) == 0) ? ((ax0_ax1_fused_ax2_fused & 7) >> 3) : ((ax0_ax1_fused_ax2_fused & 7) >> 3) + 1)) * ((float)(((((ax0_ax1_fused_ax2_fused & 7) * 2) + 2) % 8) == 0) ? (((ax0_ax1_fused_ax2_fused & 7) + 1) >> 2) : ((ax0_ax1_fused_ax2_fused & 7) + 1) >> 2)));\n  }\n}",
        "gemini1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\n#define BLOCK_SIZE_1 48\n#define BLOCK_SIZE_2 32\n\nvoid default_function_kernel(float* adaptive_pool_sum, float* data, int N) {\n  #pragma omp parallel for\n  for (int i = 0; i < N; i++) {\n    adaptive_pool_sum[i] = 0.0f;\n    int blockIdx_x = i / BLOCK_SIZE_2;\n    int threadIdx_x = i % BLOCK_SIZE_2;\n    for (int rv0 = 0; rv0 < (((((((((blockIdx_x & 1) * 4) + (threadIdx_x >> 3)) + 1) % 8) == 0) ? (((((threadIdx_x + 8) >> 5) + (blockIdx_x & 1)) >> 1) : (((((threadIdx_x + 8) >> 5) + (blockIdx_x & 1)) >> 1) + 1)); ++rv0) {\n      for (int rv1 = 0; rv1 < (((((((((threadIdx_x & 7) * 2) + 2) % 8) == 0) ? (((((threadIdx_x) & 7) + 1) >> 2) : (((((threadIdx_x) & 7) + 1) >> 2) + 1)) - (((threadIdx_x) & 7) >> 2)); ++rv1) {\n        adaptive_pool_sum[i] += data[(((((blockIdx_x >> 1) * 2) + (rv0 * 2)) + (((threadIdx_x) & 7) >> 2)) + rv1)];\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* adaptive_pool_avg, float* adaptive_pool_sum, int N) {\n  #pragma omp parallel for\n  for (int i = 0; i < N; i++) {\n    int blockIdx_x = i / BLOCK_SIZE_1;\n    int threadIdx_x = i % BLOCK_SIZE_1;\n    adaptive_pool_avg[i] = adaptive_pool_sum[i] / (((float)((((((((blockIdx_x) * 6) + (threadIdx_x >> 3)) & 7) + 1) % 8) == 0) ? (((((((blockIdx_x) * 6) + (threadIdx_x >> 3)) & 7) + 1) >> 3) : ((((((((blockIdx_x) * 6) + (threadIdx_x >> 3)) & 7) + 1) >> 3) + 1))) * ((float)((((((((threadIdx_x) & 7) * 2) + 2) % 8) == 0) ? (((((threadIdx_x) & 7) + 1) >> 2) : (((((threadIdx_x) & 7) + 1) >> 2) + 1)) - ((((threadIdx_x) & 7) >> 2)))));\n  }\n}\n\nint main() {\n  int N = 15 * 13 * 1 * 2; // Input tensor shape: [15, 13, 1, 2]\n  float* data = (float*)malloc(N * sizeof(float));\n  float* adaptive_pool_sum = (float*)malloc(N * sizeof(float));\n  float* adaptive_pool_avg = (float*)malloc(N * sizeof(float));\n\n  // Initialize data (replace with your actual data initialization)\n  for (int i = 0; i < N; i++) {\n    data[i] = (float)i;\n  }\n\n  default_function_kernel(adaptive_pool_sum, data, N);\n  default_function_kernel_1(adaptive_pool_avg, adaptive_pool_sum, N);\n\n  // Print or use the results (adaptive_pool_avg)\n  // ...\n\n  free(data);\n  free(adaptive_pool_sum);\n  free(adaptive_pool_avg);\n\n  return 0;\n}\n\n    ",
        "gemini2_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* adaptive_pool_sum, float* data) {\n  #pragma omp parallel for\n  for (int blockIdx_x = 0; blockIdx_x < 15; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 32; ++threadIdx_x) {\n      adaptive_pool_sum[((blockIdx_x * 32) + threadIdx_x)] = 0.000000e+00f;\n      for (int rv0 = 0; rv0 < ((((((((blockIdx_x & 1) * 4) + (threadIdx_x >> 3)) + 1) % 8) == 0) ? (((((threadIdx_x + 8) >> 5) + (blockIdx_x & 1)) >> 1) : (((((threadIdx_x + 8) >> 5) + (blockIdx_x & 1)) >> 1) + 1)); ++rv0) {\n        for (int rv1 = 0; rv1 < ((((((((threadIdx_x & 7) * 2) + 2) % 8) == 0) ? (((((threadIdx_x & 7) + 1) >> 2) : (((((threadIdx_x & 7) + 1) >> 2) + 1)) - ((threadIdx_x & 7) >> 2)); ++rv1) {\n          adaptive_pool_sum[((blockIdx_x * 32) + threadIdx_x)] = (adaptive_pool_sum[((blockIdx_x * 32) + threadIdx_x)] + data[(((((blockIdx_x >> 1) * 2) + (rv0 * 2)) + ((threadIdx_x & 7) >> 2)) + rv1)]);\n        }\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* adaptive_pool_avg, float* adaptive_pool_sum) {\n  #pragma omp parallel for\n  for (int blockIdx_x = 0; blockIdx_x < 15; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 48; ++threadIdx_x) {\n      adaptive_pool_avg[((blockIdx_x * 48) + threadIdx_x)] = (adaptive_pool_sum[((blockIdx_x * 48) + threadIdx_x)] / (((float)((((((((blockIdx_x * 6) + (threadIdx_x >> 3)) & 7) + 1) % 8) == 0) ? (((((blockIdx_x * 6) + (threadIdx_x >> 3)) & 7) + 1) >> 3) : (((((blockIdx_x * 6) + (threadIdx_x >> 3)) & 7) + 1) >> 3) + 1))) * ((float)((((((((threadIdx_x & 7) * 2) + 2) % 8) == 0) ? (((((threadIdx_x & 7) + 1) >> 2) : (((((threadIdx_x & 7) + 1) >> 2) + 1)) - ((threadIdx_x & 7) >> 2)))));\n    }\n  }\n}\n\n    ",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n#include <stdlib.h>\n\nvoid default_function_kernel_1(float* adaptive_pool_avg, float* adaptive_pool_sum, int num_blocks, int threads_per_block) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < num_blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads_per_block; ++threadIdx_x) {\n            int idx = blockIdx_x * threads_per_block + threadIdx_x;\n            int block_term = (((((blockIdx_x * 6) + (threadIdx_x >> 3)) & 7) + 1) % 8) == 0 ?\n                             (((((blockIdx_x * 6) + (threadIdx_x >> 3)) & 7) + 1) >> 3) :\n                             (((((blockIdx_x * 6) + (threadIdx_x >> 3)) & 7) + 1) >> 3) + 1;\n            int thread_term = (((((threadIdx_x & 7) * 2) + 2) % 8) == 0) ?\n                              (((threadIdx_x & 7) + 1) >> 2) :\n                              (((threadIdx_x & 7) + 1) >> 2) + 1;\n            int denominator = block_term * (thread_term - ((threadIdx_x & 7) >> 2));\n\n            adaptive_pool_avg[idx] = adaptive_pool_sum[idx] / (float)denominator;\n        }\n    }\n}\n\nvoid default_function_kernel(float* adaptive_pool_sum, float* data, int num_blocks, int threads_per_block) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < num_blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads_per_block; ++threadIdx_x) {\n            int idx = blockIdx_x * threads_per_block + threadIdx_x;\n            adaptive_pool_sum[idx] = 0.0f;\n            int rv0_limit = (((((blockIdx_x & 1) * 4) + (threadIdx_x >> 3) + 1) % 8) == 0) ?\n                            ((((threadIdx_x + 8) >> 5) + (blockIdx_x & 1)) >> 1) :\n                            ((((threadIdx_x + 8) >> 5) + (blockIdx_x & 1)) >> 1) + 1;\n\n            for (int rv0 = 0; rv0 < rv0_limit; ++rv0) {\n                int rv1_limit = (((((threadIdx_x & 7) * 2) + 2) % 8) == 0) ?\n                                (((threadIdx_x & 7) + 1) >> 2) :\n                                (((threadIdx_x & 7) + 1) >> 2) + 1;\n                rv1_limit -= (threadIdx_x & 7) >> 2;\n\n                for (int rv1 = 0; rv1 < rv1_limit; ++rv1) {\n                    int data_idx = (((blockIdx_x >> 1) * 2) + (rv0 * 2)) + ((threadIdx_x & 7) >> 2) + rv1;\n                    adaptive_pool_sum[idx] += data[data_idx];\n                }\n            }\n        }\n    }\n}\n\nint main() {\n    int blocks = 15 * 13 * 1 * 2;  // As per the tensor shape provided\n    int threads_per_block_1 = 48;  // Launch bounds for kernel 1\n    int threads_per_block_2 = 32;  // Launch bounds for kernel 2\n\n    int total_elements_1 = blocks * threads_per_block_1;\n    int total_elements_2 = blocks * threads_per_block_2;\n\n    float *adaptive_pool_avg = (float *)malloc(total_elements_1 * sizeof(float));\n    float *adaptive_pool_sum = (float *)malloc(total_elements_1 * sizeof(float));\n    float *data = (float *)malloc(total_elements_2 * sizeof(float));\n\n    // Initialize data and adaptive_pool_sum (example initialization)\n    for (int i = 0; i < total_elements_1; ++i) {\n        adaptive_pool_sum[i] = (float)i * 0.01f;\n    }\n    for (int i = 0; i < total_elements_2; ++i) {\n        data[i] = (float)i * 0.01f;\n    }\n\n    // Call the second function first to populate adaptive_pool_sum\n    default_function_kernel(adaptive_pool_sum, data, blocks, threads_per_block_2);\n\n    // Call the first function to compute adaptive_pool_avg\n    default_function_kernel_1(adaptive_pool_avg, adaptive_pool_sum, blocks, threads_per_block_1);\n\n    // Print the result of the first function\n    for (int i = 0; i < total_elements_1; ++i) {\n        printf(\"%f \", adaptive_pool_avg[i]);\n    }\n    printf(\"\\n\");\n\n    free(adaptive_pool_avg);\n    free(adaptive_pool_sum);\n    free(data);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\nvoid default_function_kernel_1(const float* adaptive_pool_sum, float* adaptive_pool_avg, int num_blocks) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < num_blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < 48; ++threadIdx_x) {\n            int index = (blockIdx_x * 48) + threadIdx_x;\n            int value = ((blockIdx_x * 6 + (threadIdx_x >> 3)) & 7) + 1;\n            int divisor = (value % 8 == 0) ? (value >> 3) : ((value >> 3) + 1);\n            int multiplier = (((threadIdx_x & 7) * 2 + 2) % 8 == 0) ? ((threadIdx_x & 7) + 1) >> 2 : (((threadIdx_x & 7) + 1) >> 2) + 1;\n            int subtracted = ((threadIdx_x & 7) >> 2);\n            adaptive_pool_avg[index] = adaptive_pool_sum[index] / (float)(divisor * (multiplier - subtracted));\n        }\n    }\n}\n\nvoid default_function_kernel(const float* data, float* adaptive_pool_sum, int num_blocks) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < num_blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < 32; ++threadIdx_x) {\n            int sum_index = (blockIdx_x * 32) + threadIdx_x;\n            adaptive_pool_sum[sum_index] = 0.0f;\n            int rv0_limit = (((blockIdx_x & 1) * 4 + (threadIdx_x >> 3)) + 1) % 8 == 0 ? (((threadIdx_x + 8) >> 5) + (blockIdx_x & 1)) >> 1 : (((threadIdx_x + 8) >> 5) + (blockIdx_x & 1)) >> 1 + 1;\n            int rv1_limit = (((threadIdx_x & 7) * 2 + 2) % 8 == 0) ? (((threadIdx_x & 7) + 1) >> 2) : (((threadIdx_x & 7) + 1) >> 2) + 1) - (threadIdx_x & 7) >> 2;\n            \n            for (int rv0 = 0; rv0 < rv0_limit; ++rv0) {\n                for (int rv1 = 0; rv1 < rv1_limit; ++rv1) {\n                    int data_index = (((blockIdx_x >> 1) * 2 + (rv0 * 2) + ((threadIdx_x & 7) >> 2)) + rv1);\n                    adaptive_pool_sum[sum_index] += data[data_index];\n                }\n            }\n        }\n    }\n}\n\n// Example usage\nint main() {\n    int num_blocks = 15; // Number of blocks corresponding to the first dimension\n    int total_size_sum = num_blocks * 48;\n    int total_size_data = 15 * 13 * 1 * 2; // Assuming data array size based on input shape\n\n    float* adaptive_pool_sum = (float*)malloc(total_size_sum * sizeof(float));\n    float* adaptive_pool_avg = (float*)malloc(total_size_sum * sizeof(float));\n    float* data = (float*)malloc(total_size_data * sizeof(float));\n\n    // Fill 'data' with values...\n\n    default_function_kernel(data, adaptive_pool_sum, num_blocks);\n    default_function_kernel_1(adaptive_pool_sum, adaptive_pool_avg, num_blocks);\n\n    // Use 'adaptive_pool_avg' as needed...\n\n    free(data);\n    free(adaptive_pool_sum);\n    free(adaptive_pool_avg);\n\n    return 0;\n}\n",
        "gemini3_c": "void default_function_kernel(float* adaptive_pool_sum, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 195; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 32; ++i3_s) {\n      adaptive_pool_sum[((i0_i1_fused_i2_fused * 32) + i3_s)] = 0.000000e+00f;\n      for (int rv0 = 0; rv0 < ((((((((i0_i1_fused_i2_fused & 1) * 4) + (i3_s >> 3)) + 1) % 8) == 0) ? (((((i3_s + 8) >> 5) + (i0_i1_fused_i2_fused & 1)) >> 1) : (((((((i3_s + 8) >> 5) + (i0_i1_fused_i2_fused & 1)) >> 1) + 1)); ++rv0) {\n        for (int rv1 = 0; rv1 < (((((((((i3_s & 7) * 2) + 2) % 8) == 0) ? (((((i3_s & 7) + 1) >> 2) : ((((((i3_s & 7) + 1) >> 2) + 1)) - (((i3_s & 7) >> 2)); ++rv1) {\n          adaptive_pool_sum[((i0_i1_fused_i2_fused * 32) + i3_s)] = (adaptive_pool_sum[((i0_i1_fused_i2_fused * 32) + i3_s)] + data[((((((i0_i1_fused_i2_fused >> 1) * 2) + (rv0 * 2)) + ((i3_s & 7) >> 2)) + rv1)]);\n        }\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* adaptive_pool_avg, float* adaptive_pool_sum) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 195; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 48; ++i3_s) {\n      adaptive_pool_avg[((i0_i1_fused_i2_fused * 48) + i3_s)] = (adaptive_pool_sum[((i0_i1_fused_i2_fused * 48) + i3_s)] / (((float)(((((((((i0_i1_fused_i2_fused * 6) + (i3_s >> 3)) & 7) + 1) % 8) == 0) ? (((((((i0_i1_fused_i2_fused * 6) + (i3_s >> 3)) & 7) + 1) >> 3) : ((((((((i0_i1_fused_i2_fused * 6) + (i3_s >> 3)) & 7) + 1) >> 3) + 1))) * ((float)(((((((((i3_s & 7) * 2) + 2) % 8) == 0) ? (((((i3_s & 7) + 1) >> 2) : ((((((i3_s & 7) + 1) >> 2) + 1)) - ((((i3_s & 7) >> 2)))));\n    }\n  }\n}\n//|End-of-Code|\n    "
    },
    {
        "op_name": "sum",
        "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[27];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 27; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 2210; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 27; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 27) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 27; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1865; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 16) + (((int)threadIdx.x) >> 1)) < 29835) {\n      normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 15, 18, 17), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([27], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((27,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(27):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(2210, 27):\n            data_1 = T.Buffer((59670,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] + data_1[k0_k1_fused_k2_fused_k3_fused_outer * 27 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(27):\n            data_red_1[0] = data_red_1[0] + data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]",
        "op_args": [
            13,
            15,
            18,
            17
        ],
        "input_shape": "[[13, 15, 18, 17]]",
        "output_shape": "[[]]",
        "deepseek_c": "",
        "starcoder_c": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[18];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 18; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1865; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 18; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 18) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 18; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[27];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 27; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 2310; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 27; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 27) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 27; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}",
        "gemini1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* data, float* data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n\n  // Simulate threadIdx.x using OpenMP\n  #pragma omp parallel for\n  for (int i = 0; i < 32; ++i) {\n    int threadIdx_x = i;\n    for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1865; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n      if (((k0_k1_fused_k2_fused_k3_fused_outer * 16) + (threadIdx_x >> 1)) < 29835) {\n        normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + threadIdx_x)]);\n      }\n    }\n\n    // Simulate __shfl_down_sync using OpenMP\n    #pragma omp barrier\n    if (threadIdx_x < 16) {\n      red_buf0[0] += normal_reduce_temp0[0];\n    }\n    #pragma omp barrier\n    if (threadIdx_x < 8) {\n      red_buf0[0] += normal_reduce_temp0[0];\n    }\n    #pragma omp barrier\n    if (threadIdx_x < 4) {\n      red_buf0[0] += normal_reduce_temp0[0];\n    }\n    #pragma omp barrier\n    if (threadIdx_x < 2) {\n      red_buf0[0] += normal_reduce_temp0[0];\n    }\n    #pragma omp barrier\n    if (threadIdx_x < 1) {\n      red_buf0[0] += normal_reduce_temp0[0];\n    }\n    #pragma omp barrier\n    if (threadIdx_x == 0) {\n      data_red[0] = red_buf0[0];\n    }\n  }\n}\n\n    ",
        "gemini2_c": "#include <stdio.h>\n\nvoid default_function_kernel(float* data, float* data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1865; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 16) + 0) < 29835) {\n      normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + 0)]);\n    }\n  }\n  red_buf0[0] = normal_reduce_temp0[0];\n  data_red[0] = red_buf0[0];\n}\n",
        "gpt1_c": "#include <omp.h>\n#include <float.h>\n\nvoid default_function_kernel(const float* data, float* data_red) {\n    float normal_reduce_temp0 = 0.0f;\n    int k0_k1_fused_k2_fused_k3_fused_outer;\n    \n    // Parallelize the outer loop using OpenMP\n    #pragma omp parallel for reduction(+:normal_reduce_temp0) schedule(static)\n    for (k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1865; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n        int start_idx = k0_k1_fused_k2_fused_k3_fused_outer * 32;\n        int end_idx = start_idx + 32;\n        for (int idx = start_idx; idx < end_idx; ++idx) {\n            if (idx < 29835) {\n                normal_reduce_temp0 += data[idx];\n            }\n        }\n    }\n\n    // Reduction step (assuming we only have one thread writing to data_red)\n    data_red[0] = normal_reduce_temp0;\n}\n\n// Example usage\nint main() {\n    // Dimensions of data: [13, 15, 18, 17]\n    const int total_size = 13 * 15 * 18 * 17;\n    float* data = (float*)malloc(total_size * sizeof(float));\n    float* data_red = (float*)malloc(sizeof(float));\n\n    // Fill 'data' with values...\n\n    default_function_kernel(data, data_red);\n\n    // Use 'data_red[0]' as needed...\n\n    free(data);\n    free(data_red);\n\n    return 0;\n}\n",
        "gpt2_c": "#include <stdio.h>\n\nvoid default_function_kernel(float* data, float* data_red) {\n    const int num_elements = 13 * 15 * 18 * 17; // Total number of elements in the tensor\n    const int num_outer = 1865; // Number of outer iterations\n    const int num_threads = 32; // Number of threads\n\n    float normal_reduce_temp0 = 0.0f;\n    \n    // Perform reduction across the data\n    for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < num_outer; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n        int index = (k0_k1_fused_k2_fused_k3_fused_outer * 32) + 0; // Considering threadIdx.x == 0 for simplicity\n        if ((k0_k1_fused_k2_fused_k3_fused_outer * 16) < num_elements) {\n            normal_reduce_temp0 += data[index];\n        }\n    }\n\n    // Reduction across the threads using a simplified approach\n    float red_buf0 = normal_reduce_temp0;\n    \n    // Reduction loop\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        red_buf0 += red_buf0; // In place of __shfl_down_sync\n    }\n\n    // Store the result\n    data_red[0] = red_buf0;\n}\n\n// Example usage:\n// float data[13 * 15 * 18 * 17];\n// float data_red[1];\n// default_function_kernel(data, data_red);\n\n|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* data, float* data_red) {\n  #pragma omp parallel for\n  for (int32_t threadIdx_x = 0; threadIdx_x < 32; ++threadIdx_x) {\n    float normal_reduce_temp0[1];\n    normal_reduce_temp0[0] = 0.000000e+00f;\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1865; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n      if (((k0_k1_fused_k2_fused_k3_fused_outer * 16) + (threadIdx_x >> 1)) < 29835) {\n        normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + threadIdx_x)]);\n      }\n    }\n    float red_buf0[1];\n    red_buf0[0] = normal_reduce_temp0[0];\n    // Implement reduction using OpenMP\n    #pragma omp atomic\n    data_red[0] += red_buf0[0];\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "fast_softmax",
        "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 260; ++i0_i1_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      T_softmax_maxelem[0] = -3.402823e+38f;\n      for (int32_t k = 0; k < 15; ++k) {\n        T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[(((i0_i1_fused * 45) + (i2 * 15)) + k)]);\n      }\n      T_softmax_expsum[0] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 15; ++k_1) {\n          int32_t v_ = ((int32_t)(floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0_i1_fused * 45) + (i2 * 15)) + k_1)] - T_softmax_maxelem[0])));\n      }\n      for (int32_t i3_s = 0; i3_s < 15; ++i3_s) {\n          int32_t v__1 = ((int32_t)(floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_norm[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0_i1_fused * 45) + (i2 * 15)) + i3_s)] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 195) {\n    T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 15; ++k) {\n    if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 195) {\n        int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(52) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)])) / T_softmax_expsum[(((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 15)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) < 195) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 15; ++k) {\n    if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) < 195) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 960) + (((int)threadIdx.x) * 15)) + k)]);\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 20, 3, 15), \"float32\"), T_softmax_norm: T.Buffer((13, 20, 3, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(260):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            for i2 in range(3):\n                T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n                T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((11700,), data=data.data)\n                for k in range(15):\n                    T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused * 45 + i2 * 15 + k])\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(15):\n                    cse_var_1: T.int32 = i0_i1_fused * 45 + i2 * 15 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0])\n                for i3_s in range(15):\n                    cse_var_2: T.int32 = i0_i1_fused * 45 + i2 * 15 + i3_s\n                    T_softmax_norm_1 = T.Buffer((11700,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]",
        "op_args": [
            13,
            20,
            3,
            15
        ],
        "input_shape": "[[13, 20, 3, 15]]",
        "output_shape": "[[13, 20, 3, 15]]",
        "deepseek_c": "void default_function_kernel(float* T_softmax_maxelem, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3 = 0; i3 < 15; ++i3) {\n          T_softmax_maxelem[((((i0 * 60) + (i1 * 3)) + (i2 * 20)) + i3)] = -3.402823e+38f;\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3 = 0; i3 < 15; ++i3) {\n          for (int32_t k = 0; k < 15; ++k) {\n            T_softmax_maxelem[((((i0 * 60) + (i1 * 3)) + (i2 * 20)) + i3)] = fmaxf(T_softmax_maxelem[((((i0 * 60) + (i1 * 3)) + (i2 * 20)) + i3)], data[(((((i0 * 960) + ((i1 * 48) + (i2 * 16)) + (i3 * 15)) + k) + 0) + 0)]);\n          }\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 260; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      T_softmax_maxelem[((i0_i1_fused * 3) + i2)] = -3.402823e+38f;\n      for (int32_t k = 0; k < 15; ++k) {\n        T_softmax_maxelem[((i0_i1_fused * 3) + i2)] = max(T_softmax_maxelem[((i0_i1_fused * 3) + i2)], data[(((i0_i1_fused * 45) + (i2 * 15)) + k)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 780; ++i0_i1_fused_i2_fused) {\n    T_softmax_expsum[i0_i1_fused_i2_fused] = 0.000000e+00f;\n  }\n  for (int32_t k = 0; k < 15; ++k) {\n    #pragma omp parallel for\n    for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 780; ++i0_i1_fused_i2_fused_1) {\n      int32_t v_ = ((int32_t)(floorf(((max(min((data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_expsum[i0_i1_fused_i2_fused_1] = (T_softmax_expsum[i0_i1_fused_i2_fused_1] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((i0_i1_fused_i2_fused_1 * 15) + k)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 780; ++i0_i1_fused_i2_fused_2) {\n    for (int32_t i3 = 0; i3 < 15; ++i3) {\n      int32_t v_1 = ((int32_t)(floorf(((max(min((data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_norm[((i0_i1_fused_i2_fused_2 * 15) + i3)] = (max(((*(float *)(&(v_1))) * ((((((((((((((1.987569e-04f * (max(min((data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((i0_i1_fused_i2_fused_2 * 15) + i3)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2])) / T_softmax_expsum[i0_i1_fused_i2_fused_2]);\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1300; ++i0_i1_fused_i2_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i3 = 0; i3 < 15; ++i3) {\n      T_softmax_maxelem[0] = -3.402823e+38f;\n      for (int32_t k = 0; k < 15; ++k) {\n        T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[(((i0_i1_fused_i2_fused * 15) + k) + (i3 * 450))]);\n      }\n      T_softmax_expsum[0] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 15; ++k_1) {\n        int v_ = ((int)(floorf(((max(min((data[(((i0_i1_fused_i2_fused * 15) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23);\n        T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0_i1_fused_i2_fused * 15) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused_i2_fused * 15) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0_i1_fused_i2_fused * 15) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused_i2_fused * 15) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0_i1_fused_i2_fused * 15) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused_i2_fused * 15) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0_i1_fused_i2_fused * 15) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused_i2_fused * 15) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0_i1_fused_i2_fused * 15) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused_i2_fused * 15) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0_i1_fused_i2_fused * 15) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused_i2_fused * 15) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0_i1_fused_i2_fused * 15) + k_1)] - T_softmax_maxelem[0])))))))))))))))))));\n      }\n      for (int32_t k_2 = 0; k_2 < 15; ++k_2) {\n        int v_ = ((int)(floorf(((max(min((data[(((i0_i1_fused_i2_fused * 15) + k_2)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23);\n        T_softmax_norm[(((i0_i1_fused_i2_fused * 15) + k_2) + (i3 * 450))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0_i1_fused_i2_fused * 15) + k_2)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused_i2_fused * 15) + k_2)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0_i1_fused_i2_fused * 15) + k_2)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused_i2_fused * 15) + k_2)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0_i1_fused_i2_fused * 15) + k_2)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused_i2_fused * 15) + k_2)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0_i1_fused_i2_fused * 15) + k_2)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused_i2_fused * 15) + k_2)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0_i1_fused_i2_fused * 15) + k_2)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused_i2_fused * 15) + k_2)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0_i1_fused_i2_fused * 15) + k_2)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused_i2_fused * 15) + k_2)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0_i1_fused_i2_fused * 15) + k_2)] - T_softmax_maxelem[0])))))))))))))))))));\n      }\n    }\n  }\n}",
        "gemini1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\n#define max(a, b) ((a) > (b) ? (a) : (b))\n#define min(a, b) ((a) < (b) ? (a) : (b))\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n  #pragma omp parallel for\n  for (int blockIdx_x = 0; blockIdx_x < 13; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 64; ++threadIdx_x) {\n      if (((blockIdx_x * 16) + (threadIdx_x >> 2)) < 195) {\n        T_softmax_maxelem[(blockIdx_x * 64) + threadIdx_x] = -3.402823e+38f;\n        for (int k = 0; k < 15; ++k) {\n          if (((blockIdx_x * 16) + (threadIdx_x >> 2)) < 195) {\n            T_softmax_maxelem[(blockIdx_x * 64) + threadIdx_x] = max(T_softmax_maxelem[(blockIdx_x * 64) + threadIdx_x], data[((blockIdx_x * 960) + (threadIdx_x * 15)) + k]);\n          }\n        }\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* T_softmax_expsum, float* T_softmax_maxelem, float* data) {\n  #pragma omp parallel for\n  for (int blockIdx_x = 0; blockIdx_x < 13; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 32; ++threadIdx_x) {\n      if (((blockIdx_x * 8) + (threadIdx_x >> 2)) < 195) {\n        T_softmax_expsum[(blockIdx_x * 32) + threadIdx_x] = 0.000000e+00f;\n        for (int k = 0; k < 15; ++k) {\n          if (((blockIdx_x * 8) + (threadIdx_x >> 2)) < 195) {\n            int v_ = ((int)(floorf(((max(min((data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n            T_softmax_expsum[(blockIdx_x * 32) + threadIdx_x] = (T_softmax_expsum[(blockIdx_x * 32) + threadIdx_x] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((blockIdx_x * 480) + (threadIdx_x * 15)) + k] - T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x])));\n          }\n        }\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_2(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int blockIdx_x = 0; blockIdx_x < 13 * 20; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 52; ++threadIdx_x) {\n      int v_ = ((int)(floorf(((max(min((data[(blockIdx_x * 52) + threadIdx_x] - T_softmax_maxelem[((blockIdx_x * 52) + threadIdx_x) / 15]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_norm[(blockIdx_x * 52) + threadIdx_x] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(blockIdx_x * 52) + threadIdx_x] - T_softmax_maxelem[((blockIdx_x * 52) + threadIdx_x) / 15]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(blockIdx_x * 52) + threadIdx_x] - T_softmax_maxelem[((blockIdx_x * 52) + threadIdx_x) / 15]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(blockIdx_x * 52) + threadIdx_x] - T_softmax_maxelem[((blockIdx_x * 52) + threadIdx_x) / 15]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(blockIdx_x * 52) + threadIdx_x] - T_softmax_maxelem[((blockIdx_x * 52) + threadIdx_x) / 15]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(blockIdx_x * 52) + threadIdx_x] - T_softmax_maxelem[((blockIdx_x * 52) + threadIdx_x) / 15]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(blockIdx_x * 52) + threadIdx_x] - T_softmax_maxelem[((blockIdx_x * 52) + threadIdx_x) / 15]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(blockIdx_x * 52) + threadIdx_x] - T_softmax_maxelem[((blockIdx_x * 52) + threadIdx_x) / 15]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(blockIdx_x * 52) + threadIdx_x] - T_softmax_maxelem[((blockIdx_x * 52) + threadIdx_x) / 15]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(blockIdx_x * 52) + threadIdx_x] - T_softmax_maxelem[((blockIdx_x * 52) + threadIdx_x) / 15]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(blockIdx_x * 52)",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n#include <float.h>\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n  // Input tensor shape: [[13, 20, 3, 15]]\n  // T_softmax_maxelem shape: [13, 20, 3]\n  // data shape: [13, 20, 3, 15]\n  for (int blockIdx_x = 0; blockIdx_x < 13; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 20; ++threadIdx_x) {\n      for (int k = 0; k < 3; ++k) {\n        T_softmax_maxelem[((blockIdx_x * 60) + (threadIdx_x * 3)) + k] = -FLT_MAX;\n        for (int i = 0; i < 15; ++i) {\n          T_softmax_maxelem[((blockIdx_x * 60) + (threadIdx_x * 3)) + k] = fmaxf(T_softmax_maxelem[((blockIdx_x * 60) + (threadIdx_x * 3)) + k], data[((blockIdx_x * 960) + (threadIdx_x * 45)) + (k * 15) + i]);\n        }\n      }\n    }\n  }\n}\n\n    ",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <float.h> // For FLT_MIN and FLT_MAX\n#include <math.h>  // For floorf, max, and min\n\nvoid default_function_kernel_1(float* T_softmax_expsum, float* T_softmax_maxelem, float* data, int num_blocks, int threads_per_block) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < num_blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads_per_block; ++threadIdx_x) {\n            if ((blockIdx_x * 8 + (threadIdx_x >> 2)) < 195) {\n                T_softmax_expsum[blockIdx_x * threads_per_block + threadIdx_x] = 0.0f;\n            }\n            for (int k = 0; k < 15; ++k) {\n                if ((blockIdx_x * 8 + (threadIdx_x >> 2)) < 195) {\n                    float diff = data[(blockIdx_x * 480 + threadIdx_x * 15) + k] - T_softmax_maxelem[blockIdx_x * threads_per_block + threadIdx_x];\n                    float clamped = max(min(diff, 88.37627f), -88.37626f);\n                    int v_ = ((int)(floorf((clamped * 1.442695f) + 0.5f)) + 127) << 23;\n                    float exp_val = *(float *)(&v_);\n\n                    float term = diff - (floorf(clamped * 1.442695f + 0.5f) * 0.6931472f);\n                    float exp_approx = ((((((((((((((1.987569e-04f * term) + 1.398200e-03f) * term) + 8.333452e-03f) * term) + 4.166580e-02f) * term) + 1.666667e-01f) * term) + 5.000000e-01f) * term) + 1.000000e+00f) * exp_val);\n\n                    T_softmax_expsum[blockIdx_x * threads_per_block + threadIdx_x] += exp_approx;\n                }\n            }\n        }\n    }\n}\n\nvoid default_function_kernel_2(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data, int num_blocks, int threads_per_block) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < num_blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads_per_block; ++threadIdx_x) {\n            float diff = data[blockIdx_x * threads_per_block + threadIdx_x] - T_softmax_maxelem[(blockIdx_x * threads_per_block + threadIdx_x) / 15];\n            float clamped = max(min(diff, 88.37627f), -88.37626f);\n            int v_ = ((int)(floorf((clamped * 1.442695f) + 0.5f)) + 127) << 23;\n            float exp_val = *(float *)(&v_);\n\n            float term = diff - (floorf(clamped * 1.442695f + 0.5f) * 0.6931472f);\n            float exp_approx = ((((((((((((((1.987569e-04f * term) + 1.398200e-03f) * term) + 8.333452e-03f) * term) + 4.166580e-02f) * term) + 1.666667e-01f) * term) + 5.000000e-01f) * term) + 1.000000e+00f) * exp_val);\n\n            T_softmax_norm[blockIdx_x * threads_per_block + threadIdx_x] = exp_approx / T_softmax_expsum[(blockIdx_x * threads_per_block + threadIdx_x) / 15];\n        }\n    }\n}\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data, int num_blocks, int threads_per_block) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < num_blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads_per_block; ++threadIdx_x) {\n            if ((blockIdx_x * 16 + (threadIdx_x >> 2)) < 195) {\n                T_softmax_maxelem[blockIdx_x * threads_per_block + threadIdx_x] = -FLT_MAX;\n            }\n            for (int k = 0; k < 15; ++k) {\n                if ((blockIdx_x * 16 + (threadIdx_x >> 2)) < 195) {\n                    T_softmax_maxelem[blockIdx_x * threads_per_block + threadIdx_x] = fmax(T_softmax_maxelem[blockIdx_x * threads_per_block + threadIdx_x], data[(blockIdx_x * 960 + threadIdx_x * 15) + k]);\n                }\n            }\n        }\n    }\n}\n\nint main() {\n    int blocks = 13 * 20 * 3 * 15;  // As per the tensor shape provided\n    int threads_per_block_1 = 32;   // Launch bounds for kernel 1\n    int threads_per_block_2 = 52;   // Launch bounds for kernel 2\n    int threads_per_block_3 = 64;   // Launch bounds for kernel 3\n\n    int total_elements_1 = blocks * threads_per_block_1;\n    int total_elements_2 = blocks * threads_per_block_2;\n    int total_elements_3 = blocks * threads_per_block_3;\n\n    float *T_softmax_expsum = (float *)malloc(total_elements_1 * sizeof(float));\n    float *T_softmax_maxelem = (float *)malloc(total_elements_3 * sizeof(float));\n    float *T_softmax_norm = (float *)malloc(total_elements_2 * sizeof(float));\n    float *data = (float *)malloc(total_elements_3 * sizeof(float));\n\n    // Initialize data and T_softmax_maxelem (example initialization)\n    for (int i = 0; i < total_elements_3; ++i) {\n        data[i] = (float)i * 0.01f;\n    }\n\n    // Call the first function to compute T_softmax_maxelem\n    default_function_kernel(T_softmax_maxelem, data, blocks, threads_per_block_3);\n\n    // Call the second function to compute T_softmax_expsum\n    default_function_kernel_1(T_softmax_expsum, T_softmax_maxelem, data, blocks, threads_per_block_1);\n\n    // Call the third function to compute T_softmax_norm\n    default_function_kernel_2(T_softmax_expsum, T_softmax_maxelem, T_softmax_norm, data, blocks, threads_per_block_2);\n\n    // Print the result of the third function\n    for (int i = 0; i < total_elements_2; ++i) {\n        printf(\"%f \", T_softmax_norm[i]);\n    }\n    printf(\"\\n\");\n\n    free(T_softmax_expsum);\n    free(T_softmax_maxelem);\n    free(T_softmax_norm);\n    free(data);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <stdio.h>\n#include <math.h>\n#include <float.h>\n\nvoid default_function_kernel_1(float* T_softmax_expsum, float* T_softmax_maxelem, float* data) {\n    // Assuming blockIdx.x and threadIdx.x are provided or initialized elsewhere\n    int blockIdx_x = 0; // Example value, should be set appropriately\n    int threadIdx_x = 0; // Example value, should be set appropriately\n\n    if (((blockIdx_x * 8) + (threadIdx_x >> 2)) < 195) {\n        T_softmax_expsum[(blockIdx_x * 32) + threadIdx_x] = 0.0f;\n    }\n    for (int k = 0; k < 15; ++k) {\n        if (((blockIdx_x * 8) + (threadIdx_x >> 2)) < 195) {\n            int index = ((blockIdx_x * 480) + (threadIdx_x * 15)) + k;\n            float max_elem = T_softmax_maxelem[(blockIdx_x * 32) + threadIdx_x];\n            float data_value = data[index];\n            float diff = data_value - max_elem;\n            float exp_val = expf(diff);\n            T_softmax_expsum[(blockIdx_x * 32) + threadIdx_x] += exp_val;\n        }\n    }\n}\n\nvoid default_function_kernel_2(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data) {\n    // Assuming blockIdx.x and threadIdx.x are provided or initialized elsewhere\n    int blockIdx_x = 0; // Example value, should be set appropriately\n    int threadIdx_x = 0; // Example value, should be set appropriately\n\n    int index = (blockIdx_x * 52) + threadIdx_x;\n    float max_elem = T_softmax_maxelem[index / 15];\n    float data_value = data[index];\n    float diff = data_value - max_elem;\n    float exp_val = expf(diff);\n    T_softmax_norm[index] = exp_val / T_softmax_expsum[index / 15];\n}\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n    // Assuming blockIdx.x and threadIdx.x are provided or initialized elsewhere\n    int blockIdx_x = 0; // Example value, should be set appropriately\n    int threadIdx_x = 0; // Example value, should be set appropriately\n\n    if (((blockIdx_x * 16) + (threadIdx_x >> 2)) < 195) {\n        T_softmax_maxelem[(blockIdx_x * 64) + threadIdx_x] = -FLT_MAX;\n    }\n    for (int k = 0; k < 15; ++k) {\n        if (((blockIdx_x * 16) + (threadIdx_x >> 2)) < 195) {\n            int index = ((blockIdx_x * 960) + (threadIdx_x * 15)) + k;\n            T_softmax_maxelem[(blockIdx_x * 64) + threadIdx_x] = fmaxf(T_softmax_maxelem[(blockIdx_x * 64) + threadIdx_x], data[index]);\n        }\n    }\n}\n\n// Example usage:\n// float T_softmax_expsum[...];\n// float T_softmax_maxelem[...];\n// float T_softmax_norm[...];\n// float data[...];\n// int blockIdx_x = ...;\n// int threadIdx_x = ...;\n\n// default_function_kernel_1(T_softmax_expsum, T_softmax_maxelem, data);\n// default_function_kernel_2(T_softmax_expsum, T_softmax_maxelem, T_softmax_norm, data);\n// default_function_kernel(T_softmax_maxelem, data);\n\n|End-of-Code|\n",
        "gemini3_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 195; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 16; ++i3_s) {\n      if (((i0_i1_fused_i2_fused * 16) + i3_s) < 195) {\n        T_softmax_maxelem[((i0_i1_fused_i2_fused * 16) + i3_s)] = -3.402823e+38f;\n      }\n      for (int32_t k = 0; k < 15; ++k) {\n        if (((i0_i1_fused_i2_fused * 16) + i3_s) < 195) {\n          T_softmax_maxelem[((i0_i1_fused_i2_fused * 16) + i3_s)] = fmaxf(T_softmax_maxelem[((i0_i1_fused_i2_fused * 16) + i3_s)], data[(((i0_i1_fused_i2_fused * 960) + (i3_s * 15)) + k)]);\n        }\n      }\n    }\n  }\n}\n"
    },
    {
        "op_name": "acos",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1496; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = acosf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(17) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 17) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 17) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 17, 1, 11), \"float32\"), compute: T.Buffer((8, 17, 1, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(1496):\n            compute_1 = T.Buffer((1496,), data=compute.data)\n            data_1 = T.Buffer((1496,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.acos(data_1[i0_i1_fused_i2_fused_i3_fused])",
        "op_args": [
            8,
            17,
            1,
            11
        ],
        "input_shape": "[[8, 17, 1, 11]]",
        "output_shape": "[[8, 17, 1, 11]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 1; ++i2) {\n        for (int32_t i3 = 0; i3 < 11; ++i3) {\n          compute[((((i0 * 187) + (i1 * 11)) + (i2 * 11)) + i3)] = acosf(data[((((i0 * 187) + (i1 * 11)) + (i2 * 11)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1496; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = acosf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1368; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = acosf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 136; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 11; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 11) + i3_s)] = acosf(data[((i0_i1_fused_i2_fused * 11) + i3_s)]);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "asin",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        for (int32_t i3_s = 0; i3_s < 7; ++i3_s) {\n          compute[((((i0 * 847) + (i1 * 77)) + (i2 * 7)) + i3_s)] = asinf(data[((((i0 * 847) + (i1 * 77)) + (i2 * 7)) + i3_s)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 7623) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 11, 11, 7), \"float32\"), compute: T.Buffer((9, 11, 11, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(9):\n            for i1, i2, i3_s in T.grid(11, 11, 7):\n                cse_var_1: T.int32 = i0 * 847 + i1 * 77 + i2 * 7 + i3_s\n                compute_1 = T.Buffer((7623,), data=compute.data)\n                data_1 = T.Buffer((7623,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])",
        "op_args": [
            9,
            11,
            11,
            7
        ],
        "input_shape": "[[9, 11, 11, 7]]",
        "output_shape": "[[9, 11, 11, 7]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        for (int32_t i3 = 0; i3 < 7; ++i3) {\n          compute[((((i0 * 770) + (i1 * 70)) + (i2 * 7)) + i3)] = asinf(data[((((i0 * 770) + (i1 * 70)) + (i2 * 7)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 7623; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = asinf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 7623; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = asinf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1089; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 7; ++i3_s) {\n      if (((i0_i1_fused_i2_fused * 7) + i3_s) < 7623) {\n        compute[((i0_i1_fused_i2_fused * 7) + i3_s)] = asinf(data[((i0_i1_fused_i2_fused * 7) + i3_s)]);\n      }\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "batch_to_space_nd",
        "c_code": "void default_function_kernel(float* T_strided_slice, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    float T_transpose[3040];\n    float T_reshape[80];\n    for (int32_t ax1 = 0; ax1 < 19; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        for (int32_t ax1_1 = 0; ax1_1 < 2; ++ax1_1) {\n          for (int32_t ax4 = 0; ax4 < 2; ++ax4) {\n            for (int32_t ax5 = 0; ax5 < 20; ++ax5) {\n              T_reshape[(((ax1_1 * 40) + (ax4 * 20)) + ax5)] = data[((((((ax2 * 4560) + (ax1_1 * 2280)) + (ax0 * 760)) + (ax1 * 40)) + (ax4 * 20)) + ax5)];\n            }\n          }\n        }\n        for (int32_t ax3 = 0; ax3 < 2; ++ax3) {\n          for (int32_t ax4_1 = 0; ax4_1 < 2; ++ax4_1) {\n            for (int32_t ax5_1 = 0; ax5_1 < 20; ++ax5_1) {\n              T_transpose[(((((ax1 * 160) + (ax2 * 80)) + (ax3 * 40)) + (ax4_1 * 20)) + ax5_1)] = T_reshape[(((ax4_1 * 40) + (ax3 * 20)) + ax5_1)];\n            }\n          }\n        }\n      }\n    }\n    for (int32_t ax1_2 = 0; ax1_2 < 38; ++ax1_2) {\n      for (int32_t ax2_1 = 0; ax2_1 < 4; ++ax2_1) {\n        for (int32_t ax3_1 = 0; ax3_1 < 20; ++ax3_1) {\n          T_strided_slice[((((ax0 * 3040) + (ax1_2 * 80)) + (ax2_1 * 20)) + ax3_1)] = T_transpose[(((ax1_2 * 80) + (ax2_1 * 20)) + ax3_1)];\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data) {\n  T_strided_slice[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = data[(((((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 3)) % 20) / 10) * 4560) + (((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 10) / 5) * 2280)) + ((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 3)) / 20) * 40)) + (((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 3)) % 10) / 5) * 20)) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 20))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 19, 2, 20), \"float32\"), T_strided_slice: T.Buffer((3, 38, 4, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(3):\n            T_transpose = T.allocate([3040], \"float32\", \"global\")\n            T_reshape = T.allocate([80], \"float32\", \"global\")\n            for ax1, ax2 in T.grid(19, 2):\n                T_reshape_1 = T.Buffer((80,), data=T_reshape)\n                for ax1_1, ax4, ax5 in T.grid(2, 2, 20):\n                    cse_var_1: T.int32 = ax4 * 20\n                    data_1 = T.Buffer((11400,), data=data.data)\n                    T_reshape_1[ax1_1 * 40 + cse_var_1 + ax5] = data_1[ax2 * 4560 + ax1_1 * 2280 + ax0 * 760 + ax1 * 40 + cse_var_1 + ax5]\n                for ax3, ax4, ax5 in T.grid(2, 2, 20):\n                    T_transpose_1 = T.Buffer((3040,), data=T_transpose)\n                    T_transpose_1[ax1 * 160 + ax2 * 80 + ax3 * 40 + ax4 * 20 + ax5] = T_reshape_1[ax4 * 40 + ax3 * 20 + ax5]\n            for ax1, ax2, ax3 in T.grid(38, 4, 20):\n                cse_var_3: T.int32 = ax1 * 80\n                cse_var_2: T.int32 = ax2 * 20\n                T_strided_slice_1 = T.Buffer((9120,), data=T_strided_slice.data)\n                T_transpose_1 = T.Buffer((3040,), data=T_transpose)\n                T_strided_slice_1[ax0 * 3040 + cse_var_3 + cse_var_2 + ax3] = T_transpose_1[cse_var_3 + cse_var_2 + ax3]",
        "op_args": [
            15,
            19,
            2,
            20
        ],
        "input_shape": "[[15, 19, 2, 20]]",
        "output_shape": "[[3, 38, 4, 20]]",
        "deepseek_c": "void default_function_kernel(float* T_strided_slice, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 570; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n      T_strided_slice[((ax0_ax1_fused_ax2_fused * 20) + ax3)] = data[((((((ax0_ax1_fused_ax2_fused / 10) * 4560) + (((ax0_ax1_fused_ax2_fused % 10) / 5) * 2280)) + (((ax0_ax1_fused_ax2_fused / 20) * 40)) + (((ax0_ax1_fused_ax2_fused % 10) / 5) * 20)) + (((ax0_ax1_fused_ax2_fused % 20) * 20)) + ax3))];\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* T_strided_slice, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 15; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n      T_strided_slice[((ax0_ax1_fused_ax2_fused * 20) + ax3)] = data[((((((ax0_ax1_fused_ax2_fused / 10) * 4560) + (((ax0_ax1_fused_ax2_fused % 10) / 5) * 2280)) + ((ax3 / 10) * 40)) + (((ax0_ax1_fused_ax2_fused % 10) / 5) * 20)) + (ax3 % 20))];\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_strided_slice, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 140; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n      T_strided_slice[((ax0_ax1_fused_ax2_fused * 20) + ax3)] = data[(((((((ax0_ax1_fused_ax2_fused % 10) / 5) * 4560) + (((ax0_ax1_fused_ax2_fused % 20) / 10) * 2280)) + ((ax0_ax1_fused_ax2_fused / 20) * 40)) + ((ax0_ax1_fused_ax2_fused % 5) * 20)) + ax3)];\n    }\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* T_strided_slice, float* data) {\n  #pragma omp parallel for\n  for (int blockIdx_x = 0; blockIdx_x < 15; blockIdx_x++) {\n    for (int threadIdx_x = 0; threadIdx_x < 24; threadIdx_x++) {\n      T_strided_slice[((blockIdx_x * 24) + threadIdx_x)] = data[(((((((((((blockIdx_x * 3) + (threadIdx_x >> 3)) % 20) / 10) * 4560) + (((((((blockIdx_x * 6) + (threadIdx_x >> 2)) % 10) / 5) * 2280)) + ((((((blockIdx_x * 3) + (threadIdx_x >> 3)) / 20) * 40)) + (((((((blockIdx_x * 3) + (threadIdx_x >> 3)) % 10) / 5) * 20)) + (((blockIdx_x * 4) + threadIdx_x) % 20)))];\n    }\n  }\n}\n",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel(float* T_strided_slice, float* data) {\n  #pragma omp parallel for\n  for (int i = 0; i < 15; i++) {\n    for (int j = 0; j < 24; j++) {\n      T_strided_slice[(i * 24) + j] = data[((((((((((i * 3) + (j >> 3)) % 20) / 10) * 4560) + (((((((i * 6) + (j >> 2)) % 10) / 5) * 2280)) + (((((i * 3) + (j >> 3)) / 20) * 40)) + (((((i * 3) + (j >> 3)) % 10) / 5) * 20)) + (((i * 4) + j) % 20))];\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* T_strided_slice, float* data) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 15;\n    int blockDim_x = 24;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * 24) + threadIdx_x;\n            int calc_index = ((((((blockIdx_x * 3) + (threadIdx_x >> 3)) % 20) / 10) * 4560) +\n                              (((((blockIdx_x * 6) + (threadIdx_x >> 2)) % 10) / 5) * 2280) +\n                              ((((blockIdx_x * 3) + (threadIdx_x >> 3)) / 20) * 40) +\n                              ((((blockIdx_x * 3) + (threadIdx_x >> 3)) % 10) / 5) * 20 +\n                              (((blockIdx_x * 4) + threadIdx_x) % 20));\n            T_strided_slice[idx] = data[calc_index];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* T_strided_slice, float* data) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 15 * 19 * 2; // Based on input tensor shape (15 * 19 * 2)\n    int blockDim_x = 24;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int calc_index = (((((blockIdx_x * 3 + (threadIdx_x >> 3)) % 20) / 10) * 4560) +\n                              (((((blockIdx_x * 6 + (threadIdx_x >> 2)) % 10) / 5) * 2280) +\n                              (((blockIdx_x * 3 + (threadIdx_x >> 3)) / 20) * 40)) +\n                              (((((blockIdx_x * 3 + (threadIdx_x >> 3)) % 10) / 5) * 20) +\n                              ((blockIdx_x * 4 + threadIdx_x) % 20));\n            T_strided_slice[idx] = data[calc_index];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* T_strided_slice, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1140; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 20; ++i3_s) {\n      T_strided_slice[((i0_i1_fused_i2_fused * 20) + i3_s)] = data[(((((((((i0_i1_fused_i2_fused / 114) * 3) + (i3_s >> 3)) % 20) / 10) * 4560) + (((((i0_i1_fused_i2_fused / 114) * 6) + (i3_s >> 2)) % 10) / 5) * 2280)) + (((((i0_i1_fused_i2_fused / 114) * 3) + (i3_s >> 3)) / 20) * 40)) + (((((i0_i1_fused_i2_fused / 114) * 3) + (i3_s >> 3)) % 10) / 5) * 20)) + (((((i0_i1_fused_i2_fused / 114) * 4) + i3_s) % 20))];\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "global_pool_max",
        "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 22; ++ax0_ax1_fused) {\n    adaptive_pool_max[ax0_ax1_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 4; ++rv0) {\n      adaptive_pool_max[ax0_ax1_fused] = max(adaptive_pool_max[ax0_ax1_fused], data[((ax0_ax1_fused * 4) + rv0)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(22) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((int)threadIdx.x)] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 4; ++rv0) {\n    adaptive_pool_max[((int)threadIdx.x)] = max(adaptive_pool_max[((int)threadIdx.x)], data[((((int)threadIdx.x) * 4) + rv0)]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 2, 4, 1), \"float32\"), adaptive_pool_max: T.Buffer((11, 2, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(22):\n            adaptive_pool_max_1 = T.Buffer((22,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0 in range(4):\n                data_1 = T.Buffer((88,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused], data_1[ax0_ax1_fused * 4 + rv0])",
        "op_args": [
            11,
            2,
            4,
            1
        ],
        "input_shape": "[[11, 2, 4, 1]]",
        "output_shape": "[[11, 2, 1, 1]]",
        "deepseek_c": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 88; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = -3.402823e+38f;\n    for (int rv0 = 0; rv0 < 4; ++rv0) {\n      adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = max(adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused], data[((ax0_ax1_fused_ax2_fused_ax3_fused * 4) + rv0)]);\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 22; ++ax0_ax1_fused) {\n    adaptive_pool_max[ax0_ax1_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 4; ++rv0) {\n      adaptive_pool_max[ax0_ax1_fused] = max(adaptive_pool_max[ax0_ax1_fused], data[((ax0_ax1_fused * 4) + rv0)]);\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 44; ++ax0_ax1_fused_ax2_fused) {\n    adaptive_pool_max[ax0_ax1_fused_ax2_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 4; ++rv0) {\n      adaptive_pool_max[ax0_ax1_fused_ax2_fused] = max(adaptive_pool_max[ax0_ax1_fused_ax2_fused], data[((ax0_ax1_fused_ax2_fused * 4) + rv0)]);\n    }\n  }\n}",
        "gemini1_c": "#include <omp.h>\n#include <math.h> // For the max function\n\nvoid default_function_kernel(float* adaptive_pool_max, float* data) {\n  // Calculate the number of threads needed\n  int num_threads = 11 * 2 * 22;\n\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i = 0; i < num_threads; ++i) {\n    // Calculate threadIdx.x from the global thread index i\n    int threadIdx_x = i;\n\n    adaptive_pool_max[threadIdx_x] = -3.402823e+38f; // Initialize with negative infinity\n    for (int rv0 = 0; rv0 < 4; ++rv0) {\n      adaptive_pool_max[threadIdx_x] = fmaxf(adaptive_pool_max[threadIdx_x], data[((threadIdx_x * 4) + rv0)]);\n    }\n  }\n}\n//|End-of-Code|\n    ",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* adaptive_pool_max, float* data) {\n  // Input tensor shape: [[11, 2, 4, 1]]\n  // adaptive_pool_max shape: [22]\n  // data shape: [11, 2, 4, 1]\n  for (int threadIdx_x = 0; threadIdx_x < 22; ++threadIdx_x) {\n    adaptive_pool_max[threadIdx_x] = -3.402823e+38f; // Initialize with negative infinity\n    for (int rv0 = 0; rv0 < 4; ++rv0) {\n      adaptive_pool_max[threadIdx_x] = fmaxf(adaptive_pool_max[threadIdx_x], data[((threadIdx_x * 4) + rv0)]);\n    }\n  }\n}\n",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <float.h> // For FLT_MAX\n#include <math.h>  // For fmax\n\nvoid default_function_kernel(float* adaptive_pool_max, float* data, int num_threads) {\n    #pragma omp parallel for\n    for (int threadIdx_x = 0; threadIdx_x < num_threads; ++threadIdx_x) {\n        adaptive_pool_max[threadIdx_x] = -FLT_MAX;\n        for (int rv0 = 0; rv0 < 4; ++rv0) {\n            adaptive_pool_max[threadIdx_x] = fmax(adaptive_pool_max[threadIdx_x], data[threadIdx_x * 4 + rv0]);\n        }\n    }\n}\n\nint main() {\n    int num_threads = 22;  // Launch bounds\n\n    int total_elements = num_threads;\n    int data_elements = num_threads * 4;\n\n    float *adaptive_pool_max = (float *)malloc(total_elements * sizeof(float));\n    float *data = (float *)malloc(data_elements * sizeof(float));\n\n    // Initialize data (example initialization)\n    for (int i = 0; i < data_elements; ++i) {\n        data[i] = (float)i * 0.01f;\n    }\n\n    // Call the function to compute adaptive_pool_max\n    default_function_kernel(adaptive_pool_max, data, num_threads);\n\n    // Print the result\n    for (int i = 0; i < total_elements; ++i) {\n        printf(\"%f \", adaptive_pool_max[i]);\n    }\n    printf(\"\\n\");\n\n    free(adaptive_pool_max);\n    free(data);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <stdio.h>\n#include <float.h> // For FLT_MIN\n\nvoid default_function_kernel(float* adaptive_pool_max, float* data) {\n    // Assuming threadIdx.x is provided or initialized elsewhere\n    int threadIdx_x = 0; // Example value, should be set appropriately\n\n    // Initialize adaptive_pool_max with the minimum possible float value\n    adaptive_pool_max[threadIdx_x] = -FLT_MAX;\n\n    // Iterate over the second dimension of size 4\n    for (int rv0 = 0; rv0 < 4; ++rv0) {\n        int data_index = (threadIdx_x * 4) + rv0;\n        adaptive_pool_max[threadIdx_x] = fmaxf(adaptive_pool_max[threadIdx_x], data[data_index]);\n    }\n}\n\n// Example usage:\n// float adaptive_pool_max[...];\n// float data[...];\n// int threadIdx_x = ...;\n\n// default_function_kernel(adaptive_pool_max, data);\n\n|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 88; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 22; ++i3_s) {\n      adaptive_pool_max[((i0_i1_fused_i2_fused * 22) + i3_s)] = -3.402823e+38f;\n      for (int rv0 = 0; rv0 < 4; ++rv0) {\n        adaptive_pool_max[((i0_i1_fused_i2_fused * 22) + i3_s)] = fmaxf(adaptive_pool_max[((i0_i1_fused_i2_fused * 22) + i3_s)], data[((((i0_i1_fused_i2_fused * 22) + i3_s) * 4) + rv0)]);\n      }\n    }\n  }\n}\n"
    },
    {
        "op_name": "asinh",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 240; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 19; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 19) + i3)] = asinhf(data[((i0_i1_fused_i2_fused * 19) + i3)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 4, 15, 19), \"float32\"), compute: T.Buffer((4, 4, 15, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(240):\n            for i3 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 19 + i3\n                compute_1 = T.Buffer((4560,), data=compute.data)\n                data_1 = T.Buffer((4560,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])",
        "op_args": [
            4,
            4,
            15,
            19
        ],
        "input_shape": "[[4, 4, 15, 19]]",
        "output_shape": "[[4, 4, 15, 19]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        for (int32_t i3 = 0; i3 < 19; ++i3) {\n          compute[((((i0 * 6840) + (i1 * 1680)) + (i2 * 112)) + i3)] = asinhf(data[((((i0 * 6840) + (i1 * 1680)) + (i2 * 112)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1216; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = asinhf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1560; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = asinhf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 240; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 19; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 19) + i3_s)] = asinhf(data[((i0_i1_fused_i2_fused * 19) + i3_s)]);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "global_pool_avg",
        "c_code": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    float adaptive_pool_sum[1];\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      adaptive_pool_sum[0] = 0.000000e+00f;\n      for (int32_t rv0 = 0; rv0 < 15; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 15; ++rv1) {\n          adaptive_pool_sum[0] = (adaptive_pool_sum[0] + data[((((ax0 * 1800) + (ax1 * 225)) + (rv0 * 15)) + rv1)]);\n        }\n      }\n      adaptive_pool_avg[((ax0 * 8) + ax1)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] * 4.444444e-03f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 15; ++rv0) {\n    for (int rv1 = 0; rv1 < 15; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] + data[((((((int)blockIdx.x) * 4500) + (((int)threadIdx.x) * 225)) + (rv0 * 15)) + rv1)]);\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 8, 15, 15), \"float32\"), adaptive_pool_avg: T.Buffer((5, 8, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(5):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            for ax1 in range(8):\n                adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n                adaptive_pool_sum_1[0] = T.float32(0)\n                for rv0, rv1 in T.grid(15, 15):\n                    data_1 = T.Buffer((9000,), data=data.data)\n                    adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0 * 1800 + ax1 * 225 + rv0 * 15 + rv1]\n                adaptive_pool_avg_1 = T.Buffer((40,), data=adaptive_pool_avg.data)\n                adaptive_pool_avg_1[ax0 * 8 + ax1] = adaptive_pool_sum_1[0] * T.float32(0.0044444444444444444)",
        "op_args": [
            5,
            8,
            15,
            15
        ],
        "input_shape": "[[5, 8, 15, 15]]",
        "output_shape": "[[5, 8, 1, 1]]",
        "deepseek_c": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 40; ++ax0_ax1_fused) {\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < 15; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 15; ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + data[((((ax0_ax1_fused * 225) + (rv0 * 15)) + rv1) + 600)]);\n      }\n    }\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 0)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 1)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 2)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 3)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 4)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 5)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 6)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 7)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 8)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 9)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 10)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 11)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 12)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 13)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 14)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 15)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 16)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 17)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 18)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n    adaptive_pool_avg[((ax0_ax1_fused * 20) + 19)] = (adaptive_pool_sum[0] * 4.444444e-03f);\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 40; ++ax0_ax1_fused) {\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < 15; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 15; ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + data[((((ax0_ax1_fused * 225) + (rv0 * 15)) + rv1) + 1575)]);\n      }\n    }\n    adaptive_pool_avg[ax0_ax1_fused] = (adaptive_pool_sum[0] * 4.444444e-03f);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 400; ++ax0_ax1_fused_ax2_fused) {\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < 15; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 15; ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + data[(((ax0_ax1_fused_ax2_fused * 225) + (rv0 * 15)) + rv1)]);\n      }\n    }\n    adaptive_pool_avg[ax0_ax1_fused_ax2_fused] = (adaptive_pool_sum[0] * 4.444444e-03f);\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* adaptive_pool_sum, float* data) {\n  // Calculate the number of threads needed\n  int num_threads = 5 * 8 * 20;\n\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i = 0; i < num_threads; ++i) {\n    // Calculate blockIdx.x and threadIdx.x from the global thread index i\n    int blockIdx_x = i / 20;\n    int threadIdx_x = i % 20;\n\n    adaptive_pool_sum[((blockIdx_x * 20) + threadIdx_x)] = 0.0f;\n    for (int rv0 = 0; rv0 < 15; ++rv0) {\n      for (int rv1 = 0; rv1 < 15; ++rv1) {\n        adaptive_pool_sum[((blockIdx_x * 20) + threadIdx_x)] += data[((blockIdx_x * 4500) + (threadIdx_x * 225) + (rv0 * 15) + rv1)];\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* adaptive_pool_avg, float* adaptive_pool_sum) {\n  // Calculate the number of threads needed\n  int num_threads = 5 * 8 * 20;\n\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i = 0; i < num_threads; ++i) {\n    // Calculate blockIdx.x and threadIdx.x from the global thread index i\n    int blockIdx_x = i / 20;\n    int threadIdx_x = i % 20;\n\n    adaptive_pool_avg[((blockIdx_x * 20) + threadIdx_x)] = adaptive_pool_sum[((blockIdx_x * 20) + threadIdx_x)] * 4.444444e-03f;\n  }\n}\n//|End-of-Code|\n    ",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* adaptive_pool_sum, float* data) {\n  // Input tensor shape: [[5, 8, 15, 15]]\n  // adaptive_pool_sum shape: [5, 8]\n  // data shape: [5, 8, 15, 15]\n  for (int blockIdx_x = 0; blockIdx_x < 5; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 8; ++threadIdx_x) {\n      adaptive_pool_sum[((blockIdx_x * 20) + threadIdx_x)] = 0.0f;\n      for (int rv0 = 0; rv0 < 15; ++rv0) {\n        for (int rv1 = 0; rv1 < 15; ++rv1) {\n          adaptive_pool_sum[((blockIdx_x * 20) + threadIdx_x)] += data[((blockIdx_x * 4500) + (threadIdx_x * 225)) + (rv0 * 15) + rv1];\n        }\n      }\n    }\n  }\n}\n",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n#include <stdlib.h>\n\nvoid default_function_kernel_1(float* adaptive_pool_avg, float* adaptive_pool_sum, int num_blocks, int threads_per_block) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < num_blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads_per_block; ++threadIdx_x) {\n            adaptive_pool_avg[blockIdx_x * threads_per_block + threadIdx_x] = adaptive_pool_sum[blockIdx_x * threads_per_block + threadIdx_x] * 4.444444e-03f;\n        }\n    }\n}\n\nvoid default_function_kernel(float* adaptive_pool_sum, float* data, int num_blocks, int threads_per_block) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < num_blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads_per_block; ++threadIdx_x) {\n            int idx = blockIdx_x * threads_per_block + threadIdx_x;\n            adaptive_pool_sum[idx] = 0.0f;\n            for (int rv0 = 0; rv0 < 15; ++rv0) {\n                for (int rv1 = 0; rv1 < 15; ++rv1) {\n                    adaptive_pool_sum[idx] += data[(blockIdx_x * 4500 + threadIdx_x * 225 + rv0 * 15 + rv1)];\n                }\n            }\n        }\n    }\n}\n\nint main() {\n    int num_blocks = 5 * 8;  // As per the tensor shape provided\n    int threads_per_block = 20;  // Launch bounds\n\n    int total_elements = num_blocks * threads_per_block;\n    int data_elements = num_blocks * threads_per_block * 15 * 15;\n\n    float *adaptive_pool_avg = (float *)malloc(total_elements * sizeof(float));\n    float *adaptive_pool_sum = (float *)malloc(total_elements * sizeof(float));\n    float *data = (float *)malloc(data_elements * sizeof(float));\n\n    // Initialize data (example initialization)\n    for (int i = 0; i < data_elements; ++i) {\n        data[i] = (float)i * 0.01f;\n    }\n\n    // Call the first function to compute adaptive_pool_sum\n    default_function_kernel(adaptive_pool_sum, data, num_blocks, threads_per_block);\n\n    // Call the second function to compute adaptive_pool_avg\n    default_function_kernel_1(adaptive_pool_avg, adaptive_pool_sum, num_blocks, threads_per_block);\n\n    // Print the result of the second function\n    for (int i = 0; i < total_elements; ++i) {\n        printf(\"%f \", adaptive_pool_avg[i]);\n    }\n    printf(\"\\n\");\n\n    free(adaptive_pool_avg);\n    free(adaptive_pool_sum);\n    free(data);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <stdio.h>\n\nvoid default_function_kernel_1(float* adaptive_pool_avg, float* adaptive_pool_sum) {\n    // Assuming blockIdx.x and threadIdx.x are provided or initialized elsewhere\n    int blockIdx_x = 0; // Example value, should be set appropriately\n    int threadIdx_x = 0; // Example value, should be set appropriately\n\n    adaptive_pool_avg[(blockIdx_x * 20) + threadIdx_x] = adaptive_pool_sum[(blockIdx_x * 20) + threadIdx_x] * 0.004444444f;\n}\n\nvoid default_function_kernel(float* adaptive_pool_sum, float* data) {\n    // Assuming blockIdx.x and threadIdx.x are provided or initialized elsewhere\n    int blockIdx_x = 0; // Example value, should be set appropriately\n    int threadIdx_x = 0; // Example value, should be set appropriately\n\n    int sum_index = (blockIdx_x * 20) + threadIdx_x;\n    adaptive_pool_sum[sum_index] = 0.0f;\n\n    for (int rv0 = 0; rv0 < 15; ++rv0) {\n        for (int rv1 = 0; rv1 < 15; ++rv1) {\n            int data_index = (((blockIdx_x * 4500) + (threadIdx_x * 225)) + (rv0 * 15)) + rv1;\n            adaptive_pool_sum[sum_index] += data[data_index];\n        }\n    }\n}\n\n// Example usage:\n// float adaptive_pool_avg[...];\n// float adaptive_pool_sum[...];\n// float data[...];\n// int blockIdx_x = ...;\n// int threadIdx_x = ...;\n\n// default_function_kernel_1(adaptive_pool_avg, adaptive_pool_sum);\n// default_function_kernel(adaptive_pool_sum, data);\n\n|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* adaptive_pool_sum, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 600; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 20; ++i3_s) {\n      adaptive_pool_sum[((i0_i1_fused_i2_fused * 20) + i3_s)] = 0.000000e+00f;\n      for (int rv0 = 0; rv0 < 15; ++rv0) {\n        for (int rv1 = 0; rv1 < 15; ++rv1) {\n          adaptive_pool_sum[((i0_i1_fused_i2_fused * 20) + i3_s)] = (adaptive_pool_sum[((i0_i1_fused_i2_fused * 20) + i3_s)] + data[(((((i0_i1_fused_i2_fused * 4500) + (i3_s * 225)) + (rv0 * 15)) + rv1))]);\n        }\n      }\n    }\n  }\n}\n"
    },
    {
        "op_name": "atanh",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3 = 0; i3 < 11; ++i3) {\n          compute[((((i0 * 297) + (i1 * 33)) + (i2 * 11)) + i3)] = atanhf(data[((((i0 * 297) + (i1 * 33)) + (i2 * 11)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(33) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 9, 3, 11), \"float32\"), compute: T.Buffer((18, 9, 3, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(18):\n            for i1, i2, i3 in T.grid(9, 3, 11):\n                cse_var_1: T.int32 = i0 * 297 + i1 * 33 + i2 * 11 + i3\n                compute_1 = T.Buffer((5346,), data=compute.data)\n                data_1 = T.Buffer((5346,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])",
        "op_args": [
            18,
            9,
            3,
            11
        ],
        "input_shape": "[[18, 9, 3, 11]]",
        "output_shape": "[[18, 9, 3, 11]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3 = 0; i3 < 11; ++i3) {\n          compute[((((i0 * 297) + (i1 * 33)) + (i2 * 11)) + i3)] = atanhf(data[((((i0 * 297) + (i1 * 33)) + (i2 * 11)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 5346; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = atanhf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 4054; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = atanhf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 594; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 11; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 11) + i3_s)] = atanhf(data[((i0_i1_fused_i2_fused * 11) + i3_s)]);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "dilate",
        "c_code": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2464; ++i0_i1_fused_i2_fused_i3_fused) {\n    DilatedInput[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(44) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data) {\n  DilatedInput[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 11, 16, 7), \"float32\"), DilatedInput: T.Buffer((2, 11, 16, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(2464):\n            DilatedInput_1 = T.Buffer((2464,), data=DilatedInput.data)\n            data_1 = T.Buffer((2464,), data=data.data)\n            DilatedInput_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused]",
        "op_args": [
            2,
            11,
            16,
            7
        ],
        "input_shape": "[[2, 11, 16, 7]]",
        "output_shape": "[[2, 11, 16, 7]]",
        "deepseek_c": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2464; ++i0_i1_fused_i2_fused_i3_fused) {\n    DilatedInput[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2464; ++i0_i1_fused_i2_fused_i3_fused) {\n    DilatedInput[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}",
        "codellama_c": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1728; ++i0_i1_fused_i2_fused_i3_fused) {\n    DilatedInput[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int blockIdx_x = 0; blockIdx_x < 2; blockIdx_x++) {\n    for (int threadIdx_x = 0; threadIdx_x < 44; threadIdx_x++) {\n      DilatedInput[((blockIdx_x * 44) + threadIdx_x)] = data[((blockIdx_x * 44) + threadIdx_x)];\n    }\n  }\n}\n",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int i = 0; i < 2; i++) {\n    for (int j = 0; j < 44; j++) {\n      DilatedInput[(i * 44) + j] = data[(i * 44) + j];\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* DilatedInput, float* data) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 2 * 11 * 16 * 7 / 44;\n    int blockDim_x = 44;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * 44) + threadIdx_x;\n            DilatedInput[idx] = data[idx];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* DilatedInput, float* data) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 2 * 11 * 16; // Based on input tensor shape (2 * 11 * 16)\n    int blockDim_x = 44;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            DilatedInput[idx] = data[idx];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 352; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 44; ++i3_s) {\n      DilatedInput[((i0_i1_fused_i2_fused * 44) + i3_s)] = data[((i0_i1_fused_i2_fused * 44) + i3_s)];\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "ceil",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 88; ++i0_i1_fused) {\n    for (int32_t i3 = 0; i3 < 17; ++i3) {\n      compute[((i0_i1_fused * 17) + i3)] = ceilf(data[((i0_i1_fused * 17) + i3)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 187) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 11, 1, 17), \"float32\"), compute: T.Buffer((8, 11, 1, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(88):\n            for i3 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused * 17 + i3\n                compute_1 = T.Buffer((1496,), data=compute.data)\n                data_1 = T.Buffer((1496,), data=data.data)\n                compute_1[cse_var_1] = T.ceil(data_1[cse_var_1])",
        "op_args": [
            8,
            11,
            1,
            17
        ],
        "input_shape": "[[8, 11, 1, 17]]",
        "output_shape": "[[8, 11, 1, 17]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 1; ++i2) {\n        for (int32_t i3 = 0; i3 < 17; ++i3) {\n          compute[((((i0 * 176) + (i1 * 16)) + (i2 * 17)) + i3)] = ceilf(data[((((i0 * 176) + (i1 * 16)) + (i2 * 17)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1496; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ceilf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1368; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ceilf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 88; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 17; ++i3_s) {\n      if (((i0_i1_fused_i2_fused * 4) + (i3_s >> 3)) < 187) {\n        compute[((i0_i1_fused_i2_fused * 17) + i3_s)] = ceilf(data[((i0_i1_fused_i2_fused * 17) + i3_s)]);\n      }\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "flatten",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i = 0; i < 17; ++i) {\n    for (int32_t j = 0; j < 270; ++j) {\n      compute[((i * 270) + j)] = data[((i * 270) + j)];\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 3, 9, 10), \"float32\"), compute: T.Buffer((17, 270), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i in T.parallel(17):\n            for j in range(270):\n                cse_var_1: T.int32 = i * 270 + j\n                compute_1 = T.Buffer((4590,), data=compute.data)\n                data_1 = T.Buffer((4590,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1]",
        "op_args": [
            17,
            3,
            9,
            10
        ],
        "input_shape": "[[17, 3, 9, 10]]",
        "output_shape": "[[17, 270]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          compute[((((i0 * 270) + (i1 * 90)) + (i2 * 10)) + i3)] = data[((((i0 * 270) + (i1 * 90)) + (i2 * 10)) + i3)];\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 4680; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1780; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int blockIdx_x = 0; blockIdx_x < 17; blockIdx_x++) {\n    for (int threadIdx_x = 0; threadIdx_x < 30; threadIdx_x++) {\n      compute[((blockIdx_x * 30) + threadIdx_x)] = data[((blockIdx_x * 30) + threadIdx_x)];\n    }\n  }\n}\n",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int i = 0; i < 17; i++) {\n    for (int j = 0; j < 30; j++) {\n      compute[(i * 30) + j] = data[(i * 30) + j];\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 17 * 3 * 9; // Adjusted for the given tensor shape\n    int blockDim_x = 30;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            compute[idx] = data[idx];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* compute, float* data) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 17 * 3 * 9; // Based on input tensor shape (17 * 3 * 9)\n    int blockDim_x = 30;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            compute[idx] = data[idx];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4590; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 30; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 30) + i3_s)] = data[((i0_i1_fused_i2_fused * 30) + i3_s)];\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "erf",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 312; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = erff(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 6, 1, 4), \"float32\"), compute: T.Buffer((13, 6, 1, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(312):\n            compute_1 = T.Buffer((312,), data=compute.data)\n            data_1 = T.Buffer((312,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.erf(data_1[i0_i1_fused_i2_fused_i3_fused])",
        "op_args": [
            13,
            6,
            1,
            4
        ],
        "input_shape": "[[13, 6, 1, 4]]",
        "output_shape": "[[13, 6, 1, 4]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i3 = 0; i3 < 4; ++i3) {\n        compute[(((i0 * 24) + (i1 * 4)) + i3)] = erff(data[(((i0 * 24) + (i1 * 4)) + i3)]);\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 312; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = erff(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 504; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = erff(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 78; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 4; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 4) + i3_s)] = erff(data[((i0_i1_fused_i2_fused * 4) + i3_s)]);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "fifo_buffer",
        "c_code": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused = 0; i_j_fused < 255; ++i_j_fused) {\n    for (int32_t k = 0; k < 7; ++k) {\n      for (int32_t l = 0; l < 18; ++l) {\n        new_buffer[(((i_j_fused * 126) + (k * 18)) + l)] = data[(((i_j_fused * 126) + (k * 18)) + l)];\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(34) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 17, 7, 18), \"float32\"), buffer: T.Buffer((15, 17, 7, 18), \"float32\"), new_buffer: T.Buffer((15, 17, 7, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused in T.parallel(255):\n            for k, l in T.grid(7, 18):\n                cse_var_1: T.int32 = i_j_fused * 126 + k * 18 + l\n                new_buffer_1 = T.Buffer((32130,), data=new_buffer.data)\n                data_1 = T.Buffer((32130,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]",
        "op_args": [
            15,
            17,
            7,
            18
        ],
        "input_shape": "[[15, 17, 7, 18], [15, 17, 7, 18]]",
        "output_shape": "[[15, 17, 7, 18]]",
        "deepseek_c": "void default_function_kernel(float* data, float* new_buffer) {\n  for (int32_t i = 0; i < 15; ++i) {\n    for (int32_t j = 0; j < 17; ++j) {\n      for (int32_t k = 0; k < 7; ++k) {\n        for (int32_t l = 0; l < 18; ++l) {\n          new_buffer[((((i * 238) + (j * 14)) + (k * 2)) + l)] = data[((((i * 238) + (j * 14)) + (k * 2)) + l)];\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* data, float* new_buffer) {\n  for (int32_t i1 = 0; i1 < 17; ++i1) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      for (int32_t i3 = 0; i3 < 18; ++i3) {\n        new_buffer[(((i1 * 126) + (i2 * 18)) + i3)] = data[(((i1 * 126) + (i2 * 18)) + i3)];\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 4550; ++i0_i1_fused_i2_fused_i3_fused) {\n    new_buffer[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* data, float* new_buffer) {\n  // Calculate the number of threads per block (34 in this case)\n  const int threadsPerBlock = 34;\n\n  // Calculate the number of blocks needed based on the input tensor shape\n  const int numBlocks = 15 * 17 * 7 * 18 / threadsPerBlock; \n\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock; ++threadIdx) {\n      int index = (blockIdx * threadsPerBlock) + threadIdx;\n      new_buffer[index] = data[index];\n    }\n  }\n}\n//|End-of-Code|",
        "gemini2_c": "void default_function_kernel(float* data, float* new_buffer) {\n  int blockIdx_x = 0; // Assuming only one block is used\n  int threadIdx_x = 0; \n  for (int i = 0; i < 2; i++) { // Looping through the two input tensors\n    for (int j = 0; j < 15; j++) {\n      for (int k = 0; k < 17; k++) {\n        for (int l = 0; l < 7; l++) {\n          for (int m = 0; m < 18; m++) {\n            int index = (i * 15 * 17 * 7 * 18) + (j * 17 * 7 * 18) + (k * 7 * 18) + (l * 18) + m;\n            new_buffer[index] = data[index];\n          }\n        }\n      }\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* data, float* new_buffer, int size) {\n    #pragma omp parallel for\n    for (int i = 0; i < size; ++i) {\n        new_buffer[i] = data[i];\n    }\n}\n\nint main() {\n    int size = 15 * 17 * 7 * 18;  // Calculate the size of the input tensor\n    float* data = (float*)malloc(size * sizeof(float));\n    float* new_buffer = (float*)malloc(size * sizeof(float));\n\n    // Initialize data (for example purposes)\n    for (int i = 0; i < size; ++i) {\n        data[i] = (float)i;\n    }\n\n    // Call the function\n    default_function_kernel(data, new_buffer, size);\n\n    // Free the allocated memory\n    free(data);\n    free(new_buffer);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* data, float* new_buffer, int total_elements) {\n    #pragma omp parallel for\n    for (int idx = 0; idx < total_elements; ++idx) {\n        new_buffer[idx] = data[idx];\n    }\n}\n\nint main() {\n    int blocks = 15 * 17 * 7 * 18;  // Calculating total blocks based on the given tensor shape\n    int threads_per_block = 34;     // Launch bounds\n\n    int total_elements = blocks * threads_per_block;\n    float *data = (float *)malloc(total_elements * sizeof(float));\n    float *new_buffer = (float *)malloc(total_elements * sizeof(float));\n\n    // Initialize data (example initialization)\n    for (int i = 0; i < total_elements; ++i) {\n        data[i] = (float)i;\n    }\n\n    // Call the function\n    default_function_kernel(data, new_buffer, total_elements);\n\n    // Print the result\n    for (int i = 0; i < total_elements; ++i) {\n        printf(\"%f \", new_buffer[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(new_buffer);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1785; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 34; ++i3_s) {\n      new_buffer[((i0_i1_fused_i2_fused * 34) + i3_s)] = data[((i0_i1_fused_i2_fused * 34) + i3_s)];\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "exp",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 19; ++i1) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 10; ++i3) {\n        compute[(((i1 * 40) + (i2 * 10)) + i3)] = expf(data[(((i1 * 40) + (i2 * 10)) + i3)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = __expf(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 19, 4, 10), \"float32\"), compute: T.Buffer((1, 19, 4, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(19, 4, 10):\n            cse_var_1: T.int32 = i1 * 40 + i2 * 10 + i3\n            compute_1 = T.Buffer((760,), data=compute.data)\n            data_1 = T.Buffer((760,), data=data.data)\n            compute_1[cse_var_1] = T.exp(data_1[cse_var_1])",
        "op_args": [
            1,
            19,
            4,
            10
        ],
        "input_shape": "[[1, 19, 4, 10]]",
        "output_shape": "[[1, 19, 4, 10]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 1; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          compute[((((i0 * 760) + (i1 * 40)) + (i2 * 10)) + i3)] = expf(data[((((i0 * 760) + (i1 * 40)) + (i2 * 10)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 760; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = expf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 760; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = expf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 760; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 10; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 10) + i3_s)] = expf(data[((i0_i1_fused_i2_fused * 10) + i3_s)]);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "fast_erf",
        "c_code": "void default_function_kernel(float* T_fast_erf, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 648; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 6; ++ax3) {\n      T_fast_erf[((ax0_ax1_fused_ax2_fused * 6) + ax3)] = ((max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused_ax2_fused * 6) + ax3)], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 12, 6, 6), \"float32\"), T_fast_erf: T.Buffer((9, 12, 6, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(648):\n            for ax3 in range(6):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 6 + ax3\n                T_fast_erf_1 = T.Buffer((3888,), data=T_fast_erf.data)\n                data_1 = T.Buffer((3888,), data=data.data)\n                T_fast_erf_1[cse_var_1] = T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))",
        "op_args": [
            9,
            12,
            6,
            6
        ],
        "input_shape": "[[9, 12, 6, 6]]",
        "output_shape": "[[9, 12, 6, 6]]",
        "deepseek_c": "void default_function_kernel(float* T_fast_erf, float* data) {\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3888; ++i0_i1_fused_i2_fused_i3_fused) {\n    T_fast_erf[i0_i1_fused_i2_fused_i3_fused] = ((max(min(data[i0_i1_fused_i2_fused_i3_fused], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[i0_i1_fused_i2_fused_i3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[i0_i1_fused_i2_fused_i3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[i0_i1_fused_i2_fused_i3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[i0_i1_fused_i2_fused_i3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[i0_i1_fused_i2_fused_i3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[i0_i1_fused_i2_fused_i3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[i0_i1_fused_i2_fused_i3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[i0_i1_fused_i2_fused_i3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[i0_i1_fused_i2_fused_i3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[i0_i1_fused_i2_fused_i3_fused], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[i0_i1_fused_i2_fused_i3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[i0_i1_fused_i2_fused_i3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[i0_i1_fused_i2_fused_i3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[i0_i1_fused_i2_fused_i3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[i0_i1_fused_i2_fused_i3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[i0_i1_fused_i2_fused_i3_fused], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n  }\n}",
        "starcoder_c": "#include <math.h>\nvoid default_function_kernel(float* T_fast_erf, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 3888; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_fast_erf[ax0_ax1_fused_ax2_fused_ax3_fused] = ((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((fmaxf(fminf(data[ax0_ax1_fused_ax2_fused_ax3_f",
        "codellama_c": "void default_function_kernel(float* T_fast_erf, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 576; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_fast_erf[ax0_ax1_fused_ax2_fused_ax3_fused] = ((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* T_fast_erf, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 648; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 6; ++i3_s) {\n      T_fast_erf[((i0_i1_fused_i2_fused * 6) + i3_s)] = ((max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((i0_i1_fused_i2_fused * 6) + i3_s)], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n    }\n  }\n}"
    },
    {
        "op_name": "depth_to_space",
        "c_code": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 504; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 6; ++i3) {\n      depth_to_space[((i0_i1_fused_i2_fused * 6) + i3)] = data[((((((i0_i1_fused_i2_fused / 36) * 270) + (((i0_i1_fused_i2_fused % 36) % 2) * 108)) + ((i3 % 2) * 54)) + (((i0_i1_fused_i2_fused % 36) / 2) * 3)) + (i3 / 2))];\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) >> 2) * 270) + (((((((int)blockIdx.x) & 3) * 9) + (((int)threadIdx.x) / 6)) % 2) * 108)) + (((((int)threadIdx.x) % 6) % 2) * 54)) + (((((((int)blockIdx.x) & 3) * 9) + (((int)threadIdx.x) / 6)) / 2) * 3)) + ((((int)threadIdx.x) % 6) / 2))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 5, 18, 3), \"float32\"), depth_to_space: T.Buffer((14, 1, 36, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(504):\n            for i3 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused % 36\n                depth_to_space_1 = T.Buffer((3024,), data=depth_to_space.data)\n                data_1 = T.Buffer((3780,), data=data.data)\n                depth_to_space_1[i0_i1_fused_i2_fused * 6 + i3] = data_1[i0_i1_fused_i2_fused // 36 * 270 + T.truncmod(cse_var_1, 2) * 108 + T.truncmod(i3, 2) * 54 + T.Div(cse_var_1, 2) * 3 + T.Div(i3, 2)]",
        "op_args": [
            14,
            5,
            18,
            3
        ],
        "input_shape": "[[14, 5, 18, 3]]",
        "output_shape": "[[14, 1, 36, 6]]",
        "deepseek_c": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        for (int32_t i3 = 0; i3 < 3; ++i3) {\n          depth_to_space[((((i0 * 54) + (i1 * 18)) + (i2 * 3)) + i3)] = data[((((((i0 * 270) + ((i1 * 54) + ((i2 % 2) * 108))) + ((i3 % 2) * 54)) + (((i2 / 2) * 3) + (i3 / 2))) + (((((i0 >> 2) * 9) + (i1 * 9)) + (i2 / 2)) % 2) * 108))];\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1890; ++i0_i1_fused_i2_fused_i3_fused) {\n    depth_to_space[i0_i1_fused_i2_fused_i3_fused] = data[(((((((i0_i1_fused_i2_fused_i3_fused / 108) * 270) + ((((i0_i1_fused_i2_fused_i3_fused % 108) / 9) % 2) * 108)) + ((((i0_i1_fused_i2_fused_i3_fused % 9) % 2) * 54)) + ((((i0_i1_fused_i2_fused_i3_fused % 108) / 9) / 2) * 3)) + (((i0_i1_fused_i2_fused_i3_fused % 9) / 2))) + (((i0_i1_fused_i2_fused_i3_fused % 108) / 18) * 162)) + (((i0_i1_fused_i2_fused_i3_fused % 9) % 2) * 81))];\n  }\n}",
        "codellama_c": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1080; ++i0_i1_fused_i2_fused_i3_fused) {\n    depth_to_space[i0_i1_fused_i2_fused_i3_fused] = data[((((((i0_i1_fused_i2_fused_i3_fused % 270) / 108) * 270) + ((i0_i1_fused_i2_fused_i3_fused % 108) % 2) * 108)) + ((i0_i1_fused_i2_fused_i3_fused % 108) / 2) * 3) + ((i0_i1_fused_i2_fused_i3_fused % 270) / 270))];\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int blockIdx_x = 0; blockIdx_x < 14; blockIdx_x++) {\n    for (int threadIdx_x = 0; threadIdx_x < 54; threadIdx_x++) {\n      depth_to_space[((blockIdx_x * 54) + threadIdx_x)] = data[((((((((blockIdx_x >> 2) * 270) + (((((((blockIdx_x & 3) * 9) + (threadIdx_x / 6)) % 2) * 108)) + (((((threadIdx_x % 6) % 2) * 54)) + (((((((blockIdx_x & 3) * 9) + (threadIdx_x / 6)) / 2) * 3)) + (((threadIdx_x % 6) / 2)))];\n    }\n  }\n}\n",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int i = 0; i < 14; i++) {\n    for (int j = 0; j < 54; j++) {\n      depth_to_space[(i * 54) + j] = data[((((((i >> 2) * 270) + ((((((i & 3) * 9) + (j / 6)) % 2) * 108)) + (((((j % 6) % 2) * 54)) + (((((i & 3) * 9) + (j / 6)) / 2) * 3)) + ((j % 6) / 2))];\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* data, float* depth_to_space) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 14;\n    int blockDim_x = 54;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * 54) + threadIdx_x;\n            int calc_index = (((((blockIdx_x >> 2) * 270) +\n                                (((((blockIdx_x & 3) * 9) + (threadIdx_x / 6)) % 2) * 108)) +\n                               (((threadIdx_x % 6) % 2) * 54)) +\n                              ((((blockIdx_x & 3) * 9) + (threadIdx_x / 6)) / 2) * 3) +\n                             ((threadIdx_x % 6) / 2);\n            depth_to_space[idx] = data[calc_index];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* data, float* depth_to_space) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 14 * 5 * 18; // Based on input tensor shape (14 * 5 * 18)\n    int blockDim_x = 54;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int calc_index = (((((blockIdx_x >> 2) * 270) +\n                                (((((blockIdx_x & 3) * 9) + (threadIdx_x / 6)) % 2) * 108)) +\n                               (((threadIdx_x % 6) % 2) * 54)) +\n                              ((((blockIdx_x & 3) * 9) + (threadIdx_x / 6)) / 2) * 3) +\n                             ((threadIdx_x % 6) / 2);\n            depth_to_space[idx] = data[calc_index];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1260; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 54; ++i3_s) {\n      depth_to_space[((i0_i1_fused_i2_fused * 54) + i3_s)] = data[((((((i0_i1_fused_i2_fused / 126) >> 2) * 270) + (((((((i0_i1_fused_i2_fused / 126) & 3) * 9) + (i3_s / 6)) % 2) * 108)) + (((i3_s % 6) % 2) * 54)) + (((((i0_i1_fused_i2_fused / 126) & 3) * 9) + (i3_s / 6)) / 2) * 3)) + ((i3_s % 6) / 2))];\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "fast_exp",
        "c_code": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 19; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n            int32_t v_ = ((int32_t)(floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_fast_exp[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((ax0 * 608) + (ax1 * 32)) + (ax2 * 16)) + ax3)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 19, 2, 16), \"float32\"), T_fast_exp: T.Buffer((15, 19, 2, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(15):\n            for ax1, ax2, ax3 in T.grid(19, 2, 16):\n                cse_var_1: T.int32 = ax0 * 608 + ax1 * 32 + ax2 * 16 + ax3\n                T_fast_exp_1 = T.Buffer((9120,), data=T_fast_exp.data)\n                data_1 = T.Buffer((9120,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])",
        "op_args": [
            15,
            19,
            2,
            16
        ],
        "input_shape": "[[15, 19, 2, 16]]",
        "output_shape": "[[15, 19, 2, 16]]",
        "deepseek_c": "void default_function_kernel(float* T_fast_exp, float* data) {\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        for (int32_t i3 = 0; i3 < 16; ++i3) {\n          int32_t v_ = ((int32_t)(floorf(((max(min(data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_fast_exp[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((i0 * 192) + (i1 * 96)) + (i2 * 48)) + (i3 * 3))]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 570; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n      int32_t v_ = ((int32_t)(floorf(((max(min(data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_fast_exp[((ax0_ax1_fused_ax2_fused * 16) + ax3)] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((ax0_ax1_fused_ax2_fused / 2) * 16 + ax3)]);\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 720; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n      T_fast_exp[((ax0_ax1_fused_ax2_fused * 16) + ax3)] = max(((data[((ax0_ax1_fused_ax2_fused * 16) + ax3)] * ((((((((((((((1.987569e-04f * (max(min(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((ax0_ax1_fused_ax2_fused * 16) + ax3)]);\n    }\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 570; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 16; ++i3_s) {\n      int v_ = ((int)(floorf(((max(min(data[((i0_i1_fused_i2_fused * 16) + i3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_fast_exp[((i0_i1_fused_i2_fused * 16) + i3_s)] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((i0_i1_fused_i2_fused * 16) + i3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((i0_i1_fused_i2_fused * 16) + i3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((i0_i1_fused_i2_fused * 16) + i3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((i0_i1_fused_i2_fused * 16) + i3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((i0_i1_fused_i2_fused * 16) + i3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((i0_i1_fused_i2_fused * 16) + i3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((i0_i1_fused_i2_fused * 16) + i3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((i0_i1_fused_i2_fused * 16) + i3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((i0_i1_fused_i2_fused * 16) + i3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((i0_i1_fused_i2_fused * 16) + i3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((i0_i1_fused_i2_fused * 16) + i3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((i0_i1_fused_i2_fused * 16) + i3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((i0_i1_fused_i2_fused * 16) + i3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((i0_i1_fused_i2_fused * 16) + i3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((i0_i1_fused_i2_fused * 16) + i3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((i0_i1_fused_i2_fused * 16) + i3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((i0_i1_fused_i2_fused * 16) + i3_s)]);\n    }\n  }\n}"
    },
    {
        "op_name": "fast_tanh",
        "c_code": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 252; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 12; ++ax3) {\n        T_fast_tanh[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)] = ((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 24) + (ax2 * 12)) + ax3)]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 14, 2, 12), \"float32\"), T_fast_tanh: T.Buffer((18, 14, 2, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(252):\n            for ax2, ax3 in T.grid(2, 12):\n                cse_var_1: T.int32 = ax0_ax1_fused * 24 + ax2 * 12 + ax3\n                T_fast_tanh_1 = T.Buffer((6048,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((6048,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))",
        "op_args": [
            18,
            14,
            2,
            12
        ],
        "input_shape": "[[18, 14, 2, 12]]",
        "output_shape": "[[18, 14, 2, 12]]",
        "deepseek_c": "void default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  for (size_t k0 = 0; k0 < 18; ++k0) {\n    for (size_t k1 = 0; k1 < 14; ++k1) {\n      for (size_t k2 = 0; k2 < 2; ++k2) {\n        for (size_t k3 = 0; k3 < 12; ++k3) {\n          T_fast_tanh[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((k0 * 336) + (k1 * 24)) + (k2 * 12)) + k3)]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "#include <math.h>\n#include <stdlib.h>\nvoid default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 504; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 12; ++ax3) {\n      T_fast_tanh[((ax0_ax1_fused_ax2_fused * 12) + ax3)] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax",
        "codellama_c": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 504; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 12; ++ax3) {\n      T_fast_tanh[((ax0_ax1_fused_ax2_fused * 12) + ax3)] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 12) + ax3)]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n    }\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 504; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 12; ++i3_s) {\n      T_fast_tanh[((i0_i1_fused_i2_fused * 12) + i3_s)] = ((fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[((i0_i1_fused_i2_fused * 12) + i3_s)])) * (((fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[((i0_i1_fused_i2_fused * 12) + i3_s)])) * fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[((i0_i1_fused_i2_fused * 12) + i3_s)]))) * (((fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[((i0_i1_fused_i2_fused * 12) + i3_s)])) * fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[((i0_i1_fused_i2_fused * 12) + i3_s)]))) * (((fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[((i0_i1_fused_i2_fused * 12) + i3_s)])) * fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[((i0_i1_fused_i2_fused * 12) + i3_s)]))) * (((fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[((i0_i1_fused_i2_fused * 12) + i3_s)])) * fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[((i0_i1_fused_i2_fused * 12) + i3_s)]))) * (((fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[((i0_i1_fused_i2_fused * 12) + i3_s)])) * fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[((i0_i1_fused_i2_fused * 12) + i3_s)]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[((i0_i1_fused_i2_fused * 12) + i3_s)])) * fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[((i0_i1_fused_i2_fused * 12) + i3_s)]))) * (((fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[((i0_i1_fused_i2_fused * 12) + i3_s)])) * fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[((i0_i1_fused_i2_fused * 12) + i3_s)]))) * (((fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[((i0_i1_fused_i2_fused * 12) + i3_s)])) * fmaxf(-9.000000e+00f, fminf(9.000000e+00f, data[((i0_i1_fused_i2_fused * 12) + i3_s)]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n    }\n  }\n}"
    },
    {
        "op_name": "flip",
        "c_code": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 8; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 4; ++ax3) {\n        T_reverse_sequence[(((ax0_ax1_fused * 52) + (ax2 * 4)) + ax3)] = data[((((((ax0_ax1_fused & 1) * 52) + (ax2 * 4)) + ax3) + 312) - ((ax0_ax1_fused >> 1) * 104))];\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(13) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 13) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) & 7) * 13) + ((int)threadIdx.x)) + 312) - ((((int)blockIdx.x) >> 3) * 104))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 2, 13, 4), \"float32\"), T_reverse_sequence: T.Buffer((4, 2, 13, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2, ax3 in T.grid(13, 4):\n                cse_var_1: T.int32 = ax2 * 4\n                T_reverse_sequence_1 = T.Buffer((416,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((416,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused * 52 + cse_var_1 + ax3] = data_1[ax0_ax1_fused % 2 * 52 + cse_var_1 + ax3 + 312 - ax0_ax1_fused // 2 * 104]",
        "op_args": [
            4,
            2,
            13,
            4
        ],
        "input_shape": "[[4, 2, 13, 4]]",
        "output_shape": "[[4, 2, 13, 4]]",
        "deepseek_c": "#pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_outer_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner_outer_inner_inner",
        "starcoder_c": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 104; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 4; ++ax3) {\n      T_reverse_sequence[((ax0_ax1_fused_ax2_fused * 4) + ax3)] = data[((((ax0_ax1_fused_ax2_fused % 13) * 4) + ax3) + 312) - ((ax0_ax1_fused_ax2_fused / 13) * 104));\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 112; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 4; ++ax3) {\n      T_reverse_sequence[((ax0_ax1_fused_ax2_fused * 4) + ax3)] = data[((((ax0_ax1_fused_ax2_fused % 28) * 4) + ax3) + 312)];\n    }\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int blockIdx_x = 0; blockIdx_x < 4; blockIdx_x++) {\n    for (int threadIdx_x = 0; threadIdx_x < 13; threadIdx_x++) {\n      T_reverse_sequence[((blockIdx_x * 13) + threadIdx_x)] = data[((((((blockIdx_x & 7) * 13) + threadIdx_x) + 312) - ((blockIdx_x >> 3) * 104)))];\n    }\n  }\n}\n",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int i = 0; i < 4; i++) {\n    for (int j = 0; j < 13; j++) {\n      T_reverse_sequence[(i * 13) + j] = data[(((((i & 7) * 13) + j) + 312) - ((i >> 3) * 104))];\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* T_reverse_sequence, float* data) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 4 * 2 * 4; // Adjusted for the given tensor shape\n    int blockDim_x = 13;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int src_idx = ((((blockIdx_x & 7) * 13) + threadIdx_x) + 312) - ((blockIdx_x >> 3) * 104);\n            T_reverse_sequence[idx] = data[src_idx];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* T_reverse_sequence, float* data) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 4 * 2 * 4; // Based on input tensor shape (4 * 2 * 4)\n    int blockDim_x = 13;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int src_idx = (((blockIdx_x & 7) * 13) + threadIdx_x + 312) - ((blockIdx_x >> 3) * 104);\n            T_reverse_sequence[idx] = data[src_idx];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 104; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 13; ++i3_s) {\n      T_reverse_sequence[((i0_i1_fused_i2_fused * 13) + i3_s)] = data[((((i0_i1_fused_i2_fused & 7) * 13) + i3_s) + 312) - ((i0_i1_fused_i2_fused >> 3) * 104)];\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "floor",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 594; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 10; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 10) + i3)] = floorf(data[((i0_i1_fused_i2_fused * 10) + i3)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 18, 11, 10), \"float32\"), compute: T.Buffer((3, 18, 11, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(594):\n            for i3 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 10 + i3\n                compute_1 = T.Buffer((5940,), data=compute.data)\n                data_1 = T.Buffer((5940,), data=data.data)\n                compute_1[cse_var_1] = T.floor(data_1[cse_var_1])",
        "op_args": [
            3,
            18,
            11,
            10
        ],
        "input_shape": "[[3, 18, 11, 10]]",
        "output_shape": "[[3, 18, 11, 10]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          compute[((((i0 * 1980) + (i1 * 110)) + (i2 * 10)) + i3)] = floorf(data[((((i0 * 1980) + (i1 * 110)) + (i2 * 10)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 5940; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = floorf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 5580; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = floorf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5940; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 10; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 10) + i3_s)] = floorf(data[((i0_i1_fused_i2_fused * 10) + i3_s)]);\n    }\n  }\n}"
    },
    {
        "op_name": "isnan",
        "c_code": "void default_function_kernel(int8_t* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 112; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 5; ++i3_s) {\n        compute[(((i0_i1_fused * 95) + (i2 * 5)) + i3_s)] = ((int8_t)(data[(((i0_i1_fused * 95) + (i2 * 5)) + i3_s)] != data[(((i0_i1_fused * 95) + (i2 * 5)) + i3_s)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 665) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 8, 19, 5), \"float32\"), compute: T.Buffer((14, 8, 19, 5), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(112):\n            for i2, i3_s in T.grid(19, 5):\n                cse_var_1: T.int32 = i0_i1_fused * 95 + i2 * 5 + i3_s\n                compute_1 = T.Buffer((10640,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((10640,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))",
        "op_args": [
            14,
            8,
            19,
            5
        ],
        "input_shape": "[[14, 8, 19, 5]]",
        "output_shape": "[[14, 8, 19, 5]]",
        "deepseek_c": "void default_function_kernel(int8_t* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        for (int32_t i3 = 0; i3 < 5; ++i3) {\n          compute[((((i0 * 760) + (i1 * 95)) + (i2 * 5)) + i3)] = ((int8_t)(data[((((i0 * 760) + (i1 * 95)) + (i2 * 5)) + i3)] != data[((((i0 * 760) + (i1 * 95)) + (i2 * 5)) + i3])));\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(int8_t* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 10640; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((int8_t)(data[i0_i1_fused_i2_fused_i3_fused]!= data[i0_i1_fused_i2_fused_i3_fused]));\n  }\n}",
        "codellama_c": "void default_function_kernel(int8_t* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 13600; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((int8_t)(data[i0_i1_fused_i2_fused_i3_fused] != data[i0_i1_fused_i2_fused_i3_fused]));\n  }\n}",
        "gemini3_c": "void default_function_kernel(signed char* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 665; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 32; ++i3_s) {\n      if (((i0_i1_fused_i2_fused * 2) + (i3_s >> 4)) < 665) {\n        compute[((i0_i1_fused_i2_fused * 32) + i3_s)] = ((signed char)(data[((i0_i1_fused_i2_fused * 32) + i3_s)] != data[((i0_i1_fused_i2_fused * 32) + i3_s)]));\n      }\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "log",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        for (int32_t i3 = 0; i3 < 17; ++i3) {\n          compute[((((i0 * 1836) + (i1 * 102)) + (i2 * 17)) + i3)] = logf(data[((((i0 * 1836) + (i1 * 102)) + (i2 * 17)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(51) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] = __logf(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 18, 6, 17), \"float32\"), compute: T.Buffer((3, 18, 6, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(3):\n            for i1, i2, i3 in T.grid(18, 6, 17):\n                cse_var_1: T.int32 = i0 * 1836 + i1 * 102 + i2 * 17 + i3\n                compute_1 = T.Buffer((5508,), data=compute.data)\n                data_1 = T.Buffer((5508,), data=data.data)\n                compute_1[cse_var_1] = T.log(data_1[cse_var_1])",
        "op_args": [
            3,
            18,
            6,
            17
        ],
        "input_shape": "[[3, 18, 6, 17]]",
        "output_shape": "[[3, 18, 6, 17]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        for (int32_t i3 = 0; i3 < 17; ++i3) {\n          compute[((((i0 * 1824) + (i1 * 102)) + (i2 * 17)) + i3)] = logf(data[((((i0 * 1824) + (i1 * 102)) + (i2 * 17)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "# C code\ndefault_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1836; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = logf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3812; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = logf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1539; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 17; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 17) + i3_s)] = logf(data[((i0_i1_fused_i2_fused * 17) + i3_s)]);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "log10",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 42; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      for (int32_t i3 = 0; i3 < 6; ++i3) {\n        compute[(((i0_i1_fused * 18) + (i2 * 6)) + i3)] = log10f(data[(((i0_i1_fused * 18) + (i2 * 6)) + i3)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 6, 3, 6), \"float32\"), compute: T.Buffer((7, 6, 3, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(42):\n            for i2, i3 in T.grid(3, 6):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2 * 6 + i3\n                compute_1 = T.Buffer((756,), data=compute.data)\n                data_1 = T.Buffer((756,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])",
        "op_args": [
            7,
            6,
            3,
            6
        ],
        "input_shape": "[[7, 6, 3, 6]]",
        "output_shape": "[[7, 6, 3, 6]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3 = 0; i3 < 6; ++i3) {\n          compute[((((i0 * 108) + (i1 * 18)) + (i2 * 6)) + i3)] = log10f(data[((((i0 * 108) + (i1 * 18)) + (i2 * 6)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 756; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = log10f(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 252; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = log10f(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 126; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 6; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 6) + i3_s)] = log10f(data[((i0_i1_fused_i2_fused * 6) + i3_s)]);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "log2",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i3 = 0; i3 < 13; ++i3) {\n        compute[(((i0 * 104) + (i1 * 13)) + i3)] = log2f(data[(((i0 * 104) + (i1 * 13)) + i3)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 143) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 8, 1, 13), \"float32\"), compute: T.Buffer((11, 8, 1, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(11):\n            for i1, i3 in T.grid(8, 13):\n                cse_var_1: T.int32 = i0 * 104 + i1 * 13 + i3\n                compute_1 = T.Buffer((1144,), data=compute.data)\n                data_1 = T.Buffer((1144,), data=data.data)\n                compute_1[cse_var_1] = T.log2(data_1[cse_var_1])",
        "op_args": [
            11,
            8,
            1,
            13
        ],
        "input_shape": "[[11, 8, 1, 13]]",
        "output_shape": "[[11, 8, 1, 13]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 1; ++i2) {\n        for (int32_t i3 = 0; i3 < 13; ++i3) {\n          compute[((((i0 * 104) + (i1 * 13)) + (i2 * 13)) + i3)] = log2f(data[((((i0 * 104) + (i1 * 13)) + (i2 * 13)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1144; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = log2f(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1104; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = log2f(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 143; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 32; ++i3_s) {\n      if (((i0_i1_fused_i2_fused * 4) + (i3_s >> 3)) < 143) {\n        compute[((i0_i1_fused_i2_fused * 32) + i3_s)] = log2f(data[((i0_i1_fused_i2_fused * 32) + i3_s)]);\n      }\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "max",
        "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[22];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 22; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 330; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 22; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 22) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 22; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 12; ++k0) {\n    for (int k1 = 0; k1 < 5; ++k1) {\n      for (int k2 = 0; k2 < 11; ++k2) {\n        for (int k3 = 0; k3 < 11; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 605) + (k1 * 121)) + (k2 * 11)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 5, 11, 11), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([22], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((22,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(22):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(330, 22):\n            data_1 = T.Buffer((7260,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 22 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(22):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])",
        "op_args": [
            12,
            5,
            11,
            11
        ],
        "input_shape": "[[12, 5, 11, 11]]",
        "output_shape": "[[]]",
        "deepseek_c": "void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 12; ++k0) {\n    for (int k1 = 0; k1 < 5; ++k1) {\n      for (int k2 = 0; k2 < 11; ++k2) {\n        for (int k3 = 0; k3 < 11; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 605) + (k1 * 121)) + (k2 * 11)) + k3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 12; ++k0) {\n    for (int k1 = 0; k1 < 5; ++k1) {\n      for (int k2 = 0; k2 < 11; ++k2) {\n        for (int k3 = 0; k3 < 11; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 605) + (k1 * 121)) + (k2 * 11)) + k3)]);\n        }\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 12; ++k0) {\n    for (int k1 = 0; k1 < 5; ++k1) {\n      for (int k2 = 0; k2 < 11; ++k2) {\n        for (int k3 = 0; k3 < 11; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 605) + (k1 * 121)) + (k2 * 11)) + k3)]);\n        }\n      }\n    }\n  }\n}",
        "gemini1_c": "#include <omp.h>\n#include <math.h> // For the max function\n\nvoid default_function_kernel(float* data, float* data_red) {\n  // Initialize data_red with negative infinity\n  data_red[0] = -3.402823e+38f;\n\n  // No need for OpenMP here, as the loop is already processing the entire tensor\n  for (int k0 = 0; k0 < 12; ++k0) {\n    for (int k1 = 0; k1 < 5; ++k1) {\n      for (int k2 = 0; k2 < 11; ++k2) {\n        for (int k3 = 0; k3 < 11; ++k3) {\n          data_red[0] = fmaxf(data_red[0], data[((k0 * 605) + (k1 * 121) + (k2 * 11) + k3)]);\n        }\n      }\n    }\n  }\n}\n//|End-of-Code|",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* data, float* data_red) {\n  // Input tensor shape: [[12, 5, 11, 11]]\n  // data_red shape: [1]\n  data_red[0] = -3.402823e+38f; // Initialize with negative infinity\n  for (int k0 = 0; k0 < 12; ++k0) {\n    for (int k1 = 0; k1 < 5; ++k1) {\n      for (int k2 = 0; k2 < 11; ++k2) {\n        for (int k3 = 0; k3 < 11; ++k3) {\n          data_red[0] = fmaxf(data_red[0], data[((k0 * 605) + (k1 * 121) + (k2 * 11) + k3)]);\n        }\n      }\n    }\n  }\n}\n",
        "gpt1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n#include <float.h>\n\nvoid default_function_kernel(float* restrict data, float* restrict data_red, int size0, int size1, int size2, int size3) {\n    data_red[0] = -FLT_MAX;\n\n    #pragma omp parallel for collapse(4) reduction(max: data_red[0])\n    for (int k0 = 0; k0 < size0; ++k0) {\n        for (int k1 = 0; k1 < size1; ++k1) {\n            for (int k2 = 0; k2 < size2; ++k2) {\n                for (int k3 = 0; k3 < size3; ++k3) {\n                    data_red[0] = fmax(data_red[0], data[(k0 * size1 * size2 * size3) + (k1 * size2 * size3) + (k2 * size3) + k3]);\n                }\n            }\n        }\n    }\n}\n\nint main() {\n    int size0 = 12, size1 = 5, size2 = 11, size3 = 11;\n    int total_size = size0 * size1 * size2 * size3;\n    float* data = (float*)malloc(total_size * sizeof(float));\n    float data_red[1];\n\n    // Initialize data with appropriate values\n    // For demonstration, initializing it with dummy values\n    for (int i = 0; i < total_size; ++i) {\n        data[i] = rand() / (float)RAND_MAX;\n    }\n\n    // Call the function\n    default_function_kernel(data, data_red, size0, size1, size2, size3);\n\n    // Print the result\n    printf(\"Max value: %f\\n\", data_red[0]);\n\n    // Free allocated memory\n    free(data);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <stdio.h>\n#include <float.h>  // For FLT_MIN\n\nvoid default_function_kernel(float* data, float* data_red) {\n    // Initialize data_red[0] to the smallest possible float value\n    data_red[0] = -FLT_MAX;\n\n    // Iterate over the dimensions of the tensor\n    for (int k0 = 0; k0 < 12; ++k0) {\n        for (int k1 = 0; k1 < 5; ++k1) {\n            for (int k2 = 0; k2 < 11; ++k2) {\n                for (int k3 = 0; k3 < 11; ++k3) {\n                    // Compute the index in the flattened tensor\n                    int index = ((k0 * 605) + (k1 * 121) + (k2 * 11) + k3);\n                    // Update the maximum value\n                    data_red[0] = fmaxf(data_red[0], data[index]);\n                }\n            }\n        }\n    }\n}\n\n// Example usage:\n// float data[...];\n// float data_red[1];\n// default_function_kernel(data, data_red);\n\n|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* data, float* data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0 = 0; k0 < 12; ++k0) {\n    for (int32_t k1 = 0; k1 < 5; ++k1) {\n      for (int32_t k2 = 0; k2 < 11; ++k2) {\n        for (int32_t k3 = 0; k3 < 11; ++k3) {\n          data_red[0] = fmaxf(data_red[0], data[((((k0 * 605) + (k1 * 121)) + (k2 * 11)) + k3)]);\n        }\n      }\n    }\n  }\n}\n"
    },
    {
        "op_name": "min",
        "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[14];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 14; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 144; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 14; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = min(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 14) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 14; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = min(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 3.402823e+38f;\n  for (int k0 = 0; k0 < 6; ++k0) {\n    for (int k1 = 0; k1 < 3; ++k1) {\n      for (int k2 = 0; k2 < 16; ++k2) {\n        for (int k3 = 0; k3 < 7; ++k3) {\n          data_red[0] = min(data_red[0], data[((((k0 * 336) + (k1 * 112)) + (k2 * 7)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 3, 16, 7), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([14], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((14,), data=data_red_rf, align=32)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(14):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(144, 14):\n            data_1 = T.Buffer((2016,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 14 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(14):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])",
        "op_args": [
            6,
            3,
            16,
            7
        ],
        "input_shape": "[[6, 3, 16, 7]]",
        "output_shape": "[[]]",
        "deepseek_c": "void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 3.402823e+38f;\n  for (int k0 = 0; k0 < 6; ++k0) {\n    for (int k1 = 0; k1 < 3; ++k1) {\n      for (int k2 = 0; k2 < 16; ++k2) {\n        for (int k3 = 0; k3 < 7; ++k3) {\n          data_red[0] = min(data_red[0], data[((((k0 * 336) + (k1 * 112)) + (k2 * 7)) + k3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 3.402823e+38f;\n  for (int k0 = 0; k0 < 6; ++k0) {\n    for (int k1 = 0; k1 < 3; ++k1) {\n      for (int k2 = 0; k2 < 16; ++k2) {\n        for (int k3 = 0; k3 < 7; ++k3) {\n          data_red[0] = min(data_red[0], data[((((k0 * 336) + (k1 * 112)) + (k2 * 7)) + k3)]);\n        }\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 3.402823e+38f;\n  for (int k0 = 0; k0 < 6; ++k0) {\n    for (int k1 = 0; k1 < 3; ++k1) {\n      for (int k2 = 0; k2 < 16; ++k2) {\n        for (int k3 = 0; k3 < 7; ++k3) {\n          data_red[0] = min(data_red[0], data[((((k0 * 336) + (k1 * 112)) + (k2 * 7)) + k3)]);\n        }\n      }\n    }\n  }\n}",
        "gemini1_c": "#include <omp.h>\n#include <math.h> // For the min function\n\nvoid default_function_kernel(float* data, float* data_red) {\n  // Initialize data_red with positive infinity\n  data_red[0] = 3.402823e+38f;\n\n  // No need for OpenMP here, as the loop is already processing the entire tensor\n  for (int k0 = 0; k0 < 6; ++k0) {\n    for (int k1 = 0; k1 < 3; ++k1) {\n      for (int k2 = 0; k2 < 16; ++k2) {\n        for (int k3 = 0; k3 < 7; ++k3) {\n          data_red[0] = fminf(data_red[0], data[((k0 * 336) + (k1 * 112) + (k2 * 7) + k3)]);\n        }\n      }\n    }\n  }\n}\n//|End-of-Code|\n    ",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* data, float* data_red) {\n  // Input tensor shape: [[6, 3, 16, 7]]\n  // data_red shape: [1]\n  data_red[0] = 3.402823e+38f; // Initialize with positive infinity\n  for (int k0 = 0; k0 < 6; ++k0) {\n    for (int k1 = 0; k1 < 3; ++k1) {\n      for (int k2 = 0; k2 < 16; ++k2) {\n        for (int k3 = 0; k3 < 7; ++k3) {\n          data_red[0] = fminf(data_red[0], data[((k0 * 336) + (k1 * 112) + (k2 * 7) + k3)]);\n        }\n      }\n    }\n  }\n}\n",
        "gpt1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n#include <float.h>\n\nvoid default_function_kernel(float* restrict data, float* restrict data_red, int size0, int size1, int size2, int size3) {\n    data_red[0] = FLT_MAX;\n\n    #pragma omp parallel for collapse(4) reduction(min: data_red[0])\n    for (int k0 = 0; k0 < size0; ++k0) {\n        for (int k1 = 0; k1 < size1; ++k1) {\n            for (int k2 = 0; k2 < size2; ++k2) {\n                for (int k3 = 0; k3 < size3; ++k3) {\n                    data_red[0] = fmin(data_red[0], data[(k0 * size1 * size2 * size3) + (k1 * size2 * size3) + (k2 * size3) + k3]);\n                }\n            }\n        }\n    }\n}\n\nint main() {\n    int size0 = 6, size1 = 3, size2 = 16, size3 = 7;\n    int total_size = size0 * size1 * size2 * size3;\n    float* data = (float*)malloc(total_size * sizeof(float));\n    float data_red[1];\n\n    // Initialize data with appropriate values\n    // For demonstration, initializing it with dummy values\n    for (int i = 0; i < total_size; ++i) {\n        data[i] = rand() / (float)RAND_MAX;\n    }\n\n    // Call the function\n    default_function_kernel(data, data_red, size0, size1, size2, size3);\n\n    // Print the result\n    printf(\"Min value: %f\\n\", data_red[0]);\n\n    // Free allocated memory\n    free(data);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <stdio.h>\n#include <float.h>  // For FLT_MAX\n\nvoid default_function_kernel(float* data, float* data_red) {\n    // Initialize data_red[0] to the largest possible float value\n    data_red[0] = FLT_MAX;\n\n    // Iterate over the dimensions of the tensor\n    for (int k0 = 0; k0 < 6; ++k0) {\n        for (int k1 = 0; k1 < 3; ++k1) {\n            for (int k2 = 0; k2 < 16; ++k2) {\n                for (int k3 = 0; k3 < 7; ++k3) {\n                    // Compute the index in the flattened tensor\n                    int index = ((k0 * 336) + (k1 * 112) + (k2 * 7) + k3);\n                    // Update the minimum value\n                    data_red[0] = fminf(data_red[0], data[index]);\n                }\n            }\n        }\n    }\n}\n\n// Example usage:\n// float data[...];\n// float data_red[1];\n// default_function_kernel(data, data_red);\n\n|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* data, float* data_red) {\n  data_red[0] = 3.402823e+38f;\n  for (int32_t k0 = 0; k0 < 6; ++k0) {\n    for (int32_t k1 = 0; k1 < 3; ++k1) {\n      for (int32_t k2 = 0; k2 < 16; ++k2) {\n        for (int32_t k3 = 0; k3 < 7; ++k3) {\n          data_red[0] = fminf(data_red[0], data[((((k0 * 336) + (k1 * 112)) + (k2 * 7)) + k3)]);\n        }\n      }\n    }\n  }\n}\n"
    },
    {
        "op_name": "leaky_relu",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2592; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < data[i0_i1_fused_i2_fused_i3_fused]) ? data[i0_i1_fused_i2_fused_i3_fused] : (data[i0_i1_fused_i2_fused_i3_fused] * 5.000000e-01f));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] * 5.000000e-01f));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 12, 18, 4), \"float32\"), compute: T.Buffer((3, 12, 18, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(2592):\n            compute_1 = T.Buffer((2592,), data=compute.data)\n            data_1 = T.Buffer((2592,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.if_then_else(T.float32(0) < data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused] * T.float32(0.5))",
        "op_args": [
            3,
            12,
            18,
            4
        ],
        "input_shape": "[[3, 12, 18, 4]]",
        "output_shape": "[[3, 12, 18, 4]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        for (int32_t i3 = 0; i3 < 4; ++i3) {\n          compute[((((i0 * 864) + (i1 * 72)) + (i2 * 4)) + i3)] = ((0.000000e+00f < data[((((i0 * 864) + (i1 * 72)) + (i2 * 4)) + i3)]) ? data[((((i0 * 864) + (i1 * 72)) + (i2 * 4)) + i3)] : (data[((((i0 * 864) + (i1 * 72)) + (i2 * 4)) + i3)] * 5.000000e-01f));\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2592; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < data[i0_i1_fused_i2_fused_i3_fused])? data[i0_i1_fused_i2_fused_i3_fused] : (data[i0_i1_fused_i2_fused_i3_fused] * 5.000000e-01f));\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 288; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < data[i0_i1_fused_i2_fused_i3_fused]) ? data[i0_i1_fused_i2_fused_i3_fused] : (data[i0_i1_fused_i2_fused_i3_fused] * 5.000000e-01f));\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 648; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 4; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 4) + i3_s)] = (0.000000e+00f < data[((i0_i1_fused_i2_fused * 4) + i3_s)]) ? data[((i0_i1_fused_i2_fused * 4) + i3_s)] : (data[((i0_i1_fused_i2_fused * 4) + i3_s)] * 5.000000e-01f);\n    }\n  }\n}"
    },
    {
        "op_name": "log_softmax",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  float T_softmax_maxelem[495];\n  float compute_1[1];\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        T_softmax_maxelem[(((i0 * 33) + (i1 * 3)) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 4; ++k) {\n          T_softmax_maxelem[(((i0 * 33) + (i1 * 3)) + i2)] = max(T_softmax_maxelem[(((i0 * 33) + (i1 * 3)) + i2)], data[((((i0 * 132) + (i1 * 12)) + (i2 * 4)) + k)]);\n        }\n      }\n    }\n  }\n  for (int32_t i0_outer_outer_inner = 0; i0_outer_outer_inner < 5; ++i0_outer_outer_inner) {\n    for (int32_t i3_outer_outer_inner = 0; i3_outer_outer_inner < 4; ++i3_outer_outer_inner) {\n      for (int32_t i0_outer_inner = 0; i0_outer_inner < 3; ++i0_outer_inner) {\n        for (int32_t i1_outer_inner = 0; i1_outer_inner < 11; ++i1_outer_inner) {\n          for (int32_t i2_outer_inner = 0; i2_outer_inner < 3; ++i2_outer_inner) {\n            compute_1[0] = 0.000000e+00f;\n            for (int32_t k_1 = 0; k_1 < 4; ++k_1) {\n              compute_1[0] = (compute_1[0] + expf((data[(((((i0_outer_outer_inner * 396) + (i0_outer_inner * 132)) + (i1_outer_inner * 12)) + (i2_outer_inner * 4)) + k_1)] - T_softmax_maxelem[((((i0_outer_outer_inner * 99) + (i0_outer_inner * 33)) + (i1_outer_inner * 3)) + i2_outer_inner)])));\n            }\n            compute[(((((i0_outer_outer_inner * 396) + (i0_outer_inner * 132)) + (i1_outer_inner * 12)) + (i2_outer_inner * 4)) + i3_outer_outer_inner)] = ((data[(((((i0_outer_outer_inner * 396) + (i0_outer_inner * 132)) + (i1_outer_inner * 12)) + (i2_outer_inner * 4)) + i3_outer_outer_inner)] - T_softmax_maxelem[((((i0_outer_outer_inner * 99) + (i0_outer_inner * 33)) + (i1_outer_inner * 3)) + i2_outer_inner)]) - logf(compute_1[0]));\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) < 495) {\n    compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 4; ++k) {\n    if (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) < 495) {\n      compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 495) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2))]) - __logf(compute_1[((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) < 495) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 4; ++k) {\n    if (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) < 495) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 4)) + k)]);\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 11, 3, 4), \"float32\"), compute: T.Buffer((15, 11, 3, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_softmax_maxelem = T.allocate([495], \"float32\", \"global\")\n        compute_1 = T.allocate([1], \"float32\", \"global\")\n        T_softmax_maxelem_1 = T.Buffer((495,), data=T_softmax_maxelem)\n        data_1 = T.Buffer((1980,), data=data.data)\n        for i0, i1, i2 in T.grid(15, 11, 3):\n            T_softmax_maxelem_1[i0 * 33 + i1 * 3 + i2] = T.float32(-3.4028234663852886e+38)\n            for k in range(4):\n                cse_var_1: T.int32 = i0 * 33 + i1 * 3 + i2\n                T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 132 + i1 * 12 + i2 * 4 + k])\n        for i0_outer_outer_inner, i3_outer_outer_inner, i0_outer_inner, i1_outer_inner, i2_outer_inner in T.grid(5, 4, 3, 11, 3):\n            cse_var_2: T.int32 = i0_outer_outer_inner * 396 + i0_outer_inner * 132 + i1_outer_inner * 12 + i2_outer_inner * 4 + i3_outer_outer_inner\n            compute_2 = T.Buffer((1,), data=compute_1, align=4)\n            compute_2[0] = T.float32(0)\n            for k in range(4):\n                compute_2[0] = compute_2[0] + T.exp(data_1[i0_outer_outer_inner * 396 + i0_outer_inner * 132 + i1_outer_inner * 12 + i2_outer_inner * 4 + k] - T_softmax_maxelem_1[i0_outer_outer_inner * 99 + i0_outer_inner * 33 + i1_outer_inner * 3 + i2_outer_inner])\n            compute_3 = T.Buffer((1980,), data=compute.data)\n            compute_3[cse_var_2] = data_1[cse_var_2] - T_softmax_maxelem_1[i0_outer_outer_inner * 99 + i0_outer_inner * 33 + i1_outer_inner * 3 + i2_outer_inner] - T.log(compute_2[0])",
        "op_args": [
            15,
            11,
            3,
            4
        ],
        "input_shape": "[[15, 11, 3, 4]]",
        "output_shape": "[[15, 11, 3, 4]]",
        "deepseek_c": "void default_function_kernel(float* T_softmax_maxelem, float* compute, float* compute_1, float* data) {\n  for (int k0 = 0; k0 < 15; ++k0) {\n    for (int k1 = 0; k1 < 11; ++k1) {\n      for (int k2 = 0; k2 < 3; ++k2) {\n        for (int k3 = 0; k3 < 4; ++k3) {\n          T_softmax_maxelem[((((k0 * 132) + (k1 * 12)) + (k2 * 4)) + k3)] = -3.402823e+38f;\n        }\n        for (int k4 = 0; k4 < 4; ++k4) {\n          T_softmax_maxelem[((((k0 * 132) + (k1 * 12)) + (k2 * 4)) + k4)] = max(T_softmax_maxelem[((((k0 * 132) + (k1 * 12)) + (k2 * 4)) + k4)], data[(((((k0 * 528) + (k1 * 48)) + (k2 * 16)) + (k4 * 4)) + k3)]);\n        }\n      }\n      for (int k5 = 0; k5 < 15; ++k5) {\n        for (int k6 = 0; k6 < 11; ++k6) {\n          for (int k7 = 0; k7 < 3; ++k7) {\n            compute[(((((k5 * 44) + (k6 * 4)) + (k7 * 4)) + k0) + k1) + k2] = 0.000000e+00f;\n          }\n          for (int k8 = 0; k8 < 4; ++k8) {\n            for (int k9 = 0; k9 < 4; ++k9) {\n              compute[(((((k5 * 44) + (k6 * 4)) + (k8 * 4)) + k0) + k1) + k2)] = (compute[(((((k5 * 44) + (k6 * 4)) + (k8 * 4)) + k0) + k1) + k2)] + expf((data[(((((k5 * 528) + (k6 * 48)) + (k8 * 16)) + (k9 * 4)) + k0) + k1) + k2] - T_softmax_maxelem[(((((k5 * 88) + (k6 * 8)) + (k8 * 4)) + k0) + k1) + k2)]));\n            }\n          }\n        }\n      }\n      for (int k10 = 0; k10 < 15; ++k10) {\n        for (int k11 = 0; k11 < 11; ++k11) {\n          for (int k12 = 0; k12 < 3; ++k12) {\n            compute_1[(((((k10 * 44) + (k11 * 4)) + (k12 * 4)) + k0) + k1) + k2)] = 0.000000e+00f;\n          }\n          for (int k13 = 0; k13 < 4; ++k13) {\n            compute_1[(((((k10 * 44) + (k11 * 4)) + (k13 * 4)) + k0) + k1) + k2)] = (compute_1[(((((k10 * 44) + (k11 * 4)) + (k13 * 4)) + k0) + k1) + k2)] + compute[(((((k10 * 44) + (k11 * 4)) + (k13 * 4)) + k0) + k1) + k2)]);\n          }\n        }\n      }\n      for (int k14 = 0; k14 < 15; ++k14) {\n        for (int k15 = 0; k15 < 11; ++k15) {\n          for (int k16 = 0; k16 < 3; ++k16) {\n            compute[(((((k14 * 44) + (k15 * 4)) + (k16 * 4)) + k0) + k1) + k2)] = ((data[(((((k14 * 528) + (k15 * 48)) + (k16 * 16)) + k0) + k1) + k2)] - T_softmax_maxelem[(((((k14 * 88) + (k15 * 8)) + (k16 * 4)) + k0) + k1) + k2)]) - logf(compute_1[(((((k14 * 44) + (k15 * 4)) + (k16 * 4)) + k0) + k1) + k2)]));\n          }\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* T_softmax_maxelem, float* compute, float* compute_1, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 165; ++i0_i1_fused_i2_fused) {\n    T_softmax_maxelem[i0_i1_fused_i2_fused] = -3.402823e+38f;\n    for (int32_t k = 0; k < 4; ++k) {\n      T_softmax_maxelem[i0_i1_fused_i2_fused] = max(T_softmax_maxelem[i0_i1_fused_i2_fused], data[((i0_i1_fused_i2_fused * 4) + k)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 165; ++i0_i1_fused_i2_fused_1) {\n    for (int32_t i3 = 0; i3 < 4; ++i3) {\n      compute[((i0_i1_fused_i2_fused_1 * 4) + i3)] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 4; ++k_1) {\n        compute[((i0_i1_fused_i2_fused_1 * 4) + i3)] = (compute[((i0_i1_fused_i2_fused_1 * 4) + i3)] + expf((data[((i0_i1_fused_i2_fused_1 * 4) + k_1)] - T_softmax_maxelem[i0_i1_fused_i2_fused_1])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 165; ++i0_i1_fused_i2_fused_2) {\n    for (int32_t i3_1 = 0; i3_1 < 4; ++i3_1) {\n      compute_1[((i0_i1_fused_i2_fused_2 * 4) + i3_1)] = ((data[((i0_i1_fused_i2_fused_2 * 4) + i3_1)] - T_softmax_maxelem[i0_i1_fused_i2_fused_2]) - logf(compute[((i0_i1_fused_i2_fused_2 * 4) + i3_1)]));\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_softmax_maxelem, float* compute, float* compute_1, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 455; ++i0_i1_fused_i2_fused) {\n    float T_softmax_maxelem_s[1];\n    float compute_2[1];\n    for (int32_t i3 = 0; i3 < 4; ++i3) {\n      T_softmax_maxelem_s[0] = -3.402823e+38f;\n      for (int32_t k = 0; k < 4; ++k) {\n        T_softmax_maxelem_s[0] = max(T_softmax_maxelem_s[0], data[(((i0_i1_fused_i2_fused * 4) + k) * 4) + i3]);\n      }\n      compute_2[0] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 4; ++k_1) {\n        compute_2[0] = (compute_2[0] + expf((data[(((i0_i1_fused_i2_fused * 4) + k_1) * 4) + i3] - T_softmax_maxelem_s[0])));\n      }\n      compute[(((i0_i1_fused_i2_fused * 4) + k_1) * 4) + i3] = ((data[(((i0_i1_fused_i2_fused * 4) + k_1) * 4) + i3] - T_softmax_maxelem_s[0]) - logf(compute_2[0]));\n    }\n  }\n}",
        "gemini1_c": "#include <omp.h>\n#include <math.h> // For expf, logf, and fmaxf\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data, float* compute, float* compute_1) {\n  // Calculate the number of threads needed\n  int num_threads = 15 * 11 * 8;\n\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i = 0; i < num_threads; ++i) {\n    // Calculate blockIdx.x and threadIdx.x from the global thread index i\n    int blockIdx_x = i / 8;\n    int threadIdx_x = i % 8;\n\n    if ((blockIdx_x * 8 + threadIdx_x) < 495) {\n      T_softmax_maxelem[blockIdx_x * 8 + threadIdx_x] = -3.402823e+38f; // Initialize with negative infinity\n      for (int k = 0; k < 4; ++k) {\n        T_softmax_maxelem[blockIdx_x * 8 + threadIdx_x] = fmaxf(T_softmax_maxelem[blockIdx_x * 8 + threadIdx_x], data[(blockIdx_x * 32 + threadIdx_x * 4) + k]);\n      }\n    }\n\n    // Calculate compute values\n    if ((blockIdx_x * 2 + threadIdx_x / 4) < 495) {\n      compute[blockIdx_x * 2 + threadIdx_x / 4] = 0.0f;\n      for (int k = 0; k < 4; ++k) {\n        compute[blockIdx_x * 2 + threadIdx_x / 4] += expf(data[(blockIdx_x * 8 + threadIdx_x * 4) + k] - T_softmax_maxelem[blockIdx_x * 2 + threadIdx_x / 4]);\n      }\n    }\n\n    // Calculate compute_1 values\n    if ((blockIdx_x * 8 + threadIdx_x) < 495) {\n      compute_1[blockIdx_x * 8 + threadIdx_x] = (data[blockIdx_x * 32 + threadIdx_x] - T_softmax_maxelem[blockIdx_x * 8 + threadIdx_x]) - logf(compute[blockIdx_x * 8 + threadIdx_x / 4]);\n    }\n  }\n}\n//|End-of-Code|\n    ",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data, float* compute, float* compute_1) {\n  // Input tensor shape: [[15, 11, 3, 4]]\n  // T_softmax_maxelem shape: [495]\n  // data shape: [15, 11, 3, 4]\n  // compute shape: [495]\n  // compute_1 shape: [495]\n  for (int blockIdx_x = 0; blockIdx_x < 15; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 11; ++threadIdx_x) {\n      int idx = (blockIdx_x * 11) + threadIdx_x;\n      if (idx < 495) {\n        T_softmax_maxelem[idx] = -3.402823e+38f; // Initialize with negative infinity\n        for (int k = 0; k < 4; ++k) {\n          T_softmax_maxelem[idx] = fmaxf(T_softmax_maxelem[idx], data[((blockIdx_x * 32) + (threadIdx_x * 4)) + k]);\n        }\n        compute[idx] = 0.0f;\n        for (int k = 0; k < 4; ++k) {\n          compute[idx] += expf(data[((blockIdx_x * 32) + (threadIdx_x * 4)) + k] - T_softmax_maxelem[idx]);\n        }\n        compute_1[idx] = compute[idx];\n        compute[idx] = (data[((blockIdx_x * 32) + (threadIdx_x * 4))] - T_softmax_maxelem[idx]) - logf(compute_1[idx]);\n      }\n    }\n  }\n}\n",
        "gpt1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel_1(float* restrict T_softmax_maxelem, float* restrict compute, float* restrict data, int size) {\n    #pragma omp parallel for\n    for (int idx = 0; idx < size; ++idx) {\n        if (idx < 495) {\n            compute[idx] = 0.0f;\n        }\n        for (int k = 0; k < 4; ++k) {\n            if (idx < 495) {\n                compute[idx] += expf(data[(idx * 4) + k] - T_softmax_maxelem[idx]);\n            }\n        }\n    }\n}\n\nint main() {\n    int size = 15 * 11 * 3 * 4; // Given input tensor shape\n    float* T_softmax_maxelem = (float*)malloc(size * sizeof(float));\n    float* compute = (float*)malloc(size * sizeof(float));\n    float* data = (float*)malloc(size * sizeof(float));\n\n    // Initialize data and T_softmax_maxelem with appropriate values\n    // For demonstration, initializing them with dummy values\n    for (int i = 0; i < size; ++i) {\n        data[i] = rand() / (float)RAND_MAX;\n        T_softmax_maxelem[i] = rand() / (float)RAND_MAX;\n    }\n\n    // Call the function\n    default_function_kernel_1(T_softmax_maxelem, compute, data, size);\n\n    // Free allocated memory\n    free(T_softmax_maxelem);\n    free(compute);\n    free(data);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <stdio.h>\n#include <math.h>   // For expf and logf\n#include <float.h>  // For FLT_MIN\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n    // Iterate over the blocks and threads in the original CUDA code\n    int num_blocks = 15;  // Based on tensor shape and CUDA code\n    int num_threads_per_block = 8;  // Based on __launch_bounds__(8)\n\n    for (int blockIdx_x = 0; blockIdx_x < num_blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < num_threads_per_block; ++threadIdx_x) {\n            int index = (blockIdx_x * 8) + threadIdx_x;\n\n            if (index < 495) {\n                // Initialize T_softmax_maxelem with the minimum possible float value\n                T_softmax_maxelem[index] = -FLT_MAX;\n\n                // Compute maximum value for the given tensor slice\n                for (int k = 0; k < 4; ++k) {\n                    int data_index = ((blockIdx_x * 32) + (threadIdx_x * 4)) + k;\n                    if (index < 495) {\n                        T_softmax_maxelem[index] = fmaxf(T_softmax_maxelem[index], data[data_index]);\n                    }\n                }\n            }\n        }\n    }\n}\n\n// Example usage:\n// float T_softmax_maxelem[...];\n// float data[...];\n// default_function_kernel(T_softmax_maxelem, data);\n\n|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* T_softmax_maxelem, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 495; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n      if (((i0_i1_fused_i2_fused * 8) + i3_s) < 495) {\n        T_softmax_maxelem[((i0_i1_fused_i2_fused * 8) + i3_s)] = -3.402823e+38f;\n      }\n      for (int k = 0; k < 4; ++k) {\n        if (((i0_i1_fused_i2_fused * 8) + i3_s) < 495) {\n          T_softmax_maxelem[((i0_i1_fused_i2_fused * 8) + i3_s)] = fmaxf(T_softmax_maxelem[((i0_i1_fused_i2_fused * 8) + i3_s)], data[(((((i0_i1_fused_i2_fused * 32) + (i3_s * 4)) + k))]);\n        }\n      }\n    }\n  }\n}\n"
    },
    {
        "op_name": "negative",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          compute[((((i0 * 540) + (i1 * 60)) + (i2 * 10)) + i3)] = (data[((((i0 * 540) + (i1 * 60)) + (i2 * 10)) + i3)] * -1.000000e+00f);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 9, 6, 10), \"float32\"), compute: T.Buffer((4, 9, 6, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(4):\n            for i1, i2, i3 in T.grid(9, 6, 10):\n                cse_var_1: T.int32 = i0 * 540 + i1 * 60 + i2 * 10 + i3\n                compute_1 = T.Buffer((2160,), data=compute.data)\n                data_1 = T.Buffer((2160,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)",
        "op_args": [
            4,
            9,
            6,
            10
        ],
        "input_shape": "[[4, 9, 6, 10]]",
        "output_shape": "[[4, 9, 6, 10]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          compute[((((i0 * 540) + (i1 * 60)) + (i2 * 10)) + i3)] = (data[((((i0 * 540) + (i1 * 60)) + (i2 * 10)) + i3)] * -1.000000e+00f);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2160; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = (data[i0_i1_fused_i2_fused_i3_fused] * -1.000000e+00f);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2880; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = (data[i0_i1_fused_i2_fused_i3_fused] * -1.000000e+00f);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 216; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 8) + i3_s)] = (data[((i0_i1_fused_i2_fused * 8) + i3_s)] * -1.000000e+00f);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "lrn",
        "c_code": "void default_function_kernel(float* T_divide, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1560; ++ax0_ax1_fused_ax2_fused) {\n    float tensor[1];\n    for (int32_t ax3 = 0; ax3 < 9; ++ax3) {\n      tensor[0] = 0.000000e+00f;\n      tensor[0] = (tensor[0] + (data[((ax0_ax1_fused_ax2_fused * 9) + ax3)] * data[((ax0_ax1_fused_ax2_fused * 9) + ax3)]));\n      T_divide[((ax0_ax1_fused_ax2_fused * 9) + ax3)] = (data[((ax0_ax1_fused_ax2_fused * 9) + ax3)] / powf((2.000000e+00f + (1.000000e-04f * tensor[0])), 7.500000e-01f));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(26) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  T_divide[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))])), 7.500000e-01f));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 1755) {\n    tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 1755) {\n    tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 10, 13, 9), \"float32\"), T_divide: T.Buffer((12, 10, 13, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(1560):\n            tensor = T.allocate([1], \"float32\", \"global\")\n            for ax3 in range(9):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 9 + ax3\n                tensor_1 = T.Buffer((1,), data=tensor, align=4)\n                tensor_1[0] = T.float32(0)\n                data_1 = T.Buffer((14040,), data=data.data)\n                tensor_1[0] = tensor_1[0] + data_1[cse_var_1] * data_1[cse_var_1]\n                T_divide_1 = T.Buffer((14040,), data=T_divide.data)\n                T_divide_1[cse_var_1] = data_1[cse_var_1] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[0], T.float32(0.75))",
        "op_args": [
            12,
            10,
            13,
            9
        ],
        "input_shape": "[[12, 10, 13, 9]]",
        "output_shape": "[[12, 10, 13, 9]]",
        "deepseek_c": "void default_function_kernel(float* T_divide, float* data, float* tensor) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_outer = 0; ax0_ax1_fused_ax2_fused_outer < 1560; ++ax0_ax1_fused_ax2_fused_outer) {\n    for (int32_t ax3 = 0; ax3 < 9; ++ax3) {\n      tensor[((ax0_ax1_fused_ax2_fused_outer * 9) + ax3)] = 0.000000e+00f;\n    }\n    for (int32_t ax3_1 = 0; ax3_1 < 9; ++ax3_1) {\n      tensor[((ax0_ax1_fused_ax2_fused_outer * 9) + ax3_1)] = (tensor[((ax0_ax1_fused_ax2_fused_outer * 9) + ax3_1)] + (data[((ax0_ax1_fused_ax2_fused_outer * 9) + ax3_1)] * data[((ax0_ax1_fused_ax2_fused_outer * 9) + ax3_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_1 < 1560; ++ax0_ax1_fused_ax2_fused_outer_1) {\n    for (int32_t ax3_2 = 0; ax3_2 < 9; ++ax3_2) {\n      T_divide[((ax0_ax1_fused_ax2_fused_outer_1 * 9) + ax3_2)] = (data[((ax0_ax1_fused_ax2_fused_outer_1 * 9) + ax3_2)] / powf((2.000000e+00f + (1.000000e-04f * tensor[((ax0_ax1_fused_ax2_fused_outer_1 * 9) + ax3_2)])), 7.500000e-01f));\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* T_divide, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1755; ++i0_i1_fused_i2_fused_i3_fused) {\n    float tensor[1];\n    tensor[0] = 0.000000e+00f;\n    tensor[0] = (tensor[0] + (data[i0_i1_fused_i2_fused_i3_fused] * data[i0_i1_fused_i2_fused_i3_fused]));\n    T_divide[i0_i1_fused_i2_fused_i3_fused] = (data[i0_i1_fused_i2_fused_i3_fused] / powf((2.000000e+00f + (1.000000e-04f * tensor[0])), 7.500000e-01f));\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_divide, float* data, float* tensor) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 1560; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    tensor[ax0_ax1_fused_ax2_fused_ax3_fused] = 0.000000e+00f;\n    tensor[ax0_ax1_fused_ax2_fused_ax3_fused] = (tensor[ax0_ax1_fused_ax2_fused_ax3_fused] + (data[ax0_ax1_fused_ax2_fused_ax3_fused] * data[ax0_ax1_fused_ax2_fused_ax3_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_1 < 1560; ++ax0_ax1_fused_ax2_fused_ax3_fused_1) {\n    T_divide[ax0_ax1_fused_ax2_fused_ax3_fused_1] = (data[ax0_ax1_fused_ax2_fused_ax3_fused_1] / powf((2.000000e+00f + (1.000000e-04f * tensor[ax0_ax1_fused_ax2_fused_ax3_fused_1])), 7.500000e-01f));\n  }\n}",
        "gemini1_c": "#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel_1(float* T_divide, float* data, float* tensor) {\n  #pragma omp parallel for\n  for (int block_x = 0; block_x < 12; ++block_x) {\n    for (int thread_x = 0; thread_x < 26; ++thread_x) {\n      T_divide[((block_x * 26) + thread_x)] = data[((block_x * 26) + thread_x)] / powf((2.0f + (1.0e-4f * tensor[((block_x * 26) + thread_x)])), 0.75f);\n    }\n  }\n}\n\nvoid default_function_kernel(float* data, float* tensor) {\n  #pragma omp parallel for\n  for (int block_x = 0; block_x < 12; ++block_x) {\n    for (int thread_x = 0; thread_x < 64; ++thread_x) {\n      if (((block_x * 8) + (thread_x >> 3)) < 1755) {\n        tensor[((block_x * 64) + thread_x)] = 0.0f;\n      }\n      if (((block_x * 8) + (thread_x >> 3)) < 1755) {\n        tensor[((block_x * 64) + thread_x)] += data[((block_x * 64) + thread_x)] * data[((block_x * 64) + thread_x)];\n      }\n    }\n  }\n}\n//End-of-Code|",
        "gemini2_c": "void default_function_kernel(float* data, float* tensor) {\n  for (int blockIdx_x = 0; blockIdx_x < 28; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 64; ++threadIdx_x) {\n      if (((((int)blockIdx_x) * 8) + (((int)threadIdx_x) >> 3)) < 1755) {\n        tensor[((((int)blockIdx_x) * 64) + ((int)threadIdx_x))] = 0.000000e+00f;\n      }\n      if (((((int)blockIdx_x) * 8) + (((int)threadIdx_x) >> 3)) < 1755) {\n        tensor[((((int)blockIdx_x) * 64) + ((int)threadIdx_x))] = (tensor[((((int)blockIdx_x) * 64) + ((int)threadIdx_x))] + (data[((((int)blockIdx_x) * 64) + ((int)threadIdx_x))] * data[((((int)blockIdx_x) * 64) + ((int)threadIdx_x))]));\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* T_divide, float* data, float* tensor) {\n  for (int blockIdx_x = 0; blockIdx_x < 28; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 26; ++threadIdx_x) {\n      T_divide[((((int)blockIdx_x) * 26) + ((int)threadIdx_x))] = (data[((((int)blockIdx_x) * 26) + ((int)threadIdx_x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx_x) * 26) + ((int)threadIdx_x))])), 7.500000e-01f));\n    }\n  }\n}\n",
        "gpt1_c": "#include <omp.h>\n#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel_1(float* T_divide, float* data, float* tensor, int size) {\n    #pragma omp parallel for\n    for (int i = 0; i < size; ++i) {\n        T_divide[i] = data[i] / powf((2.0f + (1.0e-04f * tensor[i])), 0.75f);\n    }\n}\n\nvoid default_function_kernel(float* data, float* tensor, int size) {\n    int limit = size / 8;\n    #pragma omp parallel for\n    for (int i = 0; i < size; ++i) {\n        if ((i / 8) < limit) {\n            tensor[i] = 0.0f;\n        }\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < size; ++i) {\n        if ((i / 8) < limit) {\n            tensor[i] += data[i] * data[i];\n        }\n    }\n}\n\nint main() {\n    int size = 12 * 10 * 13 * 9;\n    float* T_divide = (float*)malloc(size * sizeof(float));\n    float* data = (float*)malloc(size * sizeof(float));\n    float* tensor = (float*)malloc(size * sizeof(float));\n\n    // Initialize data and tensor with some values\n    for (int i = 0; i < size; ++i) {\n        data[i] = 1.0f; // Example initialization\n        tensor[i] = 0.5f; // Example initialization\n    }\n\n    // Call the functions\n    default_function_kernel_1(T_divide, data, tensor, size);\n    default_function_kernel(data, tensor, size);\n\n    // Clean up\n    free(T_divide);\n    free(data);\n    free(tensor);\n\n    return 0;\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <math.h>\n#include <stdio.h>\n\nvoid default_function_kernel_1(float* T_divide, float* data, float* tensor) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 12;\n    int blockDim_x = 26;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * 26) + threadIdx_x;\n            T_divide[idx] = data[idx] / powf(2.0f + (1.0e-04f * tensor[idx]), 0.75f);\n        }\n    }\n}\n\nvoid default_function_kernel(float* data, float* tensor) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 3510 / 64 + 1;\n    int blockDim_x = 64;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int global_idx = (blockIdx_x * 8) + (threadIdx_x >> 3);\n            int idx = (blockIdx_x * 64) + threadIdx_x;\n            if (global_idx < 1755) {\n                tensor[idx] = 0.0f;\n                tensor[idx] += data[idx] * data[idx];\n            }\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* data, float* tensor) {\n  #pragma omp parallel for\n  for (int32_t blockIdx_x = 0; blockIdx_x < 220; ++blockIdx_x) {\n    for (int32_t threadIdx_x = 0; threadIdx_x < 64; ++threadIdx_x) {\n      if (((blockIdx_x * 8) + (threadIdx_x >> 3)) < 1755) {\n        tensor[(blockIdx_x * 64) + threadIdx_x] = 0.000000e+00f;\n      }\n      if (((blockIdx_x * 8) + (threadIdx_x >> 3)) < 1755) {\n        tensor[(blockIdx_x * 64) + threadIdx_x] = (tensor[(blockIdx_x * 64) + threadIdx_x] + (data[(blockIdx_x * 64) + threadIdx_x] * data[(blockIdx_x * 64) + threadIdx_x]));\n      }\n    }\n  }\n}\n"
    },
    {
        "op_name": "prod",
        "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[21];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 21; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 1.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 300; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 21; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 21) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 1.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 21; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] * data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 1.000000e+00f;\n  for (int k0 = 0; k0 < 15; ++k0) {\n    for (int k1 = 0; k1 < 20; ++k1) {\n      for (int k2 = 0; k2 < 3; ++k2) {\n        for (int k3 = 0; k3 < 7; ++k3) {\n          data_red[0] = (data_red[0] * data[((((k0 * 420) + (k1 * 21)) + (k2 * 7)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 20, 3, 7), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([21], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((21,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(21):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(300, 21):\n            data_1 = T.Buffer((6300,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] * data_1[k0_k1_fused_k2_fused_k3_fused_outer * 21 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(21):\n            data_red_1[0] = data_red_1[0] * data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]",
        "op_args": [
            15,
            20,
            3,
            7
        ],
        "input_shape": "[[15, 20, 3, 7]]",
        "output_shape": "[[]]",
        "deepseek_c": "void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 1.000000e+00f;\n  for (int k0 = 0; k0 < 15; ++k0) {\n    for (int k1 = 0; k1 < 20; ++k1) {\n      for (int k2 = 0; k2 < 3; ++k2) {\n        for (int k3 = 0; k3 < 7; ++k3) {\n          data_red[0] = (data_red[0] * data[((((k0 * 420) + (k1 * 21)) + (k2 * 7)) + k3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 1.000000e+00f;\n  for (int k0 = 0; k0 < 15; ++k0) {\n    for (int k1 = 0; k1 < 20; ++k1) {\n      for (int k2 = 0; k2 < 3; ++k2) {\n        for (int k3 = 0; k3 < 7; ++k3) {\n          data_red[0] = (data_red[0] * data[((((k0 * 420) + (k1 * 21)) + (k2 * 7)) + k3)]);\n        }\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 1.000000e+00f;\n  for (int k0 = 0; k0 < 15; ++k0) {\n    for (int k1 = 0; k1 < 20; ++k1) {\n      for (int k2 = 0; k2 < 3; ++k2) {\n        for (int k3 = 0; k3 < 7; ++k3) {\n          data_red[0] = (data_red[0] * data[((((k0 * 420) + (k1 * 21)) + (k2 * 7)) + k3)]);\n        }\n      }\n    }\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* data, float* data_red) {\n  // Initialize data_red with 1.0\n  data_red[0] = 1.000000e+00f;\n\n  // No need for OpenMP here, as the loop is already processing the entire tensor\n  for (int k0 = 0; k0 < 15; ++k0) {\n    for (int k1 = 0; k1 < 20; ++k1) {\n      for (int k2 = 0; k2 < 3; ++k2) {\n        for (int k3 = 0; k3 < 7; ++k3) {\n          data_red[0] *= data[((k0 * 420) + (k1 * 21) + (k2 * 7) + k3)];\n        }\n      }\n    }\n  }\n}\n//|End-of-Code|\n    ",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* data, float* data_red) {\n  // Input tensor shape: [[15, 20, 3, 7]]\n  // data_red shape: [1]\n  data_red[0] = 1.000000e+00f; // Initialize with 1.0\n  for (int k0 = 0; k0 < 15; ++k0) {\n    for (int k1 = 0; k1 < 20; ++k1) {\n      for (int k2 = 0; k2 < 3; ++k2) {\n        for (int k3 = 0; k3 < 7; ++k3) {\n          data_red[0] *= data[((k0 * 420) + (k1 * 21) + (k2 * 7) + k3)];\n        }\n      }\n    }\n  }\n}\n",
        "gpt1_c": "void default_function_kernel(float* restrict data, float* restrict data_red, int size0, int size1, int size2, int size3) {\n    data_red[0] = 1.0f;\n\n    #pragma omp parallel for collapse(4) reduction(*: data_red[0])\n    for (int k0 = 0; k0 < size0; ++k0) {\n        for (int k1 = 0; k1 < size1; ++k1) {\n            for (int k2 = 0; k2 < size2; ++k2) {\n                for (int k3 = 0; k3 < size3; ++k3) {\n                    data_red[0] *= data[(k0 * size1 * size2 * size3) + (k1 * size2 * size3) + (k2 * size3) + k3];\n                }\n            }\n        }\n    }\n}\n",
        "gpt2_c": "#include <stdio.h>\n\nvoid default_function_kernel(float* data, float* data_red) {\n    // Initialize data_red[0] to 1.0\n    data_red[0] = 1.0f;\n\n    // Iterate over the tensor dimensions\n    for (int k0 = 0; k0 < 15; ++k0) {\n        for (int k1 = 0; k1 < 20; ++k1) {\n            for (int k2 = 0; k2 < 3; ++k2) {\n                for (int k3 = 0; k3 < 7; ++k3) {\n                    // Compute the index in the flattened data array\n                    int index = (k0 * 420) + (k1 * 21) + (k2 * 7) + k3;\n                    \n                    // Multiply data_red[0] by the value at the computed index\n                    data_red[0] *= data[index];\n                }\n            }\n        }\n    }\n}\n\n// Example usage:\n// float data[15 * 20 * 3 * 7];\n// float data_red[1];\n// default_function_kernel(data, data_red);\n\n|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* data, float* data_red) {\n  data_red[0] = 1.000000e+00f;\n  for (int32_t k0 = 0; k0 < 15; ++k0) {\n    for (int32_t k1 = 0; k1 < 20; ++k1) {\n      for (int32_t k2 = 0; k2 < 3; ++k2) {\n        for (int32_t k3 = 0; k3 < 7; ++k3) {\n          data_red[0] = (data_red[0] * data[((((k0 * 420) + (k1 * 21)) + (k2 * 7)) + k3)]);\n        }\n      }\n    }\n  }\n}\n"
    },
    {
        "op_name": "mirror_pad",
        "c_code": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      MirrorPadInput[((i0 * 9) + i1)] = data[((((3 <= i0) ? (4 - i0) : ((i0 < 1) ? (0 - i0) : (i0 - 1))) * 6) + ((i1 == 8) ? (13 - i1) : ((i1 < 2) ? (1 - i1) : (i1 - 2))))];\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(11) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) < 45) {\n    MirrorPadInput[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))] = data[((((27 <= ((((int)blockIdx.x) * 11) + ((int)threadIdx.x))) ? (4 - (((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) / 9)) : ((((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) < 9) ? 0 : ((((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) / 9) - 1))) * 6) + (((((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 9) == 8) ? (13 - (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 9)) : (((((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 9) < 2) ? (1 - (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 9)) : ((((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 9) - 2))))];\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 6), \"float32\"), MirrorPadInput: T.Buffer((5, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(5):\n            for i1 in range(9):\n                MirrorPadInput_1 = T.Buffer((45,), data=MirrorPadInput.data)\n                data_1 = T.Buffer((12,), data=data.data)\n                MirrorPadInput_1[i0 * 9 + i1] = data_1[T.if_then_else(3 <= i0, 4 - i0, T.if_then_else(i0 < 1, 0 - i0, i0 - 1)) * 6 + T.if_then_else(i1 == 8, 13 - i1, T.if_then_else(i1 < 2, 1 - i1, i1 - 2))]",
        "op_args": [
            14,
            16,
            2,
            6
        ],
        "input_shape": "[[2, 6]]",
        "output_shape": "[[5, 9]]",
        "deepseek_c": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      MirrorPadInput[((i0 * 6) + i1)] = data[((((27 <= ((i0 * 6) + i1)) ? (4 - ((i1 / 9))) : ((((i0 * 6) + i1) < 9) ? 0 : (((i1 / 9) - 1) * 6) + (((((i0 * 2) + i1) % 9) == 8) ? (13 - ((i1 % 9))) : (((((i0 * 2) + i1) % 9) < 2) ? (1 - ((i1 % 9))) : (((i1 % 9) - 2)))))) * 6) + (((((i0 * 2) + i1) % 9) == 8) ? (13 - ((i1 % 9))) : (((((i0 * 2) + i1) % 9) < 2) ? (1 - ((i1 % 9))) : (((i1 % 9) - 2))))))];\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 45; ++i0_i1_fused) {\n    MirrorPadInput[i0_i1_fused] = data[((((27 <= i0_i1_fused)? (4 - (i0_i1_fused / 9)) : ((i0_i1_fused < 9)? 0 : ((i0_i1_fused / 9) - 1))) * 6) + (((i0_i1_fused % 9) == 8)? (13 - (i0_i1_fused % 9)) : (((i0_i1_fused % 9) < 2)? (1 - (i0_i1_fused % 9)) : ((i0_i1_fused % 9) - 2))))];\n  }\n}",
        "codellama_c": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 45; ++i0_i1_fused_i2_fused) {\n    MirrorPadInput[i0_i1_fused_i2_fused] = data[((((27 <= i0_i1_fused_i2_fused) ? (4 - (i0_i1_fused_i2_fused / 9)) : ((i0_i1_fused_i2_fused < 9) ? 0 : (i0_i1_fused_i2_fused / 9) - 1))) * 6) + ((((i0_i1_fused_i2_fused % 9) == 8)) ? (13 - (i0_i1_fused_i2_fused % 9)) : (((i0_i1_fused_i2_fused % 9) < 2)) ? (1 - (i0_i1_fused_i2_fused % 9)) : ((i0_i1_fused_i2_fused % 9) - 2)))];\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int blockIdx_x = 0; blockIdx_x < 2; blockIdx_x++) {\n    for (int threadIdx_x = 0; threadIdx_x < 11; threadIdx_x++) {\n      int idx = (blockIdx_x * 11) + threadIdx_x;\n      if (idx < 45) {\n        MirrorPadInput[idx] = data[(((27 <= idx) ? (4 - (idx / 9)) : ((idx < 9) ? 0 : ((idx / 9) - 1))) * 6) + (((((blockIdx_x * 2) + threadIdx_x) % 9) == 8) ? (13 - (((blockIdx_x * 2) + threadIdx_x) % 9)) : (((((blockIdx_x * 2) + threadIdx_x) % 9) < 2) ? (1 - (((blockIdx_x * 2) + threadIdx_x) % 9)) : (((blockIdx_x * 2) + threadIdx_x) % 9 - 2)))];\n      }\n    }\n  }\n}\n",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int i = 0; i < 2; i++) {\n    for (int j = 0; j < 11; j++) {\n      if ((i * 11) + j < 45) {\n        MirrorPadInput[(i * 11) + j] = data[((27 <= ((i * 11) + j)) ? (4 - (((i * 11) + j) / 9)) : (((((i * 11) + j) < 9) ? 0 : (((((i * 11) + j) / 9) - 1))) * 6) + (((((i * 2) + j) % 9) == 8) ? (13 - (((i * 2) + j) % 9)) : (((((i * 2) + j) % 9) < 2) ? (1 - (((i * 2) + j) % 9)) : (((i * 2) + j) % 9) - 2))))];\n      }\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* MirrorPadInput, float* data) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = (45 + 10) / 11; // Adjusted for the given tensor shape\n    int blockDim_x = 11;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            if (idx < 45) {\n                int calc_index = ((27 <= idx) ? (4 - (idx / 9)) : ((idx < 9) ? 0 : ((idx / 9) - 1))) * 6 +\n                                 (((idx * 2) % 9 == 8) ? (13 - (idx * 2 % 9)) : \n                                  ((idx * 2 % 9 < 2) ? (1 - (idx * 2 % 9)) : (idx * 2 % 9 - 2)));\n                MirrorPadInput[idx] = data[calc_index];\n            }\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* MirrorPadInput, float* data) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = (45 + 10) / 11; // Number of blocks required to cover 45 elements with 11 threads per block\n    int blockDim_x = 11;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            if (idx < 45) {\n                int calc_index = (((27 <= idx) ? (4 - (idx / 9)) : ((idx < 9) ? 0 : ((idx / 9) - 1))) * 6) +\n                                 (((((blockIdx_x * 2) + threadIdx_x) % 9) == 8) ? (13 - ((blockIdx_x * 2 + threadIdx_x) % 9)) :\n                                  ((((blockIdx_x * 2 + threadIdx_x) % 9) < 2) ? (1 - ((blockIdx_x * 2 + threadIdx_x) % 9)) :\n                                   ((blockIdx_x * 2 + threadIdx_x) % 9 - 2)));\n                MirrorPadInput[idx] = data[calc_index];\n            }\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 45; ++i0_i1_fused) {\n    if (i0_i1_fused < 45) {\n      MirrorPadInput[i0_i1_fused] = data[((27 <= i0_i1_fused) ? (4 - (i0_i1_fused / 9)) : (((i0_i1_fused < 9) ? 0 : ((i0_i1_fused / 9) - 1))) * 6) + ((((i0_i1_fused % 9) == 8) ? (13 - (i0_i1_fused % 9)) : (((i0_i1_fused % 9) < 2) ? (1 - (i0_i1_fused % 9)) : ((i0_i1_fused % 9) - 2))))];\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "round",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      for (int32_t i3 = 0; i3 < 11; ++i3) {\n        compute[(((i0 * 22) + (i2 * 11)) + i3)] = roundf(data[(((i0 * 22) + (i2 * 11)) + i3)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) < 374) {\n    compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 1, 2, 11), \"float32\"), compute: T.Buffer((17, 1, 2, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(17):\n            for i2, i3 in T.grid(2, 11):\n                cse_var_1: T.int32 = i0 * 22 + i2 * 11 + i3\n                compute_1 = T.Buffer((374,), data=compute.data)\n                data_1 = T.Buffer((374,), data=data.data)\n                compute_1[cse_var_1] = T.round(data_1[cse_var_1])",
        "op_args": [
            17,
            1,
            2,
            11
        ],
        "input_shape": "[[17, 1, 2, 11]]",
        "output_shape": "[[17, 1, 2, 11]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 1; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        for (int32_t i3 = 0; i3 < 11; ++i3) {\n          compute[((((i0 * 22) + (i1 * 22)) + (i2 * 11)) + i3)] = roundf(data[((((i0 * 22) + (i1 * 22)) + (i2 * 11)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 374; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = roundf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 374; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = roundf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 374; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 3; ++i3_s) {\n      if (((i0_i1_fused_i2_fused * 3) + i3_s) < 374) {\n        compute[((i0_i1_fused_i2_fused * 3) + i3_s)] = roundf(data[((i0_i1_fused_i2_fused * 3) + i3_s)]);\n      }\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "pool1d",
        "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 19; ++ax0) {\n    float pad_temp[13];\n    for (int32_t ax2_s = 0; ax2_s < 13; ++ax2_s) {\n      pad_temp[ax2_s] = (((1 <= ax2_s) && (ax2_s < 12)) ? data[(((ax0 * 11) + ax2_s) - 1)] : -3.402823e+38f);\n    }\n    for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n      pool_max[((ax0 * 6) + ax2)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        pool_max[((ax0 * 6) + ax2)] = max(pool_max[((ax0 * 6) + ax2)], pad_temp[((ax2 * 2) + rv0)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  if (((((int)blockIdx.x) * 7) + ((int)threadIdx.x)) < 114) {\n    pool_max[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    if (((((int)blockIdx.x) * 7) + ((int)threadIdx.x)) < 114) {\n      pool_max[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))], (((1 <= ((((((int)blockIdx.x) + ((int)threadIdx.x)) % 6) * 2) + rv0)) && (((rv0 >> 1) + ((((int)blockIdx.x) + ((int)threadIdx.x)) % 6)) < 6)) ? data[(((((((((int)blockIdx.x) * 7) + ((int)threadIdx.x)) / 6) * 11) + (((((int)blockIdx.x) + ((int)threadIdx.x)) % 6) * 2)) + rv0) - 1)] : -3.402823e+38f));\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 1, 11), \"float32\"), pool_max: T.Buffer((19, 1, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(19):\n            pad_temp = T.allocate([13], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((13,), data=pad_temp, align=32)\n            for ax2_s in range(13):\n                data_1 = T.Buffer((209,), data=data.data)\n                pad_temp_1[ax2_s] = T.if_then_else(1 <= ax2_s and ax2_s < 12, data_1[ax0 * 11 + ax2_s - 1], T.float32(-3.4028234663852886e+38))\n            for ax2 in range(6):\n                pool_max_1 = T.Buffer((114,), data=pool_max.data)\n                pool_max_1[ax0 * 6 + ax2] = T.float32(-3.4028234663852886e+38)\n                for rv0 in range(3):\n                    cse_var_1: T.int32 = ax0 * 6 + ax2\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 2 + rv0])",
        "op_args": [
            19,
            1,
            5,
            11
        ],
        "input_shape": "[[19, 1, 11]]",
        "output_shape": "[[19, 1, 6]]",
        "deepseek_c": "#include <stdint.h>\n#include <stddef.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n\nvoid default_function_kernel(float* data, float* pool_max) {\n  for (int32_t _b_y_inner_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused_inner_k_fused_inner_j_fused",
        "starcoder_c": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 19; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n      pool_max[((ax0_ax1_fused * 6) + ax2)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        pool_max[((ax0_ax1_fused * 6) + ax2)] = max(pool_max[((ax0_ax1_fused * 6) + ax2)], (((1 <= ((ax2 * 2) + rv0)) && (((rv0 >> 1) + ax2) < 6))? data[((((ax0_ax1_fused * 11) + (ax2 * 2)) + rv0) - 1)] : -3.402823e+38f));\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 19; ++ax0) {\n    pool_max[ax0] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n      pool_max[ax0] = max(pool_max[ax0], (((1 <= (((ax0 * 2) + rv0)) && ((rv0 >> 1) + (ax0 % 6)) < 6)) ? data[(((ax0 * 11) + (ax0 % 6) * 2) + rv0) - 1)] : -3.402823e+38f));\n    }\n  }\n}",
        "gemini1_c": "#include <omp.h>\n#include <math.h> // For the max function\n\nvoid default_function_kernel(float* data, float* pool_max) {\n  // Calculate the number of threads needed\n  int num_threads = 19 * 7;\n\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i = 0; i < num_threads; ++i) {\n    // Calculate blockIdx.x and threadIdx.x from the global thread index i\n    int blockIdx_x = i / 7;\n    int threadIdx_x = i % 7;\n\n    if ((blockIdx_x * 7 + threadIdx_x) < 114) {\n      pool_max[blockIdx_x * 7 + threadIdx_x] = -3.402823e+38f; // Initialize with negative infinity\n      for (int rv0 = 0; rv0 < 3; ++rv0) {\n        if ((blockIdx_x * 7 + threadIdx_x) < 114) {\n          // Calculate the index for data based on the conditions\n          int data_index = -1;\n          if (1 <= (((blockIdx_x + threadIdx_x) % 6) * 2 + rv0) && ((rv0 >> 1) + ((blockIdx_x + threadIdx_x) % 6)) < 6) {\n            data_index = (((((blockIdx_x * 7 + threadIdx_x) / 6) * 11) + (((blockIdx_x + threadIdx_x) % 6) * 2)) + rv0) - 1;\n          }\n\n          pool_max[blockIdx_x * 7 + threadIdx_x] = fmaxf(pool_max[blockIdx_x * 7 + threadIdx_x], (data_index >= 0) ? data[data_index] : -3.402823e+38f);\n        }\n      }\n    }\n  }\n}\n//|End-of-Code|\n    ",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* data, float* pool_max) {\n  // Input tensor shape: [[19, 1, 11]]\n  // pool_max shape: [114]\n  // data shape: [19, 1, 11]\n  for (int blockIdx_x = 0; blockIdx_x < 16; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 7; ++threadIdx_x) {\n      int idx = (blockIdx_x * 7) + threadIdx_x;\n      if (idx < 114) {\n        pool_max[idx] = -3.402823e+38f; // Initialize with negative infinity\n        for (int rv0 = 0; rv0 < 3; ++rv0) {\n          int row = (blockIdx_x + threadIdx_x) % 6;\n          int col = (row * 2) + rv0;\n          if (1 <= col && (rv0 >> 1) + row < 6) {\n            pool_max[idx] = fmaxf(pool_max[idx], data[((blockIdx_x * 7 + threadIdx_x) / 6 * 11 + row * 2 + rv0) - 1]);\n          }\n        }\n      }\n    }\n  }\n}\n",
        "gpt1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n#include <float.h>\n\nvoid default_function_kernel(float* restrict data, float* restrict pool_max, int size) {\n    int block_size = 7;\n    int num_blocks = (size + block_size - 1) / block_size;\n    int idx;\n\n    #pragma omp parallel for private(idx)\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < block_size; ++threadIdx) {\n            idx = blockIdx * block_size + threadIdx;\n            if (idx < 114) {\n                pool_max[idx] = -FLT_MAX;\n            }\n        }\n    }\n\n    #pragma omp parallel for private(idx)\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < block_size; ++threadIdx) {\n            idx = blockIdx * block_size + threadIdx;\n            if (idx < 114) {\n                for (int rv0 = 0; rv0 < 3; ++rv0) {\n                    if (1 <= (((idx % 6) * 2) + rv0) && ((rv0 >> 1) + (idx % 6)) < 6) {\n                        int data_idx = (((idx / 6) * 11) + ((idx % 6) * 2) + rv0 - 1);\n                        pool_max[idx] = fmax(pool_max[idx], data[data_idx]);\n                    } else {\n                        pool_max[idx] = fmax(pool_max[idx], -FLT_MAX);\n                    }\n                }\n            }\n        }\n    }\n}\n\nint main() {\n    int size = 19 * 1 * 11; // Given input tensor shape\n    float* data = (float*)malloc(size * sizeof(float));\n    float* pool_max = (float*)malloc(114 * sizeof(float));\n\n    // Initialize data with appropriate values\n    // For demonstration, initializing it with dummy values\n    for (int i = 0; i < size; ++i) {\n        data[i] = rand() / (float)RAND_MAX;\n    }\n\n    // Call the function\n    default_function_kernel(data, pool_max, size);\n\n    // Print the result (optional)\n    for (int i = 0; i < 114; ++i) {\n        printf(\"pool_max[%d] = %f\\n\", i, pool_max[i]);\n    }\n\n    // Free allocated memory\n    free(data);\n    free(pool_max);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <stdio.h>\n#include <float.h>  // For FLT_MAX\n\nvoid default_function_kernel(float* data, float* data_red) {\n    // Initialize data_red[0] to the largest possible float value\n    data_red[0] = FLT_MAX;\n\n    // Iterate over the dimensions of the tensor\n    for (int k0 = 0; k0 < 6; ++k0) {\n        for (int k1 = 0; k1 < 3; ++k1) {\n            for (int k2 = 0; k2 < 16; ++k2) {\n                for (int k3 = 0; k3 < 7; ++k3) {\n                    // Compute the index in the flattened tensor\n                    int index = ((k0 * 336) + (k1 * 112) + (k2 * 7) + k3);\n                    // Update the minimum value\n                    data_red[0] = fminf(data_red[0], data[index]);\n                }\n            }\n        }\n    }\n}\n\n// Example usage:\n// float data[6 * 3 * 16 * 7];\n// float data_red[1];\n// default_function_kernel(data, data_red);\n\n|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 114; ++i0_i1_fused_i2_fused) {\n    pool_max[i0_i1_fused_i2_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n      if ((1 <= (((i0_i1_fused_i2_fused % 6) * 2) + rv0)) && (((rv0 >> 1) + (i0_i1_fused_i2_fused % 6)) < 6)) {\n        pool_max[i0_i1_fused_i2_fused] = fmaxf(pool_max[i0_i1_fused_i2_fused], data[((((((i0_i1_fused_i2_fused / 6) * 11) + ((i0_i1_fused_i2_fused % 6) * 2)) + rv0) - 1)]);\n      }\n    }\n  }\n}\n"
    },
    {
        "op_name": "shape",
        "c_code": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_shape[ax0] = ((ax0 == 3) ? 11 : ((ax0 == 2) ? 1 : ((ax0 == 1) ? 5 : 14)));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 11 : ((((int)threadIdx.x) == 2) ? 1 : ((((int)threadIdx.x) == 1) ? 5 : 14)));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 5, 1, 11), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 11, T.if_then_else(ax0 == 2, 1, T.if_then_else(ax0 == 1, 5, 14)))",
        "op_args": [
            14,
            5,
            1,
            11
        ],
        "input_shape": "[[14, 5, 1, 11]]",
        "output_shape": "[[4]]",
        "deepseek_c": "void default_function_kernel(int* T_shape) {\n  T_shape[0] = 14;\n  T_shape[1] = 5;\n  T_shape[2] = 1;\n  T_shape[3] = 11;\n}",
        "starcoder_c": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_shape[ax0] = ((ax0 == 3)? 11 : ((ax0 == 2)? 1 : ((ax0 == 1)? 5 : 14)));\n  }\n}",
        "codellama_c": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 70; ++ax0_ax1_fused_ax2_fused) {\n    T_shape[ax0_ax1_fused_ax2_fused] = ((ax0_ax1_fused_ax2_fused == 69) ? 11 : (((ax0_ax1_fused_ax2_fused == 68) ? 1 : (((ax0_ax1_fused_ax2_fused == 67) ? 5 : 14))));\n  }\n}",
        "gemini1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\nvoid default_function_kernel(int* T_shape) {\n  #pragma omp parallel for\n  for (int i = 0; i < 4; i++) {\n    T_shape[i] = (i == 3) ? 11 : (i == 2) ? 1 : (i == 1) ? 5 : 14;\n  }\n}\n",
        "gemini2_c": "void default_function_kernel(int* T_shape) {\n  //  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 11 : ((((int)threadIdx.x) == 2) ? 1 : ((((int)threadIdx.x) == 1) ? 5 : 14)));\n  T_shape[0] = 14;\n  T_shape[1] = 5;\n  T_shape[2] = 1;\n  T_shape[3] = 11;\n}\n",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(int* T_shape) {\n    #pragma omp parallel for\n    for (int threadIdx_x = 0; threadIdx_x < 4; ++threadIdx_x) {\n        if (threadIdx_x == 3) {\n            T_shape[threadIdx_x] = 11;\n        } else if (threadIdx_x == 2) {\n            T_shape[threadIdx_x] = 1;\n        } else if (threadIdx_x == 1) {\n            T_shape[threadIdx_x] = 5;\n        } else {\n            T_shape[threadIdx_x] = 14;\n        }\n    }\n}\n\nint main() {\n    int T_shape[4];\n    \n    // Call the function\n    default_function_kernel(T_shape);\n\n    // Print the result\n    for (int i = 0; i < 4; ++i) {\n        printf(\"%d \", T_shape[i]);\n    }\n    printf(\"\\n\");\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(int* T_shape) {\n    #pragma omp parallel for\n    for (int threadIdx_x = 0; threadIdx_x < 4; ++threadIdx_x) {\n        if (threadIdx_x == 3) {\n            T_shape[threadIdx_x] = 11;\n        } else if (threadIdx_x == 2) {\n            T_shape[threadIdx_x] = 1;\n        } else if (threadIdx_x == 1) {\n            T_shape[threadIdx_x] = 5;\n        } else {\n            T_shape[threadIdx_x] = 14;\n        }\n    }\n}\n\nint main() {\n    int T_shape[4];\n    \n    // Call the function\n    default_function_kernel(T_shape);\n\n    // Print the result\n    for (int i = 0; i < 4; ++i) {\n        printf(\"%d \", T_shape[i]);\n    }\n    printf(\"\\n\");\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(int* T_shape) {\n  for (int32_t threadIdx_x = 0; threadIdx_x < 4; ++threadIdx_x) {\n    T_shape[threadIdx_x] = (threadIdx_x == 3) ? 11 : (threadIdx_x == 2) ? 1 : (threadIdx_x == 1) ? 5 : 14;\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "pool2d",
        "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 11; ++ax0) {\n    float pad_temp[247];\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n        for (int32_t ax3_s = 0; ax3_s < 13; ++ax3_s) {\n          pad_temp[((ax2 * 13) + ax3_s)] = (((1 <= ax2) && (1 <= ax3_s)) ? data[(((((ax0 * 648) + (ax1 * 216)) + (ax2 * 12)) + ax3_s) - 13)] : -3.402823e+38f);\n        }\n      }\n      for (int32_t ax2_1 = 0; ax2_1 < 9; ++ax2_1) {\n        for (int32_t ax3 = 0; ax3 < 6; ++ax3) {\n          pool_max[((((ax0 * 162) + (ax1 * 54)) + (ax2_1 * 6)) + ax3)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              pool_max[((((ax0 * 162) + (ax1 * 54)) + (ax2_1 * 6)) + ax3)] = max(pool_max[((((ax0 * 162) + (ax1 * 54)) + (ax2_1 * 6)) + ax3)], pad_temp[((((ax2_1 * 26) + (rv0 * 13)) + (ax3 * 2)) + rv1)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], (((1 <= (((((int)blockIdx.x) % 9) * 2) + rv0)) && (1 <= ((((int)threadIdx.x) * 2) + rv1))) ? data[(((((((int)blockIdx.x) * 24) + (rv0 * 12)) + (((int)threadIdx.x) * 2)) + rv1) - 13)] : -3.402823e+38f));\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 3, 18, 12), \"float32\"), pool_max: T.Buffer((11, 3, 9, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(11):\n            pad_temp = T.allocate([247], \"float32\", \"global\")\n            for ax1 in range(3):\n                pad_temp_1 = T.Buffer((247,), data=pad_temp)\n                for ax2, ax3_s in T.grid(19, 13):\n                    data_1 = T.Buffer((7128,), data=data.data)\n                    pad_temp_1[ax2 * 13 + ax3_s] = T.if_then_else(1 <= ax2 and 1 <= ax3_s, data_1[ax0 * 648 + ax1 * 216 + ax2 * 12 + ax3_s - 13], T.float32(-3.4028234663852886e+38))\n                for ax2, ax3 in T.grid(9, 6):\n                    pool_max_1 = T.Buffer((1782,), data=pool_max.data)\n                    pool_max_1[ax0 * 162 + ax1 * 54 + ax2 * 6 + ax3] = T.float32(-3.4028234663852886e+38)\n                    for rv0, rv1 in T.grid(3, 3):\n                        cse_var_1: T.int32 = ax0 * 162 + ax1 * 54 + ax2 * 6 + ax3\n                        pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 26 + rv0 * 13 + ax3 * 2 + rv1])",
        "op_args": [
            11,
            3,
            18,
            4
        ],
        "input_shape": "[[11, 3, 18, 12]]",
        "output_shape": "[[11, 3, 9, 6]]",
        "deepseek_c": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 198; ++ax0_ax1_fused_ax2_fused) {\n    pool_max[ax0_ax1_fused_ax2_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n        pool_max[ax0_ax1_fused_ax2_fused] = max(pool_max[ax0_ax1_fused_ax2_fused], (((1 <= (ax0_ax1_fused_ax2_fused + rv0)) && (1 <= (ax0_ax1_fused_ax2_fused + rv1))) ? data[((((ax0_ax1_fused_ax2_fused * 12) + (rv0 * 4)) + rv1) - 5)] : -3.402823e+38f));\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 33; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 6; ++ax3) {\n        pool_max[(((ax0_ax1_fused * 54) + (ax2 * 6)) + ax3)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            pool_max[(((ax0_ax1_fused * 54) + (ax2 * 6)) + ax3)] = max(pool_max[(((ax0_ax1_fused * 54) + (ax2 * 6)) + ax3)], (((1 <= (((ax2 * 2) + rv0))) && (1 <= (((ax3 * 2) + rv1))))? data[(((((ax0_ax1_fused * 216) + (rv0 * 108)) + (ax2 * 12)) + (ax3 * 2)) + rv1) - 13] : -3.402823e+38f));\n          }\n        }\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 546; ++ax0_ax1_fused_ax2_fused) {\n    float pad_temp[1];\n    for (int32_t ax3 = 0; ax3 < 6; ++ax3) {\n      pad_temp[0] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n          pad_temp[0] = max(pad_temp[0], (((1 <= (((ax0_ax1_fused_ax2_fused % 36) / 12) * 2) + rv0)) ? data[((((((ax0_ax1_fused_ax2_fused / 12) * 24) + (rv0 * 12)) + ((ax0_ax1_fused_ax2_fused % 12) * 2)) + rv1) - 13)] : -3.402823e+38f));\n        }\n      }\n      pool_max[((ax0_ax1_fused_ax2_fused * 6) + ax3)] = pad_temp[0];\n    }\n  }\n}",
        "gemini1_c": "#include <omp.h>\n#include <math.h> // For the max function\n\nvoid default_function_kernel(float* data, float* pool_max) {\n  // Calculate the number of threads needed\n  int num_threads = 11 * 6;\n\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i = 0; i < num_threads; ++i) {\n    // Calculate blockIdx.x and threadIdx.x from the global thread index i\n    int blockIdx_x = i / 6;\n    int threadIdx_x = i % 6;\n\n    pool_max[blockIdx_x * 6 + threadIdx_x] = -3.402823e+38f; // Initialize with negative infinity\n    for (int rv0 = 0; rv0 < 3; ++rv0) {\n      for (int rv1 = 0; rv1 < 3; ++rv1) {\n        // Calculate the index for data based on the conditions\n        int data_index = -1;\n        if (1 <= ((blockIdx_x % 9) * 2 + rv0) && 1 <= ((threadIdx_x * 2) + rv1)) {\n          data_index = (((blockIdx_x * 24) + (rv0 * 12) + (threadIdx_x * 2) + rv1) - 13);\n        }\n\n        pool_max[blockIdx_x * 6 + threadIdx_x] = fmaxf(pool_max[blockIdx_x * 6 + threadIdx_x], (data_index >= 0) ? data[data_index] : -3.402823e+38f);\n      }\n    }\n  }\n}\n//|End-of-Code|\n    ",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* data, float* pool_max) {\n  // Input tensor shape: [[11, 3, 18, 12]]\n  // pool_max shape: [66]\n  // data shape: [11, 3, 18, 12]\n  for (int blockIdx_x = 0; blockIdx_x < 11; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 6; ++threadIdx_x) {\n      int idx = (blockIdx_x * 6) + threadIdx_x;\n      pool_max[idx] = -3.402823e+38f; // Initialize with negative infinity\n      for (int rv0 = 0; rv0 < 3; ++rv0) {\n        for (int rv1 = 0; rv1 < 3; ++rv1) {\n          int row = (blockIdx_x % 9) * 2 + rv0;\n          int col = (threadIdx_x * 2) + rv1;\n          if (1 <= row && 1 <= col) {\n            pool_max[idx] = fmaxf(pool_max[idx], data[((blockIdx_x * 24 + rv0 * 12 + threadIdx_x * 2 + rv1) - 13)]);\n          }\n        }\n      }\n    }\n  }\n}\n",
        "gpt1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n#include <float.h>\n\nvoid default_function_kernel(float* restrict data, float* restrict pool_max, int size) {\n    int block_size = 6;\n    int num_blocks = (size + block_size - 1) / block_size;\n    int idx;\n\n    #pragma omp parallel for private(idx)\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < block_size; ++threadIdx) {\n            idx = blockIdx * block_size + threadIdx;\n            if (idx < 99) {\n                pool_max[idx] = -FLT_MAX;\n            }\n        }\n    }\n\n    #pragma omp parallel for private(idx)\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < block_size; ++threadIdx) {\n            idx = blockIdx * block_size + threadIdx;\n            if (idx < 99) {\n                for (int rv0 = 0; rv0 < 3; ++rv0) {\n                    for (int rv1 = 0; rv1 < 3; ++rv1) {\n                        int cond1 = (((blockIdx % 9) * 2) + rv0) >= 1;\n                        int cond2 = ((threadIdx * 2) + rv1) >= 1;\n                        int data_idx = ((blockIdx * 24) + (rv0 * 12) + (threadIdx * 2) + rv1 - 13);\n                        if (cond1 && cond2 && data_idx >= 0 && data_idx < size) {\n                            pool_max[idx] = fmax(pool_max[idx], data[data_idx]);\n                        } else {\n                            pool_max[idx] = fmax(pool_max[idx], -FLT_MAX);\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nint main() {\n    int size0 = 11, size1 = 3, size2 = 18, size3 = 12;\n    int total_size = size0 * size1 * size2 * size3;\n    float* data = (float*)malloc(total_size * sizeof(float));\n    float* pool_max = (float*)malloc(99 * sizeof(float));\n\n    // Initialize data with appropriate values\n    // For demonstration, initializing it with dummy values\n    for (int i = 0; i < total_size; ++i) {\n        data[i] = rand() / (float)RAND_MAX;\n    }\n\n    // Call the function\n    default_function_kernel(data, pool_max, total_size);\n\n    // Print the result (optional)\n    for (int i = 0; i < 99; ++i) {\n        printf(\"pool_max[%d] = %f\\n\", i, pool_max[i]);\n    }\n\n    // Free allocated memory\n    free(data);\n    free(pool_max);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <stdio.h>\n#include <float.h>  // For FLT_MIN\n\nvoid default_function_kernel(float* data, float* pool_max) {\n    // Number of blocks and threads per block\n    const int num_blocks = 11;\n    const int threads_per_block = 6;\n\n    // Iterate over blocks\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        // Iterate over threads within the block\n        for (int threadIdx = 0; threadIdx < threads_per_block; ++threadIdx) {\n            // Initialize pool_max[blockIdx * 6 + threadIdx] to the smallest possible float value\n            int index = (blockIdx * 6) + threadIdx;\n            pool_max[index] = -FLT_MAX;\n\n            // Iterate over the 3x3 region\n            for (int rv0 = 0; rv0 < 3; ++rv0) {\n                for (int rv1 = 0; rv1 < 3; ++rv1) {\n                    // Check bounds for the condition\n                    int condition1 = ((blockIdx % 9) * 2) + rv0;\n                    int condition2 = (threadIdx * 2) + rv1;\n\n                    // Determine the data index based on conditions\n                    float value = (1 <= condition1 && 1 <= condition2)\n                        ? data[(((blockIdx * 24) + (rv0 * 12) + (threadIdx * 2) + rv1) - 13)]\n                        : -FLT_MAX;\n\n                    // Update the maximum value\n                    pool_max[index] = fmaxf(pool_max[index], value);\n                }\n            }\n        }\n    }\n}\n\n// Example usage:\n// float data[11 * 3 * 18 * 12];\n// float pool_max[11 * 6];\n// default_function_kernel(data, pool_max);\n\n|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 66; ++i0_i1_fused_i2_fused) {\n    pool_max[i0_i1_fused_i2_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n        if ((1 <= (((i0_i1_fused_i2_fused % 9) * 2) + rv0)) && (1 <= (((i0_i1_fused_i2_fused / 6) * 2) + rv1))) {\n          pool_max[i0_i1_fused_i2_fused] = fmaxf(pool_max[i0_i1_fused_i2_fused], data[((((((i0_i1_fused_i2_fused % 9) * 24) + (rv0 * 12)) + ((i0_i1_fused_i2_fused / 6) * 2)) + rv1) - 13)]);\n        }\n      }\n    }\n  }\n}\n"
    },
    {
        "op_name": "sigmoid",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 7; ++i1) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      for (int32_t i3 = 0; i3 < 4; ++i3) {\n        compute[(((i1 * 68) + (i2 * 4)) + i3)] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - data[(((i1 * 68) + (i2 * 4)) + i3)]))));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))]))));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 7, 17, 4), \"float32\"), compute: T.Buffer((1, 7, 17, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(7, 17, 4):\n            cse_var_1: T.int32 = i1 * 68 + i2 * 4 + i3\n            compute_1 = T.Buffer((476,), data=compute.data)\n            data_1 = T.Buffer((476,), data=data.data)\n            compute_1[cse_var_1] = T.sigmoid(data_1[cse_var_1])",
        "op_args": [
            1,
            7,
            17,
            4
        ],
        "input_shape": "[[1, 7, 17, 4]]",
        "output_shape": "[[1, 7, 17, 4]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 1; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        for (int32_t i3 = 0; i3 < 4; ++i3) {\n          compute[((((i0 * 476) + (i1 * 68)) + (i2 * 4)) + i3)] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - data[((((i0 * 476) + (i1 * 68)) + (i2 * 4)) + i3)]))));\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 476; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - data[i0_i1_fused_i2_fused_i3_fused]))));\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 784; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - data[i0_i1_fused_i2_fused_i3_fused]))));\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 119; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 4; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 4) + i3_s)] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - data[((i0_i1_fused_i2_fused * 4) + i3_s)]))));\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "pool3d",
        "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 170; ++ax0_ax1_fused) {\n    float pad_temp[27];\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 4; ++ax3) {\n        for (int32_t ax4 = 0; ax4 < 8; ++ax4) {\n          for (int32_t ax2_1 = 0; ax2_1 < 3; ++ax2_1) {\n            for (int32_t ax3_1 = 0; ax3_1 < 3; ++ax3_1) {\n              for (int32_t ax4_s = 0; ax4_s < 3; ++ax4_s) {\n                pad_temp[(((ax2_1 * 9) + (ax3_1 * 3)) + ax4_s)] = (((((1 <= ((ax2 * 2) + ax2_1)) && (((ax2_1 >> 1) + ax2) < 4)) && (1 <= ((ax3 * 2) + ax3_1))) && (1 <= ((ax4 * 2) + ax4_s))) ? data[((((((((ax0_ax1_fused * 896) + (ax2 * 256)) + (ax2_1 * 128)) + (ax3 * 32)) + (ax3_1 * 16)) + (ax4 * 2)) + ax4_s) - 145)] : -3.402823e+38f);\n              }\n            }\n          }\n          pool_max[((((ax0_ax1_fused * 128) + (ax2 * 32)) + (ax3 * 8)) + ax4)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n                pool_max[((((ax0_ax1_fused * 128) + (ax2 * 32)) + (ax3 * 8)) + ax4)] = max(pool_max[((((ax0_ax1_fused * 128) + (ax2 * 32)) + (ax3 * 8)) + ax4)], pad_temp[(((rv0 * 9) + (rv1 * 3)) + rv2)]);\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], (((((1 <= ((((((int)blockIdx.x) & 1) * 4) + ((((int)threadIdx.x) >> 5) * 2)) + rv0)) && (((((((int)threadIdx.x) >> 5) + (rv0 >> 1)) >> 1) + (((int)blockIdx.x) & 1)) < 2)) && (1 <= ((((((int)threadIdx.x) & 31) >> 3) * 2) + rv1))) && (1 <= (((((int)threadIdx.x) & 7) * 2) + rv2))) ? data[((((((((((((int)blockIdx.x) >> 1) * 896) + ((((int)blockIdx.x) & 1) * 512)) + ((((int)threadIdx.x) >> 5) * 256)) + (rv0 * 128)) + (((((int)threadIdx.x) & 31) >> 3) * 32)) + (rv1 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv2) - 145)] : -3.402823e+38f));\n      }\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 17, 7, 8, 16), \"float32\"), pool_max: T.Buffer((10, 17, 4, 4, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(170):\n            pad_temp = T.allocate([27], \"float32\", \"global\")\n            for ax2, ax3, ax4 in T.grid(4, 4, 8):\n                pad_temp_1 = T.Buffer((27,), data=pad_temp)\n                for ax2_1, ax3_1, ax4_s in T.grid(3, 3, 3):\n                    cse_var_1: T.int32 = ax4 * 2\n                    data_1 = T.Buffer((152320,), data=data.data)\n                    pad_temp_1[ax2_1 * 9 + ax3_1 * 3 + ax4_s] = T.if_then_else(1 <= ax2 * 2 + ax2_1 and ax2_1 // 2 + ax2 < 4 and 1 <= ax3 * 2 + ax3_1 and 1 <= cse_var_1 + ax4_s, data_1[ax0_ax1_fused * 896 + ax2 * 256 + ax2_1 * 128 + ax3 * 32 + ax3_1 * 16 + cse_var_1 + ax4_s - 145], T.float32(-3.4028234663852886e+38))\n                pool_max_1 = T.Buffer((21760,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 128 + ax2 * 32 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_2: T.int32 = ax0_ax1_fused * 128 + ax2 * 32 + ax3 * 8 + ax4\n                    pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[rv0 * 9 + rv1 * 3 + rv2])",
        "op_args": [
            10,
            17,
            7,
            8
        ],
        "input_shape": "[[10, 17, 7, 8, 16]]",
        "output_shape": "[[10, 17, 4, 4, 8]]",
        "deepseek_c": "void default_function_kernel(float* data, float* pool_max) {\n  for (int32_t ax0_ax1_fused_ax2_fused_outer = 0; ax0_ax1_fused_ax2_fused_outer < 1190; ++ax0_ax1_fused_ax2_fused_outer) {\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      for (int32_t ax4 = 0; ax4 < 16; ++ax4) {\n        pool_max[((((ax0_ax1_fused_ax2_fused_outer * 128) + (ax3 * 16)) + ax4) - 145)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n              pool_max[((((ax0_ax1_fused_ax2_fused_outer * 128) + (ax3 * 16)) + ax4) - 145)] = fmaxf(pool_max[((((ax0_ax1_fused_ax2_fused_outer * 128) + (ax3 * 16)) + ax4) - 145)], (((((1 <= (((((ax0_ax1_fused_ax2_fused_outer >> 1) * 4) + (((ax3 >> 1) * 2) + rv0)) + rv1)) && (((((((ax3 >> 1) + (rv0 >> 1)) >> 1) + (((ax0_ax1_fused_ax2_fused_outer >> 1) * 2)) + (rv1 >> 1)) + (ax3 & 1)) < 2)) && (1 <= (((((ax0_ax1_fused_ax2_fused_outer & 1) * 8) + (((ax3 & 1) * 4) + rv2))) + rv2))) ? data[((((((((((ax0_ax1_fused_ax2_fused_outer >> 1) * 896) + (((ax0_ax1_fused_ax2_fused_outer & 1) * 512)) + (((ax3 >> 1) * 256) + (rv0 * 128))) + (((ax3 & 1) * 32) + (rv1 * 16))) + (((ax0_ax1_fused_ax2_fused_outer & 1) * 8) + (rv2 * 2))) + ax3) - 145) + (((((ax0_ax1_fused_ax2_fused_outer >> 1) * 4) + (((ax3 >> 1) * 2) + rv0)) + rv1) * 32)) + (((((ax0_ax1_fused_ax2_fused_outer & 1) * 8) + (((ax3 & 1) * 4) + rv2))) + rv2)))] : -3.402823e+38f));\n            }\n          }\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 170; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n        for (int32_t ax4 = 0; ax4 < 8; ++ax4) {\n          pool_max[(((ax0_ax1_fused * 128) + (ax2 * 64)) + (ax3 * 8)) + ax4] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n                pool_max[(((ax0_ax1_fused * 128) + (ax2 * 64)) + (ax3 * 8)) + ax4] = max(pool_max[(((ax0_ax1_fused * 128) + (ax2 * 64)) + (ax3 * 8)) + ax4], (((((1 <= (((ax2 * 2) + rv0)) && (((rv0 >> 1) + ax2) < 2)) && (1 <= ((ax3 * 2) + rv1))) && (1 <= ((ax4 * 2) + rv2)))? data[(((((ax0_ax1_fused * 896) + (ax2 * 512)) + (rv0 * 128)) + ((ax3 * 32) + (rv1 * 16))) + ((ax4 * 2) + rv2)) - 145] : -3.402823e+38f));\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 1368; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n        for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n          pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = max(pool_max[ax0_ax1_fused_ax2_fused_ax3_fused], (((((1 <= (((ax0_ax1_fused_ax2_fused_ax3_fused % 16) * 2) + rv0)) && (((ax0_ax1_fused_ax2_fused_ax3_fused % 16) + (rv0 >> 1)) < 16)) && (1 <= (((ax0_ax1_fused_ax2_fused_ax3_fused / 16) * 2) + rv1))) && (1 <= (((ax0_ax1_fused_ax2_fused_ax3_fused % 2) * 2) + rv2))) ? data[(((((((((ax0_ax1_fused_ax2_fused_ax3_fused / 16) * 896) + (ax0_ax1_fused_ax2_fused_ax3_fused % 2 * 448)) + (rv0 * 128)) + ((ax0_ax1_fused_ax2_fused_ax3_fused / 16) * 32)) + (rv1 * 16)) + (ax0_ax1_fused_ax2_fused_ax3_fused % 2 * 2)) + rv2) - 145)] : -3.402823e+38f));\n        }\n      }\n    }\n  }\n}",
        "gemini1_c": "#include <omp.h>\n#include <math.h> // For the max function\n\nvoid default_function_kernel(float* data, float* pool_max) {\n  // Calculate the number of threads needed\n  int num_threads = 10 * 64;\n\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i = 0; i < num_threads; ++i) {\n    // Calculate blockIdx.x and threadIdx.x from the global thread index i\n    int blockIdx_x = i / 64;\n    int threadIdx_x = i % 64;\n\n    pool_max[blockIdx_x * 64 + threadIdx_x] = -3.402823e+38f; // Initialize with negative infinity\n    for (int rv0 = 0; rv0 < 3; ++rv0) {\n      for (int rv1 = 0; rv1 < 3; ++rv1) {\n        for (int rv2 = 0; rv2 < 3; ++rv2) {\n          // Calculate the index for data based on the conditions\n          int data_index = -1;\n          if (1 <= (((blockIdx_x & 1) * 4 + ((threadIdx_x >> 5) * 2)) + rv0) &&\n              (((((threadIdx_x >> 5) + (rv0 >> 1)) >> 1) + (blockIdx_x & 1)) < 2) &&\n              1 <= (((((threadIdx_x & 31) >> 3) * 2) + rv1)) &&\n              1 <= (((((threadIdx_x & 7) * 2) + rv2))) {\n            data_index = (((((((((blockIdx_x >> 1) * 896) + ((blockIdx_x & 1) * 512)) + ((threadIdx_x >> 5) * 256)) + (rv0 * 128)) +\n                             (((threadIdx_x & 31) >> 3) * 32)) + (rv1 * 16)) + (((threadIdx_x & 7) * 2)) + rv2) - 145);\n          }\n\n          pool_max[blockIdx_x * 64 + threadIdx_x] = fmaxf(pool_max[blockIdx_x * 64 + threadIdx_x], (data_index >= 0) ? data[data_index] : -3.402823e+38f);\n        }\n      }\n    }\n  }\n}\n//|End-of-Code|\n    ",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* data, float* pool_max) {\n  // Input tensor shape: [[10, 17, 7, 8, 16]]\n  // pool_max shape: [1024]\n  // data shape: [10, 17, 7, 8, 16]\n  for (int blockIdx_x = 0; blockIdx_x < 16; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 64; ++threadIdx_x) {\n      int idx = (blockIdx_x * 64) + threadIdx_x;\n      pool_max[idx] = -3.402823e+38f; // Initialize with negative infinity\n      for (int rv0 = 0; rv0 < 3; ++rv0) {\n        for (int rv1 = 0; rv1 < 3; ++rv1) {\n          for (int rv2 = 0; rv2 < 3; ++rv2) {\n            int row = (blockIdx_x & 1) * 4 + (threadIdx_x >> 5) * 2 + rv0;\n            int col1 = (threadIdx_x >> 5) + (rv0 >> 1) >> 1 + (blockIdx_x & 1);\n            int col2 = (threadIdx_x & 31) >> 3 * 2 + rv1;\n            int col3 = (threadIdx_x & 7) * 2 + rv2;\n            if (1 <= row && col1 < 2 && 1 <= col2 && 1 <= col3) {\n              pool_max[idx] = fmaxf(pool_max[idx], data[((blockIdx_x >> 1) * 896 + (blockIdx_x & 1) * 512 + (threadIdx_x >> 5) * 256 + rv0 * 128 + (threadIdx_x & 31) >> 3 * 32 + rv1 * 16 + (threadIdx_x & 7) * 2 + rv2) - 145]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n",
        "gpt1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n#include <float.h>\n\nvoid default_function_kernel(float* restrict data, float* restrict pool_max, int total_size) {\n    int block_size = 64;\n    int num_blocks = (total_size + block_size - 1) / block_size;\n    int idx;\n\n    #pragma omp parallel for private(idx)\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < block_size; ++threadIdx) {\n            idx = blockIdx * block_size + threadIdx;\n            if (idx < 1088) {\n                pool_max[idx] = -FLT_MAX;\n            }\n        }\n    }\n\n    #pragma omp parallel for private(idx)\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < block_size; ++threadIdx) {\n            idx = blockIdx * block_size + threadIdx;\n            if (idx < 1088) {\n                for (int rv0 = 0; rv0 < 3; ++rv0) {\n                    for (int rv1 = 0; rv1 < 3; ++rv1) {\n                        for (int rv2 = 0; rv2 < 3; ++rv2) {\n                            int cond1 = (((blockIdx & 1) * 4) + ((threadIdx >> 5) * 2) + rv0) >= 1;\n                            int cond2 = ((((threadIdx >> 5) + (rv0 >> 1)) >> 1) + (blockIdx & 1)) < 2;\n                            int cond3 = (((((threadIdx & 31) >> 3) * 2) + rv1) >= 1);\n                            int cond4 = ((((threadIdx & 7) * 2) + rv2) >= 1);\n                            int data_idx = ((((blockIdx >> 1) * 896) + ((blockIdx & 1) * 512) + ((threadIdx >> 5) * 256) + (rv0 * 128) + (((threadIdx & 31) >> 3) * 32) + (rv1 * 16) + ((threadIdx & 7) * 2) + rv2) - 145);\n                            if (cond1 && cond2 && cond3 && cond4 && data_idx >= 0 && data_idx < total_size) {\n                                pool_max[idx] = fmax(pool_max[idx], data[data_idx]);\n                            } else {\n                                pool_max[idx] = fmax(pool_max[idx], -FLT_MAX);\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nint main() {\n    int size0 = 10, size1 = 17, size2 = 7, size3 = 8, size4 = 16;\n    int total_size = size0 * size1 * size2 * size3 * size4;\n    float* data = (float*)malloc(total_size * sizeof(float));\n    float* pool_max = (float*)malloc(1088 * sizeof(float));\n\n    // Initialize data with appropriate values\n    // For demonstration, initializing it with dummy values\n    for (int i = 0; i < total_size; ++i) {\n        data[i] = rand() / (float)RAND_MAX;\n    }\n\n    // Call the function\n    default_function_kernel(data, pool_max, total_size);\n\n    // Print the result (optional)\n    for (int i = 0; i < 1088; ++i) {\n        printf(\"pool_max[%d] = %f\\n\", i, pool_max[i]);\n    }\n\n    // Free allocated memory\n    free(data);\n    free(pool_max);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <stdio.h>\n#include <float.h>  // For FLT_MIN\n#include <math.h>   // For fmaxf\n\nvoid default_function_kernel(float* data, float* pool_max) {\n    // Number of blocks and threads per block\n    const int num_blocks = 10;\n    const int threads_per_block = 64;\n\n    // Iterate over blocks\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        // Iterate over threads within the block\n        for (int threadIdx = 0; threadIdx < threads_per_block; ++threadIdx) {\n            // Initialize pool_max[blockIdx * 64 + threadIdx] to the smallest possible float value\n            int index = (blockIdx * 64) + threadIdx;\n            pool_max[index] = -FLT_MAX;\n\n            // Iterate over the 3x3x3 region\n            for (int rv0 = 0; rv0 < 3; ++rv0) {\n                for (int rv1 = 0; rv1 < 3; ++rv1) {\n                    for (int rv2 = 0; rv2 < 3; ++rv2) {\n                        // Compute the condition for valid data access\n                        int condition1 = (blockIdx & 1) * 4 + ((threadIdx >> 5) * 2) + rv0;\n                        int condition2 = (((threadIdx >> 5) + (rv0 >> 1)) >> 1) + (blockIdx & 1);\n                        int condition3 = ((threadIdx & 31) >> 3) * 2 + rv1;\n                        int condition4 = ((threadIdx & 7) * 2) + rv2;\n\n                        // Check if conditions are met\n                        float value = (1 <= condition1 && condition2 < 2 &&\n                                       1 <= condition3 && 1 <= condition4)\n                            ? data[(((((((((blockIdx >> 1) * 896) + ((blockIdx & 1) * 512)) +\n                                         ((threadIdx >> 5) * 256)) + (rv0 * 128)) +\n                                         ((((threadIdx & 31) >> 3) * 32)) + (rv1 * 16)) +\n                                         (((threadIdx & 7) * 2) + rv2)) - 145)]\n                            : -FLT_MAX;\n\n                        // Update the maximum value\n                        pool_max[index] = fmaxf(pool_max[index], value);\n                    }\n                }\n            }\n        }\n    }\n}\n\n// Example usage:\n// float data[10 * 17 * 7 * 8 * 16];\n// float pool_max[10 * 64];\n// default_function_kernel(data, pool_max);\n\n|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1360; ++i0_i1_fused_i2_fused_i3_fused) {\n    pool_max[i0_i1_fused_i2_fused_i3_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n        for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n          if ((((1 <= (((((i0_i1_fused_i2_fused_i3_fused >> 5) & 1) * 4) + ((i0_i1_fused_i2_fused_i3_fused >> 10) * 2)) + rv0)) && (((((i0_i1_fused_i2_fused_i3_fused >> 10) + (rv0 >> 1)) >> 1) + ((i0_i1_fused_i2_fused_i3_fused >> 5) & 1)) < 2)) && (1 <= (((((i0_i1_fused_i2_fused_i3_fused >> 3) & 31) * 2) + rv1))) && (1 <= (((((i0_i1_fused_i2_fused_i3_fused & 7) * 2) + rv2)))) {\n            pool_max[i0_i1_fused_i2_fused_i3_fused] = fmaxf(pool_max[i0_i1_fused_i2_fused_i3_fused], data[((((((((((i0_i1_fused_i2_fused_i3_fused >> 6) * 896) + (((i0_i1_fused_i2_fused_i3_fused >> 5) & 1) * 512)) + ((i0_i1_fused_i2_fused_i3_fused >> 10) * 256)) + (rv0 * 128)) + (((i0_i1_fused_i2_fused_i3_fused >> 3) & 31) * 32)) + (rv1 * 16)) + (((i0_i1_fused_i2_fused_i3_fused & 7) * 2)) + rv2) - 145)]);\n          }\n        }\n      }\n    }\n  }\n}\n"
    },
    {
        "op_name": "sign",
        "c_code": "void default_function_kernel(float* T_sign, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 1248; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_sign[ax0_ax1_fused_ax2_fused_ax3_fused] = ((0.000000e+00f < data[ax0_ax1_fused_ax2_fused_ax3_fused]) ? 1.000000e+00f : ((data[ax0_ax1_fused_ax2_fused_ax3_fused] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 2, 13, 3), \"float32\"), T_sign: T.Buffer((16, 2, 13, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(1248):\n            T_sign_1 = T.Buffer((1248,), data=T_sign.data)\n            data_1 = T.Buffer((1248,), data=data.data)\n            T_sign_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.if_then_else(T.float32(0) < data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(1), T.Select(data_1[ax0_ax1_fused_ax2_fused_ax3_fused] < T.float32(0), T.float32(-1), T.float32(0)))",
        "op_args": [
            16,
            2,
            13,
            3
        ],
        "input_shape": "[[16, 2, 13, 3]]",
        "output_shape": "[[16, 2, 13, 3]]",
        "deepseek_c": "void default_function_kernel(float* T_sign, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 416; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    for (int32_t ax3 = 0; ax3 < 3; ++ax3) {\n      T_sign[((ax0_ax1_fused_ax2_fused_outer_outer * 3) + ax3)] = ((0.000000e+00f < data[((ax0_ax1_fused_ax2_fused_outer_outer * 3) + ax3)]) ? 1.000000e+00f : ((data[((ax0_ax1_fused_ax2_fused_outer_outer * 3) + ax3)] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* T_sign, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 1248; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_sign[ax0_ax1_fused_ax2_fused_ax3_fused] = ((0.000000e+00f < data[ax0_ax1_fused_ax2_fused_ax3_fused])? 1.000000e+00f : ((data[ax0_ax1_fused_ax2_fused_ax3_fused] < 0.000000e+00f)? -1.000000e+00f : 0.000000e+00f));\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_sign, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 576; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_sign[ax0_ax1_fused_ax2_fused_ax3_fused] = ((0.000000e+00f < data[ax0_ax1_fused_ax2_fused_ax3_fused]) ? 1.000000e+00f : ((data[ax0_ax1_fused_ax2_fused_ax3_fused] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* T_sign, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 416; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 3; ++i3_s) {\n      T_sign[((i0_i1_fused_i2_fused * 3) + i3_s)] = (0.000000e+00f < data[((i0_i1_fused_i2_fused * 3) + i3_s)]) ? 1.000000e+00f : ((data[((i0_i1_fused_i2_fused * 3) + i3_s)] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "relu",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 6930; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = max(data[i0_i1_fused_i2_fused_i3_fused], 0.000000e+00f);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 0.000000e+00f);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 14, 3, 11), \"float32\"), compute: T.Buffer((15, 14, 3, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(6930):\n            compute_1 = T.Buffer((6930,), data=compute.data)\n            data_1 = T.Buffer((6930,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.max(data_1[i0_i1_fused_i2_fused_i3_fused], T.float32(0))",
        "op_args": [
            15,
            14,
            3,
            11
        ],
        "input_shape": "[[15, 14, 3, 11]]",
        "output_shape": "[[15, 14, 3, 11]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3 = 0; i3 < 11; ++i3) {\n          compute[((((i0 * 462) + (i1 * 33)) + (i2 * 11)) + i3)] = max(data[((((i0 * 462) + (i1 * 33)) + (i2 * 11)) + i3)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 6930; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = max(data[i0_i1_fused_i2_fused_i3_fused], 0.000000e+00f);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 630; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = max(data[i0_i1_fused_i2_fused_i3_fused], 0.000000e+00f);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 630; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 11; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 11) + i3_s)] = fmaxf(data[((i0_i1_fused_i2_fused * 11) + i3_s)], 0.000000e+00f);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "sqrt",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 180) + (i1 * 9)) + i2)] = sqrtf(data[(((i0 * 180) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 20, 9, 1), \"float32\"), compute: T.Buffer((7, 20, 9, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(20, 9):\n                cse_var_1: T.int32 = i0 * 180 + i1 * 9 + i2\n                compute_1 = T.Buffer((1260,), data=compute.data)\n                data_1 = T.Buffer((1260,), data=data.data)\n                compute_1[cse_var_1] = T.sqrt(data_1[cse_var_1])",
        "op_args": [
            7,
            20,
            9,
            1
        ],
        "input_shape": "[[7, 20, 9, 1]]",
        "output_shape": "[[7, 20, 9, 1]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        for (int32_t i3 = 0; i3 < 1; ++i3) {\n          compute[((((i0 * 180) + (i1 * 9)) + (i2 * 1)) + i3)] = sqrtf(data[((((i0 * 180) + (i1 * 9)) + (i2 * 1)) + i3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1360; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sqrtf(data[i0_i1_fused_i2_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1260; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 1; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 1) + i3_s)] = sqrtf(data[((i0_i1_fused_i2_fused * 1) + i3_s)]);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "scale_shift_nchw",
        "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused < 1482; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused) {\n    for (int32_t b_inner = 0; b_inner < 5; ++b_inner) {\n      for (int32_t i_inner = 0; i_inner < 6; ++i_inner) {\n        ScaleShift[((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused / 741) * 22230) + (b_inner * 4446)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 741) / 19) * 114)) + (i_inner * 19)) + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 19))] = ((data[((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused / 741) * 22230) + (b_inner * 4446)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 741) / 19) * 114)) + (i_inner * 19)) + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 19))] * Scale[((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 741) / 57)]) + Shift[((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 741) / 57)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 11115) {\n    ScaleShift[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) % 2223) / 171)]) + Shift[((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) % 2223) / 171)]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 13, 18, 19), \"float32\"), Scale: T.Buffer((13,), \"float32\"), Shift: T.Buffer((13,), \"float32\"), ScaleShift: T.Buffer((10, 13, 18, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused in T.parallel(1482):\n            for b_inner, i_inner in T.grid(5, 6):\n                cse_var_3: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 741\n                cse_var_2: T.int32 = cse_var_3 // 57\n                cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused // 741 * 22230 + b_inner * 4446 + cse_var_3 // 19 * 114 + i_inner * 19 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 19\n                ScaleShift_1 = T.Buffer((44460,), data=ScaleShift.data)\n                data_1 = T.Buffer((44460,), data=data.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale[cse_var_2] + Shift[cse_var_2]",
        "op_args": [
            10,
            13,
            18,
            19
        ],
        "input_shape": "[[10, 13, 18, 19], [13], [13]]",
        "output_shape": "[[10, 13, 18, 19]]",
        "deepseek_c": "void default_function_kernel(float* Scale, float* Shift, float* data, float* ScaleShift) {\n  #pragma omp parallel for\n  for (int32_t b = 0; b < 10; ++b) {\n    for (int32_t i = 0; i < 13; ++i) {\n      for (int32_t j = 0; j < 18; ++j) {\n        for (int32_t k = 0; k < 19; ++k) {\n          ScaleShift[((((b * 4194) + (i * 312)) + (j * 17)) + k)] = ((data[((((b * 4194) + (i * 312)) + (j * 17)) + k)] * Scale[i]) + Shift[i]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "#include <stdio.h>\n\nvoid default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_inner_fused_j_inner_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_inner_fused_j_inner_inner_fused < 11115; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_inner_fused_j_inner_inner_fused) {\n    for (int32_t j_inner = 0; j_inner < 19; ++j_inner) {\n      ScaleShift[(((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_inner_fused_j_inner_inner_fused / 11115) * 323) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_inner_fused_j_inner_inner_fused % 11115)) * 19) + j_inner)] = ((data[(((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_inner_fused_j_inner_inner_fused / 11115) * 323) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_inner_fused_j_inner_inner_fused % 11115)) * 19) + j_inner)] * Scale[((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_",
        "codellama_c": "#pragma omp parallel for\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused < 130; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused) {\n    for (int32_t j_inner = 0; j_inner < 19; ++j_inner) {\n      ScaleShift[((((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused % 13) * 324) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused / 13) * 26)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused % 13) * 19)) + j_inner)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused / 13) * 26)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused % 13) * 19))) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int blockIdx_x = 0; blockIdx_x < 10; blockIdx_x++) {\n    for (int threadIdx_x = 0; threadIdx_x < 32; threadIdx_x++) {\n      int idx = (blockIdx_x * 8) + (threadIdx_x >> 2);\n      if (idx < 11115) {\n        ScaleShift[(blockIdx_x * 32) + threadIdx_x] = (data[(blockIdx_x * 32) + threadIdx_x] * Scale[(((blockIdx_x * 16) + (threadIdx_x >> 1)) % 2223) / 171]) + Shift[(((blockIdx_x * 16) + (threadIdx_x >> 1)) % 2223) / 171];\n      }\n    }\n  }\n}\n",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int i = 0; i < 10; i++) {\n    for (int j = 0; j < 32; j++) {\n      if ((i * 8) + (j >> 2) < 11115) {\n        ScaleShift[(i * 32) + j] = (data[(i * 32) + j] * Scale[(((i * 16) + (j >> 1)) % 2223) / 171]) + Shift[(((i * 16) + (j >> 1)) % 2223) / 171];\n      }\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 10 * 13 * 18; // Adjusted for the given tensor shape\n    int blockDim_x = 32;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int cond = ((blockIdx_x * 8) + (threadIdx_x >> 2));\n            if (cond < 11115) {\n                int scale_shift_idx = (blockIdx_x * 32) + threadIdx_x;\n                int data_idx = scale_shift_idx;\n                int scale_shift_mod_idx = (((blockIdx_x * 16) + (threadIdx_x >> 1)) % 2223) / 171;\n                ScaleShift[scale_shift_idx] = (data[data_idx] * Scale[scale_shift_mod_idx]) + Shift[scale_shift_mod_idx];\n            }\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 10 * 13 * 18; // Based on input tensor shape (10 * 13 * 18)\n    int blockDim_x = 32;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int cond = (blockIdx_x * 8) + (threadIdx_x >> 2);\n            if (cond < 11115) {\n                int scale_shift_idx = (((blockIdx_x * 16) + (threadIdx_x >> 1)) % 2223) / 171;\n                ScaleShift[idx] = (data[idx] * Scale[scale_shift_idx]) + Shift[scale_shift_idx];\n            }\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 35568; ++i0_i1_fused_i2_fused_i3_fused) {\n    if (i0_i1_fused_i2_fused_i3_fused < 11115) {\n      ScaleShift[i0_i1_fused_i2_fused_i3_fused] = (data[i0_i1_fused_i2_fused_i3_fused] * Scale[((i0_i1_fused_i2_fused_i3_fused / 32) % 13)]) + Shift[((i0_i1_fused_i2_fused_i3_fused / 32) % 13)];\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "tan",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        for (int32_t i3 = 0; i3 < 20; ++i3) {\n          compute[((((i0 * 1320) + (i1 * 220)) + (i2 * 20)) + i3)] = tanf(data[((((i0 * 1320) + (i1 * 220)) + (i2 * 20)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(55) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 55) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 55) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 6, 11, 20), \"float32\"), compute: T.Buffer((13, 6, 11, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(13):\n            for i1, i2, i3 in T.grid(6, 11, 20):\n                cse_var_1: T.int32 = i0 * 1320 + i1 * 220 + i2 * 20 + i3\n                compute_1 = T.Buffer((17160,), data=compute.data)\n                data_1 = T.Buffer((17160,), data=data.data)\n                compute_1[cse_var_1] = T.tan(data_1[cse_var_1])",
        "op_args": [
            13,
            6,
            11,
            20
        ],
        "input_shape": "[[13, 6, 11, 20]]",
        "output_shape": "[[13, 6, 11, 20]]",
        "deepseek_c": "void default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 171600; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = tanf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "starcoder_c": "# C code\ndefault_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 17160; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = tanf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n</s>\n",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 10860; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = tanf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1430; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 20; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 20) + i3_s)] = tanf(data[((i0_i1_fused_i2_fused * 20) + i3_s)]);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "prelu",
        "c_code": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 46512; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < data[i0_i1_fused_i2_fused_i3_fused]) ? data[i0_i1_fused_i2_fused_i3_fused] : (data[i0_i1_fused_i2_fused_i3_fused] * Scale[(i0_i1_fused_i2_fused_i3_fused % 19)]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(38) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] * Scale[(((int)threadIdx.x) % 19)]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 17, 16, 19), \"float32\"), Scale: T.Buffer((19,), \"float32\"), compute: T.Buffer((9, 17, 16, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(46512):\n            compute_1 = T.Buffer((46512,), data=compute.data)\n            data_1 = T.Buffer((46512,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.if_then_else(T.float32(0) < data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused] * Scale[i0_i1_fused_i2_fused_i3_fused % 19])",
        "op_args": [
            9,
            17,
            16,
            19
        ],
        "input_shape": "[[9, 17, 16, 19], [19]]",
        "output_shape": "[[9, 17, 16, 19]]",
        "deepseek_c": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 26208; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    compute[ax0_ax1_fused_ax2_fused_ax3_fused] = ((0.000000e+00f < data[ax0_ax1_fused_ax2_fused_ax3_fused]) ? data[ax0_ax1_fused_ax2_fused_ax3_fused] : (data[ax0_ax1_fused_ax2_fused_ax3_fused] * Scale[((ax0_ax1_fused_ax2_fused_ax3_fused % 19) + 19) % 19)]));\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2432; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < data[i0_i1_fused_i2_fused_i3_fused])? data[i0_i1_fused_i2_fused_i3_fused] : (data[i0_i1_fused_i2_fused_i3_fused] * Scale[i0_i1_fused_i2_fused_i3_fused % 19]));\n  }\n}",
        "codellama_c": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3888; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < data[i0_i1_fused_i2_fused_i3_fused]) ? data[i0_i1_fused_i2_fused_i3_fused] : (data[i0_i1_fused_i2_fused_i3_fused] * Scale[(i0_i1_fused_i2_fused_i3_fused % 19)]));\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2592; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 19; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 19) + i3_s)] = (0.000000e+00f < data[((i0_i1_fused_i2_fused * 19) + i3_s)]) ? data[((i0_i1_fused_i2_fused * 19) + i3_s)] : (data[((i0_i1_fused_i2_fused * 19) + i3_s)] * Scale[i3_s % 19]);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "tanh",
        "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 20; ++i1) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      for (int32_t i3 = 0; i3 < 13; ++i3) {\n        compute[(((i1 * 182) + (i2 * 13)) + i3)] = tanhf(data[(((i1 * 182) + (i2 * 13)) + i3)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = tanhf(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 20, 14, 13), \"float32\"), compute: T.Buffer((1, 20, 14, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(20, 14, 13):\n            cse_var_1: T.int32 = i1 * 182 + i2 * 13 + i3\n            compute_1 = T.Buffer((3640,), data=compute.data)\n            data_1 = T.Buffer((3640,), data=data.data)\n            compute_1[cse_var_1] = T.tanh(data_1[cse_var_1])",
        "op_args": [
            1,
            20,
            14,
            13
        ],
        "input_shape": "[[1, 20, 14, 13]]",
        "output_shape": "[[1, 20, 14, 13]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        for (int32_t ax3 = 0; ax3 < 28; ++ax3) {\n          compute[((((i0 * 364) + (i1 * 26)) + (i2 * 13)) + ax3)] = tanhf(data[((((i0 * 364) + (i1 * 26)) + (i2 * 13)) + ax3)]);\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3640; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = tanhf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 4280; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = tanhf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 280; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 13; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 13) + i3_s)] = tanhf(data[((i0_i1_fused_i2_fused * 13) + i3_s)]);\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "scale_shift_nchwc",
        "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused < 196; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused) {\n    for (int32_t cb_outer_inner = 0; cb_outer_inner < 7; ++cb_outer_inner) {\n      for (int32_t i_inner = 0; i_inner < 16; ++i_inner) {\n        ScaleShift[((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused / 28) * 3136) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused & 1) * 1568)) + (i_inner * 98)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 28) >> 1) * 7)) + cb_outer_inner)] = ((data[((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused / 28) * 3136) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused & 1) * 1568)) + (i_inner * 98)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 28) >> 1) * 7)) + cb_outer_inner)] * Scale[(((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused & 1) * 7) + cb_outer_inner)]) + Shift[(((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused & 1) * 7) + cb_outer_inner)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[((((((((int)blockIdx.x) % 49) * 2) + (((int)threadIdx.x) >> 5)) / 49) * 7) + ((((int)blockIdx.x) + ((int)threadIdx.x)) % 7))]) + Shift[((((((((int)blockIdx.x) % 49) * 2) + (((int)threadIdx.x) >> 5)) / 49) * 7) + ((((int)blockIdx.x) + ((int)threadIdx.x)) % 7))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 2, 16, 14, 7), \"float32\"), Scale: T.Buffer((2, 7), \"float32\"), Shift: T.Buffer((2, 7), \"float32\"), ScaleShift: T.Buffer((7, 2, 16, 14, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused in T.parallel(196):\n            for cb_outer_inner, i_inner in T.grid(7, 16):\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 2\n                cse_var_2: T.int32 = cse_var_3 * 7 + cb_outer_inner\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused // 28 * 3136 + cse_var_3 * 1568 + i_inner * 98 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 28 // 2 * 7 + cb_outer_inner\n                ScaleShift_1 = T.Buffer((21952,), data=ScaleShift.data)\n                data_1 = T.Buffer((21952,), data=data.data)\n                Scale_1 = T.Buffer((14,), data=Scale.data)\n                Shift_1 = T.Buffer((14,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]",
        "op_args": [
            7,
            7,
            16,
            14
        ],
        "input_shape": "[[7, 2, 16, 14, 7], [2, 7], [2, 7]]",
        "output_shape": "[[7, 2, 16, 14, 7]]",
        "deepseek_c": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_outer_outer_fused = 0; b_outer_outer_outer_outer_outer_fused < 16; ++b_outer_outer_outer_outer_outer_fused) {\n    for (int32_t b_outer_outer_outer_outer_inner_fused = 0; b_outer_outer_outer_outer_inner_fused < 14; ++b_outer_outer_outer_outer_inner_fused) {\n      for (int32_t b_outer_outer_inner_fused = 0; b_outer_outer_inner_fused < 2; ++b_outer_outer_inner_fused) {\n        for (int32_t b_outer_fused = 0; b_outer_fused < 7; ++b_outer_fused) {\n          for (int32_t b_inner_fused = 0; b_inner_fused < 64; ++b_inner_fused) {\n            ScaleShift[((((((((b_outer_fused * 128) + (b_outer_outer_inner_fused * 64)) + (b_outer_outer_outer_outer_inner_fused * 32)) + (b_outer_outer_outer_outer_outer_fused * 2)) + b_outer_outer_inner_fused) * 64) + b_inner_fused) + (((((b_outer_outer_outer_outer_inner_fused * 2) + (((b_outer_outer_inner_fused * 2) + b_outer_outer_outer_outer_outer_fused) % 2)) / 2) * 4) + (b_inner_fused / 16)))] = ((data[((((((((b_outer_fused * 128) + (b_outer_outer_inner_fused * 64)) + (b_outer_outer_outer_outer_inner_fused * 32)) + (b_outer_outer_outer_outer_outer_fused * 2)) + b_outer_outer_inner_fused) * 64) + b_inner_fused) + (((((b_outer_outer_outer_outer_inner_fused * 2) + (((b_outer_outer_inner_fused * 2) + b_outer_outer_outer_outer_outer_fused) % 2)) / 2) * 4) + (b_inner_fused / 16)))] * Scale[((((((((b_outer_outer_outer_outer_inner_fused * 2) + (((b_outer_outer_inner_fused * 2) + b_outer_outer_outer_outer_outer_fused) % 2)) / 2) * 7) + (((((b_outer_outer_outer_outer_inner_fused * 2) + (((b_outer_outer_inner_fused * 2) + b_outer_outer_outer_outer_outer_fused) % 2)) / 2) * 4) + (b_inner_fused / 16))) * 7) + (((((b_outer_outer_outer_outer_inner_fused * 2) + (((b_outer_outer_inner_fused * 2) + b_outer_outer_outer_outer_outer_fused) % 2)) / 2) * 4) + (b_inner_fused / 16)))])) + Shift[((((((((b_outer_outer_outer_outer_inner_fused * 2) + (((b_outer_outer_inner_fused * 2) + b_outer_outer_outer_outer_outer_fused) % 2)) / 2) * 7) + (((((b_outer_outer_outer_outer_inner_fused * 2) + (((b_outer_outer_inner_fused * 2) + b_outer_outer_outer_outer_outer_fused) % 2)) / 2) * 4) + (b_inner_fused / 16))) * 7) + (((((b_outer_outer_outer_outer_inner_fused * 2) + (((b_outer_outer_inner_fused * 2) + b_outer_outer_outer_outer_outer_fused) % 2)) / 2) * 4) + (b_inner_fused / 16)))]);\n          }\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "#include <stdio.h>\n\nvoid default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cc_inner_fused_i_inner_fused_j_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cc_inner_fused_i_inner_fused_j_inner_fused < 1568; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cc_inner_fused_i_inner_fused_j_inner_fused) {\n    for (int32_t b_inner = 0; b_inner < 2; ++b_inner) {\n      for (int32_t cc_inner = 0; cc_inner < 2; ++cc_inner) {\n        ScaleShift[((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cc_inner_fused_i_inner_fused_j_inner_fused >> 10) * 112) + (cc_inner * 56)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cc_inner_fused_i_inner_fused_j_inner_fused & 511) >> 5) * 2)) + (b_inner * 2)) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cc_inner_fused_i_inner_fused_j_inner_fused & 31))] = ((data[((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cc_inner_fused_i_inner_fused_j_inner_fused >> 10) * 112) + (cc_inner * 56)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cc_inner_fused_i_inner_fused_j_inner_",
        "codellama_c": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused_i4_fused = 0; i0_i1_fused_i2_fused_i3_fused_i4_fused < 1248; ++i0_i1_fused_i2_fused_i3_fused_i4_fused) {\n    ScaleShift[i0_i1_fused_i2_fused_i3_fused_i4_fused] = ((data[i0_i1_fused_i2_fused_i3_fused_i4_fused] * Scale[((i0_i1_fused_i2_fused_i3_fused_i4_fused % 14) * 2) + (i0_i1_fused_i2_fused_i3_fused_i4_fused / 14)]) + Shift[((i0_i1_fused_i2_fused_i3_fused_i4_fused % 14) * 2) + (i0_i1_fused_i2_fused_i3_fused_i4_fused / 14)]);\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int blockIdx_x = 0; blockIdx_x < 7; blockIdx_x++) {\n    for (int threadIdx_x = 0; threadIdx_x < 64; threadIdx_x++) {\n      ScaleShift[(blockIdx_x * 64) + threadIdx_x] = (data[(blockIdx_x * 64) + threadIdx_x] * Scale[(((((blockIdx_x % 49) * 2) + (threadIdx_x >> 5)) / 49) * 7) + ((blockIdx_x + threadIdx_x) % 7)]) + Shift[(((((blockIdx_x % 49) * 2) + (threadIdx_x >> 5)) / 49) * 7) + ((blockIdx_x + threadIdx_x) % 7)];\n    }\n  }\n}\n",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int i = 0; i < 7; i++) {\n    for (int j = 0; j < 64; j++) {\n      ScaleShift[(i * 64) + j] = (data[(i * 64) + j] * Scale[(((((i % 49) * 2) + (j >> 5)) / 49) * 7) + ((i + j) % 7)]) + Shift[(((((i % 49) * 2) + (j >> 5)) / 49) * 7) + ((i + j) % 7)];\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 7 * 2 * 16 * 14; // Adjusted for the given tensor shape\n    int blockDim_x = 64;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int scale_shift_idx = (((((blockIdx_x % 49) * 2) + (threadIdx_x >> 5)) / 49) * 7) + ((blockIdx_x + threadIdx_x) % 7);\n            ScaleShift[idx] = (data[idx] * Scale[scale_shift_idx]) + Shift[scale_shift_idx];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 7 * 2 * 16 * 14; // Based on input tensor shape (7 * 2 * 16 * 14)\n    int blockDim_x = 64;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int scale_shift_idx = (((((blockIdx_x % 49) * 2) + (threadIdx_x >> 5)) / 49) * 7) + ((blockIdx_x + threadIdx_x) % 7);\n            ScaleShift[idx] = (data[idx] * Scale[scale_shift_idx]) + Shift[scale_shift_idx];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused_i4_fused = 0; i0_i1_fused_i2_fused_i3_fused_i4_fused < 15680; ++i0_i1_fused_i2_fused_i3_fused_i4_fused) {\n    ScaleShift[i0_i1_fused_i2_fused_i3_fused_i4_fused] = (data[i0_i1_fused_i2_fused_i3_fused_i4_fused] * Scale[((((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 64) % 49) * 2) + ((i0_i1_fused_i2_fused_i3_fused_i4_fused >> 5) / 49)) * 7) + (((i0_i1_fused_i2_fused_i3_fused_i4_fused / 64) + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 64)) % 7))]) + Shift[((((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 64) % 49) * 2) + ((i0_i1_fused_i2_fused_i3_fused_i4_fused >> 5) / 49)) * 7) + (((i0_i1_fused_i2_fused_i3_fused_i4_fused / 64) + (i0_i1_fused_i2_fused_i3_fused_i4_fused % 64)) % 7))];\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "matmul",
        "c_code": "void default_function_kernel(float* T_matmul, float* left_matrix, float* right_matrix) {\n  for (int32_t ax1_outer_outer_outer = 0; ax1_outer_outer_outer < 2; ++ax1_outer_outer_outer) {\n    for (int32_t ax0_inner_init = 0; ax0_inner_init < 2; ++ax0_inner_init) {\n      T_matmul[((ax0_inner_init * 2) + ax1_outer_outer_outer)] = 0.000000e+00f;\n    }\n    for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n      for (int32_t ax0_inner = 0; ax0_inner < 2; ++ax0_inner) {\n        T_matmul[((ax0_inner * 2) + ax1_outer_outer_outer)] = (T_matmul[((ax0_inner * 2) + ax1_outer_outer_outer)] + (left_matrix[((ax0_inner * 2) + k_inner)] * right_matrix[((k_inner * 2) + ax1_outer_outer_outer)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_matmul, float* __restrict__ left_matrix, float* __restrict__ right_matrix) {\n  float T_matmul_local[2];\n  __shared__ float left_matrix_shared[4];\n  __shared__ float right_matrix_shared[4];\n  T_matmul_local[0] = 0.000000e+00f;\n  T_matmul_local[1] = 0.000000e+00f;\n  for (int ax0_ax1_fused_outer_outer = 0; ax0_ax1_fused_outer_outer < 2; ++ax0_ax1_fused_outer_outer) {\n    left_matrix_shared[((ax0_ax1_fused_outer_outer * 2) + ((int)threadIdx.x))] = left_matrix[((ax0_ax1_fused_outer_outer * 2) + ((int)threadIdx.x))];\n  }\n  for (int ax0_ax1_fused_outer_outer_1 = 0; ax0_ax1_fused_outer_outer_1 < 2; ++ax0_ax1_fused_outer_outer_1) {\n    right_matrix_shared[((ax0_ax1_fused_outer_outer_1 * 2) + ((int)threadIdx.x))] = right_matrix[((ax0_ax1_fused_outer_outer_1 * 2) + ((int)threadIdx.x))];\n  }\n  __syncthreads();\n  for (int k_inner = 0; k_inner < 2; ++k_inner) {\n    T_matmul_local[0] = (T_matmul_local[0] + (left_matrix_shared[((((int)threadIdx.x) * 2) + k_inner)] * right_matrix_shared[(k_inner * 2)]));\n    T_matmul_local[1] = (T_matmul_local[1] + (left_matrix_shared[((((int)threadIdx.x) * 2) + k_inner)] * right_matrix_shared[((k_inner * 2) + 1)]));\n  }\n  T_matmul[(((int)threadIdx.x) * 2)] = T_matmul_local[0];\n  T_matmul[((((int)threadIdx.x) * 2) + 1)] = T_matmul_local[1];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(left_matrix: T.Buffer((2, 2), \"float32\"), right_matrix: T.Buffer((2, 2), \"float32\"), T_matmul: T.Buffer((2, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax1_outer_outer_outer in range(2):\n            T_matmul_1 = T.Buffer((4,), data=T_matmul.data)\n            for ax0_inner_init in range(2):\n                T_matmul_1[ax0_inner_init * 2 + ax1_outer_outer_outer] = T.float32(0)\n            for k_inner, ax0_inner in T.grid(2, 2):\n                cse_var_2: T.int32 = ax0_inner * 2\n                cse_var_1: T.int32 = cse_var_2 + ax1_outer_outer_outer\n                left_matrix_1 = T.Buffer((4,), data=left_matrix.data)\n                right_matrix_1 = T.Buffer((4,), data=right_matrix.data)\n                T_matmul_1[cse_var_1] = T_matmul_1[cse_var_1] + left_matrix_1[cse_var_2 + k_inner] * right_matrix_1[k_inner * 2 + ax1_outer_outer_outer]",
        "op_args": [
            7,
            2,
            2,
            2
        ],
        "input_shape": "[[2, 2], [2, 2]]",
        "output_shape": "[[2, 2]]",
        "deepseek_c": "void default_function_kernel(float* T_matmul, float* left_matrix, float* right_matrix) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_outer_outer = 0; ax0_ax1_fused_outer_outer < 4; ++ax0_ax1_fused_outer_outer) {\n    float T_matmul_local[2];\n    T_matmul_local[0] = 0.000000e+00f;\n    T_matmul_local[1] = 0.000000e+00f;\n    for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n      T_matmul_local[0] = (T_matmul_local[0] + (left_matrix[((ax0_ax1_fused_outer_outer * 2) + k_inner)] * right_matrix[k_inner]));\n      T_matmul_local[1] = (T_matmul_local[1] + (left_matrix[((ax0_ax1_fused_outer_outer * 2) + k_inner)] * right_matrix[((k_inner * 2) + 1)]));\n    }\n    T_matmul[((ax0_ax1_fused_outer_outer * 2) + 0)] = T_matmul_local[0];\n    T_matmul[((ax0_ax1_fused_outer_outer * 2) + 1)] = T_matmul_local[1];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* T_matmul, float* left_matrix, float* right_matrix) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 2; ++ax0_ax1_fused) {\n    float T_matmul_local[2];\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      T_matmul_local[ax1] = 0.000000e+00f;\n      for (int32_t k = 0; k < 2; ++k) {\n        T_matmul_local[ax1] = (T_matmul_local[ax1] + (left_matrix[((ax0_ax1_fused * 2) + k)] * right_matrix[((k * 2) + ax1)]));\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 2; ++ax1_1) {\n      T_matmul[((ax0_ax1_fused * 2) + ax1_1)] = T_matmul_local[ax1_1];\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_matmul, float* left_matrix, float* right_matrix) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 4; ++ax0_ax1_fused) {\n    float T_matmul_local[1];\n    T_matmul_local[0] = 0.000000e+00f;\n    for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n      T_matmul_local[0] = (T_matmul_local[0] + (left_matrix[((ax0_ax1_fused * 2) + k_inner)] * right_matrix[k_inner]));\n    }\n    T_matmul[ax0_ax1_fused] = T_matmul_local[0];\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* T_matmul, float* left_matrix, float* right_matrix) {\n  #pragma omp parallel for\n  for (int thread_x = 0; thread_x < 2; ++thread_x) {\n    float T_matmul_local[2] = {0.0f, 0.0f};\n    float left_matrix_shared[4];\n    float right_matrix_shared[4];\n\n    // Load data into shared memory\n    for (int ax0_ax1_fused_outer_outer = 0; ax0_ax1_fused_outer_outer < 2; ++ax0_ax1_fused_outer_outer) {\n      left_matrix_shared[(ax0_ax1_fused_outer_outer * 2) + thread_x] = left_matrix[(ax0_ax1_fused_outer_outer * 2) + thread_x];\n      right_matrix_shared[(ax0_ax1_fused_outer_outer * 2) + thread_x] = right_matrix[(ax0_ax1_fused_outer_outer * 2) + thread_x];\n    }\n\n    // Perform matrix multiplication\n    for (int k_inner = 0; k_inner < 2; ++k_inner) {\n      T_matmul_local[0] += left_matrix_shared[(thread_x * 2) + k_inner] * right_matrix_shared[k_inner * 2];\n      T_matmul_local[1] += left_matrix_shared[(thread_x * 2) + k_inner] * right_matrix_shared[(k_inner * 2) + 1];\n    }\n\n    // Store results\n    T_matmul[(thread_x * 2)] = T_matmul_local[0];\n    T_matmul[(thread_x * 2) + 1] = T_matmul_local[1];\n  }\n}\n//End-of-Code|",
        "gemini2_c": "void default_function_kernel(float* data, float* tensor) {\n  for (int blockIdx_x = 0; blockIdx_x < 28; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 64; ++threadIdx_x) {\n      if (((((int)blockIdx_x) * 8) + (((int)threadIdx_x) >> 3)) < 1755) {\n        tensor[((((int)blockIdx_x) * 64) + ((int)threadIdx_x))] = 0.000000e+00f;\n      }\n      if (((((int)blockIdx_x) * 8) + (((int)threadIdx_x) >> 3)) < 1755) {\n        tensor[((((int)blockIdx_x) * 64) + ((int)threadIdx_x))] = (tensor[((((int)blockIdx_x) * 64) + ((int)threadIdx_x))] + (data[((((int)blockIdx_x) * 64) + ((int)threadIdx_x))] * data[((((int)blockIdx_x) * 64) + ((int)threadIdx_x))]));\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* T_divide, float* data, float* tensor) {\n  for (int blockIdx_x = 0; blockIdx_x < 28; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 26; ++threadIdx_x) {\n      T_divide[((((int)blockIdx_x) * 26) + ((int)threadIdx_x))] = (data[((((int)blockIdx_x) * 26) + ((int)threadIdx_x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx_x) * 26) + ((int)threadIdx_x))])), 7.500000e-01f));\n    }\n  }\n}\n",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* T_matmul, float* left_matrix, float* right_matrix) {\n    float T_matmul_local[2];\n    float left_matrix_shared[4];\n    float right_matrix_shared[4];\n\n    // Copy left_matrix to shared memory\n    #pragma omp parallel for\n    for (int i = 0; i < 4; ++i) {\n        left_matrix_shared[i] = left_matrix[i];\n    }\n\n    // Copy right_matrix to shared memory\n    #pragma omp parallel for\n    for (int i = 0; i < 4; ++i) {\n        right_matrix_shared[i] = right_matrix[i];\n    }\n\n    // Initialize local accumulators\n    T_matmul_local[0] = 0.0f;\n    T_matmul_local[1] = 0.0f;\n\n    // Perform matrix multiplication\n    #pragma omp parallel for\n    for (int threadIdx_x = 0; threadIdx_x < 2; ++threadIdx_x) {\n        for (int k_inner = 0; k_inner < 2; ++k_inner) {\n            T_matmul_local[0] += left_matrix_shared[threadIdx_x * 2 + k_inner] * right_matrix_shared[k_inner * 2];\n            T_matmul_local[1] += left_matrix_shared[threadIdx_x * 2 + k_inner] * right_matrix_shared[k_inner * 2 + 1];\n        }\n        T_matmul[threadIdx_x * 2] = T_matmul_local[0];\n        T_matmul[threadIdx_x * 2 + 1] = T_matmul_local[1];\n    }\n}\n\nint main() {\n    float T_matmul[4];\n    float left_matrix[4] = {1.0f, 2.0f, 3.0f, 4.0f};\n    float right_matrix[4] = {5.0f, 6.0f, 7.0f, 8.0f};\n\n    // Call the function\n    default_function_kernel(T_matmul, left_matrix, right_matrix);\n\n    // Print the result\n    for (int i = 0; i < 4; ++i) {\n        printf(\"%f \", T_matmul[i]);\n    }\n    printf(\"\\n\");\n\n    return 0;\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* T_matmul, float* left_matrix, float* right_matrix) {\n    float T_matmul_local[2];\n    float left_matrix_shared[4];\n    float right_matrix_shared[4];\n\n    // Load left_matrix into shared memory\n    #pragma omp parallel for\n    for (int ax0_ax1_fused_outer_outer = 0; ax0_ax1_fused_outer_outer < 2; ++ax0_ax1_fused_outer_outer) {\n        left_matrix_shared[ax0_ax1_fused_outer_outer * 2 + omp_get_thread_num()] = left_matrix[ax0_ax1_fused_outer_outer * 2 + omp_get_thread_num()];\n    }\n\n    // Load right_matrix into shared memory\n    #pragma omp parallel for\n    for (int ax0_ax1_fused_outer_outer_1 = 0; ax0_ax1_fused_outer_outer_1 < 2; ++ax0_ax1_fused_outer_outer_1) {\n        right_matrix_shared[ax0_ax1_fused_outer_outer_1 * 2 + omp_get_thread_num()] = right_matrix[ax0_ax1_fused_outer_outer_1 * 2 + omp_get_thread_num()];\n    }\n\n    #pragma omp barrier\n\n    T_matmul_local[0] = 0.0f;\n    T_matmul_local[1] = 0.0f;\n\n    // Matrix multiplication\n    for (int k_inner = 0; k_inner < 2; ++k_inner) {\n        T_matmul_local[0] += left_matrix_shared[omp_get_thread_num() * 2 + k_inner] * right_matrix_shared[k_inner * 2];\n        T_matmul_local[1] += left_matrix_shared[omp_get_thread_num() * 2 + k_inner] * right_matrix_shared[k_inner * 2 + 1];\n    }\n\n    // Store results\n    T_matmul[omp_get_thread_num() * 2] = T_matmul_local[0];\n    T_matmul[omp_get_thread_num() * 2 + 1] = T_matmul_local[1];\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* T_matmul, float* left_matrix, float* right_matrix) {\n  for (int32_t ax1_outer_outer_outer = 0; ax1_outer_outer_outer < 2; ++ax1_outer_outer_outer) {\n    for (int32_t ax0_inner_init = 0; ax0_inner_init < 2; ++ax0_inner_init) {\n      T_matmul[(ax0_inner_init * 2) + ax1_outer_outer_outer] = 0.000000e+00f;\n    }\n    for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n      for (int32_t ax0_inner = 0; ax0_inner < 2; ++ax0_inner) {\n        T_matmul[(ax0_inner * 2) + ax1_outer_outer_outer] = (T_matmul[(ax0_inner * 2) + ax1_outer_outer_outer] + (left_matrix[(ax0_inner * 2) + k_inner] * right_matrix[(k_inner * 2) + ax1_outer_outer_outer]));\n      }\n    }\n  }\n}\n"
    },
    {
        "op_name": "softmax",
        "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    float T_softmax_maxelem[4];\n    float T_softmax_expsum[4];\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        T_softmax_maxelem[i2] = -3.402823e+38f;\n        T_softmax_maxelem[i2] = max(T_softmax_maxelem[i2], data[(((i0 * 16) + (i1 * 4)) + i2)]);\n      }\n      for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n        T_softmax_maxelem[i2_1] = expf((data[(((i0 * 16) + (i1 * 4)) + i2_1)] - T_softmax_maxelem[i2_1]));\n      }\n      for (int32_t i2_2 = 0; i2_2 < 4; ++i2_2) {\n        T_softmax_expsum[i2_2] = 0.000000e+00f;\n        T_softmax_expsum[i2_2] = (T_softmax_expsum[i2_2] + T_softmax_maxelem[i2_2]);\n      }\n      for (int32_t i2_3 = 0; i2_3 < 4; ++i2_3) {\n        T_softmax_norm[(((i0 * 16) + (i1 * 4)) + i2_3)] = (T_softmax_maxelem[i2_3] / T_softmax_expsum[i2_3]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 17) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 17) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) < 17) {\n    T_softmax_norm[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__expf((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) / T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + __expf((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 4, 4, 1), \"float32\"), T_softmax_norm: T.Buffer((17, 4, 4, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(17):\n            T_softmax_maxelem = T.allocate([4], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([4], \"float32\", \"global\")\n            for i1 in range(4):\n                T_softmax_maxelem_1 = T.Buffer((4,), data=T_softmax_maxelem, align=16)\n                data_1 = T.Buffer((272,), data=data.data)\n                for i2 in range(4):\n                    T_softmax_maxelem_1[i2] = T.float32(-3.4028234663852886e+38)\n                    T_softmax_maxelem_1[i2] = T.max(T_softmax_maxelem_1[i2], data_1[i0 * 16 + i1 * 4 + i2])\n                T_softmax_maxelem_2 = T.Buffer((4,), data=T_softmax_maxelem, align=16)\n                for i2 in range(4):\n                    T_softmax_maxelem_2[i2] = T.exp(data_1[i0 * 16 + i1 * 4 + i2] - T_softmax_maxelem_1[i2])\n                T_softmax_expsum_1 = T.Buffer((4,), data=T_softmax_expsum, align=16)\n                for i2 in range(4):\n                    T_softmax_expsum_1[i2] = T.float32(0)\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_maxelem_2[i2]\n                for i2 in range(4):\n                    T_softmax_norm_1 = T.Buffer((272,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[i0 * 16 + i1 * 4 + i2] = T_softmax_maxelem_2[i2] / T_softmax_expsum_1[i2]",
        "op_args": [
            17,
            4,
            4,
            1
        ],
        "input_shape": "[[17, 4, 4, 1]]",
        "output_shape": "[[17, 4, 4, 1]]",
        "deepseek_c": "void default_function_kernel(float* T_softmax_maxelem, float* T_softmax_expsum, float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_outer = 0; ax0_ax1_fused_ax2_fused_outer < 272; ++ax0_ax1_fused_ax2_fused_outer) {\n    T_softmax_maxelem[ax0_ax1_fused_ax2_fused_outer] = -3.402823e+38f;\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 272; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    T_softmax_maxelem[ax0_ax1_fused_ax2_fused_outer_outer] = max(T_softmax_maxelem[ax0_ax1_fused_ax2_fused_outer_outer], data[ax0_ax1_fused_ax2_fused_outer_outer]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_1 < 272; ++ax0_ax1_fused_ax2_fused_outer_1) {\n    T_softmax_expsum[ax0_ax1_fused_ax2_fused_outer_1] = 0.000000e+00f;\n    T_softmax_expsum[ax0_ax1_fused_ax2_fused_outer_1] = (T_softmax_expsum[ax0_ax1_fused_ax2_fused_outer_1] + expf((data[ax0_ax1_fused_ax2_fused_outer_1] - T_softmax_maxelem[ax0_ax1_fused_ax2_fused_outer_1])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_outer_2 = 0; ax0_ax1_fused_ax2_fused_outer_2 < 272; ++ax0_ax1_fused_ax2_fused_outer_2) {\n    T_softmax_norm[ax0_ax1_fused_ax2_fused_outer_2] = (expf((data[ax0_ax1_fused_ax2_fused_outer_2] - T_softmax_maxelem[ax0_ax1_fused_ax2_fused_outer_2])) / T_softmax_expsum[ax0_ax1_fused_ax2_fused_outer_2]);\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 272; ++i0_i1_fused_i2_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    T_softmax_maxelem[0] = -3.402823e+38f;\n    for (int32_t k = 0; k < 1; ++k) {\n      T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[((i0_i1_fused_i2_fused * 1) + k)]);\n    }\n    T_softmax_expsum[0] = 0.000000e+00f;\n    for (int32_t k_1 = 0; k_1 < 1; ++k_1) {\n      T_softmax_expsum[0] = (T_softmax_expsum[0] + expf((data[((i0_i1_fused_i2_fused * 1) + k_1)] - T_softmax_maxelem[0])));\n    }\n    for (int32_t k_2 = 0; k_2 < 1; ++k_2) {\n      T_softmax_norm[((i0_i1_fused_i2_fused * 1) + k_2)] = (expf((data[((i0_i1_fused_i2_fused * 1) + k_2)] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 112; ++i0_i1_fused_i2_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    T_softmax_maxelem[0] = -3.402823e+38f;\n    T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[i0_i1_fused_i2_fused]);\n    T_softmax_expsum[0] = 0.000000e+00f;\n    T_softmax_expsum[0] = (T_softmax_expsum[0] + expf((data[i0_i1_fused_i2_fused] - T_softmax_maxelem[0])));\n    T_softmax_norm[i0_i1_fused_i2_fused] = (expf((data[i0_i1_fused_i2_fused] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n  }\n}",
        "gemini1_c": "#include <omp.h>\n#include <math.h> // For expf, fmaxf\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n  // Calculate the number of threads needed\n  int num_threads = 17 * 32;\n\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i = 0; i < num_threads; ++i) {\n    // Calculate blockIdx.x and threadIdx.x from the global thread index i\n    int blockIdx_x = i / 32;\n    int threadIdx_x = i % 32;\n\n    if (((blockIdx_x * 2) + (threadIdx_x >> 4)) < 17) {\n      T_softmax_maxelem[blockIdx_x * 32 + threadIdx_x] = -3.402823e+38f; // Initialize with negative infinity\n      T_softmax_maxelem[blockIdx_x * 32 + threadIdx_x] = fmaxf(T_softmax_maxelem[blockIdx_x * 32 + threadIdx_x], data[blockIdx_x * 32 + threadIdx_x]);\n    }\n  }\n}\n\nvoid default_function_kernel_2(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data) {\n  // Calculate the number of threads needed\n  int num_threads = 17 * 64;\n\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i = 0; i < num_threads; ++i) {\n    // Calculate blockIdx.x and threadIdx.x from the global thread index i\n    int blockIdx_x = i / 64;\n    int threadIdx_x = i % 64;\n\n    if (((blockIdx_x * 4) + (threadIdx_x >> 4)) < 17) {\n      T_softmax_norm[blockIdx_x * 64 + threadIdx_x] = expf((data[blockIdx_x * 64 + threadIdx_x] - T_softmax_maxelem[blockIdx_x * 64 + threadIdx_x]) / T_softmax_expsum[blockIdx_x * 64 + threadIdx_x]);\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* T_softmax_expsum, float* T_softmax_maxelem, float* data) {\n  // Calculate the number of threads needed\n  int num_threads = 17 * 8;\n\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i = 0; i < num_threads; ++i) {\n    // Calculate blockIdx.x and threadIdx.x from the global thread index i\n    int blockIdx_x = i / 8;\n    int threadIdx_x = i % 8;\n\n    T_softmax_expsum[blockIdx_x * 8 + threadIdx_x] = 0.0f;\n    T_softmax_expsum[blockIdx_x * 8 + threadIdx_x] += expf((data[blockIdx_x * 8 + threadIdx_x] - T_softmax_maxelem[blockIdx_x * 8 + threadIdx_x]));\n  }\n}\n//|End-of-Code|\n    ",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n  // Input tensor shape: [[17, 4, 4, 1]]\n  // T_softmax_maxelem shape: [544]\n  // data shape: [544]\n  for (int blockIdx_x = 0; blockIdx_x < 17; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 32; ++threadIdx_x) {\n      int idx = (blockIdx_x * 32) + threadIdx_x;\n      if ((blockIdx_x * 2 + (threadIdx_x >> 4)) < 17) {\n        T_softmax_maxelem[idx] = -3.402823e+38f; // Initialize with negative infinity\n        T_softmax_maxelem[idx] = fmaxf(T_softmax_maxelem[idx], data[idx]);\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_2(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data) {\n  // Input tensor shape: [[17, 4, 4, 1]]\n  // T_softmax_expsum shape: [544]\n  // T_softmax_maxelem shape: [544]\n  // T_softmax_norm shape: [544]\n  // data shape: [544]\n  for (int blockIdx_x = 0; blockIdx_x < 17; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 64; ++threadIdx_x) {\n      int idx = (blockIdx_x * 64) + threadIdx_x;\n      if ((blockIdx_x * 4 + (threadIdx_x >> 4)) < 17) {\n        T_softmax_norm[idx] = expf((data[idx] - T_softmax_maxelem[idx]) / T_softmax_expsum[idx]);\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* T_softmax_expsum, float* T_softmax_maxelem, float* data) {\n  // Input tensor shape: [[17, 4, 4, 1]]\n  // T_softmax_expsum shape: [544]\n  // T_softmax_maxelem shape: [544]\n  // data shape: [544]\n  for (int blockIdx_x = 0; blockIdx_x < 68; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 8; ++threadIdx_x) {\n      int idx = (blockIdx_x * 8) + threadIdx_x;\n      T_softmax_expsum[idx] = 0.0f;\n      T_softmax_expsum[idx] += expf((data[idx] - T_softmax_maxelem[idx]));\n    }\n  }\n}\n",
        "gpt1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\n// Function to calculate T_softmax_maxelem\nvoid default_function_kernel(float* T_softmax_maxelem, float* data, int num_blocks, int block_size) {\n    #pragma omp parallel for\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < block_size; ++threadIdx) {\n            int idx = (blockIdx * block_size) + threadIdx;\n            if ((((blockIdx * 2) + (threadIdx >> 4)) < 17)) {\n                T_softmax_maxelem[idx] = -3.402823e+38f;\n            }\n        }\n    }\n\n    #pragma omp parallel for\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < block_size; ++threadIdx) {\n            int idx = (blockIdx * block_size) + threadIdx;\n            if ((((blockIdx * 2) + (threadIdx >> 4)) < 17)) {\n                T_softmax_maxelem[idx] = fmaxf(T_softmax_maxelem[idx], data[idx]);\n            }\n        }\n    }\n}\n\n// Function to calculate T_softmax_norm\nvoid default_function_kernel_2(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data, int num_blocks, int block_size) {\n    #pragma omp parallel for\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < block_size; ++threadIdx) {\n            int idx = (blockIdx * block_size) + threadIdx;\n            if ((((blockIdx * 4) + (threadIdx >> 4)) < 17)) {\n                T_softmax_norm[idx] = expf(data[idx] - T_softmax_maxelem[idx]) / T_softmax_expsum[idx];\n            }\n        }\n    }\n}\n\n// Function to calculate T_softmax_expsum\nvoid default_function_kernel_1(float* T_softmax_expsum, float* T_softmax_maxelem, float* data, int num_blocks, int block_size) {\n    #pragma omp parallel for\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < block_size; ++threadIdx) {\n            int idx = (blockIdx * block_size) + threadIdx;\n            T_softmax_expsum[idx] = 0.0f;\n        }\n    }\n\n    #pragma omp parallel for\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < block_size; ++threadIdx) {\n            int idx = (blockIdx * block_size) + threadIdx;\n            if ((((blockIdx * 8) + threadIdx) < num_blocks * block_size)) {\n                T_softmax_expsum[idx] += expf(data[idx] - T_softmax_maxelem[idx]);\n            }\n        }\n    }\n}\n\nint main() {\n    int num_blocks = 17; // Example number of blocks\n    int block_size_1 = 32;\n    int block_size_2 = 64;\n    int block_size_3 = 8;\n\n    // Allocate memory for tensors\n    int total_size_1 = num_blocks * block_size_1;\n    int total_size_2 = num_blocks * block_size_2;\n    int total_size_3 = num_blocks * block_size_3;\n\n    float* data = (float*)malloc(total_size_1 * sizeof(float));\n    float* T_softmax_maxelem = (float*)malloc(total_size_1 * sizeof(float));\n    float* T_softmax_norm = (float*)malloc(total_size_2 * sizeof(float));\n    float* T_softmax_expsum = (float*)malloc(total_size_3 * sizeof(float));\n\n    // Initialize data with some values\n    for (int i = 0; i < total_size_1; ++i) {\n        data[i] = (rand() % 100) / 10.0f;\n    }\n\n    // Call the functions\n    default_function_kernel(T_softmax_maxelem, data, num_blocks, block_size_1);\n    default_function_kernel_1(T_softmax_expsum, T_softmax_maxelem, data, num_blocks, block_size_3);\n    default_function_kernel_2(T_softmax_expsum, T_softmax_maxelem, T_softmax_norm, data, num_blocks, block_size_2);\n\n    // Print results (optional)\n    printf(\"T_softmax_maxelem[0]: %f\\n\", T_softmax_maxelem[0]);\n    printf(\"T_softmax_norm[0]: %f\\n\", T_softmax_norm[0]);\n    printf(\"T_softmax_expsum[0]: %f\\n\", T_softmax_expsum[0]);\n\n    // Free allocated memory\n    free(data);\n    free(T_softmax_maxelem);\n    free(T_softmax_norm);\n    free(T_softmax_expsum);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <stdio.h>\n#include <math.h>\n#include <float.h> // For FLT_MIN\n\nvoid default_function_kernel(float* data, float* T_softmax_maxelem) {\n    const int num_blocks = 17; // Number of blocks\n    const int threads_per_block = 32; // Number of threads per block\n    \n    // Compute the size of the data tensor\n    int tensor_size = 17 * 4 * 4 * 1; // Shape [17, 4, 4, 1]\n\n    // Initialize T_softmax_maxelem\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < threads_per_block; ++threadIdx) {\n            int idx = (blockIdx * threads_per_block) + threadIdx;\n            if (((blockIdx * 2) + (threadIdx >> 4)) < 17) {\n                T_softmax_maxelem[idx] = -FLT_MAX; // Use FLT_MAX to represent the minimum value\n            }\n        }\n    }\n\n    // Compute the maximum element\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < threads_per_block; ++threadIdx) {\n            int idx = (blockIdx * threads_per_block) + threadIdx;\n            if (((blockIdx * 2) + (threadIdx >> 4)) < 17) {\n                T_softmax_maxelem[idx] = fmaxf(T_softmax_maxelem[idx], data[idx]);\n            }\n        }\n    }\n}\n\n// Example usage:\n// float data[17 * 4 * 4 * 1];\n// float T_softmax_maxelem[17 * 32];\n// default_function_kernel(data, T_softmax_maxelem);\n\n|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* T_softmax_maxelem, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 68; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 32; ++i3_s) {\n      if (((i0_i1_fused_i2_fused * 2) + (i3_s >> 4)) < 17) {\n        T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s)] = -3.402823e+38f;\n      }\n      if (((i0_i1_fused_i2_fused * 2) + (i3_s >> 4)) < 17) {\n        T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s)] = fmaxf(T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s)], data[((i0_i1_fused_i2_fused * 32) + i3_s)]);\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* T_softmax_expsum, float* T_softmax_maxelem, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 17; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n      T_softmax_expsum[((i0_i1_fused_i2_fused * 8) + i3_s)] = 0.000000e+00f;\n      T_softmax_expsum[((i0_i1_fused_i2_fused * 8) + i3_s)] = (T_softmax_expsum[((i0_i1_fused_i2_fused * 8) + i3_s)] + expf((data[((i0_i1_fused_i2_fused * 8) + i3_s)] - T_softmax_maxelem[((i0_i1_fused_i2_fused * 8) + i3_s)])));\n    }\n  }\n}\n\nvoid default_function_kernel_2(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 17; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 64; ++i3_s) {\n      if (((i0_i1_fused_i2_fused * 4) + (i3_s >> 4)) < 17) {\n        T_softmax_norm[((i0_i1_fused_i2_fused * 64) + i3_s)] = (expf((data[((i0_i1_fused_i2_fused * 64) + i3_s)] - T_softmax_maxelem[((i0_i1_fused_i2_fused * 64) + i3_s)])) / T_softmax_expsum[((i0_i1_fused_i2_fused * 64) + i3_s)]);\n      }\n    }\n  }\n}\n"
    },
    {
        "op_name": "combination_op",
        "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 392; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n      T_add[((ax0_ax1_fused_ax2_fused * 16) + ax3)] = (sqrtf(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)]) + cosf(data_1[((ax0_ax1_fused_ax2_fused * 16) + ax3)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 4, 7, 16), \"float32\"), data_1: T.Buffer((14, 4, 7, 16), \"float32\"), T_add: T.Buffer((14, 4, 7, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(392):\n            for ax3 in range(16):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 16 + ax3\n                T_add_1 = T.Buffer((6272,), data=T_add.data)\n                data_2 = T.Buffer((6272,), data=data.data)\n                data_3 = T.Buffer((6272,), data=data_1.data)\n                T_add_1[cse_var_1] = T.sqrt(data_2[cse_var_1]) + T.cos(data_3[cse_var_1])",
        "op_args": [
            14,
            4,
            7,
            16
        ],
        "input_shape": "[[14, 4, 7, 16], [14, 4, 7, 16]]",
        "output_shape": "[[14, 4, 7, 16]]",
        "deepseek_c": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_inner_inner = 0; ax0_ax1_fused_ax2_fused_inner_inner < 392; ++ax0_ax1_fused_ax2_fused_inner_inner) {\n    T_add[ax0_ax1_fused_ax2_fused_inner_inner] = (sqrtf(data[ax0_ax1_fused_ax2_fused_inner_inner]) + cosf(data_1[ax0_ax1_fused_ax2_fused_inner_inner]));\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 392; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n      T_add[((ax0_ax1_fused_ax2_fused * 16) + ax3)] = (sqrtf(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)]) + cosf(data_1[((ax0_ax1_fused_ax2_fused * 16) + ax3)]));\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 576; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n      T_add[((ax0_ax1_fused_ax2_fused * 16) + ax3)] = (sqrtf(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)]) + cosf(data_1[((ax0_ax1_fused_ax2_fused * 16) + ax3)]));\n    }\n  }\n}",
        "gemini3_c": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 784; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 16; ++i3_s) {\n      T_add[((i0_i1_fused_i2_fused * 16) + i3_s)] = (sqrtf(data[((i0_i1_fused_i2_fused * 16) + i3_s)]) + __cosf(data_1[((i0_i1_fused_i2_fused * 16) + i3_s)]));\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "multi_out_op",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        for (int32_t i3 = 0; i3 < 17; ++i3) {\n          compute[((((i0 * 3468) + (i1 * 204)) + (i2 * 17)) + i3)] = sqrtf((data[((((i0 * 3468) + (i1 * 204)) + (i2 * 17)) + i3)] + data_1[((((i0 * 3468) + (i1 * 204)) + (i2 * 17)) + i3)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 68; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 12; ++i2_1) {\n      for (int32_t i3_1 = 0; i3_1 < 17; ++i3_1) {\n        compute_1[(((i0_i1_fused * 204) + (i2_1 * 17)) + i3_1)] = cosf((data[(((i0_i1_fused * 204) + (i2_1 * 17)) + i3_1)] + data_1[(((i0_i1_fused * 204) + (i2_1 * 17)) + i3_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __cosf((data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = sqrtf((data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 17, 12, 17), \"float32\"), data_1: T.Buffer((4, 17, 12, 17), \"float32\"), compute: T.Buffer((4, 17, 12, 17), \"float32\"), compute_1: T.Buffer((4, 17, 12, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_2 = T.Buffer((13872,), data=data.data)\n        data_3 = T.Buffer((13872,), data=data_1.data)\n        for i0 in T.parallel(4):\n            for i1, i2, i3 in T.grid(17, 12, 17):\n                cse_var_1: T.int32 = i0 * 3468 + i1 * 204 + i2 * 17 + i3\n                compute_2 = T.Buffer((13872,), data=compute.data)\n                compute_2[cse_var_1] = T.sqrt(data_2[cse_var_1] + data_3[cse_var_1])\n        for i0_i1_fused in T.parallel(68):\n            for i2, i3 in T.grid(12, 17):\n                cse_var_2: T.int32 = i0_i1_fused * 204 + i2 * 17 + i3\n                compute_2 = T.Buffer((13872,), data=compute_1.data)\n                compute_2[cse_var_2] = T.cos(data_2[cse_var_2] + data_3[cse_var_2])",
        "op_args": [
            4,
            17,
            12,
            17
        ],
        "input_shape": "[[4, 17, 12, 17], [4, 17, 12, 17], [4, 17, 12, 17]]",
        "output_shape": "[[4, 17, 12, 17]]",
        "deepseek_c": "void default_function_kernel(float* compute, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        for (int32_t i3 = 0; i3 < 17; ++i3) {\n          compute[((((i0 * 3468) + (i1 * 198)) + (i2 * 17)) + i3)] = sqrtf((data[((((i0 * 3468) + (i1 * 198)) + (i2 * 17)) + i3)] + data_1[((((i0 * 3468) + (i1 * 198)) + (i2 * 17)) + i3)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 4; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 17; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 12; ++i2_1) {\n        for (int32_t i3_1 = 0; i3_1 < 17; ++i3_1) {\n          compute[((((i0_1 * 3468) + (i1_1 * 198)) + (i2_1 * 17)) + i3_1)] = cosf((data[((((i0_1 * 3468) + (i1_1 * 198)) + (i2_1 * 17)) + i3_1)] + data_1[((((i0_1 * 3468) + (i1_1 * 198)) + (i2_1 * 17)) + i3_1)]));\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* compute, float* compute_1, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 16320; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sqrtf((data[i0_i1_fused_i2_fused_i3_fused] + data_1[i0_i1_fused_i2_fused_i3_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused_1 = 0; i0_i1_fused_i2_fused_i3_fused_1 < 16320; ++i0_i1_fused_i2_fused_i3_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_i3_fused_1] = cosf((data[i0_i1_fused_i2_fused_i3_fused_1] + data_1[i0_i1_fused_i2_fused_i3_fused_1]));\n  }\n}",
        "codellama_c": "void default_function_kernel(float* compute, float* compute_1, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 864; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 17; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 17) + i3)] = sqrtf((data[((i0_i1_fused_i2_fused * 17) + i3)] + data_1[((i0_i1_fused_i2_fused * 17) + i3)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 864; ++i0_i1_fused_i2_fused_1) {\n    for (int32_t i3_1 = 0; i3_1 < 17; ++i3_1) {\n      compute_1[((i0_i1_fused_i2_fused_1 * 17) + i3_1)] = cosf((data[((i0_i1_fused_i2_fused_1 * 17) + i3_1)] + data_1[((i0_i1_fused_i2_fused_1 * 17) + i3_1)]));\n    }\n  }\n}",
        "gemini1_c": "#include <omp.h>\n#include <math.h>\n\nvoid default_function_kernel(float* compute, float* data, float* data_1) {\n  // Calculate the number of threads per block for each kernel\n  const int threadsPerBlock_1 = 24;\n  const int threadsPerBlock_2 = 6;\n\n  // Calculate the number of blocks needed based on the input tensor shape\n  const int numBlocks_1 = 4 * 17 * 12 * 17 / threadsPerBlock_1;\n  const int numBlocks_2 = 4 * 17 * 12 * 17 / threadsPerBlock_2;\n\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks_1; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock_1; ++threadIdx) {\n      int index = (blockIdx * threadsPerBlock_1) + threadIdx;\n      compute[index] = cosf(data[index] + data_1[index]);\n    }\n  }\n\n  #pragma omp parallel for\n  for (int blockIdx = 0; blockIdx < numBlocks_2; ++blockIdx) {\n    for (int threadIdx = 0; threadIdx < threadsPerBlock_2; ++threadIdx) {\n      int index = (blockIdx * threadsPerBlock_2) + threadIdx;\n      compute[index] = sqrtf(data[index] + data_1[index]);\n    }\n  }\n}\n//|End-of-Code|",
        "gemini2_c": "void default_function_kernel(float* compute, float* data, float* data_1, int blockIdx_x, int threadIdx_x) {\n  int idx = (blockIdx_x * 6) + threadIdx_x;\n  compute[idx] = sqrtf(data[idx] + data_1[idx]);\n}\n",
        "gpt1_c": "#include <math.h>\n#include <omp.h>\n#include <stdio.h>\n#include <stdlib.h>\n\n// Kernel 1: Compute cosine of the sum of two data arrays\nvoid default_function_kernel_1(float* compute, float* data, float* data_1, int size) {\n    #pragma omp parallel for\n    for (int idx = 0; idx < size; ++idx) {\n        compute[idx] = cosf(data[idx] + data_1[idx]);\n    }\n}\n\n// Kernel 2: Compute square root of the sum of two data arrays\nvoid default_function_kernel(float* compute, float* data, float* data_1, int size) {\n    #pragma omp parallel for\n    for (int idx = 0; idx < size; ++idx) {\n        compute[idx] = sqrtf(data[idx] + data_1[idx]);\n    }\n}\n\nint main() {\n    int shape[4] = {4, 17, 12, 17};\n    int size = shape[0] * shape[1] * shape[2] * shape[3];\n\n    float* data = (float*)malloc(size * sizeof(float));\n    float* data_1 = (float*)malloc(size * sizeof(float));\n    float* compute_cos = (float*)malloc(size * sizeof(float));\n    float* compute_sqrt = (float*)malloc(size * sizeof(float));\n\n    // Initialize data and data_1 (for example purposes)\n    for (int i = 0; i < size; ++i) {\n        data[i] = (float)i / size;\n        data_1[i] = (float)i / (size * 2);\n    }\n\n    // Call the kernel functions\n    default_function_kernel_1(compute_cos, data, data_1, size);\n    default_function_kernel(compute_sqrt, data, data_1, size);\n\n    // Print some results (for verification purposes)\n    for (int i = 0; i < 10; ++i) {\n        printf(\"cos(%f + %f) = %f\\n\", data[i], data_1[i], compute_cos[i]);\n        printf(\"sqrt(%f + %f) = %f\\n\", data[i], data_1[i], compute_sqrt[i]);\n    }\n\n    // Free the allocated memory\n    free(data);\n    free(data_1);\n    free(compute_cos);\n    free(compute_sqrt);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <math.h>\n#include <stdio.h>\n#include <stdlib.h>\n\nvoid default_function_kernel_1(float* compute, float* data, float* data_1, int blocks, int threads) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads; ++threadIdx_x) {\n            int idx = blockIdx_x * threads + threadIdx_x;\n            compute[idx] = cosf(data[idx] + data_1[idx]);\n        }\n    }\n}\n\nvoid default_function_kernel(float* compute, float* data, float* data_1, int blocks, int threads) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads; ++threadIdx_x) {\n            int idx = blockIdx_x * threads + threadIdx_x;\n            compute[idx] = sqrtf(data[idx] + data_1[idx]);\n        }\n    }\n}\n\nint main() {\n    int blocks = 4 * 17 * 12 * 17; // As per the tensor shape provided\n    int threads_per_block_1 = 24;  // Launch bounds for kernel 1\n    int threads_per_block_2 = 6;   // Launch bounds for kernel 2\n\n    int total_elements_1 = blocks * threads_per_block_1;\n    int total_elements_2 = blocks * threads_per_block_2;\n\n    float *compute_1 = (float *)malloc(total_elements_1 * sizeof(float));\n    float *data = (float *)malloc(total_elements_1 * sizeof(float));\n    float *data_1 = (float *)malloc(total_elements_1 * sizeof(float));\n    float *compute_2 = (float *)malloc(total_elements_2 * sizeof(float));\n\n    // Initialize data and data_1 (example initialization)\n    for (int i = 0; i < total_elements_1; ++i) {\n        data[i] = (float)i * 0.01f;\n        data_1[i] = (float)i * 0.02f;\n    }\n\n    // Call the first function\n    default_function_kernel_1(compute_1, data, data_1, blocks, threads_per_block_1);\n\n    // Print the result of the first function\n    for (int i = 0; i < total_elements_1; ++i) {\n        printf(\"%f \", compute_1[i]);\n    }\n    printf(\"\\n\");\n\n    // Adjust the size for the second function if needed\n    for (int i = 0; i < total_elements_2; ++i) {\n        if (i < total_elements_1) {\n            data[i] = (float)i * 0.01f;\n            data_1[i] = (float)i * 0.02f;\n        }\n    }\n\n    // Call the second function\n    default_function_kernel(compute_2, data, data_1, blocks, threads_per_block_2);\n\n    // Print the result of the second function\n    for (int i = 0; i < total_elements_2; ++i) {\n        printf(\"%f \", compute_2[i]);\n    }\n    printf(\"\\n\");\n\n    free(compute_1);\n    free(data);\n    free(data_1);\n    free(compute_2);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* compute, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1088; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 17; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 17) + i3_s)] = sqrtf((data[((i0_i1_fused_i2_fused * 17) + i3_s)] + data_1[((i0_i1_fused_i2_fused * 17) + i3_s)]));\n    }\n  }\n}\n"
    },
    {
        "op_name": "softmax_common",
        "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    float T_softmax_maxelem[168];\n    float T_softmax_expsum[14];\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        T_softmax_maxelem[((i1 * 14) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 3; ++k) {\n          T_softmax_maxelem[((i1 * 14) + i2)] = max(T_softmax_maxelem[((i1 * 14) + i2)], data[((((i0 * 504) + (i1 * 42)) + (i2 * 3)) + k)]);\n        }\n      }\n    }\n    for (int32_t i1_1 = 0; i1_1 < 12; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n        T_softmax_expsum[i2_1] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 3; ++k_1) {\n            int32_t v_ = ((int32_t)(floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 504) + (i1_1 * 42)) + (i2_1 * 3)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)])));\n        }\n      }\n      for (int32_t i2_2 = 0; i2_2 < 14; ++i2_2) {\n        for (int32_t i3_s = 0; i3_s < 3; ++i3_s) {\n            int32_t v__1 = ((int32_t)(floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_norm[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 504) + (i1_1 * 42)) + (i2_2 * 3)) + i3_s)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)])) / T_softmax_expsum[i2_2]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 189) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 3; ++k) {\n    if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 189) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 96) + (((int)threadIdx.x) * 3)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 567) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)])) / T_softmax_expsum[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 3)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 3; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 3)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 12, 14, 3), \"float32\"), T_softmax_norm: T.Buffer((18, 12, 14, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(18):\n            T_softmax_maxelem = T.allocate([168], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([14], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((168,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((9072,), data=data.data)\n            for i1, i2 in T.grid(12, 14):\n                T_softmax_maxelem_1[i1 * 14 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(3):\n                    cse_var_1: T.int32 = i1 * 14 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 504 + i1 * 42 + i2 * 3 + k])\n            for i1 in range(12):\n                T_softmax_expsum_1 = T.Buffer((14,), data=T_softmax_expsum, align=32)\n                for i2 in range(14):\n                    T_softmax_expsum_1[i2] = T.float32(0)\n                    for k in range(3):\n                        cse_var_3: T.int32 = i1 * 14 + i2\n                        cse_var_2: T.int32 = i0 * 504 + i1 * 42 + i2 * 3 + k\n                        T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3])\n                for i2, i3_s in T.grid(14, 3):\n                    cse_var_5: T.int32 = i1 * 14 + i2\n                    cse_var_4: T.int32 = i0 * 504 + i1 * 42 + i2 * 3 + i3_s\n                    T_softmax_norm_1 = T.Buffer((9072,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_4] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5]) / T_softmax_expsum_1[i2]",
        "op_args": [
            18,
            12,
            14,
            3
        ],
        "input_shape": "[[18, 12, 14, 3]]",
        "output_shape": "[[18, 12, 14, 3]]",
        "deepseek_c": "void default_function_kernel(float* T_softmax_maxelem, float* data) {\n    for (int32_t b = 0; b < 18; ++b) {\n        for (int32_t i = 0; i < 12; ++i) {\n            for (int32_t j = 0; j < 14; ++j) {\n                for (int32_t k = 0; k < 3; ++k) {\n                    T_softmax_maxelem[((((b * 504) + (i * 42)) + (j * 3)) + k)] = -3.402823e+38f;\n                }\n            }\n        }\n    }\n    for (int32_t b = 0; b < 18; ++b) {\n        for (int32_t i = 0; i < 12; ++i) {\n            for (int32_t j = 0; j < 14; ++j) {\n                for (int32_t k = 0; k < 3; ++k) {\n                    for (int32_t l = 0; l < 3; ++l) {\n                        T_softmax_maxelem[((((b * 504) + (i * 42)) + (j * 3)) + k)] = fmaxf(T_softmax_maxelem[((((b * 504) + (i * 42)) + (j * 3)) + k)], data[((((((b * 456) + (i * 38)) + (j * 3)) + k) * 3) + l)]);\n                    }\n                }\n            }\n        }\n    }\n}",
        "starcoder_c": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 504; ++i0_i1_fused_i2_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i3 = 0; i3 < 3; ++i3) {\n      T_softmax_maxelem[0] = -3.402823e+38f;\n      for (int32_t k = 0; k < 3; ++k) {\n        T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[((i0_i1_fused_i2_fused * 3) + k)]);\n      }\n      T_softmax_expsum[0] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 3; ++k_1) {\n        T_softmax_expsum[0] = (T_softmax_expsum[0] + expf((data[((i0_i1_fused_i2_fused * 3) + k_1)] - T_softmax_maxelem[0])));\n      }\n      T_softmax_norm[((i0_i1_fused_i2_fused * 3) + i3)] = (expf((data[((i0_i1_fused_i2_fused * 3) + i3)] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_softmax_maxelem, float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2528; ++i0_i1_fused_i2_fused) {\n    T_softmax_maxelem[i0_i1_fused_i2_fused] = -3.402823e+38f;\n    for (int32_t k = 0; k < 3; ++k) {\n      T_softmax_maxelem[i0_i1_fused_i2_fused] = max(T_softmax_maxelem[i0_i1_fused_i2_fused], data[((i0_i1_fused_i2_fused * 3) + k)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 7680; ++i0_i1_fused_i2_fused_i3_fused) {\n    float T_softmax_expsum[1];\n    T_softmax_expsum[0] = 0.000000e+00f;\n    for (int32_t k = 0; k < 3; ++k) {\n        int v_ = ((int)(floorf(((max(min((data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[i0_i1_fused_i2_fused_i3_fused] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused])));\n    }\n    for (int32_t k_1 = 0; k_1 < 3; ++k_1) {\n      T_softmax_norm[((i0_i1_fused_i2_fused_i3_fused * 3) + k_1)] = (max((data[((i0_i1_fused_i2_fused_i3_fused * 3) + k_1)] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused_i3_fused * 3) + k_1)] - T_softmax_maxelem[i0_i1_fused_i2_fused_i3_fused]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f)) / T_softmax_expsum[0];\n    }\n  }\n}",
        "gemini1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\n#define NUM_THREADS 32\n#define BLOCK_SIZE 32\n\n// Define the maximum and minimum values for clamping\n#define MAX_VALUE 8.837627e+01f\n#define MIN_VALUE -8.837626e+01f\n\n// Function to calculate the maximum of two floats\nfloat max(float a, float b) {\n  return (a > b) ? a : b;\n}\n\n// Function to calculate the minimum of two floats\nfloat min(float a, float b) {\n  return (a < b) ? a : b;\n}\n\n// Function to perform the softmax operation on the input data\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n  // Calculate the number of threads needed\n  int num_threads = (189 + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n  // Create a thread pool\n  #pragma omp parallel for num_threads(num_threads)\n  for (int blockIdx_x = 0; blockIdx_x < num_threads; ++blockIdx_x) {\n    // Calculate the thread index within the block\n    for (int threadIdx_x = 0; threadIdx_x < BLOCK_SIZE; ++threadIdx_x) {\n      // Check if the thread is within the valid range\n      if (((blockIdx_x * 2) + (threadIdx_x >> 4)) < 189) {\n        // Initialize the maximum element to a very small value\n        T_softmax_maxelem[(blockIdx_x * BLOCK_SIZE) + threadIdx_x] = -3.402823e+38f;\n\n        // Find the maximum element within the corresponding 3-element slice\n        for (int k = 0; k < 3; ++k) {\n          T_softmax_maxelem[(blockIdx_x * BLOCK_SIZE) + threadIdx_x] = max(T_softmax_maxelem[(blockIdx_x * BLOCK_SIZE) + threadIdx_x], data[((blockIdx_x * 96) + (threadIdx_x * 3)) + k]);\n        }\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* T_softmax_expsum, float* T_softmax_maxelem, float* data) {\n  // Calculate the number of threads needed\n  int num_threads = (189 + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n  // Create a thread pool\n  #pragma omp parallel for num_threads(num_threads)\n  for (int blockIdx_x = 0; blockIdx_x < num_threads; ++blockIdx_x) {\n    // Calculate the thread index within the block\n    for (int threadIdx_x = 0; threadIdx_x < BLOCK_SIZE; ++threadIdx_x) {\n      // Check if the thread is within the valid range\n      if (((blockIdx_x * 2) + (threadIdx_x >> 4)) < 189) {\n        // Initialize the exponential sum to 0\n        T_softmax_expsum[(blockIdx_x * BLOCK_SIZE) + threadIdx_x] = 0.0f;\n\n        // Calculate the exponential sum for each element within the corresponding 3-element slice\n        for (int k = 0; k < 3; ++k) {\n          // Calculate the exponent using the formula exp(x - max(x))\n          float exponent = exp(max(min(data[((blockIdx_x * 96) + (threadIdx_x * 3)) + k] - T_softmax_maxelem[(blockIdx_x * BLOCK_SIZE) + threadIdx_x], MAX_VALUE), MIN_VALUE));\n\n          // Add the exponent to the exponential sum\n          T_softmax_expsum[(blockIdx_x * BLOCK_SIZE) + threadIdx_x] += exponent;\n        }\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_2(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data) {\n  // Calculate the number of threads needed\n  int num_threads = (567 + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n  // Create a thread pool\n  #pragma omp parallel for num_threads(num_threads)\n  for (int blockIdx_x = 0; blockIdx_x < num_threads; ++blockIdx_x) {\n    // Calculate the thread index within the block\n    for (int threadIdx_x = 0; threadIdx_x < BLOCK_SIZE; ++threadIdx_x) {\n      // Check if the thread is within the valid range\n      if (((blockIdx_x * 2) + (threadIdx_x >> 4)) < 567) {\n        // Calculate the normalized value using the formula exp(x - max(x)) / sum(exp(x - max(x)))\n        float exponent = exp(max(min(data[(blockIdx_x * BLOCK_SIZE) + threadIdx_x] - T_softmax_maxelem[((blockIdx_x * BLOCK_SIZE) + threadIdx_x) / 3], MAX_VALUE), MIN_VALUE));\n        T_softmax_norm[(blockIdx_x * BLOCK_SIZE) + threadIdx_x] = exponent / T_softmax_expsum[((blockIdx_x * BLOCK_SIZE) + threadIdx_x) / 3];\n      }\n    }\n  }\n}\n\nint main() {\n  // Allocate memory for the input data and output tensors\n  float* data = (float*)malloc(18 * 12 * 14 * 3 * sizeof(float));\n  float* T_softmax_maxelem = (float*)malloc(18 * 12 * 14 * sizeof(float));\n  float* T_softmax_expsum = (float*)malloc(18 * 12 * 14 * sizeof(float));\n  float* T_softmax_norm = (float*)malloc(18 * 12 * 14 * sizeof(float));\n\n  // Initialize the input data with random values\n  for (int i = 0; i < 18 * 12 * 14 * 3; ++i) {\n    data[i] = (float)rand() / RAND_MAX;\n  }\n\n  // Call the kernel functions to perform the softmax operation\n  default_function_kernel(T_softmax_maxelem, data);\n  default_function_kernel_1(T_softmax_expsum, T_softmax_maxelem, data);\n  default_function_kernel_2(T_softmax_expsum, T_softmax_maxelem, T_softmax_norm, data);\n\n  // Print the normalized values\n  for (int i = 0; i < 18 * 12 * 14; ++i) {\n    printf(\"T_softmax_norm[%d]: %f\\n\", i, T_softmax_norm[i]);\n  }\n\n  // Free the allocated memory\n  free(data);\n  free(T_softmax_maxelem);\n  free(T_softmax_expsum);\n  free(T_softmax_norm);\n\n  return 0;\n}\n\n    ",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n#include <float.h>\n\nvoid default_function_kernel(float* T_softmax_maxelem, float* data) {\n  // Input tensor shape: [18, 12, 14, 3]\n  // T_softmax_maxelem: [18, 12, 14]\n  // data: [18, 12, 14, 3]\n\n  for (int blockIdx_x = 0; blockIdx_x < 18; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 12; ++threadIdx_x) {\n      int idx = (blockIdx_x * 12 + threadIdx_x);\n      if (idx < 189) {\n        T_softmax_maxelem[idx] = -FLT_MAX;\n        for (int k = 0; k < 3; ++k) {\n          T_softmax_maxelem[idx] = fmaxf(T_softmax_maxelem[idx], data[(blockIdx_x * 96 + threadIdx_x * 3 + k)]);\n        }\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* T_softmax_expsum, float* T_softmax_maxelem, float* data) {\n  // Input tensor shape: [18, 12, 14, 3]\n  // T_softmax_expsum: [18, 12, 14]\n  // T_softmax_maxelem: [18, 12, 14]\n  // data: [18, 12, 14, 3]\n\n  for (int blockIdx_x = 0; blockIdx_x < 18; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 12; ++threadIdx_x) {\n      int idx = (blockIdx_x * 12 + threadIdx_x);\n      T_softmax_expsum[idx] = 0.0f;\n      for (int k = 0; k < 3; ++k) {\n        float val = data[(blockIdx_x * 96 + threadIdx_x * 3 + k)] - T_softmax_maxelem[idx];\n        val = fmaxf(fminf(val, 88.37627f), -88.37626f);\n        val = (floorf(val * 1.442695f + 0.5f) + 127.0f) * 6.931472e-01f;\n        T_softmax_expsum[idx] += expf(val);\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_2(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data) {\n  // Input tensor shape: [18, 12, 14, 3]\n  // T_softmax_expsum: [18, 12, 14]\n  // T_softmax_maxelem: [18, 12, 14]\n  // T_softmax_norm: [18, 12, 14]\n  // data: [18, 12, 14, 3]\n\n  for (int blockIdx_x = 0; blockIdx_x < 18; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 12; ++threadIdx_x) {\n      int idx = (blockIdx_x * 12 + threadIdx_x);\n      if (idx < 567) {\n        float val = data[(blockIdx_x * 12 + threadIdx_x)] - T_softmax_maxelem[idx / 3];\n        val = fmaxf(fminf(val, 88.37627f), -88.37626f);\n        val = (floorf(val * 1.442695f + 0.5f) + 127.0f) * 6.931472e-01f;\n        T_softmax_norm[idx] = expf(val) / T_softmax_expsum[idx / 3];\n      }\n    }\n  }\n}\n\n//|End-of-Code|",
        "gpt1_c": "#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n\n// Function to compute softmax max element\nvoid default_function_kernel(float* T_softmax_maxelem, const float* data, int blocks, int threads_per_block) {\n    #pragma omp parallel for collapse(2)\n    for (int blockIdx_x = 0; blockIdx_x < blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads_per_block; ++threadIdx_x) {\n            int idx = (blockIdx_x * 32) + threadIdx_x;\n            if (((blockIdx_x * 2) + (threadIdx_x >> 4)) < 189) {\n                T_softmax_maxelem[idx] = -3.402823e+38f;\n            }\n        }\n    }\n    \n    #pragma omp parallel for collapse(2)\n    for (int blockIdx_x = 0; blockIdx_x < blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads_per_block; ++threadIdx_x) {\n            int idx = (blockIdx_x * 32) + threadIdx_x;\n            if (((blockIdx_x * 2) + (threadIdx_x >> 4)) < 189) {\n                float local_max = T_softmax_maxelem[idx];\n                for (int k = 0; k < 3; ++k) {\n                    float value = data[((blockIdx_x * 96) + (threadIdx_x * 3)) + k];\n                    local_max = fmaxf(local_max, value);\n                }\n                T_softmax_maxelem[idx] = local_max;\n            }\n        }\n    }\n}\n\n// Function to compute softmax normalization\nvoid default_function_kernel_2(float* T_softmax_expsum, const float* T_softmax_maxelem, float* T_softmax_norm, const float* data, int blocks, int threads_per_block) {\n    #pragma omp parallel for collapse(2)\n    for (int blockIdx_x = 0; blockIdx_x < blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads_per_block; ++threadIdx_x) {\n            int idx = (blockIdx_x * 32) + threadIdx_x;\n            if (((blockIdx_x * 2) + (threadIdx_x >> 4)) < 567) {\n                float maxelem = T_softmax_maxelem[idx];\n                float data_val = data[idx];\n                float diff = data_val - maxelem;\n                float exp_val = expf(diff);\n                T_softmax_norm[idx] = exp_val / T_softmax_expsum[idx / 3];\n            }\n        }\n    }\n}\n\n// Function to compute softmax exponent sum\nvoid default_function_kernel_1(float* T_softmax_expsum, const float* T_softmax_maxelem, const float* data, int blocks, int threads_per_block) {\n    #pragma omp parallel for collapse(2)\n    for (int blockIdx_x = 0; blockIdx_x < blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads_per_block; ++threadIdx_x) {\n            int idx = (blockIdx_x * 4) + threadIdx_x;\n            T_softmax_expsum[idx] = 0.0f;\n            for (int k = 0; k < 3; ++k) {\n                float maxelem = T_softmax_maxelem[idx];\n                float data_val = data[((blockIdx_x * 12) + (threadIdx_x * 3)) + k];\n                float diff = data_val - maxelem;\n                float exp_val = expf(diff);\n                T_softmax_expsum[idx] += exp_val;\n            }\n        }\n    }\n}\n\n// Example usage\nint main() {\n    // Example tensor dimensions\n    int blocks = 1;\n    int threads_per_block = 32;\n    float data[18 * 12 * 14 * 3];\n    float T_softmax_maxelem[18 * 32];\n    float T_softmax_expsum[18 * 4];\n    float T_softmax_norm[18 * 32];\n    \n    // Initialize data\n    // ...\n\n    // Call functions\n    default_function_kernel(T_softmax_maxelem, data, blocks, threads_per_block);\n    default_function_kernel_1(T_softmax_expsum, T_softmax_maxelem, data, blocks, threads_per_block);\n    default_function_kernel_2(T_softmax_expsum, T_softmax_maxelem, T_softmax_norm, data, blocks, threads_per_block);\n\n    return 0;\n}\n",
        "gpt2_c": "#include <stdio.h>\n#include <math.h>\n#include <float.h> // For FLT_MIN\n\nvoid default_function_kernel(float* data, float* T_softmax_maxelem) {\n    const int num_blocks = 18; // Number of blocks\n    const int threads_per_block = 32; // Number of threads per block\n    const int num_elements = 18 * 12 * 14 * 3; // Total number of elements in the tensor\n    \n    // Initialize T_softmax_maxelem\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < threads_per_block; ++threadIdx) {\n            int idx = (blockIdx * threads_per_block) + threadIdx;\n            if (((blockIdx * 2) + (threadIdx >> 4)) < 189) {\n                T_softmax_maxelem[idx] = -FLT_MAX; // Use FLT_MAX to represent the minimum value\n            }\n        }\n    }\n\n    // Compute the maximum element\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < threads_per_block; ++threadIdx) {\n            int idx = (blockIdx * threads_per_block) + threadIdx;\n            if (((blockIdx * 2) + (threadIdx >> 4)) < 189) {\n                for (int k = 0; k < 3; ++k) {\n                    int data_idx = (((blockIdx * 12 * 14) + (threadIdx * 3)) + k);\n                    T_softmax_maxelem[idx] = fmaxf(T_softmax_maxelem[idx], data[data_idx]);\n                }\n            }\n        }\n    }\n}\n\n// Example usage:\n// float data[18 * 12 * 14 * 3];\n// float T_softmax_maxelem[18 * 32];\n// default_function_kernel(data, T_softmax_maxelem);\n\n|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* T_softmax_maxelem, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 189; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 32; ++i3_s) {\n      T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s)] = -3.402823e+38f;\n    }\n    for (int32_t k = 0; k < 3; ++k) {\n      for (int32_t i3_s = 0; i3_s < 32; ++i3_s) {\n        T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s)] = max(T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s)], data[((i0_i1_fused_i2_fused * 96) + (i3_s * 3) + k)]);\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_2(float* T_softmax_expsum, float* T_softmax_maxelem, float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 567; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 32; ++i3_s) {\n      int v_ = ((int)(floorf(((max(min((data[((i0_i1_fused_i2_fused * 32) + i3_s)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_norm[((i0_i1_fused_i2_fused * 32) + i3_s)] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((i0_i1_fused_i2_fused * 32) + i3_s)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 32) + i3_s)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((i0_i1_fused_i2_fused * 32) + i3_s)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 32) + i3_s)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((i0_i1_fused_i2_fused * 32) + i3_s)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 32) + i3_s)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((i0_i1_fused_i2_fused * 32) + i3_s)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 32) + i3_s)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((i0_i1_fused_i2_fused * 32) + i3_s)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 32) + i3_s)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((i0_i1_fused_i2_fused * 32) + i3_s)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 32) + i3_s)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((i0_i1_fused_i2_fused * 32) + i3_s)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 32) + i3_s)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((i0_i1_fused_i2_fused * 32) + i3_s)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 32) + i3_s)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((i0_i1_fused_i2_fused * 32) + i3_s)] - T_softmax_maxelem[((i0_i1_fused_i2_fused * 32) + i3_s) / 3])) / T_softmax_expsum[((i0_i1_fused_i2_fused * 32) + i3_s) / 3]);\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* T_softmax_expsum, float* T_softmax_maxelem, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 42; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 4; ++i3_s) {\n      T_softmax_expsum[((i0_i1_fused_i2_fused * 4) + i3_s)] = 0.000000e+00f;\n      for (int32_t k = 0; k < 3; ++k) {\n        int v_ = ((int)(floorf(((max(min((data[((i0_i1_fused_i2_fused * 12) + (i3_s * 3) + k)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 4) + i3_s)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_expsum[((i0_i1_fused_i2_fused * 4) + i3_s)] = (T_softmax_expsum[((i0_i1_fused_i2_fused * 4) + i3_s)] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((i0_i1_fused_i2_fused * 12) + (i3_s * 3) + k)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 4) + i3_s)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 12) + (i3_s * 3) + k)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 4) + i3_s)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((i0_i1_fused_i2_fused * 12) + (i3_s * 3) + k)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 4) + i3_s)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 12) + (i3_s * 3) + k)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 4) + i3_s)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((i0_i1_fused_i2_fused * 12) + (i3_s * 3) + k)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 4) + i3_s)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 12) + (i3_s * 3) + k)]) - T_softmax_maxelem[((i0_i1_fused_i2_fused * 4) + i3_s)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f)"
    },
    {
        "op_name": "space_to_depth",
        "c_code": "void default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 648; ++i0_i1_fused_i2_fused_i3_fused) {\n    space_to_depth[i0_i1_fused_i2_fused_i3_fused] = data[((((((i0_i1_fused_i2_fused_i3_fused / 36) * 36) + ((((i0_i1_fused_i2_fused_i3_fused % 36) / 3) % 3) * 12)) + ((((i0_i1_fused_i2_fused_i3_fused % 36) / 3) / 6) * 6)) + ((i0_i1_fused_i2_fused_i3_fused % 3) * 2)) + ((((i0_i1_fused_i2_fused_i3_fused % 36) / 3) % 6) / 3))];\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = data[((((((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 2)) / 9) * 36) + ((((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 36) / 3) % 3) * 12)) + ((((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 36) / 3) / 6) * 6)) + ((((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 3) * 2)) + ((((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 36) / 3) % 6) / 3))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 3, 2, 6), \"float32\"), space_to_depth: T.Buffer((18, 12, 1, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(648):\n            cse_var_1: T.int32 = i0_i1_fused_i2_fused_i3_fused % 36 // 3\n            space_to_depth_1 = T.Buffer((648,), data=space_to_depth.data)\n            data_1 = T.Buffer((648,), data=data.data)\n            space_to_depth_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused // 36 * 36 + T.truncmod(cse_var_1, 3) * 12 + T.Div(cse_var_1, 6) * 6 + i0_i1_fused_i2_fused_i3_fused % 3 * 2 + T.Div(T.truncmod(cse_var_1, 6), 3)]",
        "op_args": [
            18,
            3,
            1,
            3
        ],
        "input_shape": "[[18, 3, 2, 6]]",
        "output_shape": "[[18, 12, 1, 3]]",
        "deepseek_c": "void default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        for (int32_t i3 = 0; i3 < 2; ++i3) {\n          space_to_depth[((((i0 * 36) + (i1 * 12)) + (i2 * 2)) + i3)] = data[((((((i0 / 9) * 36) + ((i1 % 3) * 12)) + ((i2 % 3) * 6)) + (i3 % 2)) + (((((i0 / 9) * 3) + (i1 / 3)) % 3) * 2))];\n        }\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 216; ++i0_i1_fused_i2_fused_i3_fused) {\n    space_to_depth[i0_i1_fused_i2_fused_i3_fused] = data[((((((i0_i1_fused_i2_fused_i3_fused / 36) * 36) + (((i0_i1_fused_i2_fused_i3_fused % 36) / 3) % 3) * 12)) + (((i0_i1_fused_i2_fused_i3_fused % 36) / 3) / 6) * 6) + (((i0_i1_fused_i2_fused_i3_fused % 36) / 3) % 6)) + (i0_i1_fused_i2_fused_i3_fused % 3))];\n  }\n}",
        "codellama_c": "void default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 126; ++i0_i1_fused_i2_fused_i3_fused) {\n    space_to_depth[i0_i1_fused_i2_fused_i3_fused] = data[((((i0_i1_fused_i2_fused_i3_fused / 36) * 36) + ((i0_i1_fused_i2_fused_i3_fused % 36) / 3) * 6)) + ((i0_i1_fused_i2_fused_i3_fused % 36) % 3) * 2)];\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int i = 0; i < 18; i++) {\n    for (int j = 0; j < 8; j++) {\n      space_to_depth[((i * 8) + j)] = data[((((((i * 2) + (j >> 2)) / 9) * 36) + ((((((((i * 8) + j) % 36) / 3) % 3) * 12)) + ((((((((i * 8) + j) % 36) / 3) / 6) * 6)) + (((((i * 2) + j) % 3) * 2)) + ((((((((i * 8) + j) % 36) / 3) % 6) / 3))))));\n    }\n  }\n}\n//End-of-Code|",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int i = 0; i < 18; i++) {\n    for (int j = 0; j < 8; j++) {\n      space_to_depth[(i * 8) + j] = data[(((((((((i * 2) + (j >> 2)) / 9) * 36) + ((((((((i * 8) + j) % 36) / 3) % 3) * 12)) + ((((((((i * 8) + j) % 36) / 3) / 6) * 6)) + (((i * 2) + j) % 3) * 2)) + ((((((((i * 8) + j) % 36) / 3) % 6) / 3))];\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* data, float* space_to_depth) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 18 * 3; // Based on input tensor shape\n    int blockDim_x = 8;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int calc_index = (((((blockIdx_x * 2) + (threadIdx_x >> 2)) / 9) * 36) +\n                              (((((blockIdx_x * 8) + threadIdx_x) % 36) / 3) % 3) * 12) +\n                             ((((blockIdx_x * 8) + threadIdx_x) % 36) / 3 / 6) * 6) +\n                            (((blockIdx_x * 2) + threadIdx_x) % 3) * 2 +\n                            (((((blockIdx_x * 8) + threadIdx_x) % 36) / 3) % 6) / 3;\n            space_to_depth[idx] = data[calc_index];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* data, float* space_to_depth) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 18 * 3 * 2; // Based on input tensor shape (18 * 3 * 2)\n    int blockDim_x = 8;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int calc_index = (((((blockIdx_x * 2) + (threadIdx_x >> 2)) / 9) * 36) +\n                              (((((blockIdx_x * 8 + threadIdx_x) % 36) / 3) % 3) * 12) +\n                              (((((blockIdx_x * 8 + threadIdx_x) % 36) / 3) / 6) * 6) +\n                              (((blockIdx_x * 2 + threadIdx_x) % 3) * 2) +\n                              (((blockIdx_x * 8 + threadIdx_x) % 36 / 3 % 6) / 3));\n            space_to_depth[idx] = data[calc_index];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 108; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n      space_to_depth[((i0_i1_fused_i2_fused * 8) + i3_s)] = data[((((((i0_i1_fused_i2_fused * 2) + (i3_s >> 2)) / 9) * 36) + (((((i0_i1_fused_i2_fused * 8) + i3_s) % 36) / 3) % 3) * 12) + (((((i0_i1_fused_i2_fused * 8) + i3_s) % 36) / 3) / 6) * 6) + (((((i0_i1_fused_i2_fused * 2) + i3_s) % 3) * 2) + (((((i0_i1_fused_i2_fused * 8) + i3_s) % 36) / 3) % 6) / 3))];\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "strided_slice",
        "c_code": "void default_function_kernel(float* T_strided_slice, float* a) {\n  for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      T_strided_slice[((ax1 * 7) + ax2)] = a[(((ax1 * 15) + ax2) + 183)];\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  T_strided_slice[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = a[(((((int)blockIdx.x) * 15) + ((int)threadIdx.x)) + 183)];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((2, 10, 15), \"float32\"), T_strided_slice: T.Buffer((1, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax1, ax2 in T.grid(5, 7):\n            T_strided_slice_1 = T.Buffer((35,), data=T_strided_slice.data)\n            a_1 = T.Buffer((300,), data=a.data)\n            T_strided_slice_1[ax1 * 7 + ax2] = a_1[ax1 * 15 + ax2 + 183]",
        "op_args": [
            1,
            2,
            10,
            15
        ],
        "input_shape": "[[2, 10, 15]]",
        "output_shape": "[[1, 5, 7]]",
        "deepseek_c": "void default_function_kernel(float* T_strided_slice, float* a) {\n  for (int i0 = 0; i0 < 2; ++i0) {\n    for (int i1 = 0; i1 < 10; ++i1) {\n      for (int i2 = 0; i2 < 7; ++i2) {\n        T_strided_slice[((((i0 * 105) + (i1 * 7)) + i2) + 183)] = a[((((i0 * 105) + (i1 * 15)) + i2) + 183)];\n      }\n    }\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 20; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      T_strided_slice[((ax0_ax1_fused * 7) + ax2)] = a[(((((ax0_ax1_fused / 10) * 15) + (ax2 * 2)) + 183) + (ax0_ax1_fused % 10))];\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 300; ++ax0_ax1_fused_ax2_fused) {\n    T_strided_slice[ax0_ax1_fused_ax2_fused] = a[((ax0_ax1_fused_ax2_fused % 15) + 183)];\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int i = 0; i < 2; i++) {\n    for (int j = 0; j < 7; j++) {\n      T_strided_slice[(i * 7) + j] = a[((i * 15) + j) + 183];\n    }\n  }\n}\n//End-of-Code|",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int i = 0; i < 2; i++) {\n    for (int j = 0; j < 7; j++) {\n      T_strided_slice[(i * 7) + j] = a[((i * 15) + j) + 183];\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* T_strided_slice, float* a) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 2 * 10; // Based on input tensor shape (2 * 10)\n    int blockDim_x = 7;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int src_idx = (blockIdx_x * 15) + threadIdx_x + 183;\n            T_strided_slice[idx] = a[src_idx];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* T_strided_slice, float* a) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 2 * 10; // Based on input tensor shape (2 * 10)\n    int blockDim_x = 7;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int src_idx = (blockIdx_x * 15) + threadIdx_x + 183;\n            T_strided_slice[idx] = a[src_idx];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 20; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 7; ++i3_s) {\n      T_strided_slice[((i0_i1_fused_i2_fused * 7) + i3_s)] = a[((i0_i1_fused_i2_fused * 15) + i3_s + 183)];\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "unpack_NCHWc_to_nchw",
        "c_code": "void default_function_kernel(float* output_unpack, float* packed_out) {\n  #pragma omp parallel for\n  for (int32_t n_c_fused_h_fused = 0; n_c_fused_h_fused < 504; ++n_c_fused_h_fused) {\n    for (int32_t w = 0; w < 8; ++w) {\n      output_unpack[((n_c_fused_h_fused * 8) + w)] = packed_out[(((((n_c_fused_h_fused / 24) * 192) + ((n_c_fused_h_fused % 12) * 16)) + (w * 2)) + ((n_c_fused_h_fused % 24) / 12))];\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out) {\n  output_unpack[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = packed_out[(((((((int)blockIdx.x) / 3) * 192) + ((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) % 12) * 16)) + ((((int)threadIdx.x) & 7) * 2)) + ((((((int)blockIdx.x) % 3) * 2) + (((int)threadIdx.x) >> 5)) / 3))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(packed_out: T.Buffer((3, 7, 12, 8, 2), \"float32\"), output_unpack: T.Buffer((3, 14, 12, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for n_c_fused_h_fused in T.parallel(504):\n            for w in range(8):\n                output_unpack_1 = T.Buffer((4032,), data=output_unpack.data)\n                packed_out_1 = T.Buffer((4032,), data=packed_out.data)\n                output_unpack_1[n_c_fused_h_fused * 8 + w] = packed_out_1[n_c_fused_h_fused // 24 * 192 + n_c_fused_h_fused % 12 * 16 + w * 2 + n_c_fused_h_fused % 24 // 12]",
        "op_args": [
            3,
            7,
            12,
            8
        ],
        "input_shape": "[[3, 7, 12, 8, 2]]",
        "output_shape": "[[3, 14, 12, 8]]",
        "deepseek_c": "void default_function_kernel(float* output_unpack, float* packed_out) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused_i4_fused = 0; i0_i1_fused_i2_fused_i3_fused_i4_fused < 12096; ++i0_i1_fused_i2_fused_i3_fused_i4_fused) {\n    output_unpack[i0_i1_fused_i2_fused_i3_fused_i4_fused] = packed_out[((((((i0_i1_fused_i2_fused_i3_fused_i4_fused / 192) * 192) + (((i0_i1_fused_i2_fused_i3_fused_i4_fused % 192) / 16) * 16)) + (((i0_i1_fused_i2_fused_i3_fused_i4_fused % 16) * 2) + (((i0_i1_fused_i2_fused_i3_fused_i4_fused % 3) * 2) + ((i0_i1_fused_i2_fused_i3_fused_i4_fused / 12) / 3))))];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* output_unpack, float* packed_out) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 252; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      for (int32_t ax4 = 0; ax4 < 2; ++ax4) {\n        output_unpack[(((ax0_ax1_fused_ax2_fused * 16) + (ax3 * 2)) + ax4)] = packed_out[((((((ax0_ax1_fused_ax2_fused / 12) * 192) + ((ax0_ax1_fused_ax2_fused % 12) * 16)) + (ax3 * 2)) + ax4) + (((ax0_ax1_fused_ax2_fused % 12) / 6) * 128))];\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* output_unpack, float* packed_out) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused_i4_fused = 0; i0_i1_fused_i2_fused_i3_fused_i4_fused < 252; ++i0_i1_fused_i2_fused_i3_fused_i4_fused) {\n    output_unpack[i0_i1_fused_i2_fused_i3_fused_i4_fused] = packed_out[((((i0_i1_fused_i2_fused_i3_fused_i4_fused % 192) / 16) * 192) + ((i0_i1_fused_i2_fused_i3_fused_i4_fused % 16) * 2)) + (i0_i1_fused_i2_fused_i3_fused_i4_fused / 192)];\n  }\n}",
        "gemini1_c": "#include <omp.h>\n\nvoid default_function_kernel(float* output_unpack, float* packed_out) {\n  #pragma omp parallel for\n  for (int i = 0; i < 24; i++) {\n    for (int j = 0; j < 64; j++) {\n      output_unpack[(i * 64) + j] = packed_out[(((((i / 3) * 192) + (((((i * 8) + (j >> 3)) % 12) * 16)) + (((j & 7) * 2)) + (((((i % 3) * 2) + (j >> 5)) / 3))))];\n    }\n  }\n}\n//End-of-Code|",
        "gemini2_c": "#include <omp.h>\n\nvoid default_function_kernel(float* output_unpack, float* packed_out) {\n  #pragma omp parallel for\n  for (int i = 0; i < 3; i++) {\n    for (int j = 0; j < 64; j++) {\n      output_unpack[(i * 64) + j] = packed_out[(((((i / 3) * 192) + (((((i * 8) + (j >> 3)) % 12) * 16)) + (((j & 7) * 2)) + (((i % 3) * 2) + (j >> 5)) / 3))];\n    }\n  }\n}\n//End-of-Code|",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* output_unpack, float* packed_out) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 3 * 7 * 12; // Based on input tensor shape\n    int blockDim_x = 64;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int src_idx = (((((blockIdx_x / 3) * 192) +\n                             (((blockIdx_x * 8 + (threadIdx_x >> 3)) % 12) * 16)) +\n                            ((threadIdx_x & 7) * 2)) +\n                           (((blockIdx_x % 3) * 2 + (threadIdx_x >> 5)) / 3));\n            output_unpack[idx] = packed_out[src_idx];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* output_unpack, float* packed_out) {\n    int blockIdx_x, threadIdx_x;\n    int gridDim_x = 3 * 7 * 12; // Based on input tensor shape (3 * 7 * 12)\n    int blockDim_x = 64;\n\n    #pragma omp parallel for private(blockIdx_x, threadIdx_x)\n    for (blockIdx_x = 0; blockIdx_x < gridDim_x; ++blockIdx_x) {\n        for (threadIdx_x = 0; threadIdx_x < blockDim_x; ++threadIdx_x) {\n            int idx = (blockIdx_x * blockDim_x) + threadIdx_x;\n            int src_idx = (((((blockIdx_x / 3) * 192) +\n                             ((((blockIdx_x * 8) + (threadIdx_x >> 3)) % 12) * 16)) +\n                            ((threadIdx_x & 7) * 2)) +\n                           ((((blockIdx_x % 3) * 2) + (threadIdx_x >> 5)) / 3));\n            output_unpack[idx] = packed_out[src_idx];\n        }\n    }\n}\n\n//|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* output_unpack, float* packed_out) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 168; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 64; ++i3_s) {\n      output_unpack[((i0_i1_fused_i2_fused * 64) + i3_s)] = packed_out[((((((i0_i1_fused_i2_fused / 3) * 192) + ((((((i0_i1_fused_i2_fused * 8) + (i3_s >> 3)) % 12) * 16)) + (((i3_s & 7) * 2)) + (((((i0_i1_fused_i2_fused % 3) * 2) + (i3_s >> 5)) / 3))];\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "upsampling",
        "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i2 = 0; i2 < 40; ++i2) {\n      for (int32_t i3 = 0; i3 < 36; ++i3) {\n        resize[(((i0 * 1440) + (i2 * 36)) + i3)] = data[(((i0 * 360) + ((i2 / 2) * 18)) + (i3 / 2))];\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) / 40) * 360) + (((((int)blockIdx.x) % 40) / 2) * 18)) + (((int)threadIdx.x) / 2))];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 1, 20, 18), \"float32\"), resize: T.Buffer((11, 1, 40, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(11):\n            for i2, i3 in T.grid(40, 36):\n                resize_1 = T.Buffer((15840,), data=resize.data)\n                data_1 = T.Buffer((3960,), data=data.data)\n                resize_1[i0 * 1440 + i2 * 36 + i3] = data_1[i0 * 360 + T.Div(i2, 2) * 18 + T.Div(i3, 2)]",
        "op_args": [
            11,
            1,
            10,
            9
        ],
        "input_shape": "[[11, 1, 20, 18]]",
        "output_shape": "[[11, 1, 40, 36]]",
        "deepseek_c": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 7920; ++i0_i1_fused_i2_fused_i3_fused) {\n    resize[i0_i1_fused_i2_fused_i3_fused] = data[((((i0_i1_fused_i2_fused_i3_fused / 36) / 2) * 360) + ((i0_i1_fused_i2_fused_i3_fused % 36) / 2))];\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 7920; ++i0_i1_fused_i2_fused_i3_fused) {\n    resize[i0_i1_fused_i2_fused_i3_fused] = data[((((i0_i1_fused_i2_fused_i3_fused / 360) * 360) + (((i0_i1_fused_i2_fused_i3_fused % 360) / 18) * 18)) + ((i0_i1_fused_i2_fused_i3_fused % 18) / 2))];\n  }\n}",
        "codellama_c": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 7200; ++i0_i1_fused_i2_fused_i3_fused) {\n    resize[i0_i1_fused_i2_fused_i3_fused] = data[((((i0_i1_fused_i2_fused_i3_fused / 360) * 360) + (((i0_i1_fused_i2_fused_i3_fused % 360) / 2) * 18)) + ((i0_i1_fused_i2_fused_i3_fused % 360) / 2))];\n  }\n}",
        "gemini1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\nvoid default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int i = 0; i < 11 * 20 * 18; i++) {\n    int blockIdx_x = i / 36;\n    int threadIdx_x = i % 36;\n    resize[i] = data[((blockIdx_x / 40) * 360) + (((blockIdx_x % 40) / 2) * 18) + (threadIdx_x / 2)];\n  }\n}\n",
        "gemini2_c": "void default_function_kernel(float* data, float* resize) {\n  int blockIdx_x = 0;\n  for (; blockIdx_x < 11; blockIdx_x++) {\n    int threadIdx_x = 0;\n    for (; threadIdx_x < 36; threadIdx_x++) {\n      resize[((blockIdx_x * 36) + threadIdx_x)] = data[(((((blockIdx_x / 40) * 360) + (((blockIdx_x % 40) / 2) * 18)) + (threadIdx_x / 2)))];\n    }\n  }\n}\n",
        "gpt1_c": "#include <omp.h>\n#include <stdio.h>\n\nvoid default_function_kernel(float* data, float* resize, int blocks, int threads) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads; ++threadIdx_x) {\n            int idx = blockIdx_x * threads + threadIdx_x;\n            int data_idx = ((blockIdx_x / 40) * 360) + (((blockIdx_x % 40) / 2) * 18) + (threadIdx_x / 2);\n            resize[idx] = data[data_idx];\n        }\n    }\n}\n\nint main() {\n    int blocks = 20;            // As per the tensor shape provided\n    int threads = 36;           // Launch bounds\n\n    int total_elements = blocks * threads;\n    float *data = (float *)malloc(11 * 1 * 20 * 18 * sizeof(float)); // Adjust size based on tensor shape\n    float *resize = (float *)malloc(total_elements * sizeof(float));\n\n    // Initialize data (example initialization)\n    for (int i = 0; i < 11 * 1 * 20 * 18; ++i) {\n        data[i] = (float)i;\n    }\n\n    // Call the function\n    default_function_kernel(data, resize, blocks, threads);\n\n    // Print the result\n    for (int i = 0; i < total_elements; ++i) {\n        printf(\"%f \", resize[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(resize);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <omp.h>\n#include <stdio.h>\n#include <stdlib.h>\n\nvoid default_function_kernel(float* data, float* resize, int num_blocks, int threads_per_block) {\n    #pragma omp parallel for\n    for (int blockIdx_x = 0; blockIdx_x < num_blocks; ++blockIdx_x) {\n        for (int threadIdx_x = 0; threadIdx_x < threads_per_block; ++threadIdx_x) {\n            int idx = blockIdx_x * threads_per_block + threadIdx_x;\n            int data_idx = ((blockIdx_x / 40) * 360) + ((blockIdx_x % 40) / 2) * 18 + (threadIdx_x / 2);\n            resize[idx] = data[data_idx];\n        }\n    }\n}\n\nint main() {\n    int blocks = 11 * 1 * 20 * 18; // Calculating total blocks based on the given tensor shape\n    int threads_per_block = 36;    // Launch bounds\n\n    int total_elements = blocks * threads_per_block;\n    float *data = (float *)malloc(total_elements * sizeof(float));\n    float *resize = (float *)malloc(total_elements * sizeof(float));\n\n    // Initialize data (example initialization)\n    for (int i = 0; i < total_elements; ++i) {\n        data[i] = (float)i;\n    }\n\n    // Call the function\n    default_function_kernel(data, resize, blocks, threads_per_block);\n\n    // Print the result\n    for (int i = 0; i < total_elements; ++i) {\n        printf(\"%f \", resize[i]);\n    }\n    printf(\"\\n\");\n\n    free(data);\n    free(resize);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3960; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 36; ++i3_s) {\n      resize[((i0_i1_fused_i2_fused * 36) + i3_s)] = data[(((((i0_i1_fused_i2_fused / 40) * 360) + (((i0_i1_fused_i2_fused % 40) / 2) * 18)) + (i3_s / 2)))];\n    }\n  }\n}\n//|End-of-Code|"
    },
    {
        "op_name": "rms_norm",
        "c_code": "void default_function_kernel(float* T_cast, float* data, float* weight) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    float T_multiply_red[168];\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n        T_multiply_red[((ax1 * 14) + ax2)] = 0.000000e+00f;\n        for (int32_t k1 = 0; k1 < 10; ++k1) {\n          T_multiply_red[((ax1 * 14) + ax2)] = (T_multiply_red[((ax1 * 14) + ax2)] + (data[((((ax0 * 1680) + (k1 * 168)) + (ax1 * 14)) + ax2)] * data[((((ax0 * 1680) + (k1 * 168)) + (ax1 * 14)) + ax2)]));\n        }\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 10; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 12; ++ax2_1) {\n        for (int32_t ax3_s = 0; ax3_s < 14; ++ax3_s) {\n          T_cast[((((ax0 * 1680) + (ax1_1 * 168)) + (ax2_1 * 14)) + ax3_s)] = ((data[((((ax0 * 1680) + (ax1_1 * 168)) + (ax2_1 * 14)) + ax3_s)] * weight[ax1_1]) * (1.000000e+00f / sqrtf(((T_multiply_red[((ax2_1 * 14) + ax3_s)] * 1.000000e-01f) + 1.000000e-05f))));\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(35) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] * weight[((((((int)blockIdx.x) % 48) * 5) + (((int)threadIdx.x) / 7)) / 24)]) * (1.000000e+00f / sqrtf(((T_multiply_red[(((((int)blockIdx.x) / 48) * 168) + (((((int)blockIdx.x) * 35) + ((int)threadIdx.x)) % 168))] * 1.000000e-01f) + 1.000000e-05f))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 10; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + (data[((((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 21) * 1680) + (k1 * 168)) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 168))] * data[((((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 21) * 1680) + (k1 * 168)) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 168))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 10, 12, 14), \"float32\"), weight: T.Buffer((14,), \"float32\"), T_cast: T.Buffer((8, 10, 12, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(8):\n            T_multiply_red = T.allocate([168], \"float32\", \"global\")\n            T_multiply_red_1 = T.Buffer((168,), data=T_multiply_red)\n            data_1 = T.Buffer((13440,), data=data.data)\n            for ax1, ax2 in T.grid(12, 14):\n                T_multiply_red_1[ax1 * 14 + ax2] = T.float32(0)\n                for k1 in range(10):\n                    cse_var_3: T.int32 = ax1 * 14\n                    cse_var_2: T.int32 = cse_var_3 + ax2\n                    cse_var_1: T.int32 = ax0 * 1680 + k1 * 168 + cse_var_3 + ax2\n                    T_multiply_red_1[cse_var_2] = T_multiply_red_1[cse_var_2] + data_1[cse_var_1] * data_1[cse_var_1]\n            for ax1, ax2, ax3_s in T.grid(10, 12, 14):\n                cse_var_5: T.int32 = ax2 * 14\n                cse_var_4: T.int32 = ax0 * 1680 + ax1 * 168 + cse_var_5 + ax3_s\n                T_cast_1 = T.Buffer((13440,), data=T_cast.data)\n                T_cast_1[cse_var_4] = data_1[cse_var_4] * weight[ax1] * T.rsqrt(T_multiply_red_1[cse_var_5 + ax3_s] * T.float32(0.10000000000000001) + T.float32(1.0000000000000001e-05))",
        "op_args": [
            8,
            10,
            12,
            14
        ],
        "input_shape": "[[8, 10, 12, 14], [14]]",
        "output_shape": "[[8, 10, 12, 14]]",
        "deepseek_c": "void default_function_kernel(float* T_cast, float* T_multiply_red, float* data, float* weight) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 13440; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_multiply_red[ax0_ax1_fused_ax2_fused_ax3_fused] = 0.000000e+00f;\n    for (int32_t k = 0; k < 10; ++k) {\n      T_multiply_red[ax0_ax1_fused_ax2_fused_ax3_fused] = (T_multiply_red[ax0_ax1_fused_ax2_fused_ax3_fused] + (data[((((ax0_ax1_fused_ax2_fused / 168) * 1680) + (k * 168)) + (ax0_ax1_fused_ax2_fused % 168))] * data[((((((ax0_ax1_fused_ax2_fused / 168) * 1680) + (k * 168)) + (ax0_ax1_fused_ax2_fused % 168)) + (((ax0_ax1_fused_ax2_fused / 168) * 1680) + (ax0_ax1_fused_ax2_fused % 168)))]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 13440; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_cast[ax0_ax1_fused_ax2_fused_ax3_fused] = ((data[((((ax0_ax1_fused_ax2_fused / 168) * 1680) + (ax0_ax1_fused_ax2_fused % 168))] * weight[((ax0_ax1_fused_ax2_fused % 168) / 7)]) * (1.000000e+00f / sqrtf(((T_multiply_red[((ax0_ax1_fused_ax2_fused / 168) * 168)] * 1.000000e-01f) + 1.000000e-05f))));\n  }\n}",
        "starcoder_c": "void default_function_kernel(float* T_cast, float* data, float* weight) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 80; ++ax0_ax1_fused) {\n    float T_multiply_red[1];\n    for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 14; ++ax3) {\n        T_multiply_red[0] = 0.000000e+00f;\n        for (int32_t k1 = 0; k1 < 10; ++k1) {\n          T_multiply_red[0] = (T_multiply_red[0] + (data[(((ax0_ax1_fused * 1680) + (k1 * 168)) + (ax2 * 14)) + ax3] * data[(((ax0_ax1_fused * 1680) + (k1 * 168)) + (ax2 * 14)) + ax3]));\n        }\n        T_cast[((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3] = ((data[((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3] * weight[ax3]) * (1.000000e+00f / sqrtf(((T_multiply_red[0] * 1.000000e-01f) + 1.000000e-05f))));\n      }\n    }\n  }\n}",
        "codellama_c": "void default_function_kernel(float* T_cast, float* data, float* weight) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 960; ++ax0_ax1_fused_ax2_fused) {\n    float T_multiply_red[1];\n    for (int32_t ax3 = 0; ax3 < 14; ++ax3) {\n      T_multiply_red[0] = 0.000000e+00f;\n      for (int32_t k1 = 0; k1 < 10; ++k1) {\n        T_multiply_red[0] = (T_multiply_red[0] + (data[(((ax0_ax1_fused_ax2_fused * 168) + (k1 * 168)) + ax3)] * data[(((ax0_ax1_fused_ax2_fused * 168) + (k1 * 168)) + ax3)]));\n      }\n      T_cast[((ax0_ax1_fused_ax2_fused * 14) + ax3)] = ((data[((ax0_ax1_fused_ax2_fused * 14) + ax3)] * weight[((ax0_ax1_fused_ax2_fused % 24) / 12)]) * (1.000000e+00f / sqrtf(((T_multiply_red[0] * 1.000000e-01f) + 1.000000e-05f))));\n    }\n  }\n}",
        "gemini1_c": "#include <omp.h>\n#include <math.h> // For sqrtf\n\nvoid default_function_kernel_1(float* T_cast, float* T_multiply_red, float* data, float* weight) {\n  // Calculate the number of threads needed\n  int num_threads = 8 * 35;\n\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i = 0; i < num_threads; ++i) {\n    // Calculate blockIdx.x and threadIdx.x from the global thread index i\n    int blockIdx_x = i / 35;\n    int threadIdx_x = i % 35;\n\n    // Calculate the index for weight based on the conditions\n    int weight_index = (((blockIdx_x % 48) * 5 + (threadIdx_x / 7)) / 24);\n\n    // Calculate the index for T_multiply_red based on the conditions\n    int T_multiply_red_index = (((blockIdx_x / 48) * 168) + (((blockIdx_x * 35) + threadIdx_x) % 168));\n\n    T_cast[blockIdx_x * 35 + threadIdx_x] = (data[blockIdx_x * 35 + threadIdx_x] * weight[weight_index]) * (1.0f / sqrtf((T_multiply_red[T_multiply_red_index] * 1.0e-01f) + 1.0e-05f));\n  }\n}\n\nvoid default_function_kernel(float* T_multiply_red, float* data) {\n  // Calculate the number of threads needed\n  int num_threads = 8 * 32;\n\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i = 0; i < num_threads; ++i) {\n    // Calculate blockIdx.x and threadIdx.x from the global thread index i\n    int blockIdx_x = i / 32;\n    int threadIdx_x = i % 32;\n\n    T_multiply_red[blockIdx_x * 32 + threadIdx_x] = 0.0f;\n    for (int k1 = 0; k1 < 10; ++k1) {\n      // Calculate the index for data based on the conditions\n      int data_index = (((((blockIdx_x * 4) + (threadIdx_x >> 3)) / 21) * 1680) + (k1 * 168) + (((blockIdx_x * 32) + threadIdx_x) % 168));\n\n      T_multiply_red[blockIdx_x * 32 + threadIdx_x] += data[data_index] * data[data_index];\n    }\n  }\n}\n//|End-of-Code|\n    ",
        "gemini2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel_1(float* T_cast, float* T_multiply_red, float* data, float* weight) {\n  // Input tensor shape: [[8, 10, 12, 14], [14]]\n  // T_cast shape: [1120]\n  // T_multiply_red shape: [1680]\n  // data shape: [1120]\n  // weight shape: [14]\n  for (int blockIdx_x = 0; blockIdx_x < 32; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 35; ++threadIdx_x) {\n      int idx = (blockIdx_x * 35) + threadIdx_x;\n      T_cast[idx] = (data[idx] * weight[((blockIdx_x % 48) * 5 + (threadIdx_x / 7)) / 24]) * (1.0f / sqrtf((T_multiply_red[(blockIdx_x / 48) * 168 + (idx % 168)] * 0.1f) + 1e-05f));\n    }\n  }\n}\n\nvoid default_function_kernel(float* T_multiply_red, float* data) {\n  // Input tensor shape: [[8, 10, 12, 14], [14]]\n  // T_multiply_red shape: [1680]\n  // data shape: [1120]\n  for (int blockIdx_x = 0; blockIdx_x < 42; ++blockIdx_x) {\n    for (int threadIdx_x = 0; threadIdx_x < 32; ++threadIdx_x) {\n      int idx = (blockIdx_x * 32) + threadIdx_x;\n      T_multiply_red[idx] = 0.0f;\n      for (int k1 = 0; k1 < 10; ++k1) {\n        T_multiply_red[idx] += data[((blockIdx_x * 4 + (threadIdx_x >> 3)) / 21 * 1680 + k1 * 168 + (idx % 168))] * data[((blockIdx_x * 4 + (threadIdx_x >> 3)) / 21 * 1680 + k1 * 168 + (idx % 168))];\n      }\n    }\n  }\n}\n",
        "gpt1_c": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nvoid default_function_kernel_1(float* restrict T_cast, float* restrict T_multiply_red, float* restrict data, float* restrict weight, int num_blocks, int block_size) {\n    int idx;\n    \n    #pragma omp parallel for private(idx)\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < block_size; ++threadIdx) {\n            idx = (blockIdx * block_size) + threadIdx;\n            if (idx < num_blocks * block_size) {\n                int weight_index = (((blockIdx % 48) * 5) + (threadIdx / 7)) / 24;\n                float scale = 1.0f / sqrtf((T_multiply_red[(((blockIdx / 48) * 168) + (idx % 168))] * 0.1f) + 1e-5f);\n                T_cast[idx] = (data[idx] * weight[weight_index]) * scale;\n            }\n        }\n    }\n}\n\nvoid default_function_kernel(float* restrict T_multiply_red, float* restrict data, int num_blocks, int block_size) {\n    int idx;\n\n    #pragma omp parallel for private(idx)\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < block_size; ++threadIdx) {\n            idx = (blockIdx * block_size) + threadIdx;\n            if (idx < num_blocks * block_size) {\n                T_multiply_red[idx] = 0.0f;\n            }\n        }\n    }\n\n    #pragma omp parallel for private(idx)\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < block_size; ++threadIdx) {\n            idx = (blockIdx * block_size) + threadIdx;\n            if (idx < num_blocks * block_size) {\n                float sum = 0.0f;\n                for (int k1 = 0; k1 < 10; ++k1) {\n                    int data_index = (((blockIdx * 4) + (threadIdx >> 3)) / 21) * 1680 + (k1 * 168) + (idx % 168);\n                    float value = data[data_index];\n                    sum += value * value;\n                }\n                T_multiply_red[idx] = sum;\n            }\n        }\n    }\n}\n\nint main() {\n    int num_blocks_1 = 8, block_size_1 = 35;\n    int num_blocks_2 = 8, block_size_2 = 32;\n\n    // Allocate memory for tensors\n    int total_size_1 = num_blocks_1 * block_size_1;\n    int total_size_2 = num_blocks_2 * block_size_2;\n    float* data = (float*)malloc(total_size_1 * sizeof(float));\n    float* weight = (float*)malloc(14 * sizeof(float));\n    float* T_cast = (float*)malloc(total_size_1 * sizeof(float));\n    float* T_multiply_red = (float*)malloc(total_size_2 * sizeof(float));\n\n    // Initialize data and weight with some values\n    for (int i = 0; i < total_size_1; ++i) {\n        data[i] = (rand() % 100) / 10.0f;\n    }\n    for (int i = 0; i < 14; ++i) {\n        weight[i] = (rand() % 100) / 10.0f;\n    }\n\n    // Call the functions\n    default_function_kernel_1(T_cast, T_multiply_red, data, weight, num_blocks_1, block_size_1);\n    default_function_kernel(T_multiply_red, data, num_blocks_2, block_size_2);\n\n    // Print results (optional)\n    printf(\"T_cast[0]: %f\\n\", T_cast[0]);\n    printf(\"T_multiply_red[0]: %f\\n\", T_multiply_red[0]);\n\n    // Free allocated memory\n    free(data);\n    free(weight);\n    free(T_cast);\n    free(T_multiply_red);\n\n    return 0;\n}\n\n// |End-of-Code|\n",
        "gpt2_c": "#include <stdio.h>\n#include <math.h>\n\nvoid default_function_kernel(float* data, float* T_multiply_red) {\n    // Calculate the dimensions\n    const int num_blocks = 8;  // Assuming 8 blocks as per the shape\n    const int threads_per_block = 32;  // Threads per block as per the kernel\n    const int data_size = 8 * 10 * 12 * 14; // Size of data tensor\n    \n    // Initialize T_multiply_red array\n    for (int i = 0; i < num_blocks * threads_per_block; ++i) {\n        T_multiply_red[i] = 0.0f;\n    }\n    \n    // Perform the reduction operation\n    for (int blockIdx = 0; blockIdx < num_blocks; ++blockIdx) {\n        for (int threadIdx = 0; threadIdx < threads_per_block; ++threadIdx) {\n            int idx = (blockIdx * threads_per_block) + threadIdx;\n            for (int k1 = 0; k1 < 10; ++k1) {\n                int base_index = ((blockIdx * 4) + (threadIdx >> 3)) / 21 * 1680 + k1 * 168;\n                int data_idx = base_index + (idx % 168);\n                T_multiply_red[idx] += data[data_idx] * data[data_idx];\n            }\n        }\n    }\n}\n\n// Example usage:\n// float data[8 * 10 * 12 * 14];\n// float T_multiply_red[8 * 32];\n// default_function_kernel(data, T_multiply_red);\n\n|End-of-Code|\n",
        "gemini3_c": "void default_function_kernel(float* T_multiply_red, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 840; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 32; ++i3_s) {\n      T_multiply_red[((i0_i1_fused_i2_fused * 32) + i3_s)] = 0.000000e+00f;\n      for (int32_t k1 = 0; k1 < 10; ++k1) {\n        T_multiply_red[((i0_i1_fused_i2_fused * 32) + i3_s)] = (T_multiply_red[((i0_i1_fused_i2_fused * 32) + i3_s)] + (data[((((i0_i1_fused_i2_fused * 1680) + (k1 * 168)) + i3_s))] * data[((((i0_i1_fused_i2_fused * 1680) + (k1 * 168)) + i3_s))]));\n      }\n    }\n  }\n}\n\nvoid default_function_kernel_1(float* T_cast, float* T_multiply_red, float* data, float* weight) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1120; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 35; ++i3_s) {\n      T_cast[((i0_i1_fused_i2_fused * 35) + i3_s)] = ((data[((i0_i1_fused_i2_fused * 35) + i3_s)] * weight[((((i0_i1_fused_i2_fused % 48) * 5) + (i3_s / 7)) / 24)]) * (1.000000e+00f / sqrtf(((T_multiply_red[((((i0_i1_fused_i2_fused / 48) * 168) + (((i0_i1_fused_i2_fused * 35) + i3_s) % 168))] * 1.000000e-01f) + 1.000000e-05f))));\n    }\n  }\n}\n"
    }
]