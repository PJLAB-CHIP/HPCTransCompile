[
    {
      "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.",
      "input": "\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float4* __restrict__ T_softmax_norm, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float4* __restrict__ T_softmax_norm, float* __restrict__ tarray) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  float T_softmax_exp[16];\n  float normal_reduce_temp0_1[1];\n  float red_buf0_1[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k_inner = 0; k_inner < 16; ++k_inner) {\n    normal_reduce_temp0[0] = max(normal_reduce_temp0[0], tarray[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k_inner)]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  for (int i1_inner_outer = 0; i1_inner_outer < 4; ++i1_inner_outer) {\n    float4 __1;\n    float4 __2;\n      float4 v_ = *(float4*)(tarray + (((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + (i1_inner_outer * 4)));\n      float4 v__1 = make_float4(red_buf0[0], red_buf0[0], red_buf0[0], red_buf0[0]);\n      __2.x = (v_.x-v__1.x);\n      __2.y = (v_.y-v__1.y);\n      __2.z = (v_.z-v__1.z);\n      __2.w = (v_.w-v__1.w);\n    __1.x = __expf(__2.x);\n    __1.y = __expf(__2.y);\n    __1.z = __expf(__2.z);\n    __1.w = __expf(__2.w);\n    *(float4*)(T_softmax_exp + (i1_inner_outer * 4)) = __1;\n  }\n  normal_reduce_temp0_1[0] = 0.000000e+00f;\n  for (int k_inner_1 = 0; k_inner_1 < 16; ++k_inner_1) {\n    normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + T_softmax_exp[k_inner_1]);\n  }\n  uint mask_1[1];\n  float t0_1[1];\n  red_buf0_1[0] = normal_reduce_temp0_1[0];\n  mask_1[0] = __activemask();\n  t0_1[0] = __shfl_down_sync(mask_1[0], red_buf0_1[0], 16, 32);\n  red_buf0_1[0] = (red_buf0_1[0] + t0_1[0]);\n  t0_1[0] = __shfl_down_sync(mask_1[0], red_buf0_1[0], 8, 32);\n  red_buf0_1[0] = (red_buf0_1[0] + t0_1[0]);\n  t0_1[0] = __shfl_down_sync(mask_1[0], red_buf0_1[0], 4, 32);\n  red_buf0_1[0] = (red_buf0_1[0] + t0_1[0]);\n  t0_1[0] = __shfl_down_sync(mask_1[0], red_buf0_1[0], 2, 32);\n  red_buf0_1[0] = (red_buf0_1[0] + t0_1[0]);\n  t0_1[0] = __shfl_down_sync(mask_1[0], red_buf0_1[0], 1, 32);\n  red_buf0_1[0] = (red_buf0_1[0] + t0_1[0]);\n  red_buf0_1[0] = __shfl_sync(mask_1[0], red_buf0_1[0], 0, 32);\n  for (int i1_inner_outer_1 = 0; i1_inner_outer_1 < 4; ++i1_inner_outer_1) {\n    float4 __3;\n      float4 v__2 = *(float4*)(T_softmax_exp + (i1_inner_outer_1 * 4));\n      float4 v__3 = make_float4(red_buf0_1[0], red_buf0_1[0], red_buf0_1[0], red_buf0_1[0]);\n      __3.x = (v__2.x/v__3.x);\n      __3.y = (v__2.y/v__3.y);\n      __3.z = (v__2.z/v__3.z);\n      __3.w = (v__2.w/v__3.w);\n    T_softmax_norm[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + i1_inner_outer_1)] = __3;\n  }\n}\n",
      "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  int8_t T_less[2520];\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 10; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 7; ++ax4) {\n            T_less[(((((ax0 * 420) + (ax1 * 210)) + (ax2 * 70)) + (ax3 * 7)) + ax4)] = ((int8_t)(tarray[(((((ax0 * 420) + (ax1 * 210)) + (ax2 * 70)) + (ax3 * 7)) + ax4)] < tarray[(((((ax0 * 420) + (ax1 * 210)) + (ax2 * 70)) + (ax3 * 7)) + ax4)]));\n          }\n        }\n      }\n    }\n  }\n}\n\n",
      "op_name": "qqq"
    }
]