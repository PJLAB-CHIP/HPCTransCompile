[
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(2),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([4], \"float32\", \"global\")\n        for ax0, ax1 in T.grid(2, 2):\n            T_tensordot_1 = T.Buffer((T.int64(4),), data=T_tensordot, align=16)\n            T_tensordot_1[ax0 * 2 + ax1] = tarray[ax0] * tarray[ax1]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  T_tensordot[((int)threadIdx.x)] = (tarray[(((int)threadIdx.x) >> 1)] * tarray[(((int)threadIdx.x) & 1)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[4];\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      T_tensordot[((ax0 * 2) + ax1)] = (tarray[ax0] * tarray[ax1]);\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(9),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([81], \"float32\", \"global\")\n        for ax0, ax1 in T.grid(9, 9):\n            T_tensordot_1 = T.Buffer((T.int64(81),), data=T_tensordot)\n            T_tensordot_1[ax0 * 9 + ax1] = tarray[ax0] * tarray[ax1]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(81) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(81) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  T_tensordot[((int)threadIdx.x)] = (tarray[(((int)threadIdx.x) / 9)] * tarray[(((int)threadIdx.x) % 9)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[81];\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      T_tensordot[((ax0 * 9) + ax1)] = (tarray[ax0] * tarray[ax1]);\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(10),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([100], \"float32\", \"global\")\n        for ax0, ax1 in T.grid(10, 10):\n            T_tensordot_1 = T.Buffer((T.int64(100),), data=T_tensordot)\n            T_tensordot_1[ax0 * 10 + ax1] = tarray[ax0] * tarray[ax1]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(100) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(100) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  T_tensordot[((int)threadIdx.x)] = (tarray[(((int)threadIdx.x) / 10)] * tarray[(((int)threadIdx.x) % 10)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[100];\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      T_tensordot[((ax0 * 10) + ax1)] = (tarray[ax0] * tarray[ax1]);\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([36], \"float32\", \"global\")\n        for ax0, ax1 in T.grid(6, 6):\n            T_tensordot_1 = T.Buffer((T.int64(36),), data=T_tensordot)\n            T_tensordot_1[ax0 * 6 + ax1] = tarray[ax0] * tarray[ax1]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  T_tensordot[((int)threadIdx.x)] = (tarray[(((int)threadIdx.x) / 6)] * tarray[(((int)threadIdx.x) % 6)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[36];\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      T_tensordot[((ax0 * 6) + ax1)] = (tarray[ax0] * tarray[ax1]);\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(4),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([16], \"float32\", \"global\")\n        for ax0, ax1 in T.grid(4, 4):\n            T_tensordot_1 = T.Buffer((T.int64(16),), data=T_tensordot)\n            T_tensordot_1[ax0 * 4 + ax1] = tarray[ax0] * tarray[ax1]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  T_tensordot[((int)threadIdx.x)] = (tarray[(((int)threadIdx.x) >> 2)] * tarray[(((int)threadIdx.x) & 3)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[16];\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 4; ++ax1) {\n      T_tensordot[((ax0 * 4) + ax1)] = (tarray[ax0] * tarray[ax1]);\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(8), T.int64(1)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([64], \"float32\", \"global\")\n        for ax0, ax2 in T.grid(8, 8):\n            T_tensordot_1 = T.Buffer((T.int64(64),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(8),), data=tarray.data)\n            T_tensordot_1[ax0 * 8 + ax2] = tarray_1[ax0] * tarray_1[ax2]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  T_tensordot[((int)threadIdx.x)] = (tarray[(((int)threadIdx.x) >> 3)] * tarray[(((int)threadIdx.x) & 7)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[64];\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_tensordot[((ax0 * 8) + ax2)] = (tarray[ax0] * tarray[ax2]);\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(2), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([324], \"float32\", \"global\")\n        for ax0, ax1, ax2, ax3 in T.grid(2, 9, 2, 9):\n            cse_var_1: T.int32 = ax2 * 9\n            T_tensordot_1 = T.Buffer((T.int64(324),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(18),), data=tarray.data)\n            T_tensordot_1[ax0 * 162 + ax1 * 18 + cse_var_1 + ax3] = tarray_1[ax0 * 9 + ax1] * tarray_1[cse_var_1 + ax3]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  T_tensordot[((int)threadIdx.x)] = (tarray[(((int)threadIdx.x) / 18)] * tarray[(((int)threadIdx.x) % 18)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[324];\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 9; ++ax3) {\n          T_tensordot[((((ax0 * 162) + (ax1 * 18)) + (ax2 * 9)) + ax3)] = (tarray[((ax0 * 9) + ax1)] * tarray[((ax2 * 9) + ax3)]);\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(1)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([36], \"float32\", \"global\")\n        for ax0, ax2 in T.grid(6, 6):\n            T_tensordot_1 = T.Buffer((T.int64(36),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(6),), data=tarray.data)\n            T_tensordot_1[ax0 * 6 + ax2] = tarray_1[ax0] * tarray_1[ax2]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  T_tensordot[((int)threadIdx.x)] = (tarray[(((int)threadIdx.x) / 6)] * tarray[(((int)threadIdx.x) % 6)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[36];\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n      T_tensordot[((ax0 * 6) + ax2)] = (tarray[ax0] * tarray[ax2]);\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(9), T.int64(2)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([324], \"float32\", \"global\")\n        for ax0, ax1, ax2, ax3 in T.grid(9, 2, 9, 2):\n            cse_var_1: T.int32 = ax2 * 2\n            T_tensordot_1 = T.Buffer((T.int64(324),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(18),), data=tarray.data)\n            T_tensordot_1[ax0 * 36 + ax1 * 18 + cse_var_1 + ax3] = tarray_1[ax0 * 2 + ax1] * tarray_1[cse_var_1 + ax3]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  T_tensordot[((int)threadIdx.x)] = (tarray[(((int)threadIdx.x) / 18)] * tarray[(((int)threadIdx.x) % 18)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[324];\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 2; ++ax3) {\n          T_tensordot[((((ax0 * 36) + (ax1 * 18)) + (ax2 * 2)) + ax3)] = (tarray[((ax0 * 2) + ax1)] * tarray[((ax2 * 2) + ax3)]);\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(3), T.int64(8)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([576], \"float32\", \"global\")\n        for ax0, ax1, ax2, ax3 in T.grid(3, 8, 3, 8):\n            cse_var_1: T.int32 = ax2 * 8\n            T_tensordot_1 = T.Buffer((T.int64(576),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(24),), data=tarray.data)\n            T_tensordot_1[ax0 * 192 + ax1 * 24 + cse_var_1 + ax3] = tarray_1[ax0 * 8 + ax1] * tarray_1[cse_var_1 + ax3]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(576) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(576) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  T_tensordot[((int)threadIdx.x)] = (tarray[(((int)threadIdx.x) / 24)] * tarray[(((int)threadIdx.x) % 24)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[576];\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n          T_tensordot[((((ax0 * 192) + (ax1 * 24)) + (ax2 * 8)) + ax3)] = (tarray[((ax0 * 8) + ax1)] * tarray[((ax2 * 8) + ax3)]);\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(2), T.int64(3), T.int64(2)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([144], \"float32\", \"global\")\n        for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 3, 2, 2, 3, 2):\n            cse_var_2: T.int32 = ax3 * 6\n            cse_var_1: T.int32 = ax4 * 2\n            T_tensordot_1 = T.Buffer((T.int64(144),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(12),), data=tarray.data)\n            T_tensordot_1[ax0 * 72 + ax1 * 24 + ax2 * 12 + cse_var_2 + cse_var_1 + ax5] = tarray_1[ax0 * 6 + ax1 * 2 + ax2] * tarray_1[cse_var_2 + cse_var_1 + ax5]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(144) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(144) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  T_tensordot[((int)threadIdx.x)] = (tarray[(((int)threadIdx.x) / 12)] * tarray[(((int)threadIdx.x) % 12)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[144];\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 2; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 3; ++ax4) {\n            for (int32_t ax5 = 0; ax5 < 2; ++ax5) {\n              T_tensordot[((((((ax0 * 72) + (ax1 * 24)) + (ax2 * 12)) + (ax3 * 6)) + (ax4 * 2)) + ax5)] = (tarray[(((ax0 * 6) + (ax1 * 2)) + ax2)] * tarray[(((ax3 * 6) + (ax4 * 2)) + ax5)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(7), T.int64(5), T.int64(10)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([122500], \"float32\", \"global\")\n        for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(7, 5, 10, 7, 5, 10):\n            cse_var_2: T.int32 = ax3 * 50\n            cse_var_1: T.int32 = ax4 * 10\n            T_tensordot_1 = T.Buffer((T.int64(122500),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(350),), data=tarray.data)\n            T_tensordot_1[ax0 * 17500 + ax1 * 3500 + ax2 * 350 + cse_var_2 + cse_var_1 + ax5] = tarray_1[ax0 * 50 + ax1 * 10 + ax2] * tarray_1[cse_var_2 + cse_var_1 + ax5]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)256) + (((int64_t)((int)threadIdx.x)) >> (int64_t)2)) < (int64_t)30625) {\n    T_tensordot[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (tarray[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) / 175)] * tarray[(((((((((int)blockIdx.x) * 162) + (((int)threadIdx.x) >> 1)) % 175) / 25) * 50) + (((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 1)) % 25) / 5) * 10)) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 10))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[122500];\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 7; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 5; ++ax4) {\n            for (int32_t ax5 = 0; ax5 < 10; ++ax5) {\n              T_tensordot[((((((ax0 * 17500) + (ax1 * 3500)) + (ax2 * 350)) + (ax3 * 50)) + (ax4 * 10)) + ax5)] = (tarray[(((ax0 * 50) + (ax1 * 10)) + ax2)] * tarray[(((ax3 * 50) + (ax4 * 10)) + ax5)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(4), T.int64(8), T.int64(4)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([16384], \"float32\", \"global\")\n        for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 8, 4, 4, 8, 4):\n            cse_var_2: T.int32 = ax3 * 32\n            cse_var_1: T.int32 = ax4 * 4\n            T_tensordot_1 = T.Buffer((T.int64(16384),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(128),), data=tarray.data)\n            T_tensordot_1[ax0 * 4096 + ax1 * 512 + ax2 * 128 + cse_var_2 + cse_var_1 + ax5] = tarray_1[ax0 * 32 + ax1 * 4 + ax2] * tarray_1[cse_var_2 + cse_var_1 + ax5]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  T_tensordot[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (tarray[((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 7))] * tarray[(((int)threadIdx.x) & 127)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[16384];\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 4; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 8; ++ax4) {\n            for (int32_t ax5 = 0; ax5 < 4; ++ax5) {\n              T_tensordot[((((((ax0 * 4096) + (ax1 * 512)) + (ax2 * 128)) + (ax3 * 32)) + (ax4 * 4)) + ax5)] = (tarray[(((ax0 * 32) + (ax1 * 4)) + ax2)] * tarray[(((ax3 * 32) + (ax4 * 4)) + ax5)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(9), T.int64(7)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([142884], \"float32\", \"global\")\n        for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(6, 9, 7, 6, 9, 7):\n            cse_var_2: T.int32 = ax3 * 63\n            cse_var_1: T.int32 = ax4 * 7\n            T_tensordot_1 = T.Buffer((T.int64(142884),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(378),), data=tarray.data)\n            T_tensordot_1[ax0 * 23814 + ax1 * 2646 + ax2 * 378 + cse_var_2 + cse_var_1 + ax5] = tarray_1[ax0 * 63 + ax1 * 7 + ax2] * tarray_1[cse_var_2 + cse_var_1 + ax5]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)256) + (((int64_t)((int)threadIdx.x)) >> (int64_t)2)) < (int64_t)35721) {\n    T_tensordot[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (tarray[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) >> 1)) / 189)] * tarray[(((((((((int)blockIdx.x) * 268) + ((int)threadIdx.x)) % 378) / 63) * 63) + (((((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) % 63) / 7) * 7)) + (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 7))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[142884];\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 6; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 9; ++ax4) {\n            for (int32_t ax5 = 0; ax5 < 7; ++ax5) {\n              T_tensordot[((((((ax0 * 23814) + (ax1 * 2646)) + (ax2 * 378)) + (ax3 * 63)) + (ax4 * 7)) + ax5)] = (tarray[(((ax0 * 63) + (ax1 * 7)) + ax2)] * tarray[(((ax3 * 63) + (ax4 * 7)) + ax5)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(6), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([104976], \"float32\", \"global\")\n        for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(6, 6, 9, 6, 6, 9):\n            cse_var_2: T.int32 = ax3 * 54\n            cse_var_1: T.int32 = ax4 * 9\n            T_tensordot_1 = T.Buffer((T.int64(104976),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(324),), data=tarray.data)\n            T_tensordot_1[ax0 * 17496 + ax1 * 2916 + ax2 * 324 + cse_var_2 + cse_var_1 + ax5] = tarray_1[ax0 * 54 + ax1 * 9 + ax2] * tarray_1[cse_var_2 + cse_var_1 + ax5]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)64) + (((int64_t)((int)threadIdx.x)) >> (int64_t)4)) < (int64_t)6561) {\n    T_tensordot[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (tarray[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 81)] * tarray[((((((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) % 324) / 9) * 9) + (((((int)blockIdx.x) * 7) + ((int)threadIdx.x)) % 9))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[104976];\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 6; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 6; ++ax4) {\n            for (int32_t ax5 = 0; ax5 < 9; ++ax5) {\n              T_tensordot[((((((ax0 * 17496) + (ax1 * 2916)) + (ax2 * 324)) + (ax3 * 54)) + (ax4 * 9)) + ax5)] = (tarray[(((ax0 * 54) + (ax1 * 9)) + ax2)] * tarray[(((ax3 * 54) + (ax4 * 9)) + ax5)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(4), T.int64(9), T.int64(8), T.int64(4)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([1327104], \"float32\", \"global\")\n        for ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7 in T.grid(4, 9, 8, 4, 4, 9, 8, 4):\n            cse_var_3: T.int32 = ax4 * 288\n            cse_var_2: T.int32 = ax5 * 32\n            cse_var_1: T.int32 = ax6 * 4\n            T_tensordot_1 = T.Buffer((T.int64(1327104),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(1152),), data=tarray.data)\n            T_tensordot_1[ax0 * 331776 + ax1 * 36864 + ax2 * 4608 + ax3 * 1152 + cse_var_3 + cse_var_2 + cse_var_1 + ax7] = tarray_1[ax0 * 288 + ax1 * 32 + ax2 * 4 + ax3] * tarray_1[cse_var_3 + cse_var_2 + cse_var_1 + ax7]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer < 6; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 16) + (((int)blockIdx.x) >> 4)) < 81) {\n      T_tensordot[(((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 2048) + (((int)blockIdx.x) * 8)) + (((int)threadIdx.x) >> 7)) / 9)] * tarray[((((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 8192) + (((int)blockIdx.x) * 32)) + (((int)threadIdx.x) >> 5)) % 36) * 32) + (((int)threadIdx.x) & 31))]);\n    }\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[1327104];\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 4; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 4; ++ax4) {\n            for (int32_t ax5 = 0; ax5 < 9; ++ax5) {\n              for (int32_t ax6 = 0; ax6 < 8; ++ax6) {\n                for (int32_t ax7 = 0; ax7 < 4; ++ax7) {\n                  T_tensordot[((((((((ax0 * 331776) + (ax1 * 36864)) + (ax2 * 4608)) + (ax3 * 1152)) + (ax4 * 288)) + (ax5 * 32)) + (ax6 * 4)) + ax7)] = (tarray[((((ax0 * 288) + (ax1 * 32)) + (ax2 * 4)) + ax3)] * tarray[((((ax4 * 288) + (ax5 * 32)) + (ax6 * 4)) + ax7)]);\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(8), T.int64(2), T.int64(10), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([2073600], \"float32\", \"global\")\n        for ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7 in T.grid(8, 2, 10, 9, 8, 2, 10, 9):\n            cse_var_3: T.int32 = ax4 * 180\n            cse_var_2: T.int32 = ax5 * 90\n            cse_var_1: T.int32 = ax6 * 9\n            T_tensordot_1 = T.Buffer((T.int64(2073600),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(1440),), data=tarray.data)\n            T_tensordot_1[ax0 * 259200 + ax1 * 129600 + ax2 * 12960 + ax3 * 1440 + cse_var_3 + cse_var_2 + cse_var_1 + ax7] = tarray_1[ax0 * 180 + ax1 * 90 + ax2 * 9 + ax3] * tarray_1[cse_var_3 + cse_var_2 + cse_var_1 + ax7]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer < 8; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 256) + ((int)blockIdx.x)) < 2025) {\n      T_tensordot[(((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 8192) + (((int)blockIdx.x) * 32)) + (((int)threadIdx.x) >> 5)) / 45)] * tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 1440)]);\n    }\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[2073600];\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 9; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 8; ++ax4) {\n            for (int32_t ax5 = 0; ax5 < 2; ++ax5) {\n              for (int32_t ax6 = 0; ax6 < 10; ++ax6) {\n                for (int32_t ax7 = 0; ax7 < 9; ++ax7) {\n                  T_tensordot[((((((((ax0 * 259200) + (ax1 * 129600)) + (ax2 * 12960)) + (ax3 * 1440)) + (ax4 * 180)) + (ax5 * 90)) + (ax6 * 9)) + ax7)] = (tarray[((((ax0 * 180) + (ax1 * 90)) + (ax2 * 9)) + ax3)] * tarray[((((ax4 * 180) + (ax5 * 90)) + (ax6 * 9)) + ax7)]);\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(4), T.int64(5), T.int64(4), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([518400], \"float32\", \"global\")\n        for ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7 in T.grid(4, 5, 4, 9, 4, 5, 4, 9):\n            cse_var_3: T.int32 = ax4 * 180\n            cse_var_2: T.int32 = ax5 * 36\n            cse_var_1: T.int32 = ax6 * 9\n            T_tensordot_1 = T.Buffer((T.int64(518400),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(720),), data=tarray.data)\n            T_tensordot_1[ax0 * 129600 + ax1 * 25920 + ax2 * 6480 + ax3 * 720 + cse_var_3 + cse_var_2 + cse_var_1 + ax7] = tarray_1[ax0 * 180 + ax1 * 36 + ax2 * 9 + ax3] * tarray_1[cse_var_3 + cse_var_2 + cse_var_1 + ax7]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer) {\n    if ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 1024) + (((int)blockIdx.x) * 4)) + (((int)threadIdx.x) >> 8)) < 2025) {\n      T_tensordot[(((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 16384) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.x) >> 4)) / 45)] * tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 720)]);\n    }\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[518400];\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 9; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 4; ++ax4) {\n            for (int32_t ax5 = 0; ax5 < 5; ++ax5) {\n              for (int32_t ax6 = 0; ax6 < 4; ++ax6) {\n                for (int32_t ax7 = 0; ax7 < 9; ++ax7) {\n                  T_tensordot[((((((((ax0 * 129600) + (ax1 * 25920)) + (ax2 * 6480)) + (ax3 * 720)) + (ax4 * 180)) + (ax5 * 36)) + (ax6 * 9)) + ax7)] = (tarray[((((ax0 * 180) + (ax1 * 36)) + (ax2 * 9)) + ax3)] * tarray[((((ax4 * 180) + (ax5 * 36)) + (ax6 * 9)) + ax7)]);\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(1), T.int64(8), T.int64(3), T.int64(6)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([20736], \"float32\", \"global\")\n        for ax1, ax2, ax3, ax5, ax6, ax7 in T.grid(8, 3, 6, 8, 3, 6):\n            cse_var_2: T.int32 = ax5 * 18\n            cse_var_1: T.int32 = ax6 * 6\n            T_tensordot_1 = T.Buffer((T.int64(20736),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(144),), data=tarray.data)\n            T_tensordot_1[ax1 * 2592 + ax2 * 864 + ax3 * 144 + cse_var_2 + cse_var_1 + ax7] = tarray_1[ax1 * 18 + ax2 * 6 + ax3] * tarray_1[cse_var_2 + cse_var_1 + ax7]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)4) + (((int64_t)((int)threadIdx.x)) >> (int64_t)8)) < (int64_t)81) {\n    T_tensordot[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (tarray[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) >> 4)) / 9)] * tarray[((((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 1)) % 72) / 3) * 6) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 6))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[20736];\n  for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 6; ++ax3) {\n        for (int32_t ax5 = 0; ax5 < 8; ++ax5) {\n          for (int32_t ax6 = 0; ax6 < 3; ++ax6) {\n            for (int32_t ax7 = 0; ax7 < 6; ++ax7) {\n              T_tensordot[((((((ax1 * 2592) + (ax2 * 864)) + (ax3 * 144)) + (ax5 * 18)) + (ax6 * 6)) + ax7)] = (tarray[(((ax1 * 18) + (ax2 * 6)) + ax3)] * tarray[(((ax5 * 18) + (ax6 * 6)) + ax7)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(7), T.int64(7), T.int64(7), T.int64(10)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([11764900], \"float32\", \"global\")\n        for ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7 in T.grid(7, 7, 7, 10, 7, 7, 7, 10):\n            cse_var_3: T.int32 = ax4 * 490\n            cse_var_2: T.int32 = ax5 * 70\n            cse_var_1: T.int32 = ax6 * 10\n            T_tensordot_1 = T.Buffer((T.int64(11764900),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(3430),), data=tarray.data)\n            T_tensordot_1[ax0 * 1680700 + ax1 * 240100 + ax2 * 34300 + ax3 * 3430 + cse_var_3 + cse_var_2 + cse_var_1 + ax7] = tarray_1[ax0 * 490 + ax1 * 70 + ax2 * 10 + ax3] * tarray_1[cse_var_3 + cse_var_2 + cse_var_1 + ax7]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer < 45; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer) {\n    if ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 65536) + (((int)blockIdx.x) * 256)) + (((int)threadIdx.x) >> 2)) < 2941225) {\n      T_tensordot[(((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 131072) + (((int)blockIdx.x) * 512)) + (((int)threadIdx.x) >> 1)) / 1715)] * tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 3430)]);\n    }\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[11764900];\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 10; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 7; ++ax4) {\n            for (int32_t ax5 = 0; ax5 < 7; ++ax5) {\n              for (int32_t ax6 = 0; ax6 < 7; ++ax6) {\n                for (int32_t ax7 = 0; ax7 < 10; ++ax7) {\n                  T_tensordot[((((((((ax0 * 1680700) + (ax1 * 240100)) + (ax2 * 34300)) + (ax3 * 3430)) + (ax4 * 490)) + (ax5 * 70)) + (ax6 * 10)) + ax7)] = (tarray[((((ax0 * 490) + (ax1 * 70)) + (ax2 * 10)) + ax3)] * tarray[((((ax4 * 490) + (ax5 * 70)) + (ax6 * 10)) + ax7)]);\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(2), T.int64(3), T.int64(10), T.int64(7)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([6350400], \"float32\", \"global\")\n        for ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9 in T.grid(6, 2, 3, 10, 7, 6, 2, 3, 10, 7):\n            cse_var_4: T.int32 = ax5 * 420\n            cse_var_3: T.int32 = ax6 * 210\n            cse_var_2: T.int32 = ax7 * 70\n            cse_var_1: T.int32 = ax8 * 7\n            T_tensordot_1 = T.Buffer((T.int64(6350400),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(2520),), data=tarray.data)\n            T_tensordot_1[ax0 * 1058400 + ax1 * 529200 + ax2 * 176400 + ax3 * 17640 + ax4 * 2520 + cse_var_4 + cse_var_3 + cse_var_2 + cse_var_1 + ax9] = tarray_1[ax0 * 420 + ax1 * 210 + ax2 * 70 + ax3 * 7 + ax4] * tarray_1[cse_var_4 + cse_var_3 + cse_var_2 + cse_var_1 + ax9]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer < 25; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer) {\n    if ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 4096) + (((int)blockIdx.x) * 16)) + (((int)threadIdx.x) >> 6)) < 99225) {\n      T_tensordot[(((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 32768) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) >> 3)) / 315)] * tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 2520)]);\n    }\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[6350400];\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 10; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 7; ++ax4) {\n            for (int32_t ax5 = 0; ax5 < 6; ++ax5) {\n              for (int32_t ax6 = 0; ax6 < 2; ++ax6) {\n                for (int32_t ax7 = 0; ax7 < 3; ++ax7) {\n                  for (int32_t ax8 = 0; ax8 < 10; ++ax8) {\n                    for (int32_t ax9 = 0; ax9 < 7; ++ax9) {\n                      T_tensordot[((((((((((ax0 * 1058400) + (ax1 * 529200)) + (ax2 * 176400)) + (ax3 * 17640)) + (ax4 * 2520)) + (ax5 * 420)) + (ax6 * 210)) + (ax7 * 70)) + (ax8 * 7)) + ax9)] = (tarray[(((((ax0 * 420) + (ax1 * 210)) + (ax2 * 70)) + (ax3 * 7)) + ax4)] * tarray[(((((ax5 * 420) + (ax6 * 210)) + (ax7 * 70)) + (ax8 * 7)) + ax9)]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(10), T.int64(4), T.int64(2), T.int64(2), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([2073600], \"float32\", \"global\")\n        for ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9 in T.grid(10, 4, 2, 2, 9, 10, 4, 2, 2, 9):\n            cse_var_4: T.int32 = ax5 * 144\n            cse_var_3: T.int32 = ax6 * 36\n            cse_var_2: T.int32 = ax7 * 18\n            cse_var_1: T.int32 = ax8 * 9\n            T_tensordot_1 = T.Buffer((T.int64(2073600),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(1440),), data=tarray.data)\n            T_tensordot_1[ax0 * 207360 + ax1 * 51840 + ax2 * 25920 + ax3 * 12960 + ax4 * 1440 + cse_var_4 + cse_var_3 + cse_var_2 + cse_var_1 + ax9] = tarray_1[ax0 * 144 + ax1 * 36 + ax2 * 18 + ax3 * 9 + ax4] * tarray_1[cse_var_4 + cse_var_3 + cse_var_2 + cse_var_1 + ax9]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer < 8; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 256) + ((int)blockIdx.x)) < 2025) {\n      T_tensordot[(((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 8192) + (((int)blockIdx.x) * 32)) + (((int)threadIdx.x) >> 5)) / 45)] * tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 1440)]);\n    }\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[2073600];\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 4; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 2; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 9; ++ax4) {\n            for (int32_t ax5 = 0; ax5 < 10; ++ax5) {\n              for (int32_t ax6 = 0; ax6 < 4; ++ax6) {\n                for (int32_t ax7 = 0; ax7 < 2; ++ax7) {\n                  for (int32_t ax8 = 0; ax8 < 2; ++ax8) {\n                    for (int32_t ax9 = 0; ax9 < 9; ++ax9) {\n                      T_tensordot[((((((((((ax0 * 207360) + (ax1 * 51840)) + (ax2 * 25920)) + (ax3 * 12960)) + (ax4 * 1440)) + (ax5 * 144)) + (ax6 * 36)) + (ax7 * 18)) + (ax8 * 9)) + ax9)] = (tarray[(((((ax0 * 144) + (ax1 * 36)) + (ax2 * 18)) + (ax3 * 9)) + ax4)] * tarray[(((((ax5 * 144) + (ax6 * 36)) + (ax7 * 18)) + (ax8 * 9)) + ax9)]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(8), T.int64(10), T.int64(10), T.int64(3)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([207360000], \"float32\", \"global\")\n        for ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9 in T.grid(6, 8, 10, 10, 3, 6, 8, 10, 10, 3):\n            cse_var_4: T.int32 = ax5 * 2400\n            cse_var_3: T.int32 = ax6 * 300\n            cse_var_2: T.int32 = ax7 * 30\n            cse_var_1: T.int32 = ax8 * 3\n            T_tensordot_1 = T.Buffer((T.int64(207360000),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(14400),), data=tarray.data)\n            T_tensordot_1[ax0 * 34560000 + ax1 * 4320000 + ax2 * 432000 + ax3 * 43200 + ax4 * 14400 + cse_var_4 + cse_var_3 + cse_var_2 + cse_var_1 + ax9] = tarray_1[ax0 * 2400 + ax1 * 300 + ax2 * 30 + ax3 * 3 + ax4] * tarray_1[cse_var_4 + cse_var_3 + cse_var_2 + cse_var_1 + ax9]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer < 792; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 64) + (((int)blockIdx.x) >> 2)) < 50625) {\n      T_tensordot[(((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 4096) + (((int)blockIdx.x) * 16)) + (((int)threadIdx.x) >> 6)) / 225)] * tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 14400)]);\n    }\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[207360000];\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 10; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 3; ++ax4) {\n            for (int32_t ax5 = 0; ax5 < 6; ++ax5) {\n              for (int32_t ax6 = 0; ax6 < 8; ++ax6) {\n                for (int32_t ax7 = 0; ax7 < 10; ++ax7) {\n                  for (int32_t ax8 = 0; ax8 < 10; ++ax8) {\n                    for (int32_t ax9 = 0; ax9 < 3; ++ax9) {\n                      T_tensordot[((((((((((ax0 * 34560000) + (ax1 * 4320000)) + (ax2 * 432000)) + (ax3 * 43200)) + (ax4 * 14400)) + (ax5 * 2400)) + (ax6 * 300)) + (ax7 * 30)) + (ax8 * 3)) + ax9)] = (tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)] * tarray[(((((ax5 * 2400) + (ax6 * 300)) + (ax7 * 30)) + (ax8 * 3)) + ax9)]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(7), T.int64(8), T.int64(3), T.int64(3), T.int64(5)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([6350400], \"float32\", \"global\")\n        for ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9 in T.grid(7, 8, 3, 3, 5, 7, 8, 3, 3, 5):\n            cse_var_4: T.int32 = ax5 * 360\n            cse_var_3: T.int32 = ax6 * 45\n            cse_var_2: T.int32 = ax7 * 15\n            cse_var_1: T.int32 = ax8 * 5\n            T_tensordot_1 = T.Buffer((T.int64(6350400),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(2520),), data=tarray.data)\n            T_tensordot_1[ax0 * 907200 + ax1 * 113400 + ax2 * 37800 + ax3 * 12600 + ax4 * 2520 + cse_var_4 + cse_var_3 + cse_var_2 + cse_var_1 + ax9] = tarray_1[ax0 * 360 + ax1 * 45 + ax2 * 15 + ax3 * 5 + ax4] * tarray_1[cse_var_4 + cse_var_3 + cse_var_2 + cse_var_1 + ax9]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer < 25; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer) {\n    if ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 4096) + (((int)blockIdx.x) * 16)) + (((int)threadIdx.x) >> 6)) < 99225) {\n      T_tensordot[(((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 32768) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) >> 3)) / 315)] * tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 2520)]);\n    }\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[6350400];\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 3; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 5; ++ax4) {\n            for (int32_t ax5 = 0; ax5 < 7; ++ax5) {\n              for (int32_t ax6 = 0; ax6 < 8; ++ax6) {\n                for (int32_t ax7 = 0; ax7 < 3; ++ax7) {\n                  for (int32_t ax8 = 0; ax8 < 3; ++ax8) {\n                    for (int32_t ax9 = 0; ax9 < 5; ++ax9) {\n                      T_tensordot[((((((((((ax0 * 907200) + (ax1 * 113400)) + (ax2 * 37800)) + (ax3 * 12600)) + (ax4 * 2520)) + (ax5 * 360)) + (ax6 * 45)) + (ax7 * 15)) + (ax8 * 5)) + ax9)] = (tarray[(((((ax0 * 360) + (ax1 * 45)) + (ax2 * 15)) + (ax3 * 5)) + ax4)] * tarray[(((((ax5 * 360) + (ax6 * 45)) + (ax7 * 15)) + (ax8 * 5)) + ax9)]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(2), T.int64(10), T.int64(7), T.int64(1), T.int64(7)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_tensordot = T.allocate([960400], \"float32\", \"global\")\n        for ax0, ax1, ax2, ax4, ax5, ax6, ax7, ax9 in T.grid(2, 10, 7, 7, 2, 10, 7, 7):\n            cse_var_3: T.int32 = ax5 * 490\n            cse_var_2: T.int32 = ax6 * 49\n            cse_var_1: T.int32 = ax7 * 7\n            T_tensordot_1 = T.Buffer((T.int64(960400),), data=T_tensordot)\n            tarray_1 = T.Buffer((T.int64(980),), data=tarray.data)\n            T_tensordot_1[ax0 * 480200 + ax1 * 48020 + ax2 * 6860 + ax4 * 980 + cse_var_3 + cse_var_2 + cse_var_1 + ax9] = tarray_1[ax0 * 490 + ax1 * 49 + ax2 * 7 + ax4] * tarray_1[cse_var_3 + cse_var_2 + cse_var_1 + ax9]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer) {\n    if ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 16384) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.x) >> 4)) < 60025) {\n      T_tensordot[(((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 65536) + (((int)blockIdx.x) * 256)) + (((int)threadIdx.x) >> 2)) / 245)] * tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 980)]);\n    }\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[960400];\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        for (int32_t ax4 = 0; ax4 < 7; ++ax4) {\n          for (int32_t ax5 = 0; ax5 < 2; ++ax5) {\n            for (int32_t ax6 = 0; ax6 < 10; ++ax6) {\n              for (int32_t ax7 = 0; ax7 < 7; ++ax7) {\n                for (int32_t ax9 = 0; ax9 < 7; ++ax9) {\n                  T_tensordot[((((((((ax0 * 480200) + (ax1 * 48020)) + (ax2 * 6860)) + (ax4 * 980)) + (ax5 * 490)) + (ax6 * 49)) + (ax7 * 7)) + ax9)] = (tarray[((((ax0 * 490) + (ax1 * 49)) + (ax2 * 7)) + ax4)] * tarray[((((ax5 * 490) + (ax6 * 49)) + (ax7 * 7)) + ax9)]);\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(2),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([2], \"float32\", \"global\")\n        for i0 in range(2):\n            compute_1 = T.Buffer((T.int64(2),), data=compute, align=8)\n            compute_1[i0] = T.rsqrt(tarray[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[2];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    compute[i0] = (1.000000e+00f / sqrtf(tarray[i0]));\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(9),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([9], \"float32\", \"global\")\n        for i0 in range(9):\n            compute_1 = T.Buffer((T.int64(9),), data=compute, align=32)\n            compute_1[i0] = T.rsqrt(tarray[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[9];\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    compute[i0] = (1.000000e+00f / sqrtf(tarray[i0]));\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(10),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([10], \"float32\", \"global\")\n        for i0 in range(10):\n            compute_1 = T.Buffer((T.int64(10),), data=compute, align=32)\n            compute_1[i0] = T.rsqrt(tarray[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[10];\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    compute[i0] = (1.000000e+00f / sqrtf(tarray[i0]));\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([6], \"float32\", \"global\")\n        for i0 in range(6):\n            compute_1 = T.Buffer((T.int64(6),), data=compute, align=16)\n            compute_1[i0] = T.rsqrt(tarray[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[6];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    compute[i0] = (1.000000e+00f / sqrtf(tarray[i0]));\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(4),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([4], \"float32\", \"global\")\n        for i0 in range(4):\n            compute_1 = T.Buffer((T.int64(4),), data=compute, align=16)\n            compute_1[i0] = T.rsqrt(tarray[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[4];\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    compute[i0] = (1.000000e+00f / sqrtf(tarray[i0]));\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(8), T.int64(1)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([8], \"float32\", \"global\")\n        for i0 in range(8):\n            compute_1 = T.Buffer((T.int64(8),), data=compute, align=32)\n            tarray_1 = T.Buffer((T.int64(8),), data=tarray.data)\n            compute_1[i0] = T.rsqrt(tarray_1[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[8];\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    compute[i0] = (1.000000e+00f / sqrtf(tarray[i0]));\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(2), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([18], \"float32\", \"global\")\n        for i0, i1 in T.grid(2, 9):\n            cse_var_1: T.int32 = i0 * 9 + i1\n            compute_1 = T.Buffer((T.int64(18),), data=compute)\n            tarray_1 = T.Buffer((T.int64(18),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[18];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      compute[((i0 * 9) + i1)] = (1.000000e+00f / sqrtf(tarray[((i0 * 9) + i1)]));\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(1)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([6], \"float32\", \"global\")\n        for i0 in range(6):\n            compute_1 = T.Buffer((T.int64(6),), data=compute, align=16)\n            tarray_1 = T.Buffer((T.int64(6),), data=tarray.data)\n            compute_1[i0] = T.rsqrt(tarray_1[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[6];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    compute[i0] = (1.000000e+00f / sqrtf(tarray[i0]));\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(9), T.int64(2)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([18], \"float32\", \"global\")\n        for i0, i1 in T.grid(9, 2):\n            cse_var_1: T.int32 = i0 * 2 + i1\n            compute_1 = T.Buffer((T.int64(18),), data=compute)\n            tarray_1 = T.Buffer((T.int64(18),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[18];\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      compute[((i0 * 2) + i1)] = (1.000000e+00f / sqrtf(tarray[((i0 * 2) + i1)]));\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(3), T.int64(8)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([24], \"float32\", \"global\")\n        for i0, i1 in T.grid(3, 8):\n            cse_var_1: T.int32 = i0 * 8 + i1\n            compute_1 = T.Buffer((T.int64(24),), data=compute)\n            tarray_1 = T.Buffer((T.int64(24),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[24];\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      compute[((i0 * 8) + i1)] = (1.000000e+00f / sqrtf(tarray[((i0 * 8) + i1)]));\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(2), T.int64(3), T.int64(2)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([12], \"float32\", \"global\")\n        for i0, i1, i2 in T.grid(2, 3, 2):\n            cse_var_1: T.int32 = i0 * 6 + i1 * 2 + i2\n            compute_1 = T.Buffer((T.int64(12),), data=compute, align=32)\n            tarray_1 = T.Buffer((T.int64(12),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[12];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 6) + (i1 * 2)) + i2)] = (1.000000e+00f / sqrtf(tarray[(((i0 * 6) + (i1 * 2)) + i2)]));\n      }\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(7), T.int64(5), T.int64(10)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([350], \"float32\", \"global\")\n        for i0, i1, i2 in T.grid(7, 5, 10):\n            cse_var_1: T.int32 = i0 * 50 + i1 * 10 + i2\n            compute_1 = T.Buffer((T.int64(350),), data=compute)\n            tarray_1 = T.Buffer((T.int64(350),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(350) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(350) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[350];\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        compute[(((i0 * 50) + (i1 * 10)) + i2)] = (1.000000e+00f / sqrtf(tarray[(((i0 * 50) + (i1 * 10)) + i2)]));\n      }\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(4), T.int64(8), T.int64(4)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([128], \"float32\", \"global\")\n        for i0, i1, i2 in T.grid(4, 8, 4):\n            cse_var_1: T.int32 = i0 * 32 + i1 * 4 + i2\n            compute_1 = T.Buffer((T.int64(128),), data=compute)\n            tarray_1 = T.Buffer((T.int64(128),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[128];\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute[(((i0 * 32) + (i1 * 4)) + i2)] = (1.000000e+00f / sqrtf(tarray[(((i0 * 32) + (i1 * 4)) + i2)]));\n      }\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(9), T.int64(7)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([378], \"float32\", \"global\")\n        for i0, i1, i2 in T.grid(6, 9, 7):\n            cse_var_1: T.int32 = i0 * 63 + i1 * 7 + i2\n            compute_1 = T.Buffer((T.int64(378),), data=compute)\n            tarray_1 = T.Buffer((T.int64(378),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(378) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(378) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[378];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 63) + (i1 * 7)) + i2)] = (1.000000e+00f / sqrtf(tarray[(((i0 * 63) + (i1 * 7)) + i2)]));\n      }\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(6), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([324], \"float32\", \"global\")\n        for i0, i1, i2 in T.grid(6, 6, 9):\n            cse_var_1: T.int32 = i0 * 54 + i1 * 9 + i2\n            compute_1 = T.Buffer((T.int64(324),), data=compute)\n            tarray_1 = T.Buffer((T.int64(324),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[324];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 54) + (i1 * 9)) + i2)] = (1.000000e+00f / sqrtf(tarray[(((i0 * 54) + (i1 * 9)) + i2)]));\n      }\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(4), T.int64(9), T.int64(8), T.int64(4)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([1152], \"float32\", \"global\")\n        for i0, i1, i2, i3 in T.grid(4, 9, 8, 4):\n            cse_var_1: T.int32 = i0 * 288 + i1 * 32 + i2 * 4 + i3\n            compute_1 = T.Buffer((T.int64(1152),), data=compute)\n            tarray_1 = T.Buffer((T.int64(1152),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)8) + (((int64_t)((int)threadIdx.x)) >> (int64_t)7)) < (int64_t)9) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]));\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[1152];\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        for (int32_t i3 = 0; i3 < 4; ++i3) {\n          compute[((((i0 * 288) + (i1 * 32)) + (i2 * 4)) + i3)] = (1.000000e+00f / sqrtf(tarray[((((i0 * 288) + (i1 * 32)) + (i2 * 4)) + i3)]));\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(8), T.int64(2), T.int64(10), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([1440], \"float32\", \"global\")\n        for i0, i1, i2, i3 in T.grid(8, 2, 10, 9):\n            cse_var_1: T.int32 = i0 * 180 + i1 * 90 + i2 * 9 + i3\n            compute_1 = T.Buffer((T.int64(1440),), data=compute)\n            tarray_1 = T.Buffer((T.int64(1440),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)32) + (((int64_t)((int)threadIdx.x)) >> (int64_t)5)) < (int64_t)45) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]));\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[1440];\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        for (int32_t i3 = 0; i3 < 9; ++i3) {\n          compute[((((i0 * 180) + (i1 * 90)) + (i2 * 9)) + i3)] = (1.000000e+00f / sqrtf(tarray[((((i0 * 180) + (i1 * 90)) + (i2 * 9)) + i3)]));\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(4), T.int64(5), T.int64(4), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([720], \"float32\", \"global\")\n        for i0, i1, i2, i3 in T.grid(4, 5, 4, 9):\n            cse_var_1: T.int32 = i0 * 180 + i1 * 36 + i2 * 9 + i3\n            compute_1 = T.Buffer((T.int64(720),), data=compute)\n            tarray_1 = T.Buffer((T.int64(720),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(720) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(720) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[720];\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        for (int32_t i3 = 0; i3 < 9; ++i3) {\n          compute[((((i0 * 180) + (i1 * 36)) + (i2 * 9)) + i3)] = (1.000000e+00f / sqrtf(tarray[((((i0 * 180) + (i1 * 36)) + (i2 * 9)) + i3)]));\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(1), T.int64(8), T.int64(3), T.int64(6)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([144], \"float32\", \"global\")\n        for i1, i2, i3 in T.grid(8, 3, 6):\n            cse_var_1: T.int32 = i1 * 18 + i2 * 6 + i3\n            compute_1 = T.Buffer((T.int64(144),), data=compute)\n            tarray_1 = T.Buffer((T.int64(144),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(144) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(144) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[144];\n  for (int32_t i1 = 0; i1 < 8; ++i1) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      for (int32_t i3 = 0; i3 < 6; ++i3) {\n        compute[(((i1 * 18) + (i2 * 6)) + i3)] = (1.000000e+00f / sqrtf(tarray[(((i1 * 18) + (i2 * 6)) + i3)]));\n      }\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(7), T.int64(7), T.int64(7), T.int64(10)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([3430], \"float32\", \"global\")\n        for i0, i1, i2, i3 in T.grid(7, 7, 7, 10):\n            cse_var_1: T.int32 = i0 * 490 + i1 * 70 + i2 * 10 + i3\n            compute_1 = T.Buffer((T.int64(3430),), data=compute)\n            tarray_1 = T.Buffer((T.int64(3430),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)512) + (((int64_t)((int)threadIdx.x)) >> (int64_t)1)) < (int64_t)1715) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]));\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[3430];\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          compute[((((i0 * 490) + (i1 * 70)) + (i2 * 10)) + i3)] = (1.000000e+00f / sqrtf(tarray[((((i0 * 490) + (i1 * 70)) + (i2 * 10)) + i3)]));\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(2), T.int64(3), T.int64(10), T.int64(7)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([2520], \"float32\", \"global\")\n        for i0, i1, i2, i3, i4 in T.grid(6, 2, 3, 10, 7):\n            cse_var_1: T.int32 = i0 * 420 + i1 * 210 + i2 * 70 + i3 * 7 + i4\n            compute_1 = T.Buffer((T.int64(2520),), data=compute)\n            tarray_1 = T.Buffer((T.int64(2520),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)128) + (((int64_t)((int)threadIdx.x)) >> (int64_t)3)) < (int64_t)315) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]));\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[2520];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          for (int32_t i4 = 0; i4 < 7; ++i4) {\n            compute[(((((i0 * 420) + (i1 * 210)) + (i2 * 70)) + (i3 * 7)) + i4)] = (1.000000e+00f / sqrtf(tarray[(((((i0 * 420) + (i1 * 210)) + (i2 * 70)) + (i3 * 7)) + i4)]));\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(10), T.int64(4), T.int64(2), T.int64(2), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([1440], \"float32\", \"global\")\n        for i0, i1, i2, i3, i4 in T.grid(10, 4, 2, 2, 9):\n            cse_var_1: T.int32 = i0 * 144 + i1 * 36 + i2 * 18 + i3 * 9 + i4\n            compute_1 = T.Buffer((T.int64(1440),), data=compute)\n            tarray_1 = T.Buffer((T.int64(1440),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)32) + (((int64_t)((int)threadIdx.x)) >> (int64_t)5)) < (int64_t)45) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]));\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[1440];\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        for (int32_t i3 = 0; i3 < 2; ++i3) {\n          for (int32_t i4 = 0; i4 < 9; ++i4) {\n            compute[(((((i0 * 144) + (i1 * 36)) + (i2 * 18)) + (i3 * 9)) + i4)] = (1.000000e+00f / sqrtf(tarray[(((((i0 * 144) + (i1 * 36)) + (i2 * 18)) + (i3 * 9)) + i4)]));\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(8), T.int64(10), T.int64(10), T.int64(3)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([14400], \"float32\", \"global\")\n        for i0, i1, i2, i3, i4 in T.grid(6, 8, 10, 10, 3):\n            cse_var_1: T.int32 = i0 * 2400 + i1 * 300 + i2 * 30 + i3 * 3 + i4\n            compute_1 = T.Buffer((T.int64(14400),), data=compute)\n            tarray_1 = T.Buffer((T.int64(14400),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)16) + (((int64_t)((int)threadIdx.x)) >> (int64_t)6)) < (int64_t)225) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]));\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[14400];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          for (int32_t i4 = 0; i4 < 3; ++i4) {\n            compute[(((((i0 * 2400) + (i1 * 300)) + (i2 * 30)) + (i3 * 3)) + i4)] = (1.000000e+00f / sqrtf(tarray[(((((i0 * 2400) + (i1 * 300)) + (i2 * 30)) + (i3 * 3)) + i4)]));\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(7), T.int64(8), T.int64(3), T.int64(3), T.int64(5)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([2520], \"float32\", \"global\")\n        for i0, i1, i2, i3, i4 in T.grid(7, 8, 3, 3, 5):\n            cse_var_1: T.int32 = i0 * 360 + i1 * 45 + i2 * 15 + i3 * 5 + i4\n            compute_1 = T.Buffer((T.int64(2520),), data=compute)\n            tarray_1 = T.Buffer((T.int64(2520),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)128) + (((int64_t)((int)threadIdx.x)) >> (int64_t)3)) < (int64_t)315) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]));\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[2520];\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3 = 0; i3 < 3; ++i3) {\n          for (int32_t i4 = 0; i4 < 5; ++i4) {\n            compute[(((((i0 * 360) + (i1 * 45)) + (i2 * 15)) + (i3 * 5)) + i4)] = (1.000000e+00f / sqrtf(tarray[(((((i0 * 360) + (i1 * 45)) + (i2 * 15)) + (i3 * 5)) + i4)]));\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(2), T.int64(10), T.int64(7), T.int64(1), T.int64(7)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([980], \"float32\", \"global\")\n        for i0, i1, i2, i4 in T.grid(2, 10, 7, 7):\n            cse_var_1: T.int32 = i0 * 490 + i1 * 49 + i2 * 7 + i4\n            compute_1 = T.Buffer((T.int64(980),), data=compute)\n            tarray_1 = T.Buffer((T.int64(980),), data=tarray.data)\n            compute_1[cse_var_1] = T.rsqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(980) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(980) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / sqrtf(tarray[((int)threadIdx.x)]));\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[980];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        for (int32_t i4 = 0; i4 < 7; ++i4) {\n          compute[((((i0 * 490) + (i1 * 49)) + (i2 * 7)) + i4)] = (1.000000e+00f / sqrtf(tarray[((((i0 * 490) + (i1 * 49)) + (i2 * 7)) + i4)]));\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(2),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([2], \"float32\", \"global\")\n        for i0 in range(2):\n            compute_1 = T.Buffer((T.int64(2),), data=compute, align=8)\n            compute_1[i0] = T.sqrt(tarray[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[2];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    compute[i0] = sqrtf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(9),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([9], \"float32\", \"global\")\n        for i0 in range(9):\n            compute_1 = T.Buffer((T.int64(9),), data=compute, align=32)\n            compute_1[i0] = T.sqrt(tarray[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[9];\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    compute[i0] = sqrtf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(10),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([10], \"float32\", \"global\")\n        for i0 in range(10):\n            compute_1 = T.Buffer((T.int64(10),), data=compute, align=32)\n            compute_1[i0] = T.sqrt(tarray[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[10];\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    compute[i0] = sqrtf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([6], \"float32\", \"global\")\n        for i0 in range(6):\n            compute_1 = T.Buffer((T.int64(6),), data=compute, align=16)\n            compute_1[i0] = T.sqrt(tarray[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[6];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    compute[i0] = sqrtf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(4),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([4], \"float32\", \"global\")\n        for i0 in range(4):\n            compute_1 = T.Buffer((T.int64(4),), data=compute, align=16)\n            compute_1[i0] = T.sqrt(tarray[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[4];\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    compute[i0] = sqrtf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(8), T.int64(1)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([8], \"float32\", \"global\")\n        for i0 in range(8):\n            compute_1 = T.Buffer((T.int64(8),), data=compute, align=32)\n            tarray_1 = T.Buffer((T.int64(8),), data=tarray.data)\n            compute_1[i0] = T.sqrt(tarray_1[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[8];\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    compute[i0] = sqrtf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(2), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([18], \"float32\", \"global\")\n        for i0, i1 in T.grid(2, 9):\n            cse_var_1: T.int32 = i0 * 9 + i1\n            compute_1 = T.Buffer((T.int64(18),), data=compute)\n            tarray_1 = T.Buffer((T.int64(18),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[18];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      compute[((i0 * 9) + i1)] = sqrtf(tarray[((i0 * 9) + i1)]);\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(1)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([6], \"float32\", \"global\")\n        for i0 in range(6):\n            compute_1 = T.Buffer((T.int64(6),), data=compute, align=16)\n            tarray_1 = T.Buffer((T.int64(6),), data=tarray.data)\n            compute_1[i0] = T.sqrt(tarray_1[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[6];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    compute[i0] = sqrtf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(9), T.int64(2)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([18], \"float32\", \"global\")\n        for i0, i1 in T.grid(9, 2):\n            cse_var_1: T.int32 = i0 * 2 + i1\n            compute_1 = T.Buffer((T.int64(18),), data=compute)\n            tarray_1 = T.Buffer((T.int64(18),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[18];\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      compute[((i0 * 2) + i1)] = sqrtf(tarray[((i0 * 2) + i1)]);\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(3), T.int64(8)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([24], \"float32\", \"global\")\n        for i0, i1 in T.grid(3, 8):\n            cse_var_1: T.int32 = i0 * 8 + i1\n            compute_1 = T.Buffer((T.int64(24),), data=compute)\n            tarray_1 = T.Buffer((T.int64(24),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[24];\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      compute[((i0 * 8) + i1)] = sqrtf(tarray[((i0 * 8) + i1)]);\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(2), T.int64(3), T.int64(2)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([12], \"float32\", \"global\")\n        for i0, i1, i2 in T.grid(2, 3, 2):\n            cse_var_1: T.int32 = i0 * 6 + i1 * 2 + i2\n            compute_1 = T.Buffer((T.int64(12),), data=compute, align=32)\n            tarray_1 = T.Buffer((T.int64(12),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[12];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 6) + (i1 * 2)) + i2)] = sqrtf(tarray[(((i0 * 6) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(7), T.int64(5), T.int64(10)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([350], \"float32\", \"global\")\n        for i0, i1, i2 in T.grid(7, 5, 10):\n            cse_var_1: T.int32 = i0 * 50 + i1 * 10 + i2\n            compute_1 = T.Buffer((T.int64(350),), data=compute)\n            tarray_1 = T.Buffer((T.int64(350),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(350) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(350) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[350];\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        compute[(((i0 * 50) + (i1 * 10)) + i2)] = sqrtf(tarray[(((i0 * 50) + (i1 * 10)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(4), T.int64(8), T.int64(4)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([128], \"float32\", \"global\")\n        for i0, i1, i2 in T.grid(4, 8, 4):\n            cse_var_1: T.int32 = i0 * 32 + i1 * 4 + i2\n            compute_1 = T.Buffer((T.int64(128),), data=compute)\n            tarray_1 = T.Buffer((T.int64(128),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[128];\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute[(((i0 * 32) + (i1 * 4)) + i2)] = sqrtf(tarray[(((i0 * 32) + (i1 * 4)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(9), T.int64(7)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([378], \"float32\", \"global\")\n        for i0, i1, i2 in T.grid(6, 9, 7):\n            cse_var_1: T.int32 = i0 * 63 + i1 * 7 + i2\n            compute_1 = T.Buffer((T.int64(378),), data=compute)\n            tarray_1 = T.Buffer((T.int64(378),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(378) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(378) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[378];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 63) + (i1 * 7)) + i2)] = sqrtf(tarray[(((i0 * 63) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(6), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([324], \"float32\", \"global\")\n        for i0, i1, i2 in T.grid(6, 6, 9):\n            cse_var_1: T.int32 = i0 * 54 + i1 * 9 + i2\n            compute_1 = T.Buffer((T.int64(324),), data=compute)\n            tarray_1 = T.Buffer((T.int64(324),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[324];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 54) + (i1 * 9)) + i2)] = sqrtf(tarray[(((i0 * 54) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(4), T.int64(9), T.int64(8), T.int64(4)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([1152], \"float32\", \"global\")\n        for i0, i1, i2, i3 in T.grid(4, 9, 8, 4):\n            cse_var_1: T.int32 = i0 * 288 + i1 * 32 + i2 * 4 + i3\n            compute_1 = T.Buffer((T.int64(1152),), data=compute)\n            tarray_1 = T.Buffer((T.int64(1152),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)8) + (((int64_t)((int)threadIdx.x)) >> (int64_t)7)) < (int64_t)9) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = sqrtf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[1152];\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        for (int32_t i3 = 0; i3 < 4; ++i3) {\n          compute[((((i0 * 288) + (i1 * 32)) + (i2 * 4)) + i3)] = sqrtf(tarray[((((i0 * 288) + (i1 * 32)) + (i2 * 4)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(8), T.int64(2), T.int64(10), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([1440], \"float32\", \"global\")\n        for i0, i1, i2, i3 in T.grid(8, 2, 10, 9):\n            cse_var_1: T.int32 = i0 * 180 + i1 * 90 + i2 * 9 + i3\n            compute_1 = T.Buffer((T.int64(1440),), data=compute)\n            tarray_1 = T.Buffer((T.int64(1440),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)32) + (((int64_t)((int)threadIdx.x)) >> (int64_t)5)) < (int64_t)45) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = sqrtf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[1440];\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        for (int32_t i3 = 0; i3 < 9; ++i3) {\n          compute[((((i0 * 180) + (i1 * 90)) + (i2 * 9)) + i3)] = sqrtf(tarray[((((i0 * 180) + (i1 * 90)) + (i2 * 9)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(4), T.int64(5), T.int64(4), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([720], \"float32\", \"global\")\n        for i0, i1, i2, i3 in T.grid(4, 5, 4, 9):\n            cse_var_1: T.int32 = i0 * 180 + i1 * 36 + i2 * 9 + i3\n            compute_1 = T.Buffer((T.int64(720),), data=compute)\n            tarray_1 = T.Buffer((T.int64(720),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(720) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(720) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[720];\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        for (int32_t i3 = 0; i3 < 9; ++i3) {\n          compute[((((i0 * 180) + (i1 * 36)) + (i2 * 9)) + i3)] = sqrtf(tarray[((((i0 * 180) + (i1 * 36)) + (i2 * 9)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(1), T.int64(8), T.int64(3), T.int64(6)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([144], \"float32\", \"global\")\n        for i1, i2, i3 in T.grid(8, 3, 6):\n            cse_var_1: T.int32 = i1 * 18 + i2 * 6 + i3\n            compute_1 = T.Buffer((T.int64(144),), data=compute)\n            tarray_1 = T.Buffer((T.int64(144),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(144) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(144) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[144];\n  for (int32_t i1 = 0; i1 < 8; ++i1) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      for (int32_t i3 = 0; i3 < 6; ++i3) {\n        compute[(((i1 * 18) + (i2 * 6)) + i3)] = sqrtf(tarray[(((i1 * 18) + (i2 * 6)) + i3)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(7), T.int64(7), T.int64(7), T.int64(10)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([3430], \"float32\", \"global\")\n        for i0, i1, i2, i3 in T.grid(7, 7, 7, 10):\n            cse_var_1: T.int32 = i0 * 490 + i1 * 70 + i2 * 10 + i3\n            compute_1 = T.Buffer((T.int64(3430),), data=compute)\n            tarray_1 = T.Buffer((T.int64(3430),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)512) + (((int64_t)((int)threadIdx.x)) >> (int64_t)1)) < (int64_t)1715) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = sqrtf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[3430];\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          compute[((((i0 * 490) + (i1 * 70)) + (i2 * 10)) + i3)] = sqrtf(tarray[((((i0 * 490) + (i1 * 70)) + (i2 * 10)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(2), T.int64(3), T.int64(10), T.int64(7)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([2520], \"float32\", \"global\")\n        for i0, i1, i2, i3, i4 in T.grid(6, 2, 3, 10, 7):\n            cse_var_1: T.int32 = i0 * 420 + i1 * 210 + i2 * 70 + i3 * 7 + i4\n            compute_1 = T.Buffer((T.int64(2520),), data=compute)\n            tarray_1 = T.Buffer((T.int64(2520),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)128) + (((int64_t)((int)threadIdx.x)) >> (int64_t)3)) < (int64_t)315) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = sqrtf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[2520];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          for (int32_t i4 = 0; i4 < 7; ++i4) {\n            compute[(((((i0 * 420) + (i1 * 210)) + (i2 * 70)) + (i3 * 7)) + i4)] = sqrtf(tarray[(((((i0 * 420) + (i1 * 210)) + (i2 * 70)) + (i3 * 7)) + i4)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(10), T.int64(4), T.int64(2), T.int64(2), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([1440], \"float32\", \"global\")\n        for i0, i1, i2, i3, i4 in T.grid(10, 4, 2, 2, 9):\n            cse_var_1: T.int32 = i0 * 144 + i1 * 36 + i2 * 18 + i3 * 9 + i4\n            compute_1 = T.Buffer((T.int64(1440),), data=compute)\n            tarray_1 = T.Buffer((T.int64(1440),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)32) + (((int64_t)((int)threadIdx.x)) >> (int64_t)5)) < (int64_t)45) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = sqrtf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[1440];\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        for (int32_t i3 = 0; i3 < 2; ++i3) {\n          for (int32_t i4 = 0; i4 < 9; ++i4) {\n            compute[(((((i0 * 144) + (i1 * 36)) + (i2 * 18)) + (i3 * 9)) + i4)] = sqrtf(tarray[(((((i0 * 144) + (i1 * 36)) + (i2 * 18)) + (i3 * 9)) + i4)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(8), T.int64(10), T.int64(10), T.int64(3)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([14400], \"float32\", \"global\")\n        for i0, i1, i2, i3, i4 in T.grid(6, 8, 10, 10, 3):\n            cse_var_1: T.int32 = i0 * 2400 + i1 * 300 + i2 * 30 + i3 * 3 + i4\n            compute_1 = T.Buffer((T.int64(14400),), data=compute)\n            tarray_1 = T.Buffer((T.int64(14400),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)16) + (((int64_t)((int)threadIdx.x)) >> (int64_t)6)) < (int64_t)225) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = sqrtf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[14400];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          for (int32_t i4 = 0; i4 < 3; ++i4) {\n            compute[(((((i0 * 2400) + (i1 * 300)) + (i2 * 30)) + (i3 * 3)) + i4)] = sqrtf(tarray[(((((i0 * 2400) + (i1 * 300)) + (i2 * 30)) + (i3 * 3)) + i4)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(7), T.int64(8), T.int64(3), T.int64(3), T.int64(5)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([2520], \"float32\", \"global\")\n        for i0, i1, i2, i3, i4 in T.grid(7, 8, 3, 3, 5):\n            cse_var_1: T.int32 = i0 * 360 + i1 * 45 + i2 * 15 + i3 * 5 + i4\n            compute_1 = T.Buffer((T.int64(2520),), data=compute)\n            tarray_1 = T.Buffer((T.int64(2520),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)128) + (((int64_t)((int)threadIdx.x)) >> (int64_t)3)) < (int64_t)315) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = sqrtf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[2520];\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3 = 0; i3 < 3; ++i3) {\n          for (int32_t i4 = 0; i4 < 5; ++i4) {\n            compute[(((((i0 * 360) + (i1 * 45)) + (i2 * 15)) + (i3 * 5)) + i4)] = sqrtf(tarray[(((((i0 * 360) + (i1 * 45)) + (i2 * 15)) + (i3 * 5)) + i4)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(2), T.int64(10), T.int64(7), T.int64(1), T.int64(7)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([980], \"float32\", \"global\")\n        for i0, i1, i2, i4 in T.grid(2, 10, 7, 7):\n            cse_var_1: T.int32 = i0 * 490 + i1 * 49 + i2 * 7 + i4\n            compute_1 = T.Buffer((T.int64(980),), data=compute)\n            tarray_1 = T.Buffer((T.int64(980),), data=tarray.data)\n            compute_1[cse_var_1] = T.sqrt(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(980) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(980) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[980];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        for (int32_t i4 = 0; i4 < 7; ++i4) {\n          compute[((((i0 * 490) + (i1 * 49)) + (i2 * 7)) + i4)] = sqrtf(tarray[((((i0 * 490) + (i1 * 49)) + (i2 * 7)) + i4)]);\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(2),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([2], \"float32\", \"global\")\n        for i0 in range(2):\n            compute_1 = T.Buffer((T.int64(2),), data=compute, align=8)\n            compute_1[i0] = T.tan(tarray[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[2];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    compute[i0] = tanf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(9),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([9], \"float32\", \"global\")\n        for i0 in range(9):\n            compute_1 = T.Buffer((T.int64(9),), data=compute, align=32)\n            compute_1[i0] = T.tan(tarray[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[9];\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    compute[i0] = tanf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(10),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([10], \"float32\", \"global\")\n        for i0 in range(10):\n            compute_1 = T.Buffer((T.int64(10),), data=compute, align=32)\n            compute_1[i0] = T.tan(tarray[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[10];\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    compute[i0] = tanf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([6], \"float32\", \"global\")\n        for i0 in range(6):\n            compute_1 = T.Buffer((T.int64(6),), data=compute, align=16)\n            compute_1[i0] = T.tan(tarray[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[6];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    compute[i0] = tanf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(4),), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([4], \"float32\", \"global\")\n        for i0 in range(4):\n            compute_1 = T.Buffer((T.int64(4),), data=compute, align=16)\n            compute_1[i0] = T.tan(tarray[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[4];\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    compute[i0] = tanf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(8), T.int64(1)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([8], \"float32\", \"global\")\n        for i0 in range(8):\n            compute_1 = T.Buffer((T.int64(8),), data=compute, align=32)\n            tarray_1 = T.Buffer((T.int64(8),), data=tarray.data)\n            compute_1[i0] = T.tan(tarray_1[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[8];\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    compute[i0] = tanf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(2), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([18], \"float32\", \"global\")\n        for i0, i1 in T.grid(2, 9):\n            cse_var_1: T.int32 = i0 * 9 + i1\n            compute_1 = T.Buffer((T.int64(18),), data=compute)\n            tarray_1 = T.Buffer((T.int64(18),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[18];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      compute[((i0 * 9) + i1)] = tanf(tarray[((i0 * 9) + i1)]);\n    }\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(1)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([6], \"float32\", \"global\")\n        for i0 in range(6):\n            compute_1 = T.Buffer((T.int64(6),), data=compute, align=16)\n            tarray_1 = T.Buffer((T.int64(6),), data=tarray.data)\n            compute_1[i0] = T.tan(tarray_1[i0])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[6];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    compute[i0] = tanf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(9), T.int64(2)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([18], \"float32\", \"global\")\n        for i0, i1 in T.grid(9, 2):\n            cse_var_1: T.int32 = i0 * 2 + i1\n            compute_1 = T.Buffer((T.int64(18),), data=compute)\n            tarray_1 = T.Buffer((T.int64(18),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[18];\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      compute[((i0 * 2) + i1)] = tanf(tarray[((i0 * 2) + i1)]);\n    }\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(3), T.int64(8)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([24], \"float32\", \"global\")\n        for i0, i1 in T.grid(3, 8):\n            cse_var_1: T.int32 = i0 * 8 + i1\n            compute_1 = T.Buffer((T.int64(24),), data=compute)\n            tarray_1 = T.Buffer((T.int64(24),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[24];\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      compute[((i0 * 8) + i1)] = tanf(tarray[((i0 * 8) + i1)]);\n    }\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(2), T.int64(3), T.int64(2)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([12], \"float32\", \"global\")\n        for i0, i1, i2 in T.grid(2, 3, 2):\n            cse_var_1: T.int32 = i0 * 6 + i1 * 2 + i2\n            compute_1 = T.Buffer((T.int64(12),), data=compute, align=32)\n            tarray_1 = T.Buffer((T.int64(12),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[12];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 6) + (i1 * 2)) + i2)] = tanf(tarray[(((i0 * 6) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(7), T.int64(5), T.int64(10)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([350], \"float32\", \"global\")\n        for i0, i1, i2 in T.grid(7, 5, 10):\n            cse_var_1: T.int32 = i0 * 50 + i1 * 10 + i2\n            compute_1 = T.Buffer((T.int64(350),), data=compute)\n            tarray_1 = T.Buffer((T.int64(350),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(350) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(350) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[350];\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        compute[(((i0 * 50) + (i1 * 10)) + i2)] = tanf(tarray[(((i0 * 50) + (i1 * 10)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(4), T.int64(8), T.int64(4)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([128], \"float32\", \"global\")\n        for i0, i1, i2 in T.grid(4, 8, 4):\n            cse_var_1: T.int32 = i0 * 32 + i1 * 4 + i2\n            compute_1 = T.Buffer((T.int64(128),), data=compute)\n            tarray_1 = T.Buffer((T.int64(128),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[128];\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute[(((i0 * 32) + (i1 * 4)) + i2)] = tanf(tarray[(((i0 * 32) + (i1 * 4)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(9), T.int64(7)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([378], \"float32\", \"global\")\n        for i0, i1, i2 in T.grid(6, 9, 7):\n            cse_var_1: T.int32 = i0 * 63 + i1 * 7 + i2\n            compute_1 = T.Buffer((T.int64(378),), data=compute)\n            tarray_1 = T.Buffer((T.int64(378),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(378) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(378) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[378];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 63) + (i1 * 7)) + i2)] = tanf(tarray[(((i0 * 63) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(6), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([324], \"float32\", \"global\")\n        for i0, i1, i2 in T.grid(6, 6, 9):\n            cse_var_1: T.int32 = i0 * 54 + i1 * 9 + i2\n            compute_1 = T.Buffer((T.int64(324),), data=compute)\n            tarray_1 = T.Buffer((T.int64(324),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[324];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 54) + (i1 * 9)) + i2)] = tanf(tarray[(((i0 * 54) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(4), T.int64(9), T.int64(8), T.int64(4)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([1152], \"float32\", \"global\")\n        for i0, i1, i2, i3 in T.grid(4, 9, 8, 4):\n            cse_var_1: T.int32 = i0 * 288 + i1 * 32 + i2 * 4 + i3\n            compute_1 = T.Buffer((T.int64(1152),), data=compute)\n            tarray_1 = T.Buffer((T.int64(1152),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)8) + (((int64_t)((int)threadIdx.x)) >> (int64_t)7)) < (int64_t)9) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = tanf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[1152];\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        for (int32_t i3 = 0; i3 < 4; ++i3) {\n          compute[((((i0 * 288) + (i1 * 32)) + (i2 * 4)) + i3)] = tanf(tarray[((((i0 * 288) + (i1 * 32)) + (i2 * 4)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(8), T.int64(2), T.int64(10), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([1440], \"float32\", \"global\")\n        for i0, i1, i2, i3 in T.grid(8, 2, 10, 9):\n            cse_var_1: T.int32 = i0 * 180 + i1 * 90 + i2 * 9 + i3\n            compute_1 = T.Buffer((T.int64(1440),), data=compute)\n            tarray_1 = T.Buffer((T.int64(1440),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)32) + (((int64_t)((int)threadIdx.x)) >> (int64_t)5)) < (int64_t)45) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = tanf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[1440];\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        for (int32_t i3 = 0; i3 < 9; ++i3) {\n          compute[((((i0 * 180) + (i1 * 90)) + (i2 * 9)) + i3)] = tanf(tarray[((((i0 * 180) + (i1 * 90)) + (i2 * 9)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(4), T.int64(5), T.int64(4), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([720], \"float32\", \"global\")\n        for i0, i1, i2, i3 in T.grid(4, 5, 4, 9):\n            cse_var_1: T.int32 = i0 * 180 + i1 * 36 + i2 * 9 + i3\n            compute_1 = T.Buffer((T.int64(720),), data=compute)\n            tarray_1 = T.Buffer((T.int64(720),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(720) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(720) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[720];\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        for (int32_t i3 = 0; i3 < 9; ++i3) {\n          compute[((((i0 * 180) + (i1 * 36)) + (i2 * 9)) + i3)] = tanf(tarray[((((i0 * 180) + (i1 * 36)) + (i2 * 9)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(1), T.int64(8), T.int64(3), T.int64(6)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([144], \"float32\", \"global\")\n        for i1, i2, i3 in T.grid(8, 3, 6):\n            cse_var_1: T.int32 = i1 * 18 + i2 * 6 + i3\n            compute_1 = T.Buffer((T.int64(144),), data=compute)\n            tarray_1 = T.Buffer((T.int64(144),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(144) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(144) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[144];\n  for (int32_t i1 = 0; i1 < 8; ++i1) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      for (int32_t i3 = 0; i3 < 6; ++i3) {\n        compute[(((i1 * 18) + (i2 * 6)) + i3)] = tanf(tarray[(((i1 * 18) + (i2 * 6)) + i3)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(7), T.int64(7), T.int64(7), T.int64(10)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([3430], \"float32\", \"global\")\n        for i0, i1, i2, i3 in T.grid(7, 7, 7, 10):\n            cse_var_1: T.int32 = i0 * 490 + i1 * 70 + i2 * 10 + i3\n            compute_1 = T.Buffer((T.int64(3430),), data=compute)\n            tarray_1 = T.Buffer((T.int64(3430),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)512) + (((int64_t)((int)threadIdx.x)) >> (int64_t)1)) < (int64_t)1715) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = tanf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[3430];\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          compute[((((i0 * 490) + (i1 * 70)) + (i2 * 10)) + i3)] = tanf(tarray[((((i0 * 490) + (i1 * 70)) + (i2 * 10)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(2), T.int64(3), T.int64(10), T.int64(7)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([2520], \"float32\", \"global\")\n        for i0, i1, i2, i3, i4 in T.grid(6, 2, 3, 10, 7):\n            cse_var_1: T.int32 = i0 * 420 + i1 * 210 + i2 * 70 + i3 * 7 + i4\n            compute_1 = T.Buffer((T.int64(2520),), data=compute)\n            tarray_1 = T.Buffer((T.int64(2520),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)128) + (((int64_t)((int)threadIdx.x)) >> (int64_t)3)) < (int64_t)315) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = tanf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[2520];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          for (int32_t i4 = 0; i4 < 7; ++i4) {\n            compute[(((((i0 * 420) + (i1 * 210)) + (i2 * 70)) + (i3 * 7)) + i4)] = tanf(tarray[(((((i0 * 420) + (i1 * 210)) + (i2 * 70)) + (i3 * 7)) + i4)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(10), T.int64(4), T.int64(2), T.int64(2), T.int64(9)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([1440], \"float32\", \"global\")\n        for i0, i1, i2, i3, i4 in T.grid(10, 4, 2, 2, 9):\n            cse_var_1: T.int32 = i0 * 144 + i1 * 36 + i2 * 18 + i3 * 9 + i4\n            compute_1 = T.Buffer((T.int64(1440),), data=compute)\n            tarray_1 = T.Buffer((T.int64(1440),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)32) + (((int64_t)((int)threadIdx.x)) >> (int64_t)5)) < (int64_t)45) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = tanf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[1440];\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        for (int32_t i3 = 0; i3 < 2; ++i3) {\n          for (int32_t i4 = 0; i4 < 9; ++i4) {\n            compute[(((((i0 * 144) + (i1 * 36)) + (i2 * 18)) + (i3 * 9)) + i4)] = tanf(tarray[(((((i0 * 144) + (i1 * 36)) + (i2 * 18)) + (i3 * 9)) + i4)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(6), T.int64(8), T.int64(10), T.int64(10), T.int64(3)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([14400], \"float32\", \"global\")\n        for i0, i1, i2, i3, i4 in T.grid(6, 8, 10, 10, 3):\n            cse_var_1: T.int32 = i0 * 2400 + i1 * 300 + i2 * 30 + i3 * 3 + i4\n            compute_1 = T.Buffer((T.int64(14400),), data=compute)\n            tarray_1 = T.Buffer((T.int64(14400),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)16) + (((int64_t)((int)threadIdx.x)) >> (int64_t)6)) < (int64_t)225) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = tanf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[14400];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          for (int32_t i4 = 0; i4 < 3; ++i4) {\n            compute[(((((i0 * 2400) + (i1 * 300)) + (i2 * 30)) + (i3 * 3)) + i4)] = tanf(tarray[(((((i0 * 2400) + (i1 * 300)) + (i2 * 30)) + (i3 * 3)) + i4)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(7), T.int64(8), T.int64(3), T.int64(3), T.int64(5)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([2520], \"float32\", \"global\")\n        for i0, i1, i2, i3, i4 in T.grid(7, 8, 3, 3, 5):\n            cse_var_1: T.int32 = i0 * 360 + i1 * 45 + i2 * 15 + i3 * 5 + i4\n            compute_1 = T.Buffer((T.int64(2520),), data=compute)\n            tarray_1 = T.Buffer((T.int64(2520),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)128) + (((int64_t)((int)threadIdx.x)) >> (int64_t)3)) < (int64_t)315) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = tanf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[2520];\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3 = 0; i3 < 3; ++i3) {\n          for (int32_t i4 = 0; i4 < 5; ++i4) {\n            compute[(((((i0 * 360) + (i1 * 45)) + (i2 * 15)) + (i3 * 5)) + i4)] = tanf(tarray[(((((i0 * 360) + (i1 * 45)) + (i2 * 15)) + (i3 * 5)) + i4)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tan"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(tarray: T.Buffer((T.int64(2), T.int64(10), T.int64(7), T.int64(1), T.int64(7)), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([980], \"float32\", \"global\")\n        for i0, i1, i2, i4 in T.grid(2, 10, 7, 7):\n            cse_var_1: T.int32 = i0 * 490 + i1 * 49 + i2 * 7 + i4\n            compute_1 = T.Buffer((T.int64(980),), data=compute)\n            tarray_1 = T.Buffer((T.int64(980),), data=tarray.data)\n            compute_1[cse_var_1] = T.tan(tarray_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(980) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(980) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanf(tarray[((int)threadIdx.x)]);\n}\n\n\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[980];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        for (int32_t i4 = 0; i4 < 7; ++i4) {\n          compute[((((i0 * 490) + (i1 * 49)) + (i2 * 7)) + i4)] = tanf(tarray[((((i0 * 490) + (i1 * 49)) + (i2 * 7)) + i4)]);\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tan"
  }
]