[
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 20, 12, 36), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([40], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((40,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(40):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(1512):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 40\n            data_1 = T.Buffer((60480,), data=data.data)\n            data_red_rf_1[0] = data_red_rf_1[0] + data_1[cse_var_1]\n            data_red_rf_1[1] = data_red_rf_1[1] + data_1[cse_var_1 + 1]\n            data_red_rf_1[2] = data_red_rf_1[2] + data_1[cse_var_1 + 2]\n            data_red_rf_1[3] = data_red_rf_1[3] + data_1[cse_var_1 + 3]\n            data_red_rf_1[4] = data_red_rf_1[4] + data_1[cse_var_1 + 4]\n            data_red_rf_1[5] = data_red_rf_1[5] + data_1[cse_var_1 + 5]\n            data_red_rf_1[6] = data_red_rf_1[6] + data_1[cse_var_1 + 6]\n            data_red_rf_1[7] = data_red_rf_1[7] + data_1[cse_var_1 + 7]\n            data_red_rf_1[8] = data_red_rf_1[8] + data_1[cse_var_1 + 8]\n            data_red_rf_1[9] = data_red_rf_1[9] + data_1[cse_var_1 + 9]\n            data_red_rf_1[10] = data_red_rf_1[10] + data_1[cse_var_1 + 10]\n            data_red_rf_1[11] = data_red_rf_1[11] + data_1[cse_var_1 + 11]\n            data_red_rf_1[12] = data_red_rf_1[12] + data_1[cse_var_1 + 12]\n            data_red_rf_1[13] = data_red_rf_1[13] + data_1[cse_var_1 + 13]\n            data_red_rf_1[14] = data_red_rf_1[14] + data_1[cse_var_1 + 14]\n            data_red_rf_1[15] = data_red_rf_1[15] + data_1[cse_var_1 + 15]\n            data_red_rf_1[16] = data_red_rf_1[16] + data_1[cse_var_1 + 16]\n            data_red_rf_1[17] = data_red_rf_1[17] + data_1[cse_var_1 + 17]\n            data_red_rf_1[18] = data_red_rf_1[18] + data_1[cse_var_1 + 18]\n            data_red_rf_1[19] = data_red_rf_1[19] + data_1[cse_var_1 + 19]\n            data_red_rf_1[20] = data_red_rf_1[20] + data_1[cse_var_1 + 20]\n            data_red_rf_1[21] = data_red_rf_1[21] + data_1[cse_var_1 + 21]\n            data_red_rf_1[22] = data_red_rf_1[22] + data_1[cse_var_1 + 22]\n            data_red_rf_1[23] = data_red_rf_1[23] + data_1[cse_var_1 + 23]\n            data_red_rf_1[24] = data_red_rf_1[24] + data_1[cse_var_1 + 24]\n            data_red_rf_1[25] = data_red_rf_1[25] + data_1[cse_var_1 + 25]\n            data_red_rf_1[26] = data_red_rf_1[26] + data_1[cse_var_1 + 26]\n            data_red_rf_1[27] = data_red_rf_1[27] + data_1[cse_var_1 + 27]\n            data_red_rf_1[28] = data_red_rf_1[28] + data_1[cse_var_1 + 28]\n            data_red_rf_1[29] = data_red_rf_1[29] + data_1[cse_var_1 + 29]\n            data_red_rf_1[30] = data_red_rf_1[30] + data_1[cse_var_1 + 30]\n            data_red_rf_1[31] = data_red_rf_1[31] + data_1[cse_var_1 + 31]\n            data_red_rf_1[32] = data_red_rf_1[32] + data_1[cse_var_1 + 32]\n            data_red_rf_1[33] = data_red_rf_1[33] + data_1[cse_var_1 + 33]\n            data_red_rf_1[34] = data_red_rf_1[34] + data_1[cse_var_1 + 34]\n            data_red_rf_1[35] = data_red_rf_1[35] + data_1[cse_var_1 + 35]\n            data_red_rf_1[36] = data_red_rf_1[36] + data_1[cse_var_1 + 36]\n            data_red_rf_1[37] = data_red_rf_1[37] + data_1[cse_var_1 + 37]\n            data_red_rf_1[38] = data_red_rf_1[38] + data_1[cse_var_1 + 38]\n            data_red_rf_1[39] = data_red_rf_1[39] + data_1[cse_var_1 + 39]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[0]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[1]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[2]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[3]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[4]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[5]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[6]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[7]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[8]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[9]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[10]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[11]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[12]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[13]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[14]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[15]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[16]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[17]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[18]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[19]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[20]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[21]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[22]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[23]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[24]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[25]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[26]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[27]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[28]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[29]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[30]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[31]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[32]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[33]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[34]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[35]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[36]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[37]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[38]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[39]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1890; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[40];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 40; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1512; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 40);\n    data_red_rf[0] = (data_red_rf[0] + ((float*)data_1)[cse_var_1]);\n    data_red_rf[1] = (data_red_rf[1] + ((float*)data_1)[(cse_var_1 + 1)]);\n    data_red_rf[2] = (data_red_rf[2] + ((float*)data_1)[(cse_var_1 + 2)]);\n    data_red_rf[3] = (data_red_rf[3] + ((float*)data_1)[(cse_var_1 + 3)]);\n    data_red_rf[4] = (data_red_rf[4] + ((float*)data_1)[(cse_var_1 + 4)]);\n    data_red_rf[5] = (data_red_rf[5] + ((float*)data_1)[(cse_var_1 + 5)]);\n    data_red_rf[6] = (data_red_rf[6] + ((float*)data_1)[(cse_var_1 + 6)]);\n    data_red_rf[7] = (data_red_rf[7] + ((float*)data_1)[(cse_var_1 + 7)]);\n    data_red_rf[8] = (data_red_rf[8] + ((float*)data_1)[(cse_var_1 + 8)]);\n    data_red_rf[9] = (data_red_rf[9] + ((float*)data_1)[(cse_var_1 + 9)]);\n    data_red_rf[10] = (data_red_rf[10] + ((float*)data_1)[(cse_var_1 + 10)]);\n    data_red_rf[11] = (data_red_rf[11] + ((float*)data_1)[(cse_var_1 + 11)]);\n    data_red_rf[12] = (data_red_rf[12] + ((float*)data_1)[(cse_var_1 + 12)]);\n    data_red_rf[13] = (data_red_rf[13] + ((float*)data_1)[(cse_var_1 + 13)]);\n    data_red_rf[14] = (data_red_rf[14] + ((float*)data_1)[(cse_var_1 + 14)]);\n    data_red_rf[15] = (data_red_rf[15] + ((float*)data_1)[(cse_var_1 + 15)]);\n    data_red_rf[16] = (data_red_rf[16] + ((float*)data_1)[(cse_var_1 + 16)]);\n    data_red_rf[17] = (data_red_rf[17] + ((float*)data_1)[(cse_var_1 + 17)]);\n    data_red_rf[18] = (data_red_rf[18] + ((float*)data_1)[(cse_var_1 + 18)]);\n    data_red_rf[19] = (data_red_rf[19] + ((float*)data_1)[(cse_var_1 + 19)]);\n    data_red_rf[20] = (data_red_rf[20] + ((float*)data_1)[(cse_var_1 + 20)]);\n    data_red_rf[21] = (data_red_rf[21] + ((float*)data_1)[(cse_var_1 + 21)]);\n    data_red_rf[22] = (data_red_rf[22] + ((float*)data_1)[(cse_var_1 + 22)]);\n    data_red_rf[23] = (data_red_rf[23] + ((float*)data_1)[(cse_var_1 + 23)]);\n    data_red_rf[24] = (data_red_rf[24] + ((float*)data_1)[(cse_var_1 + 24)]);\n    data_red_rf[25] = (data_red_rf[25] + ((float*)data_1)[(cse_var_1 + 25)]);\n    data_red_rf[26] = (data_red_rf[26] + ((float*)data_1)[(cse_var_1 + 26)]);\n    data_red_rf[27] = (data_red_rf[27] + ((float*)data_1)[(cse_var_1 + 27)]);\n    data_red_rf[28] = (data_red_rf[28] + ((float*)data_1)[(cse_var_1 + 28)]);\n    data_red_rf[29] = (data_red_rf[29] + ((float*)data_1)[(cse_var_1 + 29)]);\n    data_red_rf[30] = (data_red_rf[30] + ((float*)data_1)[(cse_var_1 + 30)]);\n    data_red_rf[31] = (data_red_rf[31] + ((float*)data_1)[(cse_var_1 + 31)]);\n    data_red_rf[32] = (data_red_rf[32] + ((float*)data_1)[(cse_var_1 + 32)]);\n    data_red_rf[33] = (data_red_rf[33] + ((float*)data_1)[(cse_var_1 + 33)]);\n    data_red_rf[34] = (data_red_rf[34] + ((float*)data_1)[(cse_var_1 + 34)]);\n    data_red_rf[35] = (data_red_rf[35] + ((float*)data_1)[(cse_var_1 + 35)]);\n    data_red_rf[36] = (data_red_rf[36] + ((float*)data_1)[(cse_var_1 + 36)]);\n    data_red_rf[37] = (data_red_rf[37] + ((float*)data_1)[(cse_var_1 + 37)]);\n    data_red_rf[38] = (data_red_rf[38] + ((float*)data_1)[(cse_var_1 + 38)]);\n    data_red_rf[39] = (data_red_rf[39] + ((float*)data_1)[(cse_var_1 + 39)]);\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[0]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[1]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[2]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[3]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[4]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[5]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[6]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[7]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[8]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[9]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[10]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[11]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[12]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[13]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[14]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[15]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[16]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[17]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[18]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[19]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[20]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[21]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[22]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[23]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[24]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[25]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[26]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[27]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[28]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[29]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[30]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[31]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[32]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[33]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[34]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[35]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[36]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[37]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[38]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[39]);\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sum[7__20__12__36]"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 12, 4, 36), \"float32\"), compute: T.Buffer((1, 12, 4, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(12, 4, 36):\n            cse_var_1: T.int32 = i1 * 144 + i2 * 36 + i3\n            compute_1 = T.Buffer((1728,), data=compute.data)\n            data_1 = T.Buffer((1728,), data=data.data)\n            compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i1 = 0; i1 < 12; ++i1) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 36; ++i3) {\n        int32_t cse_var_1 = (((i1 * 144) + (i2 * 36)) + i3);\n        ((float*)compute_1)[cse_var_1] = coshf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cosh"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 12, 12, 8), \"float32\"), compute: T.Buffer((3, 12, 12, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(432):\n            for i3 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 8 + i3\n                compute_1 = T.Buffer((3456,), data=compute.data)\n                data_1 = T.Buffer((3456,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 432; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 8; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 8) + i3);\n      ((float*)compute_1)[cse_var_1] = coshf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cosh"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 20, 4, 4), \"float32\"), compute: T.Buffer((2, 20, 4, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(40):\n            for i2, i3 in T.grid(4, 4):\n                cse_var_1: T.int32 = i0_i1_fused * 16 + i2 * 4 + i3\n                compute_1 = T.Buffer((640,), data=compute.data)\n                data_1 = T.Buffer((640,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 40; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 4; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 16) + (i2 * 4)) + i3);\n        ((float*)compute_1)[cse_var_1] = coshf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cosh"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 40, 24, 40), \"float32\"), compute: T.Buffer((4, 40, 24, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(3840):\n            for i3 in range(40):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 40 + i3\n                compute_1 = T.Buffer((153600,), data=compute.data)\n                data_1 = T.Buffer((153600,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3840; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 40; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 40) + i3);\n      ((float*)compute_1)[cse_var_1] = coshf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cosh"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 24, 4, 20), \"float32\"), compute: T.Buffer((5, 24, 4, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(120):\n            for i2, i3 in T.grid(4, 20):\n                cse_var_1: T.int32 = i0_i1_fused * 80 + i2 * 20 + i3\n                compute_1 = T.Buffer((9600,), data=compute.data)\n                data_1 = T.Buffer((9600,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 120; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 80) + (i2 * 20)) + i3);\n        ((float*)compute_1)[cse_var_1] = coshf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cosh"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 28, 4, 28), \"float32\"), compute: T.Buffer((8, 28, 4, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(896):\n            for i3 in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                compute_1 = T.Buffer((25088,), data=compute.data)\n                data_1 = T.Buffer((25088,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 896; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 28) + i3);\n      ((float*)compute_1)[cse_var_1] = coshf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cosh"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 20, 20, 4), \"float32\"), compute: T.Buffer((2, 20, 20, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(40):\n            for i2, i3 in T.grid(20, 4):\n                cse_var_1: T.int32 = i0_i1_fused * 80 + i2 * 4 + i3\n                compute_1 = T.Buffer((3200,), data=compute.data)\n                data_1 = T.Buffer((3200,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 40; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      for (int32_t i3 = 0; i3 < 4; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 80) + (i2 * 4)) + i3);\n        ((float*)compute_1)[cse_var_1] = coshf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cosh"
  }
]