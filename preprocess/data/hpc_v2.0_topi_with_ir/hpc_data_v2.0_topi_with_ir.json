[
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 16, 16, 12), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([4], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((4,), data=data_red_rf, align=16)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(4):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(3072):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 4\n            data_1 = T.Buffer((12288,), data=data.data)\n            data_red_rf_1[0] = data_red_rf_1[0] + data_1[cse_var_1]\n            data_red_rf_1[1] = data_red_rf_1[1] + data_1[cse_var_1 + 1]\n            data_red_rf_1[2] = data_red_rf_1[2] + data_1[cse_var_1 + 2]\n            data_red_rf_1[3] = data_red_rf_1[3] + data_1[cse_var_1 + 3]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[0]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[1]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[2]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[3]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 384; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[4];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 4; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 3072; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 4);\n    data_red_rf[0] = (data_red_rf[0] + ((float*)data_1)[cse_var_1]);\n    data_red_rf[1] = (data_red_rf[1] + ((float*)data_1)[(cse_var_1 + 1)]);\n    data_red_rf[2] = (data_red_rf[2] + ((float*)data_1)[(cse_var_1 + 2)]);\n    data_red_rf[3] = (data_red_rf[3] + ((float*)data_1)[(cse_var_1 + 3)]);\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[0]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[1]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[2]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[3]);\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sum[4__16__16__12]"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 20, 12, 36), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([40], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((40,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(40):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(1512):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 40\n            data_1 = T.Buffer((60480,), data=data.data)\n            data_red_rf_1[0] = data_red_rf_1[0] + data_1[cse_var_1]\n            data_red_rf_1[1] = data_red_rf_1[1] + data_1[cse_var_1 + 1]\n            data_red_rf_1[2] = data_red_rf_1[2] + data_1[cse_var_1 + 2]\n            data_red_rf_1[3] = data_red_rf_1[3] + data_1[cse_var_1 + 3]\n            data_red_rf_1[4] = data_red_rf_1[4] + data_1[cse_var_1 + 4]\n            data_red_rf_1[5] = data_red_rf_1[5] + data_1[cse_var_1 + 5]\n            data_red_rf_1[6] = data_red_rf_1[6] + data_1[cse_var_1 + 6]\n            data_red_rf_1[7] = data_red_rf_1[7] + data_1[cse_var_1 + 7]\n            data_red_rf_1[8] = data_red_rf_1[8] + data_1[cse_var_1 + 8]\n            data_red_rf_1[9] = data_red_rf_1[9] + data_1[cse_var_1 + 9]\n            data_red_rf_1[10] = data_red_rf_1[10] + data_1[cse_var_1 + 10]\n            data_red_rf_1[11] = data_red_rf_1[11] + data_1[cse_var_1 + 11]\n            data_red_rf_1[12] = data_red_rf_1[12] + data_1[cse_var_1 + 12]\n            data_red_rf_1[13] = data_red_rf_1[13] + data_1[cse_var_1 + 13]\n            data_red_rf_1[14] = data_red_rf_1[14] + data_1[cse_var_1 + 14]\n            data_red_rf_1[15] = data_red_rf_1[15] + data_1[cse_var_1 + 15]\n            data_red_rf_1[16] = data_red_rf_1[16] + data_1[cse_var_1 + 16]\n            data_red_rf_1[17] = data_red_rf_1[17] + data_1[cse_var_1 + 17]\n            data_red_rf_1[18] = data_red_rf_1[18] + data_1[cse_var_1 + 18]\n            data_red_rf_1[19] = data_red_rf_1[19] + data_1[cse_var_1 + 19]\n            data_red_rf_1[20] = data_red_rf_1[20] + data_1[cse_var_1 + 20]\n            data_red_rf_1[21] = data_red_rf_1[21] + data_1[cse_var_1 + 21]\n            data_red_rf_1[22] = data_red_rf_1[22] + data_1[cse_var_1 + 22]\n            data_red_rf_1[23] = data_red_rf_1[23] + data_1[cse_var_1 + 23]\n            data_red_rf_1[24] = data_red_rf_1[24] + data_1[cse_var_1 + 24]\n            data_red_rf_1[25] = data_red_rf_1[25] + data_1[cse_var_1 + 25]\n            data_red_rf_1[26] = data_red_rf_1[26] + data_1[cse_var_1 + 26]\n            data_red_rf_1[27] = data_red_rf_1[27] + data_1[cse_var_1 + 27]\n            data_red_rf_1[28] = data_red_rf_1[28] + data_1[cse_var_1 + 28]\n            data_red_rf_1[29] = data_red_rf_1[29] + data_1[cse_var_1 + 29]\n            data_red_rf_1[30] = data_red_rf_1[30] + data_1[cse_var_1 + 30]\n            data_red_rf_1[31] = data_red_rf_1[31] + data_1[cse_var_1 + 31]\n            data_red_rf_1[32] = data_red_rf_1[32] + data_1[cse_var_1 + 32]\n            data_red_rf_1[33] = data_red_rf_1[33] + data_1[cse_var_1 + 33]\n            data_red_rf_1[34] = data_red_rf_1[34] + data_1[cse_var_1 + 34]\n            data_red_rf_1[35] = data_red_rf_1[35] + data_1[cse_var_1 + 35]\n            data_red_rf_1[36] = data_red_rf_1[36] + data_1[cse_var_1 + 36]\n            data_red_rf_1[37] = data_red_rf_1[37] + data_1[cse_var_1 + 37]\n            data_red_rf_1[38] = data_red_rf_1[38] + data_1[cse_var_1 + 38]\n            data_red_rf_1[39] = data_red_rf_1[39] + data_1[cse_var_1 + 39]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[0]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[1]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[2]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[3]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[4]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[5]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[6]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[7]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[8]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[9]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[10]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[11]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[12]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[13]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[14]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[15]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[16]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[17]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[18]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[19]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[20]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[21]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[22]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[23]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[24]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[25]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[26]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[27]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[28]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[29]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[30]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[31]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[32]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[33]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[34]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[35]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[36]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[37]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[38]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[39]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1890; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[40];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 40; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1512; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 40);\n    data_red_rf[0] = (data_red_rf[0] + ((float*)data_1)[cse_var_1]);\n    data_red_rf[1] = (data_red_rf[1] + ((float*)data_1)[(cse_var_1 + 1)]);\n    data_red_rf[2] = (data_red_rf[2] + ((float*)data_1)[(cse_var_1 + 2)]);\n    data_red_rf[3] = (data_red_rf[3] + ((float*)data_1)[(cse_var_1 + 3)]);\n    data_red_rf[4] = (data_red_rf[4] + ((float*)data_1)[(cse_var_1 + 4)]);\n    data_red_rf[5] = (data_red_rf[5] + ((float*)data_1)[(cse_var_1 + 5)]);\n    data_red_rf[6] = (data_red_rf[6] + ((float*)data_1)[(cse_var_1 + 6)]);\n    data_red_rf[7] = (data_red_rf[7] + ((float*)data_1)[(cse_var_1 + 7)]);\n    data_red_rf[8] = (data_red_rf[8] + ((float*)data_1)[(cse_var_1 + 8)]);\n    data_red_rf[9] = (data_red_rf[9] + ((float*)data_1)[(cse_var_1 + 9)]);\n    data_red_rf[10] = (data_red_rf[10] + ((float*)data_1)[(cse_var_1 + 10)]);\n    data_red_rf[11] = (data_red_rf[11] + ((float*)data_1)[(cse_var_1 + 11)]);\n    data_red_rf[12] = (data_red_rf[12] + ((float*)data_1)[(cse_var_1 + 12)]);\n    data_red_rf[13] = (data_red_rf[13] + ((float*)data_1)[(cse_var_1 + 13)]);\n    data_red_rf[14] = (data_red_rf[14] + ((float*)data_1)[(cse_var_1 + 14)]);\n    data_red_rf[15] = (data_red_rf[15] + ((float*)data_1)[(cse_var_1 + 15)]);\n    data_red_rf[16] = (data_red_rf[16] + ((float*)data_1)[(cse_var_1 + 16)]);\n    data_red_rf[17] = (data_red_rf[17] + ((float*)data_1)[(cse_var_1 + 17)]);\n    data_red_rf[18] = (data_red_rf[18] + ((float*)data_1)[(cse_var_1 + 18)]);\n    data_red_rf[19] = (data_red_rf[19] + ((float*)data_1)[(cse_var_1 + 19)]);\n    data_red_rf[20] = (data_red_rf[20] + ((float*)data_1)[(cse_var_1 + 20)]);\n    data_red_rf[21] = (data_red_rf[21] + ((float*)data_1)[(cse_var_1 + 21)]);\n    data_red_rf[22] = (data_red_rf[22] + ((float*)data_1)[(cse_var_1 + 22)]);\n    data_red_rf[23] = (data_red_rf[23] + ((float*)data_1)[(cse_var_1 + 23)]);\n    data_red_rf[24] = (data_red_rf[24] + ((float*)data_1)[(cse_var_1 + 24)]);\n    data_red_rf[25] = (data_red_rf[25] + ((float*)data_1)[(cse_var_1 + 25)]);\n    data_red_rf[26] = (data_red_rf[26] + ((float*)data_1)[(cse_var_1 + 26)]);\n    data_red_rf[27] = (data_red_rf[27] + ((float*)data_1)[(cse_var_1 + 27)]);\n    data_red_rf[28] = (data_red_rf[28] + ((float*)data_1)[(cse_var_1 + 28)]);\n    data_red_rf[29] = (data_red_rf[29] + ((float*)data_1)[(cse_var_1 + 29)]);\n    data_red_rf[30] = (data_red_rf[30] + ((float*)data_1)[(cse_var_1 + 30)]);\n    data_red_rf[31] = (data_red_rf[31] + ((float*)data_1)[(cse_var_1 + 31)]);\n    data_red_rf[32] = (data_red_rf[32] + ((float*)data_1)[(cse_var_1 + 32)]);\n    data_red_rf[33] = (data_red_rf[33] + ((float*)data_1)[(cse_var_1 + 33)]);\n    data_red_rf[34] = (data_red_rf[34] + ((float*)data_1)[(cse_var_1 + 34)]);\n    data_red_rf[35] = (data_red_rf[35] + ((float*)data_1)[(cse_var_1 + 35)]);\n    data_red_rf[36] = (data_red_rf[36] + ((float*)data_1)[(cse_var_1 + 36)]);\n    data_red_rf[37] = (data_red_rf[37] + ((float*)data_1)[(cse_var_1 + 37)]);\n    data_red_rf[38] = (data_red_rf[38] + ((float*)data_1)[(cse_var_1 + 38)]);\n    data_red_rf[39] = (data_red_rf[39] + ((float*)data_1)[(cse_var_1 + 39)]);\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[0]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[1]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[2]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[3]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[4]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[5]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[6]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[7]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[8]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[9]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[10]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[11]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[12]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[13]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[14]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[15]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[16]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[17]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[18]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[19]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[20]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[21]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[22]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[23]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[24]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[25]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[26]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[27]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[28]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[29]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[30]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[31]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[32]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[33]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[34]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[35]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[36]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[37]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[38]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[39]);\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sum[7__20__12__36]"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 24, 12, 24), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([64], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((64,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(64):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(216, 64):\n            data_1 = T.Buffer((13824,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] + data_1[k0_k1_fused_k2_fused_k3_fused_outer * 64 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(64):\n            data_red_1[0] = data_red_1[0] + data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 432; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[64];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 216; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 64; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 64) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sum[2__24__12__24]"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 8, 16, 40), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([64], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((64,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(64):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(720):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 64\n            data_1 = T.Buffer((46080,), data=data.data)\n            data_red_rf_1[0] = data_red_rf_1[0] + data_1[cse_var_1]\n            data_red_rf_1[1] = data_red_rf_1[1] + data_1[cse_var_1 + 1]\n            data_red_rf_1[2] = data_red_rf_1[2] + data_1[cse_var_1 + 2]\n            data_red_rf_1[3] = data_red_rf_1[3] + data_1[cse_var_1 + 3]\n            data_red_rf_1[4] = data_red_rf_1[4] + data_1[cse_var_1 + 4]\n            data_red_rf_1[5] = data_red_rf_1[5] + data_1[cse_var_1 + 5]\n            data_red_rf_1[6] = data_red_rf_1[6] + data_1[cse_var_1 + 6]\n            data_red_rf_1[7] = data_red_rf_1[7] + data_1[cse_var_1 + 7]\n            data_red_rf_1[8] = data_red_rf_1[8] + data_1[cse_var_1 + 8]\n            data_red_rf_1[9] = data_red_rf_1[9] + data_1[cse_var_1 + 9]\n            data_red_rf_1[10] = data_red_rf_1[10] + data_1[cse_var_1 + 10]\n            data_red_rf_1[11] = data_red_rf_1[11] + data_1[cse_var_1 + 11]\n            data_red_rf_1[12] = data_red_rf_1[12] + data_1[cse_var_1 + 12]\n            data_red_rf_1[13] = data_red_rf_1[13] + data_1[cse_var_1 + 13]\n            data_red_rf_1[14] = data_red_rf_1[14] + data_1[cse_var_1 + 14]\n            data_red_rf_1[15] = data_red_rf_1[15] + data_1[cse_var_1 + 15]\n            data_red_rf_1[16] = data_red_rf_1[16] + data_1[cse_var_1 + 16]\n            data_red_rf_1[17] = data_red_rf_1[17] + data_1[cse_var_1 + 17]\n            data_red_rf_1[18] = data_red_rf_1[18] + data_1[cse_var_1 + 18]\n            data_red_rf_1[19] = data_red_rf_1[19] + data_1[cse_var_1 + 19]\n            data_red_rf_1[20] = data_red_rf_1[20] + data_1[cse_var_1 + 20]\n            data_red_rf_1[21] = data_red_rf_1[21] + data_1[cse_var_1 + 21]\n            data_red_rf_1[22] = data_red_rf_1[22] + data_1[cse_var_1 + 22]\n            data_red_rf_1[23] = data_red_rf_1[23] + data_1[cse_var_1 + 23]\n            data_red_rf_1[24] = data_red_rf_1[24] + data_1[cse_var_1 + 24]\n            data_red_rf_1[25] = data_red_rf_1[25] + data_1[cse_var_1 + 25]\n            data_red_rf_1[26] = data_red_rf_1[26] + data_1[cse_var_1 + 26]\n            data_red_rf_1[27] = data_red_rf_1[27] + data_1[cse_var_1 + 27]\n            data_red_rf_1[28] = data_red_rf_1[28] + data_1[cse_var_1 + 28]\n            data_red_rf_1[29] = data_red_rf_1[29] + data_1[cse_var_1 + 29]\n            data_red_rf_1[30] = data_red_rf_1[30] + data_1[cse_var_1 + 30]\n            data_red_rf_1[31] = data_red_rf_1[31] + data_1[cse_var_1 + 31]\n            data_red_rf_1[32] = data_red_rf_1[32] + data_1[cse_var_1 + 32]\n            data_red_rf_1[33] = data_red_rf_1[33] + data_1[cse_var_1 + 33]\n            data_red_rf_1[34] = data_red_rf_1[34] + data_1[cse_var_1 + 34]\n            data_red_rf_1[35] = data_red_rf_1[35] + data_1[cse_var_1 + 35]\n            data_red_rf_1[36] = data_red_rf_1[36] + data_1[cse_var_1 + 36]\n            data_red_rf_1[37] = data_red_rf_1[37] + data_1[cse_var_1 + 37]\n            data_red_rf_1[38] = data_red_rf_1[38] + data_1[cse_var_1 + 38]\n            data_red_rf_1[39] = data_red_rf_1[39] + data_1[cse_var_1 + 39]\n            data_red_rf_1[40] = data_red_rf_1[40] + data_1[cse_var_1 + 40]\n            data_red_rf_1[41] = data_red_rf_1[41] + data_1[cse_var_1 + 41]\n            data_red_rf_1[42] = data_red_rf_1[42] + data_1[cse_var_1 + 42]\n            data_red_rf_1[43] = data_red_rf_1[43] + data_1[cse_var_1 + 43]\n            data_red_rf_1[44] = data_red_rf_1[44] + data_1[cse_var_1 + 44]\n            data_red_rf_1[45] = data_red_rf_1[45] + data_1[cse_var_1 + 45]\n            data_red_rf_1[46] = data_red_rf_1[46] + data_1[cse_var_1 + 46]\n            data_red_rf_1[47] = data_red_rf_1[47] + data_1[cse_var_1 + 47]\n            data_red_rf_1[48] = data_red_rf_1[48] + data_1[cse_var_1 + 48]\n            data_red_rf_1[49] = data_red_rf_1[49] + data_1[cse_var_1 + 49]\n            data_red_rf_1[50] = data_red_rf_1[50] + data_1[cse_var_1 + 50]\n            data_red_rf_1[51] = data_red_rf_1[51] + data_1[cse_var_1 + 51]\n            data_red_rf_1[52] = data_red_rf_1[52] + data_1[cse_var_1 + 52]\n            data_red_rf_1[53] = data_red_rf_1[53] + data_1[cse_var_1 + 53]\n            data_red_rf_1[54] = data_red_rf_1[54] + data_1[cse_var_1 + 54]\n            data_red_rf_1[55] = data_red_rf_1[55] + data_1[cse_var_1 + 55]\n            data_red_rf_1[56] = data_red_rf_1[56] + data_1[cse_var_1 + 56]\n            data_red_rf_1[57] = data_red_rf_1[57] + data_1[cse_var_1 + 57]\n            data_red_rf_1[58] = data_red_rf_1[58] + data_1[cse_var_1 + 58]\n            data_red_rf_1[59] = data_red_rf_1[59] + data_1[cse_var_1 + 59]\n            data_red_rf_1[60] = data_red_rf_1[60] + data_1[cse_var_1 + 60]\n            data_red_rf_1[61] = data_red_rf_1[61] + data_1[cse_var_1 + 61]\n            data_red_rf_1[62] = data_red_rf_1[62] + data_1[cse_var_1 + 62]\n            data_red_rf_1[63] = data_red_rf_1[63] + data_1[cse_var_1 + 63]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(64):\n            data_red_1[0] = data_red_1[0] + data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1440; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[64];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 720; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 64);\n    data_red_rf[0] = (data_red_rf[0] + ((float*)data_1)[cse_var_1]);\n    data_red_rf[1] = (data_red_rf[1] + ((float*)data_1)[(cse_var_1 + 1)]);\n    data_red_rf[2] = (data_red_rf[2] + ((float*)data_1)[(cse_var_1 + 2)]);\n    data_red_rf[3] = (data_red_rf[3] + ((float*)data_1)[(cse_var_1 + 3)]);\n    data_red_rf[4] = (data_red_rf[4] + ((float*)data_1)[(cse_var_1 + 4)]);\n    data_red_rf[5] = (data_red_rf[5] + ((float*)data_1)[(cse_var_1 + 5)]);\n    data_red_rf[6] = (data_red_rf[6] + ((float*)data_1)[(cse_var_1 + 6)]);\n    data_red_rf[7] = (data_red_rf[7] + ((float*)data_1)[(cse_var_1 + 7)]);\n    data_red_rf[8] = (data_red_rf[8] + ((float*)data_1)[(cse_var_1 + 8)]);\n    data_red_rf[9] = (data_red_rf[9] + ((float*)data_1)[(cse_var_1 + 9)]);\n    data_red_rf[10] = (data_red_rf[10] + ((float*)data_1)[(cse_var_1 + 10)]);\n    data_red_rf[11] = (data_red_rf[11] + ((float*)data_1)[(cse_var_1 + 11)]);\n    data_red_rf[12] = (data_red_rf[12] + ((float*)data_1)[(cse_var_1 + 12)]);\n    data_red_rf[13] = (data_red_rf[13] + ((float*)data_1)[(cse_var_1 + 13)]);\n    data_red_rf[14] = (data_red_rf[14] + ((float*)data_1)[(cse_var_1 + 14)]);\n    data_red_rf[15] = (data_red_rf[15] + ((float*)data_1)[(cse_var_1 + 15)]);\n    data_red_rf[16] = (data_red_rf[16] + ((float*)data_1)[(cse_var_1 + 16)]);\n    data_red_rf[17] = (data_red_rf[17] + ((float*)data_1)[(cse_var_1 + 17)]);\n    data_red_rf[18] = (data_red_rf[18] + ((float*)data_1)[(cse_var_1 + 18)]);\n    data_red_rf[19] = (data_red_rf[19] + ((float*)data_1)[(cse_var_1 + 19)]);\n    data_red_rf[20] = (data_red_rf[20] + ((float*)data_1)[(cse_var_1 + 20)]);\n    data_red_rf[21] = (data_red_rf[21] + ((float*)data_1)[(cse_var_1 + 21)]);\n    data_red_rf[22] = (data_red_rf[22] + ((float*)data_1)[(cse_var_1 + 22)]);\n    data_red_rf[23] = (data_red_rf[23] + ((float*)data_1)[(cse_var_1 + 23)]);\n    data_red_rf[24] = (data_red_rf[24] + ((float*)data_1)[(cse_var_1 + 24)]);\n    data_red_rf[25] = (data_red_rf[25] + ((float*)data_1)[(cse_var_1 + 25)]);\n    data_red_rf[26] = (data_red_rf[26] + ((float*)data_1)[(cse_var_1 + 26)]);\n    data_red_rf[27] = (data_red_rf[27] + ((float*)data_1)[(cse_var_1 + 27)]);\n    data_red_rf[28] = (data_red_rf[28] + ((float*)data_1)[(cse_var_1 + 28)]);\n    data_red_rf[29] = (data_red_rf[29] + ((float*)data_1)[(cse_var_1 + 29)]);\n    data_red_rf[30] = (data_red_rf[30] + ((float*)data_1)[(cse_var_1 + 30)]);\n    data_red_rf[31] = (data_red_rf[31] + ((float*)data_1)[(cse_var_1 + 31)]);\n    data_red_rf[32] = (data_red_rf[32] + ((float*)data_1)[(cse_var_1 + 32)]);\n    data_red_rf[33] = (data_red_rf[33] + ((float*)data_1)[(cse_var_1 + 33)]);\n    data_red_rf[34] = (data_red_rf[34] + ((float*)data_1)[(cse_var_1 + 34)]);\n    data_red_rf[35] = (data_red_rf[35] + ((float*)data_1)[(cse_var_1 + 35)]);\n    data_red_rf[36] = (data_red_rf[36] + ((float*)data_1)[(cse_var_1 + 36)]);\n    data_red_rf[37] = (data_red_rf[37] + ((float*)data_1)[(cse_var_1 + 37)]);\n    data_red_rf[38] = (data_red_rf[38] + ((float*)data_1)[(cse_var_1 + 38)]);\n    data_red_rf[39] = (data_red_rf[39] + ((float*)data_1)[(cse_var_1 + 39)]);\n    data_red_rf[40] = (data_red_rf[40] + ((float*)data_1)[(cse_var_1 + 40)]);\n    data_red_rf[41] = (data_red_rf[41] + ((float*)data_1)[(cse_var_1 + 41)]);\n    data_red_rf[42] = (data_red_rf[42] + ((float*)data_1)[(cse_var_1 + 42)]);\n    data_red_rf[43] = (data_red_rf[43] + ((float*)data_1)[(cse_var_1 + 43)]);\n    data_red_rf[44] = (data_red_rf[44] + ((float*)data_1)[(cse_var_1 + 44)]);\n    data_red_rf[45] = (data_red_rf[45] + ((float*)data_1)[(cse_var_1 + 45)]);\n    data_red_rf[46] = (data_red_rf[46] + ((float*)data_1)[(cse_var_1 + 46)]);\n    data_red_rf[47] = (data_red_rf[47] + ((float*)data_1)[(cse_var_1 + 47)]);\n    data_red_rf[48] = (data_red_rf[48] + ((float*)data_1)[(cse_var_1 + 48)]);\n    data_red_rf[49] = (data_red_rf[49] + ((float*)data_1)[(cse_var_1 + 49)]);\n    data_red_rf[50] = (data_red_rf[50] + ((float*)data_1)[(cse_var_1 + 50)]);\n    data_red_rf[51] = (data_red_rf[51] + ((float*)data_1)[(cse_var_1 + 51)]);\n    data_red_rf[52] = (data_red_rf[52] + ((float*)data_1)[(cse_var_1 + 52)]);\n    data_red_rf[53] = (data_red_rf[53] + ((float*)data_1)[(cse_var_1 + 53)]);\n    data_red_rf[54] = (data_red_rf[54] + ((float*)data_1)[(cse_var_1 + 54)]);\n    data_red_rf[55] = (data_red_rf[55] + ((float*)data_1)[(cse_var_1 + 55)]);\n    data_red_rf[56] = (data_red_rf[56] + ((float*)data_1)[(cse_var_1 + 56)]);\n    data_red_rf[57] = (data_red_rf[57] + ((float*)data_1)[(cse_var_1 + 57)]);\n    data_red_rf[58] = (data_red_rf[58] + ((float*)data_1)[(cse_var_1 + 58)]);\n    data_red_rf[59] = (data_red_rf[59] + ((float*)data_1)[(cse_var_1 + 59)]);\n    data_red_rf[60] = (data_red_rf[60] + ((float*)data_1)[(cse_var_1 + 60)]);\n    data_red_rf[61] = (data_red_rf[61] + ((float*)data_1)[(cse_var_1 + 61)]);\n    data_red_rf[62] = (data_red_rf[62] + ((float*)data_1)[(cse_var_1 + 62)]);\n    data_red_rf[63] = (data_red_rf[63] + ((float*)data_1)[(cse_var_1 + 63)]);\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sum[9__8__16__40]"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 40, 8, 28), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([24], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((24,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(24):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(1120, 24):\n            data_1 = T.Buffer((26880,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] + data_1[k0_k1_fused_k2_fused_k3_fused_outer * 24 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(24):\n            data_red_1[0] = data_red_1[0] + data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 840; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[24];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 24; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1120; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 24; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 24) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 24; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sum[3__40__8__28]"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 32, 8, 40), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([64], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((64,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(64):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(1600, 64):\n            data_1 = T.Buffer((102400,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] + data_1[k0_k1_fused_k2_fused_k3_fused_outer * 64 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(64):\n            data_red_1[0] = data_red_1[0] + data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 3200; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[64];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1600; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 64; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 64) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sum[10__32__8__40]"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 24, 40, 12), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([32], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((32,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(32):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(2520, 32):\n            data_1 = T.Buffer((80640,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] + data_1[k0_k1_fused_k2_fused_k3_fused_outer * 32 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(32):\n            data_red_1[0] = data_red_1[0] + data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 2520; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[32];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 32; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 2520; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 32; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 32; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sum"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 4, 28, 12), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([64], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((64,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(64):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(105, 64):\n            data_1 = T.Buffer((6720,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] + data_1[k0_k1_fused_k2_fused_k3_fused_outer * 64 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(64):\n            data_red_1[0] = data_red_1[0] + data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 210; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[64];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 105; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 64; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 64) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sum"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 4, 28, 4), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([14], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((14,), data=data_red_rf, align=32)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(14):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(288, 14):\n            data_1 = T.Buffer((4032,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] + data_1[k0_k1_fused_k2_fused_k3_fused_outer * 14 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[0]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[1]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[2]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[3]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[4]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[5]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[6]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[7]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[8]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[9]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[10]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[11]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[12]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[13]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 126; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[14];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 14; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 288; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 14; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 14) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[0]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[1]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[2]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[3]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[4]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[5]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[6]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[7]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[8]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[9]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[10]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[11]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[12]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[13]);\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sum"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 20, 16, 4), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([32], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((32,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(32):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(320):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 32\n            data_1 = T.Buffer((10240,), data=data.data)\n            data_red_rf_1[0] = data_red_rf_1[0] + data_1[cse_var_1]\n            data_red_rf_1[1] = data_red_rf_1[1] + data_1[cse_var_1 + 1]\n            data_red_rf_1[2] = data_red_rf_1[2] + data_1[cse_var_1 + 2]\n            data_red_rf_1[3] = data_red_rf_1[3] + data_1[cse_var_1 + 3]\n            data_red_rf_1[4] = data_red_rf_1[4] + data_1[cse_var_1 + 4]\n            data_red_rf_1[5] = data_red_rf_1[5] + data_1[cse_var_1 + 5]\n            data_red_rf_1[6] = data_red_rf_1[6] + data_1[cse_var_1 + 6]\n            data_red_rf_1[7] = data_red_rf_1[7] + data_1[cse_var_1 + 7]\n            data_red_rf_1[8] = data_red_rf_1[8] + data_1[cse_var_1 + 8]\n            data_red_rf_1[9] = data_red_rf_1[9] + data_1[cse_var_1 + 9]\n            data_red_rf_1[10] = data_red_rf_1[10] + data_1[cse_var_1 + 10]\n            data_red_rf_1[11] = data_red_rf_1[11] + data_1[cse_var_1 + 11]\n            data_red_rf_1[12] = data_red_rf_1[12] + data_1[cse_var_1 + 12]\n            data_red_rf_1[13] = data_red_rf_1[13] + data_1[cse_var_1 + 13]\n            data_red_rf_1[14] = data_red_rf_1[14] + data_1[cse_var_1 + 14]\n            data_red_rf_1[15] = data_red_rf_1[15] + data_1[cse_var_1 + 15]\n            data_red_rf_1[16] = data_red_rf_1[16] + data_1[cse_var_1 + 16]\n            data_red_rf_1[17] = data_red_rf_1[17] + data_1[cse_var_1 + 17]\n            data_red_rf_1[18] = data_red_rf_1[18] + data_1[cse_var_1 + 18]\n            data_red_rf_1[19] = data_red_rf_1[19] + data_1[cse_var_1 + 19]\n            data_red_rf_1[20] = data_red_rf_1[20] + data_1[cse_var_1 + 20]\n            data_red_rf_1[21] = data_red_rf_1[21] + data_1[cse_var_1 + 21]\n            data_red_rf_1[22] = data_red_rf_1[22] + data_1[cse_var_1 + 22]\n            data_red_rf_1[23] = data_red_rf_1[23] + data_1[cse_var_1 + 23]\n            data_red_rf_1[24] = data_red_rf_1[24] + data_1[cse_var_1 + 24]\n            data_red_rf_1[25] = data_red_rf_1[25] + data_1[cse_var_1 + 25]\n            data_red_rf_1[26] = data_red_rf_1[26] + data_1[cse_var_1 + 26]\n            data_red_rf_1[27] = data_red_rf_1[27] + data_1[cse_var_1 + 27]\n            data_red_rf_1[28] = data_red_rf_1[28] + data_1[cse_var_1 + 28]\n            data_red_rf_1[29] = data_red_rf_1[29] + data_1[cse_var_1 + 29]\n            data_red_rf_1[30] = data_red_rf_1[30] + data_1[cse_var_1 + 30]\n            data_red_rf_1[31] = data_red_rf_1[31] + data_1[cse_var_1 + 31]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(32):\n            data_red_1[0] = data_red_1[0] + data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 320; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[32];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 32; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 320; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 32);\n    data_red_rf[0] = (data_red_rf[0] + ((float*)data_1)[cse_var_1]);\n    data_red_rf[1] = (data_red_rf[1] + ((float*)data_1)[(cse_var_1 + 1)]);\n    data_red_rf[2] = (data_red_rf[2] + ((float*)data_1)[(cse_var_1 + 2)]);\n    data_red_rf[3] = (data_red_rf[3] + ((float*)data_1)[(cse_var_1 + 3)]);\n    data_red_rf[4] = (data_red_rf[4] + ((float*)data_1)[(cse_var_1 + 4)]);\n    data_red_rf[5] = (data_red_rf[5] + ((float*)data_1)[(cse_var_1 + 5)]);\n    data_red_rf[6] = (data_red_rf[6] + ((float*)data_1)[(cse_var_1 + 6)]);\n    data_red_rf[7] = (data_red_rf[7] + ((float*)data_1)[(cse_var_1 + 7)]);\n    data_red_rf[8] = (data_red_rf[8] + ((float*)data_1)[(cse_var_1 + 8)]);\n    data_red_rf[9] = (data_red_rf[9] + ((float*)data_1)[(cse_var_1 + 9)]);\n    data_red_rf[10] = (data_red_rf[10] + ((float*)data_1)[(cse_var_1 + 10)]);\n    data_red_rf[11] = (data_red_rf[11] + ((float*)data_1)[(cse_var_1 + 11)]);\n    data_red_rf[12] = (data_red_rf[12] + ((float*)data_1)[(cse_var_1 + 12)]);\n    data_red_rf[13] = (data_red_rf[13] + ((float*)data_1)[(cse_var_1 + 13)]);\n    data_red_rf[14] = (data_red_rf[14] + ((float*)data_1)[(cse_var_1 + 14)]);\n    data_red_rf[15] = (data_red_rf[15] + ((float*)data_1)[(cse_var_1 + 15)]);\n    data_red_rf[16] = (data_red_rf[16] + ((float*)data_1)[(cse_var_1 + 16)]);\n    data_red_rf[17] = (data_red_rf[17] + ((float*)data_1)[(cse_var_1 + 17)]);\n    data_red_rf[18] = (data_red_rf[18] + ((float*)data_1)[(cse_var_1 + 18)]);\n    data_red_rf[19] = (data_red_rf[19] + ((float*)data_1)[(cse_var_1 + 19)]);\n    data_red_rf[20] = (data_red_rf[20] + ((float*)data_1)[(cse_var_1 + 20)]);\n    data_red_rf[21] = (data_red_rf[21] + ((float*)data_1)[(cse_var_1 + 21)]);\n    data_red_rf[22] = (data_red_rf[22] + ((float*)data_1)[(cse_var_1 + 22)]);\n    data_red_rf[23] = (data_red_rf[23] + ((float*)data_1)[(cse_var_1 + 23)]);\n    data_red_rf[24] = (data_red_rf[24] + ((float*)data_1)[(cse_var_1 + 24)]);\n    data_red_rf[25] = (data_red_rf[25] + ((float*)data_1)[(cse_var_1 + 25)]);\n    data_red_rf[26] = (data_red_rf[26] + ((float*)data_1)[(cse_var_1 + 26)]);\n    data_red_rf[27] = (data_red_rf[27] + ((float*)data_1)[(cse_var_1 + 27)]);\n    data_red_rf[28] = (data_red_rf[28] + ((float*)data_1)[(cse_var_1 + 28)]);\n    data_red_rf[29] = (data_red_rf[29] + ((float*)data_1)[(cse_var_1 + 29)]);\n    data_red_rf[30] = (data_red_rf[30] + ((float*)data_1)[(cse_var_1 + 30)]);\n    data_red_rf[31] = (data_red_rf[31] + ((float*)data_1)[(cse_var_1 + 31)]);\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 32; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sum"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 4, 20, 24), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([9], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((9,), data=data_red_rf, align=32)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(9):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(1280, 9):\n            data_1 = T.Buffer((11520,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] + data_1[k0_k1_fused_k2_fused_k3_fused_outer * 9 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(9):\n            data_red_1[0] = data_red_1[0] + data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 360; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[9];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 9; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1280; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 9; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 9) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 9; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sum"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 20, 12, 20), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([60], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((60,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(60):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(640, 60):\n            data_1 = T.Buffer((38400,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] + data_1[k0_k1_fused_k2_fused_k3_fused_outer * 60 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[0]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[1]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[2]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[3]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[4]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[5]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[6]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[7]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[8]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[9]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[10]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[11]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[12]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[13]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[14]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[15]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[16]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[17]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[18]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[19]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[20]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[21]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[22]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[23]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[24]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[25]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[26]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[27]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[28]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[29]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[30]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[31]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[32]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[33]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[34]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[35]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[36]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[37]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[38]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[39]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[40]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[41]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[42]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[43]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[44]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[45]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[46]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[47]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[48]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[49]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[50]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[51]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[52]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[53]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[54]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[55]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[56]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[57]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[58]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[59]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1200; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[60];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 60; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 640; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 60; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 60) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[0]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[1]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[2]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[3]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[4]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[5]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[6]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[7]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[8]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[9]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[10]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[11]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[12]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[13]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[14]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[15]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[16]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[17]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[18]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[19]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[20]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[21]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[22]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[23]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[24]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[25]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[26]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[27]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[28]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[29]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[30]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[31]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[32]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[33]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[34]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[35]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[36]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[37]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[38]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[39]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[40]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[41]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[42]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[43]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[44]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[45]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[46]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[47]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[48]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[49]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[50]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[51]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[52]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[53]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[54]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[55]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[56]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[57]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[58]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[59]);\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sum"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 28, 36, 24), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([64], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((64,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(64):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(756):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 64\n            data_1 = T.Buffer((48384,), data=data.data)\n            data_red_rf_1[0] = data_red_rf_1[0] + data_1[cse_var_1]\n            data_red_rf_1[1] = data_red_rf_1[1] + data_1[cse_var_1 + 1]\n            data_red_rf_1[2] = data_red_rf_1[2] + data_1[cse_var_1 + 2]\n            data_red_rf_1[3] = data_red_rf_1[3] + data_1[cse_var_1 + 3]\n            data_red_rf_1[4] = data_red_rf_1[4] + data_1[cse_var_1 + 4]\n            data_red_rf_1[5] = data_red_rf_1[5] + data_1[cse_var_1 + 5]\n            data_red_rf_1[6] = data_red_rf_1[6] + data_1[cse_var_1 + 6]\n            data_red_rf_1[7] = data_red_rf_1[7] + data_1[cse_var_1 + 7]\n            data_red_rf_1[8] = data_red_rf_1[8] + data_1[cse_var_1 + 8]\n            data_red_rf_1[9] = data_red_rf_1[9] + data_1[cse_var_1 + 9]\n            data_red_rf_1[10] = data_red_rf_1[10] + data_1[cse_var_1 + 10]\n            data_red_rf_1[11] = data_red_rf_1[11] + data_1[cse_var_1 + 11]\n            data_red_rf_1[12] = data_red_rf_1[12] + data_1[cse_var_1 + 12]\n            data_red_rf_1[13] = data_red_rf_1[13] + data_1[cse_var_1 + 13]\n            data_red_rf_1[14] = data_red_rf_1[14] + data_1[cse_var_1 + 14]\n            data_red_rf_1[15] = data_red_rf_1[15] + data_1[cse_var_1 + 15]\n            data_red_rf_1[16] = data_red_rf_1[16] + data_1[cse_var_1 + 16]\n            data_red_rf_1[17] = data_red_rf_1[17] + data_1[cse_var_1 + 17]\n            data_red_rf_1[18] = data_red_rf_1[18] + data_1[cse_var_1 + 18]\n            data_red_rf_1[19] = data_red_rf_1[19] + data_1[cse_var_1 + 19]\n            data_red_rf_1[20] = data_red_rf_1[20] + data_1[cse_var_1 + 20]\n            data_red_rf_1[21] = data_red_rf_1[21] + data_1[cse_var_1 + 21]\n            data_red_rf_1[22] = data_red_rf_1[22] + data_1[cse_var_1 + 22]\n            data_red_rf_1[23] = data_red_rf_1[23] + data_1[cse_var_1 + 23]\n            data_red_rf_1[24] = data_red_rf_1[24] + data_1[cse_var_1 + 24]\n            data_red_rf_1[25] = data_red_rf_1[25] + data_1[cse_var_1 + 25]\n            data_red_rf_1[26] = data_red_rf_1[26] + data_1[cse_var_1 + 26]\n            data_red_rf_1[27] = data_red_rf_1[27] + data_1[cse_var_1 + 27]\n            data_red_rf_1[28] = data_red_rf_1[28] + data_1[cse_var_1 + 28]\n            data_red_rf_1[29] = data_red_rf_1[29] + data_1[cse_var_1 + 29]\n            data_red_rf_1[30] = data_red_rf_1[30] + data_1[cse_var_1 + 30]\n            data_red_rf_1[31] = data_red_rf_1[31] + data_1[cse_var_1 + 31]\n            data_red_rf_1[32] = data_red_rf_1[32] + data_1[cse_var_1 + 32]\n            data_red_rf_1[33] = data_red_rf_1[33] + data_1[cse_var_1 + 33]\n            data_red_rf_1[34] = data_red_rf_1[34] + data_1[cse_var_1 + 34]\n            data_red_rf_1[35] = data_red_rf_1[35] + data_1[cse_var_1 + 35]\n            data_red_rf_1[36] = data_red_rf_1[36] + data_1[cse_var_1 + 36]\n            data_red_rf_1[37] = data_red_rf_1[37] + data_1[cse_var_1 + 37]\n            data_red_rf_1[38] = data_red_rf_1[38] + data_1[cse_var_1 + 38]\n            data_red_rf_1[39] = data_red_rf_1[39] + data_1[cse_var_1 + 39]\n            data_red_rf_1[40] = data_red_rf_1[40] + data_1[cse_var_1 + 40]\n            data_red_rf_1[41] = data_red_rf_1[41] + data_1[cse_var_1 + 41]\n            data_red_rf_1[42] = data_red_rf_1[42] + data_1[cse_var_1 + 42]\n            data_red_rf_1[43] = data_red_rf_1[43] + data_1[cse_var_1 + 43]\n            data_red_rf_1[44] = data_red_rf_1[44] + data_1[cse_var_1 + 44]\n            data_red_rf_1[45] = data_red_rf_1[45] + data_1[cse_var_1 + 45]\n            data_red_rf_1[46] = data_red_rf_1[46] + data_1[cse_var_1 + 46]\n            data_red_rf_1[47] = data_red_rf_1[47] + data_1[cse_var_1 + 47]\n            data_red_rf_1[48] = data_red_rf_1[48] + data_1[cse_var_1 + 48]\n            data_red_rf_1[49] = data_red_rf_1[49] + data_1[cse_var_1 + 49]\n            data_red_rf_1[50] = data_red_rf_1[50] + data_1[cse_var_1 + 50]\n            data_red_rf_1[51] = data_red_rf_1[51] + data_1[cse_var_1 + 51]\n            data_red_rf_1[52] = data_red_rf_1[52] + data_1[cse_var_1 + 52]\n            data_red_rf_1[53] = data_red_rf_1[53] + data_1[cse_var_1 + 53]\n            data_red_rf_1[54] = data_red_rf_1[54] + data_1[cse_var_1 + 54]\n            data_red_rf_1[55] = data_red_rf_1[55] + data_1[cse_var_1 + 55]\n            data_red_rf_1[56] = data_red_rf_1[56] + data_1[cse_var_1 + 56]\n            data_red_rf_1[57] = data_red_rf_1[57] + data_1[cse_var_1 + 57]\n            data_red_rf_1[58] = data_red_rf_1[58] + data_1[cse_var_1 + 58]\n            data_red_rf_1[59] = data_red_rf_1[59] + data_1[cse_var_1 + 59]\n            data_red_rf_1[60] = data_red_rf_1[60] + data_1[cse_var_1 + 60]\n            data_red_rf_1[61] = data_red_rf_1[61] + data_1[cse_var_1 + 61]\n            data_red_rf_1[62] = data_red_rf_1[62] + data_1[cse_var_1 + 62]\n            data_red_rf_1[63] = data_red_rf_1[63] + data_1[cse_var_1 + 63]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[0]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[1]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[2]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[3]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[4]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[5]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[6]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[7]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[8]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[9]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[10]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[11]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[12]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[13]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[14]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[15]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[16]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[17]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[18]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[19]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[20]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[21]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[22]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[23]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[24]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[25]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[26]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[27]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[28]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[29]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[30]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[31]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[32]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[33]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[34]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[35]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[36]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[37]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[38]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[39]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[40]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[41]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[42]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[43]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[44]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[45]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[46]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[47]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[48]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[49]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[50]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[51]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[52]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[53]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[54]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[55]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[56]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[57]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[58]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[59]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[60]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[61]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[62]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[63]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1512; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[64];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 756; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 64);\n    data_red_rf[0] = (data_red_rf[0] + ((float*)data_1)[cse_var_1]);\n    data_red_rf[1] = (data_red_rf[1] + ((float*)data_1)[(cse_var_1 + 1)]);\n    data_red_rf[2] = (data_red_rf[2] + ((float*)data_1)[(cse_var_1 + 2)]);\n    data_red_rf[3] = (data_red_rf[3] + ((float*)data_1)[(cse_var_1 + 3)]);\n    data_red_rf[4] = (data_red_rf[4] + ((float*)data_1)[(cse_var_1 + 4)]);\n    data_red_rf[5] = (data_red_rf[5] + ((float*)data_1)[(cse_var_1 + 5)]);\n    data_red_rf[6] = (data_red_rf[6] + ((float*)data_1)[(cse_var_1 + 6)]);\n    data_red_rf[7] = (data_red_rf[7] + ((float*)data_1)[(cse_var_1 + 7)]);\n    data_red_rf[8] = (data_red_rf[8] + ((float*)data_1)[(cse_var_1 + 8)]);\n    data_red_rf[9] = (data_red_rf[9] + ((float*)data_1)[(cse_var_1 + 9)]);\n    data_red_rf[10] = (data_red_rf[10] + ((float*)data_1)[(cse_var_1 + 10)]);\n    data_red_rf[11] = (data_red_rf[11] + ((float*)data_1)[(cse_var_1 + 11)]);\n    data_red_rf[12] = (data_red_rf[12] + ((float*)data_1)[(cse_var_1 + 12)]);\n    data_red_rf[13] = (data_red_rf[13] + ((float*)data_1)[(cse_var_1 + 13)]);\n    data_red_rf[14] = (data_red_rf[14] + ((float*)data_1)[(cse_var_1 + 14)]);\n    data_red_rf[15] = (data_red_rf[15] + ((float*)data_1)[(cse_var_1 + 15)]);\n    data_red_rf[16] = (data_red_rf[16] + ((float*)data_1)[(cse_var_1 + 16)]);\n    data_red_rf[17] = (data_red_rf[17] + ((float*)data_1)[(cse_var_1 + 17)]);\n    data_red_rf[18] = (data_red_rf[18] + ((float*)data_1)[(cse_var_1 + 18)]);\n    data_red_rf[19] = (data_red_rf[19] + ((float*)data_1)[(cse_var_1 + 19)]);\n    data_red_rf[20] = (data_red_rf[20] + ((float*)data_1)[(cse_var_1 + 20)]);\n    data_red_rf[21] = (data_red_rf[21] + ((float*)data_1)[(cse_var_1 + 21)]);\n    data_red_rf[22] = (data_red_rf[22] + ((float*)data_1)[(cse_var_1 + 22)]);\n    data_red_rf[23] = (data_red_rf[23] + ((float*)data_1)[(cse_var_1 + 23)]);\n    data_red_rf[24] = (data_red_rf[24] + ((float*)data_1)[(cse_var_1 + 24)]);\n    data_red_rf[25] = (data_red_rf[25] + ((float*)data_1)[(cse_var_1 + 25)]);\n    data_red_rf[26] = (data_red_rf[26] + ((float*)data_1)[(cse_var_1 + 26)]);\n    data_red_rf[27] = (data_red_rf[27] + ((float*)data_1)[(cse_var_1 + 27)]);\n    data_red_rf[28] = (data_red_rf[28] + ((float*)data_1)[(cse_var_1 + 28)]);\n    data_red_rf[29] = (data_red_rf[29] + ((float*)data_1)[(cse_var_1 + 29)]);\n    data_red_rf[30] = (data_red_rf[30] + ((float*)data_1)[(cse_var_1 + 30)]);\n    data_red_rf[31] = (data_red_rf[31] + ((float*)data_1)[(cse_var_1 + 31)]);\n    data_red_rf[32] = (data_red_rf[32] + ((float*)data_1)[(cse_var_1 + 32)]);\n    data_red_rf[33] = (data_red_rf[33] + ((float*)data_1)[(cse_var_1 + 33)]);\n    data_red_rf[34] = (data_red_rf[34] + ((float*)data_1)[(cse_var_1 + 34)]);\n    data_red_rf[35] = (data_red_rf[35] + ((float*)data_1)[(cse_var_1 + 35)]);\n    data_red_rf[36] = (data_red_rf[36] + ((float*)data_1)[(cse_var_1 + 36)]);\n    data_red_rf[37] = (data_red_rf[37] + ((float*)data_1)[(cse_var_1 + 37)]);\n    data_red_rf[38] = (data_red_rf[38] + ((float*)data_1)[(cse_var_1 + 38)]);\n    data_red_rf[39] = (data_red_rf[39] + ((float*)data_1)[(cse_var_1 + 39)]);\n    data_red_rf[40] = (data_red_rf[40] + ((float*)data_1)[(cse_var_1 + 40)]);\n    data_red_rf[41] = (data_red_rf[41] + ((float*)data_1)[(cse_var_1 + 41)]);\n    data_red_rf[42] = (data_red_rf[42] + ((float*)data_1)[(cse_var_1 + 42)]);\n    data_red_rf[43] = (data_red_rf[43] + ((float*)data_1)[(cse_var_1 + 43)]);\n    data_red_rf[44] = (data_red_rf[44] + ((float*)data_1)[(cse_var_1 + 44)]);\n    data_red_rf[45] = (data_red_rf[45] + ((float*)data_1)[(cse_var_1 + 45)]);\n    data_red_rf[46] = (data_red_rf[46] + ((float*)data_1)[(cse_var_1 + 46)]);\n    data_red_rf[47] = (data_red_rf[47] + ((float*)data_1)[(cse_var_1 + 47)]);\n    data_red_rf[48] = (data_red_rf[48] + ((float*)data_1)[(cse_var_1 + 48)]);\n    data_red_rf[49] = (data_red_rf[49] + ((float*)data_1)[(cse_var_1 + 49)]);\n    data_red_rf[50] = (data_red_rf[50] + ((float*)data_1)[(cse_var_1 + 50)]);\n    data_red_rf[51] = (data_red_rf[51] + ((float*)data_1)[(cse_var_1 + 51)]);\n    data_red_rf[52] = (data_red_rf[52] + ((float*)data_1)[(cse_var_1 + 52)]);\n    data_red_rf[53] = (data_red_rf[53] + ((float*)data_1)[(cse_var_1 + 53)]);\n    data_red_rf[54] = (data_red_rf[54] + ((float*)data_1)[(cse_var_1 + 54)]);\n    data_red_rf[55] = (data_red_rf[55] + ((float*)data_1)[(cse_var_1 + 55)]);\n    data_red_rf[56] = (data_red_rf[56] + ((float*)data_1)[(cse_var_1 + 56)]);\n    data_red_rf[57] = (data_red_rf[57] + ((float*)data_1)[(cse_var_1 + 57)]);\n    data_red_rf[58] = (data_red_rf[58] + ((float*)data_1)[(cse_var_1 + 58)]);\n    data_red_rf[59] = (data_red_rf[59] + ((float*)data_1)[(cse_var_1 + 59)]);\n    data_red_rf[60] = (data_red_rf[60] + ((float*)data_1)[(cse_var_1 + 60)]);\n    data_red_rf[61] = (data_red_rf[61] + ((float*)data_1)[(cse_var_1 + 61)]);\n    data_red_rf[62] = (data_red_rf[62] + ((float*)data_1)[(cse_var_1 + 62)]);\n    data_red_rf[63] = (data_red_rf[63] + ((float*)data_1)[(cse_var_1 + 63)]);\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[0]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[1]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[2]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[3]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[4]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[5]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[6]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[7]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[8]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[9]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[10]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[11]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[12]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[13]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[14]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[15]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[16]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[17]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[18]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[19]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[20]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[21]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[22]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[23]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[24]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[25]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[26]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[27]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[28]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[29]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[30]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[31]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[32]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[33]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[34]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[35]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[36]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[37]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[38]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[39]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[40]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[41]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[42]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[43]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[44]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[45]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[46]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[47]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[48]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[49]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[50]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[51]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[52]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[53]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[54]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[55]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[56]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[57]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[58]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[59]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[60]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[61]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[62]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[63]);\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sum"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 28, 36, 16), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([32], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((32,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(32):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(1008):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 32\n            data_1 = T.Buffer((32256,), data=data.data)\n            data_red_rf_1[0] = data_red_rf_1[0] + data_1[cse_var_1]\n            data_red_rf_1[1] = data_red_rf_1[1] + data_1[cse_var_1 + 1]\n            data_red_rf_1[2] = data_red_rf_1[2] + data_1[cse_var_1 + 2]\n            data_red_rf_1[3] = data_red_rf_1[3] + data_1[cse_var_1 + 3]\n            data_red_rf_1[4] = data_red_rf_1[4] + data_1[cse_var_1 + 4]\n            data_red_rf_1[5] = data_red_rf_1[5] + data_1[cse_var_1 + 5]\n            data_red_rf_1[6] = data_red_rf_1[6] + data_1[cse_var_1 + 6]\n            data_red_rf_1[7] = data_red_rf_1[7] + data_1[cse_var_1 + 7]\n            data_red_rf_1[8] = data_red_rf_1[8] + data_1[cse_var_1 + 8]\n            data_red_rf_1[9] = data_red_rf_1[9] + data_1[cse_var_1 + 9]\n            data_red_rf_1[10] = data_red_rf_1[10] + data_1[cse_var_1 + 10]\n            data_red_rf_1[11] = data_red_rf_1[11] + data_1[cse_var_1 + 11]\n            data_red_rf_1[12] = data_red_rf_1[12] + data_1[cse_var_1 + 12]\n            data_red_rf_1[13] = data_red_rf_1[13] + data_1[cse_var_1 + 13]\n            data_red_rf_1[14] = data_red_rf_1[14] + data_1[cse_var_1 + 14]\n            data_red_rf_1[15] = data_red_rf_1[15] + data_1[cse_var_1 + 15]\n            data_red_rf_1[16] = data_red_rf_1[16] + data_1[cse_var_1 + 16]\n            data_red_rf_1[17] = data_red_rf_1[17] + data_1[cse_var_1 + 17]\n            data_red_rf_1[18] = data_red_rf_1[18] + data_1[cse_var_1 + 18]\n            data_red_rf_1[19] = data_red_rf_1[19] + data_1[cse_var_1 + 19]\n            data_red_rf_1[20] = data_red_rf_1[20] + data_1[cse_var_1 + 20]\n            data_red_rf_1[21] = data_red_rf_1[21] + data_1[cse_var_1 + 21]\n            data_red_rf_1[22] = data_red_rf_1[22] + data_1[cse_var_1 + 22]\n            data_red_rf_1[23] = data_red_rf_1[23] + data_1[cse_var_1 + 23]\n            data_red_rf_1[24] = data_red_rf_1[24] + data_1[cse_var_1 + 24]\n            data_red_rf_1[25] = data_red_rf_1[25] + data_1[cse_var_1 + 25]\n            data_red_rf_1[26] = data_red_rf_1[26] + data_1[cse_var_1 + 26]\n            data_red_rf_1[27] = data_red_rf_1[27] + data_1[cse_var_1 + 27]\n            data_red_rf_1[28] = data_red_rf_1[28] + data_1[cse_var_1 + 28]\n            data_red_rf_1[29] = data_red_rf_1[29] + data_1[cse_var_1 + 29]\n            data_red_rf_1[30] = data_red_rf_1[30] + data_1[cse_var_1 + 30]\n            data_red_rf_1[31] = data_red_rf_1[31] + data_1[cse_var_1 + 31]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[0]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[1]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[2]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[3]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[4]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[5]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[6]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[7]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[8]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[9]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[10]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[11]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[12]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[13]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[14]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[15]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[16]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[17]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[18]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[19]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[20]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[21]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[22]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[23]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[24]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[25]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[26]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[27]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[28]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[29]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[30]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[31]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1008; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[32];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 32; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1008; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 32);\n    data_red_rf[0] = (data_red_rf[0] + ((float*)data_1)[cse_var_1]);\n    data_red_rf[1] = (data_red_rf[1] + ((float*)data_1)[(cse_var_1 + 1)]);\n    data_red_rf[2] = (data_red_rf[2] + ((float*)data_1)[(cse_var_1 + 2)]);\n    data_red_rf[3] = (data_red_rf[3] + ((float*)data_1)[(cse_var_1 + 3)]);\n    data_red_rf[4] = (data_red_rf[4] + ((float*)data_1)[(cse_var_1 + 4)]);\n    data_red_rf[5] = (data_red_rf[5] + ((float*)data_1)[(cse_var_1 + 5)]);\n    data_red_rf[6] = (data_red_rf[6] + ((float*)data_1)[(cse_var_1 + 6)]);\n    data_red_rf[7] = (data_red_rf[7] + ((float*)data_1)[(cse_var_1 + 7)]);\n    data_red_rf[8] = (data_red_rf[8] + ((float*)data_1)[(cse_var_1 + 8)]);\n    data_red_rf[9] = (data_red_rf[9] + ((float*)data_1)[(cse_var_1 + 9)]);\n    data_red_rf[10] = (data_red_rf[10] + ((float*)data_1)[(cse_var_1 + 10)]);\n    data_red_rf[11] = (data_red_rf[11] + ((float*)data_1)[(cse_var_1 + 11)]);\n    data_red_rf[12] = (data_red_rf[12] + ((float*)data_1)[(cse_var_1 + 12)]);\n    data_red_rf[13] = (data_red_rf[13] + ((float*)data_1)[(cse_var_1 + 13)]);\n    data_red_rf[14] = (data_red_rf[14] + ((float*)data_1)[(cse_var_1 + 14)]);\n    data_red_rf[15] = (data_red_rf[15] + ((float*)data_1)[(cse_var_1 + 15)]);\n    data_red_rf[16] = (data_red_rf[16] + ((float*)data_1)[(cse_var_1 + 16)]);\n    data_red_rf[17] = (data_red_rf[17] + ((float*)data_1)[(cse_var_1 + 17)]);\n    data_red_rf[18] = (data_red_rf[18] + ((float*)data_1)[(cse_var_1 + 18)]);\n    data_red_rf[19] = (data_red_rf[19] + ((float*)data_1)[(cse_var_1 + 19)]);\n    data_red_rf[20] = (data_red_rf[20] + ((float*)data_1)[(cse_var_1 + 20)]);\n    data_red_rf[21] = (data_red_rf[21] + ((float*)data_1)[(cse_var_1 + 21)]);\n    data_red_rf[22] = (data_red_rf[22] + ((float*)data_1)[(cse_var_1 + 22)]);\n    data_red_rf[23] = (data_red_rf[23] + ((float*)data_1)[(cse_var_1 + 23)]);\n    data_red_rf[24] = (data_red_rf[24] + ((float*)data_1)[(cse_var_1 + 24)]);\n    data_red_rf[25] = (data_red_rf[25] + ((float*)data_1)[(cse_var_1 + 25)]);\n    data_red_rf[26] = (data_red_rf[26] + ((float*)data_1)[(cse_var_1 + 26)]);\n    data_red_rf[27] = (data_red_rf[27] + ((float*)data_1)[(cse_var_1 + 27)]);\n    data_red_rf[28] = (data_red_rf[28] + ((float*)data_1)[(cse_var_1 + 28)]);\n    data_red_rf[29] = (data_red_rf[29] + ((float*)data_1)[(cse_var_1 + 29)]);\n    data_red_rf[30] = (data_red_rf[30] + ((float*)data_1)[(cse_var_1 + 30)]);\n    data_red_rf[31] = (data_red_rf[31] + ((float*)data_1)[(cse_var_1 + 31)]);\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[0]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[1]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[2]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[3]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[4]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[5]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[6]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[7]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[8]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[9]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[10]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[11]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[12]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[13]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[14]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[15]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[16]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[17]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[18]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[19]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[20]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[21]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[22]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[23]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[24]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[25]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[26]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[27]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[28]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[29]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[30]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[31]);\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sum"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 12, 4, 36), \"float32\"), compute: T.Buffer((1, 12, 4, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(12, 4, 36):\n            cse_var_1: T.int32 = i1 * 144 + i2 * 36 + i3\n            compute_1 = T.Buffer((1728,), data=compute.data)\n            data_1 = T.Buffer((1728,), data=data.data)\n            compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i1 = 0; i1 < 12; ++i1) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 36; ++i3) {\n        int32_t cse_var_1 = (((i1 * 144) + (i2 * 36)) + i3);\n        ((float*)compute_1)[cse_var_1] = coshf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cosh"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 12, 12, 8), \"float32\"), compute: T.Buffer((3, 12, 12, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(432):\n            for i3 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 8 + i3\n                compute_1 = T.Buffer((3456,), data=compute.data)\n                data_1 = T.Buffer((3456,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 432; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 8; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 8) + i3);\n      ((float*)compute_1)[cse_var_1] = coshf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cosh"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 20, 4, 4), \"float32\"), compute: T.Buffer((2, 20, 4, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(40):\n            for i2, i3 in T.grid(4, 4):\n                cse_var_1: T.int32 = i0_i1_fused * 16 + i2 * 4 + i3\n                compute_1 = T.Buffer((640,), data=compute.data)\n                data_1 = T.Buffer((640,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 40; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 4; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 16) + (i2 * 4)) + i3);\n        ((float*)compute_1)[cse_var_1] = coshf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cosh"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 40, 24, 40), \"float32\"), compute: T.Buffer((4, 40, 24, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(3840):\n            for i3 in range(40):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 40 + i3\n                compute_1 = T.Buffer((153600,), data=compute.data)\n                data_1 = T.Buffer((153600,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3840; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 40; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 40) + i3);\n      ((float*)compute_1)[cse_var_1] = coshf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cosh"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 24, 4, 20), \"float32\"), compute: T.Buffer((5, 24, 4, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(120):\n            for i2, i3 in T.grid(4, 20):\n                cse_var_1: T.int32 = i0_i1_fused * 80 + i2 * 20 + i3\n                compute_1 = T.Buffer((9600,), data=compute.data)\n                data_1 = T.Buffer((9600,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 120; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 80) + (i2 * 20)) + i3);\n        ((float*)compute_1)[cse_var_1] = coshf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cosh"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 28, 4, 28), \"float32\"), compute: T.Buffer((8, 28, 4, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(896):\n            for i3 in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                compute_1 = T.Buffer((25088,), data=compute.data)\n                data_1 = T.Buffer((25088,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 896; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 28) + i3);\n      ((float*)compute_1)[cse_var_1] = coshf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cosh"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 20, 20, 4), \"float32\"), compute: T.Buffer((2, 20, 20, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(40):\n            for i2, i3 in T.grid(20, 4):\n                cse_var_1: T.int32 = i0_i1_fused * 80 + i2 * 4 + i3\n                compute_1 = T.Buffer((3200,), data=compute.data)\n                data_1 = T.Buffer((3200,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 40; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      for (int32_t i3 = 0; i3 < 4; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 80) + (i2 * 4)) + i3);\n        ((float*)compute_1)[cse_var_1] = coshf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cosh"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 8, 20, 12), \"float32\"), compute: T.Buffer((3, 8, 20, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(24):\n            for i2, i3 in T.grid(20, 12):\n                cse_var_1: T.int32 = i0_i1_fused * 240 + i2 * 12 + i3\n                compute_1 = T.Buffer((5760,), data=compute.data)\n                data_1 = T.Buffer((5760,), data=data.data)\n                compute_1[cse_var_1] = T.cos(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float cosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 24; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      for (int32_t i3 = 0; i3 < 12; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 240) + (i2 * 12)) + i3);\n        ((float*)compute_1)[cse_var_1] = cosf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cos"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 24, 16, 28), \"float32\"), compute: T.Buffer((3, 24, 16, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1152):\n            for i3 in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                compute_1 = T.Buffer((32256,), data=compute.data)\n                data_1 = T.Buffer((32256,), data=data.data)\n                compute_1[cse_var_1] = T.cos(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float cosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1152; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 28) + i3);\n      ((float*)compute_1)[cse_var_1] = cosf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cos"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 40, 36, 28), \"float32\"), compute: T.Buffer((4, 40, 36, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(5760):\n            for i3 in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                compute_1 = T.Buffer((161280,), data=compute.data)\n                data_1 = T.Buffer((161280,), data=data.data)\n                compute_1[cse_var_1] = T.cos(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float cosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5760; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 28) + i3);\n      ((float*)compute_1)[cse_var_1] = cosf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cos"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 12, 40, 20), \"float32\"), compute: T.Buffer((10, 12, 40, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(120):\n            for i2, i3 in T.grid(40, 20):\n                cse_var_1: T.int32 = i0_i1_fused * 800 + i2 * 20 + i3\n                compute_1 = T.Buffer((96000,), data=compute.data)\n                data_1 = T.Buffer((96000,), data=data.data)\n                compute_1[cse_var_1] = T.cos(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float cosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 120; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 40; ++i2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 800) + (i2 * 20)) + i3);\n        ((float*)compute_1)[cse_var_1] = cosf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cos"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 4, 20, 16), \"float32\"), compute: T.Buffer((4, 4, 20, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(5120):\n            compute_1 = T.Buffer((5120,), data=compute.data)\n            data_1 = T.Buffer((5120,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.cos(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float cosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 5120; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = cosf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cos"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 12, 12, 24), \"float32\"), compute: T.Buffer((10, 12, 12, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(34560):\n            compute_1 = T.Buffer((34560,), data=compute.data)\n            data_1 = T.Buffer((34560,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.cos(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float cosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 34560; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = cosf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cos"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 28, 36, 36), \"float32\"), compute: T.Buffer((9, 28, 36, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(9072):\n            for i3 in range(36):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 36 + i3\n                compute_1 = T.Buffer((326592,), data=compute.data)\n                data_1 = T.Buffer((326592,), data=data.data)\n                compute_1[cse_var_1] = T.cos(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float cosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 9072; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 36; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 36) + i3);\n      ((float*)compute_1)[cse_var_1] = cosf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cos"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 12, 16, 40), \"float32\"), compute: T.Buffer((4, 12, 16, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(30720):\n            compute_1 = T.Buffer((30720,), data=compute.data)\n            data_1 = T.Buffer((30720,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.cos(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float cosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 30720; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = cosf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cos"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 24, 12, 40), \"float32\"), compute: T.Buffer((10, 24, 12, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2880):\n            for i3 in range(40):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 40 + i3\n                compute_1 = T.Buffer((115200,), data=compute.data)\n                data_1 = T.Buffer((115200,), data=data.data)\n                compute_1[cse_var_1] = T.cos(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float cosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2880; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 40; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 40) + i3);\n      ((float*)compute_1)[cse_var_1] = cosf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "cos"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 24, 28, 8), \"float32\"), compute: T.Buffer((5, 24, 28, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(120):\n            for i2, i3_s in T.grid(28, 8):\n                cse_var_1: T.int32 = i0_i1_fused * 224 + i2 * 8 + i3_s\n                compute_1 = T.Buffer((26880,), data=compute.data)\n                data_1 = T.Buffer((26880,), data=data.data)\n                compute_1[cse_var_1] = T.acos(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float acosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 120; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 28; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n        int32_t cse_var_1 = (((i0_i1_fused * 224) + (i2 * 8)) + i3_s);\n        ((float*)compute_1)[cse_var_1] = acosf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "acos"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 16, 16, 16), \"float32\"), compute: T.Buffer((6, 16, 16, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(96):\n            for i2, i3_s in T.grid(16, 16):\n                cse_var_1: T.int32 = i0_i1_fused * 256 + i2 * 16 + i3_s\n                compute_1 = T.Buffer((24576,), data=compute.data)\n                data_1 = T.Buffer((24576,), data=data.data)\n                compute_1[cse_var_1] = T.acos(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float acosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 96; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 16; ++i3_s) {\n        int32_t cse_var_1 = (((i0_i1_fused * 256) + (i2 * 16)) + i3_s);\n        ((float*)compute_1)[cse_var_1] = acosf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "acos"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 4, 36, 40), \"float32\"), compute: T.Buffer((7, 4, 36, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(40320):\n            compute_1 = T.Buffer((40320,), data=compute.data)\n            data_1 = T.Buffer((40320,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.acos(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float acosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 40320; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = acosf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "acos"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 28, 40, 32), \"float32\"), compute: T.Buffer((2, 28, 40, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2240):\n            for i3 in range(32):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 32 + i3\n                compute_1 = T.Buffer((71680,), data=compute.data)\n                data_1 = T.Buffer((71680,), data=data.data)\n                compute_1[cse_var_1] = T.acos(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float acosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2240; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 32; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 32) + i3);\n      ((float*)compute_1)[cse_var_1] = acosf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "acos"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 28, 16, 28), \"float32\"), compute: T.Buffer((7, 28, 16, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(87808):\n            compute_1 = T.Buffer((87808,), data=compute.data)\n            data_1 = T.Buffer((87808,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.acos(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float acosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 87808; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = acosf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "acos"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 36, 24, 12), \"float32\"), compute: T.Buffer((2, 36, 24, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1728):\n            for i3_s in range(12):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 12 + i3_s\n                compute_1 = T.Buffer((20736,), data=compute.data)\n                data_1 = T.Buffer((20736,), data=data.data)\n                compute_1[cse_var_1] = T.acos(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float acosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1728; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 12; ++i3_s) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 12) + i3_s);\n      ((float*)compute_1)[cse_var_1] = acosf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "acos"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 24, 12, 8), \"float32\"), compute: T.Buffer((1, 24, 12, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(24, 12, 8):\n            cse_var_1: T.int32 = i1 * 96 + i2 * 8 + i3\n            compute_1 = T.Buffer((2304,), data=compute.data)\n            data_1 = T.Buffer((2304,), data=data.data)\n            compute_1[cse_var_1] = T.acos(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float acosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i1 = 0; i1 < 24; ++i1) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3 = 0; i3 < 8; ++i3) {\n        int32_t cse_var_1 = (((i1 * 96) + (i2 * 8)) + i3);\n        ((float*)compute_1)[cse_var_1] = acosf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "acos"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 24, 32, 4), \"float32\"), compute: T.Buffer((4, 24, 32, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(3072):\n            for i3_s in range(4):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 4 + i3_s\n                compute_1 = T.Buffer((12288,), data=compute.data)\n                data_1 = T.Buffer((12288,), data=data.data)\n                compute_1[cse_var_1] = T.acos(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float acosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3072; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 4; ++i3_s) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 4) + i3_s);\n      ((float*)compute_1)[cse_var_1] = acosf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "acos"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 8, 12, 4), \"float32\"), compute: T.Buffer((9, 8, 12, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(72):\n            for i2, i3 in T.grid(12, 4):\n                cse_var_1: T.int32 = i0_i1_fused * 48 + i2 * 4 + i3\n                compute_1 = T.Buffer((3456,), data=compute.data)\n                data_1 = T.Buffer((3456,), data=data.data)\n                compute_1[cse_var_1] = T.acos(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float acosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 72; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3 = 0; i3 < 4; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 48) + (i2 * 4)) + i3);\n        ((float*)compute_1)[cse_var_1] = acosf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "acos"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 12, 16, 20), \"float32\"), compute: T.Buffer((6, 12, 16, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(23040):\n            compute_1 = T.Buffer((23040,), data=compute.data)\n            data_1 = T.Buffer((23040,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.acos(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float acosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 23040; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = acosf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "acos"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 24, 4, 4), \"float32\"), compute: T.Buffer((5, 24, 4, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            for i3 in range(4):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 4 + i3\n                compute_1 = T.Buffer((1920,), data=compute.data)\n                data_1 = T.Buffer((1920,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 4; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 4) + i3);\n      ((float*)compute_1)[cse_var_1] = asinf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asin"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 8, 32, 16), \"float32\"), compute: T.Buffer((8, 8, 32, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2048):\n            for i3_s in range(16):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 16 + i3_s\n                compute_1 = T.Buffer((32768,), data=compute.data)\n                data_1 = T.Buffer((32768,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2048; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 16; ++i3_s) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 16) + i3_s);\n      ((float*)compute_1)[cse_var_1] = asinf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asin"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 32, 8, 32), \"float32\"), compute: T.Buffer((5, 32, 8, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1280):\n            for i3 in range(32):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 32 + i3\n                compute_1 = T.Buffer((40960,), data=compute.data)\n                data_1 = T.Buffer((40960,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1280; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 32; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 32) + i3);\n      ((float*)compute_1)[cse_var_1] = asinf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asin"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 12, 40, 36), \"float32\"), compute: T.Buffer((1, 12, 40, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            for i3 in range(36):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 36 + i3\n                compute_1 = T.Buffer((17280,), data=compute.data)\n                data_1 = T.Buffer((17280,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 36; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 36) + i3);\n      ((float*)compute_1)[cse_var_1] = asinf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asin"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 40, 4, 40), \"float32\"), compute: T.Buffer((3, 40, 4, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            for i3 in range(40):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 40 + i3\n                compute_1 = T.Buffer((19200,), data=compute.data)\n                data_1 = T.Buffer((19200,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 40; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 40) + i3);\n      ((float*)compute_1)[cse_var_1] = asinf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asin"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 8, 4, 12), \"float32\"), compute: T.Buffer((6, 8, 4, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(6):\n            for i1, i2, i3 in T.grid(8, 4, 12):\n                cse_var_1: T.int32 = i0 * 384 + i1 * 48 + i2 * 12 + i3\n                compute_1 = T.Buffer((2304,), data=compute.data)\n                data_1 = T.Buffer((2304,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        for (int32_t i3 = 0; i3 < 12; ++i3) {\n          int32_t cse_var_1 = ((((i0 * 384) + (i1 * 48)) + (i2 * 12)) + i3);\n          ((float*)compute_1)[cse_var_1] = asinf(((float*)data_1)[cse_var_1]);\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asin"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 4, 16, 32), \"float32\"), compute: T.Buffer((4, 4, 16, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(8192):\n            compute_1 = T.Buffer((8192,), data=compute.data)\n            data_1 = T.Buffer((8192,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.asin(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 8192; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = asinf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asin"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 20, 16, 28), \"float32\"), compute: T.Buffer((10, 20, 16, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(89600):\n            compute_1 = T.Buffer((89600,), data=compute.data)\n            data_1 = T.Buffer((89600,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.asin(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 89600; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = asinf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asin"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 40, 24, 40), \"float32\"), compute: T.Buffer((5, 40, 24, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(4800):\n            for i3 in range(40):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 40 + i3\n                compute_1 = T.Buffer((192000,), data=compute.data)\n                data_1 = T.Buffer((192000,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4800; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 40; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 40) + i3);\n      ((float*)compute_1)[cse_var_1] = asinf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asin"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 28, 24, 28), \"float32\"), compute: T.Buffer((2, 28, 24, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(37632):\n            compute_1 = T.Buffer((37632,), data=compute.data)\n            data_1 = T.Buffer((37632,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.asin(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 37632; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = asinf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asin"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 24, 16, 32), \"float32\"), compute: T.Buffer((9, 24, 16, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(110592):\n            compute_1 = T.Buffer((110592,), data=compute.data)\n            data_1 = T.Buffer((110592,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.asinh(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 110592; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = asinhf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asinh"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 20, 36, 8), \"float32\"), compute: T.Buffer((5, 20, 36, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(100):\n            for i2, i3_s in T.grid(36, 8):\n                cse_var_1: T.int32 = i0_i1_fused * 288 + i2 * 8 + i3_s\n                compute_1 = T.Buffer((28800,), data=compute.data)\n                data_1 = T.Buffer((28800,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 100; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 36; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n        int32_t cse_var_1 = (((i0_i1_fused * 288) + (i2 * 8)) + i3_s);\n        ((float*)compute_1)[cse_var_1] = asinhf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asinh"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 32, 32, 36), \"float32\"), compute: T.Buffer((5, 32, 32, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(5120):\n            for i3 in range(36):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 36 + i3\n                compute_1 = T.Buffer((184320,), data=compute.data)\n                data_1 = T.Buffer((184320,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5120; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 36; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 36) + i3);\n      ((float*)compute_1)[cse_var_1] = asinhf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asinh"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 28, 20, 16), \"float32\"), compute: T.Buffer((8, 28, 20, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(224):\n            for i2, i3_s in T.grid(20, 16):\n                cse_var_1: T.int32 = i0_i1_fused * 320 + i2 * 16 + i3_s\n                compute_1 = T.Buffer((71680,), data=compute.data)\n                data_1 = T.Buffer((71680,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 224; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 16; ++i3_s) {\n        int32_t cse_var_1 = (((i0_i1_fused * 320) + (i2 * 16)) + i3_s);\n        ((float*)compute_1)[cse_var_1] = asinhf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asinh"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 4, 28, 12), \"float32\"), compute: T.Buffer((8, 4, 28, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(10752):\n            compute_1 = T.Buffer((10752,), data=compute.data)\n            data_1 = T.Buffer((10752,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.asinh(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 10752; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = asinhf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asinh"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 40, 12, 40), \"float32\"), compute: T.Buffer((5, 40, 12, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(200):\n            for i2, i3 in T.grid(12, 40):\n                cse_var_1: T.int32 = i0_i1_fused * 480 + i2 * 40 + i3\n                compute_1 = T.Buffer((96000,), data=compute.data)\n                data_1 = T.Buffer((96000,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 200; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3 = 0; i3 < 40; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 480) + (i2 * 40)) + i3);\n        ((float*)compute_1)[cse_var_1] = asinhf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asinh"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 32, 40, 36), \"float32\"), compute: T.Buffer((3, 32, 40, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(138240):\n            compute_1 = T.Buffer((138240,), data=compute.data)\n            data_1 = T.Buffer((138240,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.asinh(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 138240; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = asinhf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asinh"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 12, 4, 8), \"float32\"), compute: T.Buffer((3, 12, 4, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(144):\n            for i3 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 8 + i3\n                compute_1 = T.Buffer((1152,), data=compute.data)\n                data_1 = T.Buffer((1152,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 144; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 8; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 8) + i3);\n      ((float*)compute_1)[cse_var_1] = asinhf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asinh"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 28, 16, 8), \"float32\"), compute: T.Buffer((3, 28, 16, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1344):\n            for i3_s in range(8):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 8 + i3_s\n                compute_1 = T.Buffer((10752,), data=compute.data)\n                data_1 = T.Buffer((10752,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(21) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(21) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1344; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 8) + i3_s);\n      ((float*)compute_1)[cse_var_1] = asinhf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asinh"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 24, 36, 24), \"float32\"), compute: T.Buffer((7, 24, 36, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(168):\n            for i2, i3 in T.grid(36, 24):\n                cse_var_1: T.int32 = i0_i1_fused * 864 + i2 * 24 + i3\n                compute_1 = T.Buffer((145152,), data=compute.data)\n                data_1 = T.Buffer((145152,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 168; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 36; ++i2) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 864) + (i2 * 24)) + i3);\n        ((float*)compute_1)[cse_var_1] = asinhf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "asinh"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 24, 28, 16), \"float32\"), compute: T.Buffer((1, 24, 28, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(10752):\n            compute_1 = T.Buffer((10752,), data=compute.data)\n            data_1 = T.Buffer((10752,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.atan(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 10752; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = atanf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atan"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 16, 40, 20), \"float32\"), compute: T.Buffer((4, 16, 40, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(64):\n            for i2, i3 in T.grid(40, 20):\n                cse_var_1: T.int32 = i0_i1_fused * 800 + i2 * 20 + i3\n                compute_1 = T.Buffer((51200,), data=compute.data)\n                data_1 = T.Buffer((51200,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 64; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 40; ++i2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 800) + (i2 * 20)) + i3);\n        ((float*)compute_1)[cse_var_1] = atanf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atan"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 40, 12, 24), \"float32\"), compute: T.Buffer((4, 40, 12, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(160):\n            for i2, i3 in T.grid(12, 24):\n                cse_var_1: T.int32 = i0_i1_fused * 288 + i2 * 24 + i3\n                compute_1 = T.Buffer((46080,), data=compute.data)\n                data_1 = T.Buffer((46080,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 160; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 288) + (i2 * 24)) + i3);\n        ((float*)compute_1)[cse_var_1] = atanf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atan"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 40, 4, 16), \"float32\"), compute: T.Buffer((1, 40, 4, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(160):\n            for i3 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 16 + i3\n                compute_1 = T.Buffer((2560,), data=compute.data)\n                data_1 = T.Buffer((2560,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 160; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 16; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 16) + i3);\n      ((float*)compute_1)[cse_var_1] = atanf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atan"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 8, 40, 32), \"float32\"), compute: T.Buffer((2, 8, 40, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(640):\n            for i3 in range(32):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 32 + i3\n                compute_1 = T.Buffer((20480,), data=compute.data)\n                data_1 = T.Buffer((20480,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 640; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 32; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 32) + i3);\n      ((float*)compute_1)[cse_var_1] = atanf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atan"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 40, 12, 12), \"float32\"), compute: T.Buffer((1, 40, 12, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(40):\n            for i2, i3 in T.grid(12, 12):\n                cse_var_1: T.int32 = i0_i1_fused * 144 + i2 * 12 + i3\n                compute_1 = T.Buffer((5760,), data=compute.data)\n                data_1 = T.Buffer((5760,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 40; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3 = 0; i3 < 12; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 144) + (i2 * 12)) + i3);\n        ((float*)compute_1)[cse_var_1] = atanf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atan"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 40, 28, 28), \"float32\"), compute: T.Buffer((2, 40, 28, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(80):\n            for i2, i3 in T.grid(28, 28):\n                cse_var_1: T.int32 = i0_i1_fused * 784 + i2 * 28 + i3\n                compute_1 = T.Buffer((62720,), data=compute.data)\n                data_1 = T.Buffer((62720,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 80; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 28; ++i2) {\n      for (int32_t i3 = 0; i3 < 28; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 784) + (i2 * 28)) + i3);\n        ((float*)compute_1)[cse_var_1] = atanf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atan"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 4, 20, 36), \"float32\"), compute: T.Buffer((7, 4, 20, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(20160):\n            compute_1 = T.Buffer((20160,), data=compute.data)\n            data_1 = T.Buffer((20160,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.atan(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 20160; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = atanf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atan"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 8, 32, 40), \"float32\"), compute: T.Buffer((10, 8, 32, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(102400):\n            compute_1 = T.Buffer((102400,), data=compute.data)\n            data_1 = T.Buffer((102400,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.atan(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 102400; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = atanf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atan"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 36, 36, 16), \"float32\"), compute: T.Buffer((5, 36, 36, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(6480):\n            for i3_s in range(16):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 16 + i3_s\n                compute_1 = T.Buffer((103680,), data=compute.data)\n                data_1 = T.Buffer((103680,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 6480; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 16; ++i3_s) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 16) + i3_s);\n      ((float*)compute_1)[cse_var_1] = atanf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atan"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 40, 24, 24), \"float32\"), compute: T.Buffer((7, 40, 24, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(280):\n            for i2, i3 in T.grid(24, 24):\n                cse_var_1: T.int32 = i0_i1_fused * 576 + i2 * 24 + i3\n                compute_1 = T.Buffer((161280,), data=compute.data)\n                data_1 = T.Buffer((161280,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 280; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 24; ++i2) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 576) + (i2 * 24)) + i3);\n        ((float*)compute_1)[cse_var_1] = atanhf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atanh"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 40, 40, 40), \"float32\"), compute: T.Buffer((7, 40, 40, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(11200):\n            for i3 in range(40):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 40 + i3\n                compute_1 = T.Buffer((448000,), data=compute.data)\n                data_1 = T.Buffer((448000,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 11200; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 40; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 40) + i3);\n      ((float*)compute_1)[cse_var_1] = atanhf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atanh"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 32, 32, 36), \"float32\"), compute: T.Buffer((3, 32, 32, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(110592):\n            compute_1 = T.Buffer((110592,), data=compute.data)\n            data_1 = T.Buffer((110592,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.atanh(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 110592; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = atanhf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atanh"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 32, 4, 28), \"float32\"), compute: T.Buffer((6, 32, 4, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(192):\n            for i2, i3 in T.grid(4, 28):\n                cse_var_1: T.int32 = i0_i1_fused * 112 + i2 * 28 + i3\n                compute_1 = T.Buffer((21504,), data=compute.data)\n                data_1 = T.Buffer((21504,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 192; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 28; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 112) + (i2 * 28)) + i3);\n        ((float*)compute_1)[cse_var_1] = atanhf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atanh"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 36, 32, 12), \"float32\"), compute: T.Buffer((3, 36, 32, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(3456):\n            for i3_s in range(12):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 12 + i3_s\n                compute_1 = T.Buffer((41472,), data=compute.data)\n                data_1 = T.Buffer((41472,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3456; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 12; ++i3_s) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 12) + i3_s);\n      ((float*)compute_1)[cse_var_1] = atanhf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atanh"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 20, 24, 32), \"float32\"), compute: T.Buffer((5, 20, 24, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(100):\n            for i2, i3 in T.grid(24, 32):\n                cse_var_1: T.int32 = i0_i1_fused * 768 + i2 * 32 + i3\n                compute_1 = T.Buffer((76800,), data=compute.data)\n                data_1 = T.Buffer((76800,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 100; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 24; ++i2) {\n      for (int32_t i3 = 0; i3 < 32; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 768) + (i2 * 32)) + i3);\n        ((float*)compute_1)[cse_var_1] = atanhf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atanh"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 4, 16, 8), \"float32\"), compute: T.Buffer((9, 4, 16, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(576):\n            for i3 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 8 + i3\n                compute_1 = T.Buffer((4608,), data=compute.data)\n                data_1 = T.Buffer((4608,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 576; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 8; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 8) + i3);\n      ((float*)compute_1)[cse_var_1] = atanhf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atanh"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 4, 36, 28), \"float32\"), compute: T.Buffer((6, 4, 36, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(864):\n            for i3 in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                compute_1 = T.Buffer((24192,), data=compute.data)\n                data_1 = T.Buffer((24192,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 864; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 28) + i3);\n      ((float*)compute_1)[cse_var_1] = atanhf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atanh"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 36, 20, 8), \"float32\"), compute: T.Buffer((5, 36, 20, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(3600):\n            for i3_s in range(8):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 8 + i3_s\n                compute_1 = T.Buffer((28800,), data=compute.data)\n                data_1 = T.Buffer((28800,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3600; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 8) + i3_s);\n      ((float*)compute_1)[cse_var_1] = atanhf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atanh"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 40, 32, 24), \"float32\"), compute: T.Buffer((6, 40, 32, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(184320):\n            compute_1 = T.Buffer((184320,), data=compute.data)\n            data_1 = T.Buffer((184320,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.atanh(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 184320; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = atanhf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "atanh"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 20, 36, 4), \"float32\"), compute: T.Buffer((1, 20, 36, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(20, 36, 4):\n            cse_var_1: T.int32 = i1 * 144 + i2 * 4 + i3\n            compute_1 = T.Buffer((2880,), data=compute.data)\n            data_1 = T.Buffer((2880,), data=data.data)\n            compute_1[cse_var_1] = T.ceil(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float ceilf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i1 = 0; i1 < 20; ++i1) {\n    for (int32_t i2 = 0; i2 < 36; ++i2) {\n      for (int32_t i3 = 0; i3 < 4; ++i3) {\n        int32_t cse_var_1 = (((i1 * 144) + (i2 * 4)) + i3);\n        ((float*)compute_1)[cse_var_1] = ceilf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "ceil"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 28, 16, 24), \"float32\"), compute: T.Buffer((10, 28, 16, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(4480):\n            for i3 in range(24):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 24 + i3\n                compute_1 = T.Buffer((107520,), data=compute.data)\n                data_1 = T.Buffer((107520,), data=data.data)\n                compute_1[cse_var_1] = T.ceil(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float ceilf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4480; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 24; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 24) + i3);\n      ((float*)compute_1)[cse_var_1] = ceilf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "ceil"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 32, 28, 28), \"float32\"), compute: T.Buffer((7, 32, 28, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(224):\n            for i2, i3 in T.grid(28, 28):\n                cse_var_1: T.int32 = i0_i1_fused * 784 + i2 * 28 + i3\n                compute_1 = T.Buffer((175616,), data=compute.data)\n                data_1 = T.Buffer((175616,), data=data.data)\n                compute_1[cse_var_1] = T.ceil(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float ceilf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 224; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 28; ++i2) {\n      for (int32_t i3 = 0; i3 < 28; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 784) + (i2 * 28)) + i3);\n        ((float*)compute_1)[cse_var_1] = ceilf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "ceil"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 12, 20, 40), \"float32\"), compute: T.Buffer((9, 12, 20, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(86400):\n            compute_1 = T.Buffer((86400,), data=compute.data)\n            data_1 = T.Buffer((86400,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.ceil(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float ceilf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 86400; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = ceilf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "ceil"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 8, 28, 12), \"float32\"), compute: T.Buffer((1, 8, 28, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(8, 28, 12):\n            cse_var_1: T.int32 = i1 * 336 + i2 * 12 + i3\n            compute_1 = T.Buffer((2688,), data=compute.data)\n            data_1 = T.Buffer((2688,), data=data.data)\n            compute_1[cse_var_1] = T.ceil(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float ceilf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i1 = 0; i1 < 8; ++i1) {\n    for (int32_t i2 = 0; i2 < 28; ++i2) {\n      for (int32_t i3 = 0; i3 < 12; ++i3) {\n        int32_t cse_var_1 = (((i1 * 336) + (i2 * 12)) + i3);\n        ((float*)compute_1)[cse_var_1] = ceilf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "ceil"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 16, 16, 24), \"float32\"), compute: T.Buffer((1, 16, 16, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(16, 16, 24):\n            cse_var_1: T.int32 = i1 * 384 + i2 * 24 + i3\n            compute_1 = T.Buffer((6144,), data=compute.data)\n            data_1 = T.Buffer((6144,), data=data.data)\n            compute_1[cse_var_1] = T.ceil(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float ceilf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i1 = 0; i1 < 16; ++i1) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_1 = (((i1 * 384) + (i2 * 24)) + i3);\n        ((float*)compute_1)[cse_var_1] = ceilf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "ceil"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 32, 12, 12), \"float32\"), compute: T.Buffer((8, 32, 12, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(256):\n            for i2, i3_s in T.grid(12, 12):\n                cse_var_1: T.int32 = i0_i1_fused * 144 + i2 * 12 + i3_s\n                compute_1 = T.Buffer((36864,), data=compute.data)\n                data_1 = T.Buffer((36864,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float erff(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 256; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 12; ++i3_s) {\n        int32_t cse_var_1 = (((i0_i1_fused * 144) + (i2 * 12)) + i3_s);\n        ((float*)compute_1)[cse_var_1] = erff(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "erf"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 12, 36, 32), \"float32\"), compute: T.Buffer((3, 12, 36, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1296):\n            for i3 in range(32):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 32 + i3\n                compute_1 = T.Buffer((41472,), data=compute.data)\n                data_1 = T.Buffer((41472,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float erff(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1296; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 32; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 32) + i3);\n      ((float*)compute_1)[cse_var_1] = erff(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "erf"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 16, 36, 12), \"float32\"), compute: T.Buffer((10, 16, 36, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(160):\n            for i2, i3_s in T.grid(36, 12):\n                cse_var_1: T.int32 = i0_i1_fused * 432 + i2 * 12 + i3_s\n                compute_1 = T.Buffer((69120,), data=compute.data)\n                data_1 = T.Buffer((69120,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float erff(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 160; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 36; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 12; ++i3_s) {\n        int32_t cse_var_1 = (((i0_i1_fused * 432) + (i2 * 12)) + i3_s);\n        ((float*)compute_1)[cse_var_1] = erff(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "erf"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 4, 8, 40), \"float32\"), compute: T.Buffer((4, 4, 8, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(5120):\n            compute_1 = T.Buffer((5120,), data=compute.data)\n            data_1 = T.Buffer((5120,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.erf(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float erff(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 5120; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = erff(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "erf"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 40, 4, 28), \"float32\"), compute: T.Buffer((2, 40, 4, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(320):\n            for i3 in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                compute_1 = T.Buffer((8960,), data=compute.data)\n                data_1 = T.Buffer((8960,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float erff(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 320; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 28) + i3);\n      ((float*)compute_1)[cse_var_1] = erff(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "erf"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 28, 24, 4), \"float32\"), compute: T.Buffer((9, 28, 24, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(252):\n            for i2, i3_s in T.grid(24, 4):\n                cse_var_1: T.int32 = i0_i1_fused * 96 + i2 * 4 + i3_s\n                compute_1 = T.Buffer((24192,), data=compute.data)\n                data_1 = T.Buffer((24192,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float erff(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 252; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 24; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 4; ++i3_s) {\n        int32_t cse_var_1 = (((i0_i1_fused * 96) + (i2 * 4)) + i3_s);\n        ((float*)compute_1)[cse_var_1] = erff(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "erf"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 32, 8, 4), \"float32\"), compute: T.Buffer((8, 32, 8, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2048):\n            for i3_s in range(4):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 4 + i3_s\n                compute_1 = T.Buffer((8192,), data=compute.data)\n                data_1 = T.Buffer((8192,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float erff(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2048; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 4; ++i3_s) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 4) + i3_s);\n      ((float*)compute_1)[cse_var_1] = erff(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "erf"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 24, 36, 8), \"float32\"), compute: T.Buffer((7, 24, 36, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(168):\n            for i2, i3_s in T.grid(36, 8):\n                cse_var_1: T.int32 = i0_i1_fused * 288 + i2 * 8 + i3_s\n                compute_1 = T.Buffer((48384,), data=compute.data)\n                data_1 = T.Buffer((48384,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float erff(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 168; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 36; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n        int32_t cse_var_1 = (((i0_i1_fused * 288) + (i2 * 8)) + i3_s);\n        ((float*)compute_1)[cse_var_1] = erff(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "erf"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 24, 8, 24), \"float32\"), compute: T.Buffer((1, 24, 8, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(192):\n            for i3 in range(24):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 24 + i3\n                compute_1 = T.Buffer((4608,), data=compute.data)\n                data_1 = T.Buffer((4608,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float erff(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 192; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 24; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 24) + i3);\n      ((float*)compute_1)[cse_var_1] = erff(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "erf"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 32, 16, 40), \"float32\"), compute: T.Buffer((2, 32, 16, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1024):\n            for i3 in range(40):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 40 + i3\n                compute_1 = T.Buffer((40960,), data=compute.data)\n                data_1 = T.Buffer((40960,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float erff(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1024; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 40; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 40) + i3);\n      ((float*)compute_1)[cse_var_1] = erff(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "erf"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 40, 12, 32), \"float32\"), compute: T.Buffer((9, 40, 12, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(138240):\n            compute_1 = T.Buffer((138240,), data=compute.data)\n            data_1 = T.Buffer((138240,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.exp(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = __expf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 138240; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = expf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "exp"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 40, 8, 24), \"float32\"), compute: T.Buffer((9, 40, 8, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(360):\n            for i2, i3 in T.grid(8, 24):\n                cse_var_1: T.int32 = i0_i1_fused * 192 + i2 * 24 + i3\n                compute_1 = T.Buffer((69120,), data=compute.data)\n                data_1 = T.Buffer((69120,), data=data.data)\n                compute_1[cse_var_1] = T.exp(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = __expf(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 360; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 192) + (i2 * 24)) + i3);\n        ((float*)compute_1)[cse_var_1] = expf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "exp"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 20, 4, 24), \"float32\"), compute: T.Buffer((7, 20, 4, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(140):\n            for i2, i3 in T.grid(4, 24):\n                cse_var_1: T.int32 = i0_i1_fused * 96 + i2 * 24 + i3\n                compute_1 = T.Buffer((13440,), data=compute.data)\n                data_1 = T.Buffer((13440,), data=data.data)\n                compute_1[cse_var_1] = T.exp(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = __expf(data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 140; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 96) + (i2 * 24)) + i3);\n        ((float*)compute_1)[cse_var_1] = expf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "exp"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 4, 28, 20), \"float32\"), compute: T.Buffer((10, 4, 28, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(10):\n            for i1, i2, i3 in T.grid(4, 28, 20):\n                cse_var_1: T.int32 = i0 * 2240 + i1 * 560 + i2 * 20 + i3\n                compute_1 = T.Buffer((22400,), data=compute.data)\n                data_1 = T.Buffer((22400,), data=data.data)\n                compute_1[cse_var_1] = T.exp(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 28; ++i2) {\n        for (int32_t i3 = 0; i3 < 20; ++i3) {\n          int32_t cse_var_1 = ((((i0 * 2240) + (i1 * 560)) + (i2 * 20)) + i3);\n          ((float*)compute_1)[cse_var_1] = expf(((float*)data_1)[cse_var_1]);\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "exp"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 40, 4, 40), \"float32\"), compute: T.Buffer((7, 40, 4, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(280):\n            for i2, i3 in T.grid(4, 40):\n                cse_var_1: T.int32 = i0_i1_fused * 160 + i2 * 40 + i3\n                compute_1 = T.Buffer((44800,), data=compute.data)\n                data_1 = T.Buffer((44800,), data=data.data)\n                compute_1[cse_var_1] = T.exp(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = __expf(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 280; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 40; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 160) + (i2 * 40)) + i3);\n        ((float*)compute_1)[cse_var_1] = expf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "exp"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 16, 20, 40), \"float32\"), compute: T.Buffer((10, 16, 20, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(160):\n            for i2, i3 in T.grid(20, 40):\n                cse_var_1: T.int32 = i0_i1_fused * 800 + i2 * 40 + i3\n                compute_1 = T.Buffer((128000,), data=compute.data)\n                data_1 = T.Buffer((128000,), data=data.data)\n                compute_1[cse_var_1] = T.exp(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 160; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      for (int32_t i3 = 0; i3 < 40; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 800) + (i2 * 40)) + i3);\n        ((float*)compute_1)[cse_var_1] = expf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "exp"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 8, 4, 32), \"float32\"), compute: T.Buffer((5, 8, 4, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(160):\n            for i3 in range(32):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 32 + i3\n                compute_1 = T.Buffer((5120,), data=compute.data)\n                data_1 = T.Buffer((5120,), data=data.data)\n                compute_1[cse_var_1] = T.exp(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 160; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 32; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 32) + i3);\n      ((float*)compute_1)[cse_var_1] = expf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "exp"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 32, 32, 20), \"float32\"), T_fast_erf: T.Buffer((1, 32, 32, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(32):\n            for ax2, ax3 in T.grid(32, 20):\n                cse_var_1: T.int32 = ax0_ax1_fused * 640 + ax2 * 20 + ax3\n                T_fast_erf_1 = T.Buffer((20480,), data=T_fast_erf.data)\n                data_1 = T.Buffer((20480,), data=data.data)\n                T_fast_erf_1[cse_var_1] = T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_erf_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_erf = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_erf_1 = (((DLTensor*)T_fast_erf)[0].data);\n  void* default_function_T_fast_erf_shape = (((DLTensor*)T_fast_erf)[0].shape);\n  void* default_function_T_fast_erf_strides = (((DLTensor*)T_fast_erf)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_erf_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 32; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 32; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 640) + (ax2 * 20)) + ax3);\n        float v_ = ((float*)data_1)[cse_var_1];\n        float v__1 = (v_) < (4.000000e+00f) ? (v_) : (4.000000e+00f);\n        ((float*)T_fast_erf_1)[cse_var_1] = ((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_erf"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 32, 12, 20), \"float32\"), T_fast_erf: T.Buffer((7, 32, 12, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(53760):\n            T_fast_erf_1 = T.Buffer((53760,), data=T_fast_erf.data)\n            data_1 = T.Buffer((53760,), data=data.data)\n            T_fast_erf_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_erf_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_erf = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_erf_1 = (((DLTensor*)T_fast_erf)[0].data);\n  void* default_function_T_fast_erf_shape = (((DLTensor*)T_fast_erf)[0].shape);\n  void* default_function_T_fast_erf_strides = (((DLTensor*)T_fast_erf)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_erf_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 53760; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    float v_ = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__1 = (v_) < (4.000000e+00f) ? (v_) : (4.000000e+00f);\n    ((float*)T_fast_erf_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = ((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_erf"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 4, 40, 8), \"float32\"), T_fast_erf: T.Buffer((3, 4, 40, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(12):\n            for ax2, ax3 in T.grid(40, 8):\n                cse_var_1: T.int32 = ax0_ax1_fused * 320 + ax2 * 8 + ax3\n                T_fast_erf_1 = T.Buffer((3840,), data=T_fast_erf.data)\n                data_1 = T.Buffer((3840,), data=data.data)\n                T_fast_erf_1[cse_var_1] = T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_erf_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_erf = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_erf_1 = (((DLTensor*)T_fast_erf)[0].data);\n  void* default_function_T_fast_erf_shape = (((DLTensor*)T_fast_erf)[0].shape);\n  void* default_function_T_fast_erf_strides = (((DLTensor*)T_fast_erf)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_erf_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 12; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 40; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 320) + (ax2 * 8)) + ax3);\n        float v_ = ((float*)data_1)[cse_var_1];\n        float v__1 = (v_) < (4.000000e+00f) ? (v_) : (4.000000e+00f);\n        ((float*)T_fast_erf_1)[cse_var_1] = ((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_erf"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 24, 16, 32), \"float32\"), T_fast_erf: T.Buffer((2, 24, 16, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(48):\n            for ax2, ax3 in T.grid(16, 32):\n                cse_var_1: T.int32 = ax0_ax1_fused * 512 + ax2 * 32 + ax3\n                T_fast_erf_1 = T.Buffer((24576,), data=T_fast_erf.data)\n                data_1 = T.Buffer((24576,), data=data.data)\n                T_fast_erf_1[cse_var_1] = T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_erf_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_erf = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_erf_1 = (((DLTensor*)T_fast_erf)[0].data);\n  void* default_function_T_fast_erf_shape = (((DLTensor*)T_fast_erf)[0].shape);\n  void* default_function_T_fast_erf_strides = (((DLTensor*)T_fast_erf)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_erf_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 48; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 32; ++ax3) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 512) + (ax2 * 32)) + ax3);\n        float v_ = ((float*)data_1)[cse_var_1];\n        float v__1 = (v_) < (4.000000e+00f) ? (v_) : (4.000000e+00f);\n        ((float*)T_fast_erf_1)[cse_var_1] = ((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_erf"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 4, 12, 8), \"float32\"), T_fast_erf: T.Buffer((4, 4, 12, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(192):\n            for ax3 in range(8):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 8 + ax3\n                T_fast_erf_1 = T.Buffer((1536,), data=T_fast_erf.data)\n                data_1 = T.Buffer((1536,), data=data.data)\n                T_fast_erf_1[cse_var_1] = T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_erf_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_erf = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_erf_1 = (((DLTensor*)T_fast_erf)[0].data);\n  void* default_function_T_fast_erf_shape = (((DLTensor*)T_fast_erf)[0].shape);\n  void* default_function_T_fast_erf_strides = (((DLTensor*)T_fast_erf)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_erf_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 192; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 8) + ax3);\n      float v_ = ((float*)data_1)[cse_var_1];\n      float v__1 = (v_) < (4.000000e+00f) ? (v_) : (4.000000e+00f);\n      ((float*)T_fast_erf_1)[cse_var_1] = ((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_erf"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 32, 4, 20), \"float32\"), T_fast_erf: T.Buffer((5, 32, 4, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(640):\n            for ax3 in range(20):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 20 + ax3\n                T_fast_erf_1 = T.Buffer((12800,), data=T_fast_erf.data)\n                data_1 = T.Buffer((12800,), data=data.data)\n                T_fast_erf_1[cse_var_1] = T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_erf_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_erf = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_erf_1 = (((DLTensor*)T_fast_erf)[0].data);\n  void* default_function_T_fast_erf_shape = (((DLTensor*)T_fast_erf)[0].shape);\n  void* default_function_T_fast_erf_strides = (((DLTensor*)T_fast_erf)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_erf_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 640; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 20) + ax3);\n      float v_ = ((float*)data_1)[cse_var_1];\n      float v__1 = (v_) < (4.000000e+00f) ? (v_) : (4.000000e+00f);\n      ((float*)T_fast_erf_1)[cse_var_1] = ((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_erf"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 24, 28, 4), \"float32\"), T_fast_erf: T.Buffer((7, 24, 28, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(4704):\n            cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 4\n            T_fast_erf_1 = T.Buffer((18816,), data=T_fast_erf.data)\n            data_1 = T.Buffer((18816,), data=data.data)\n            T_fast_erf_1[cse_var_1:cse_var_1 + 4] = T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * T.Broadcast(T.float32(-2.7261423674040941e-10), 4) + T.Broadcast(T.float32(2.7706814620387377e-08), 4)) + T.Broadcast(T.float32(-2.101023937939317e-06), 4)) + T.Broadcast(T.float32(-5.6925062381196767e-05), 4)) + T.Broadcast(T.float32(-0.00073499063728377223), 4)) + T.Broadcast(T.float32(-0.0029545999132096767), 4)) + T.Broadcast(T.float32(-0.016096033155918121), 4)) / (T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(4), 4)), T.Broadcast(T.float32(-4), 4)) * T.Broadcast(T.float32(-1.4566071513399947e-05), 4) + T.Broadcast(T.float32(-0.00021337404905352741), 4)) + T.Broadcast(T.float32(-0.001682827016338706), 4)) + T.Broadcast(T.float32(-0.0073733292520046234), 4)) + T.Broadcast(T.float32(-0.014264739118516445), 4))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_erf_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_erf = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_erf_1 = (((DLTensor*)T_fast_erf)[0].data);\n  void* default_function_T_fast_erf_shape = (((DLTensor*)T_fast_erf)[0].shape);\n  void* default_function_T_fast_erf_strides = (((DLTensor*)T_fast_erf)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_erf_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4704; ++ax0_ax1_fused_ax2_fused) {\n    int32_t cse_var_1 = (ax0_ax1_fused_ax2_fused * 4);\n    int32_t4 v_ = int32_t4((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3));\n    float4 v__1 = float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3]);\n    float4 v__2 = (float4)(4.000000e+00f, 4.000000e+00f, 4.000000e+00f, 4.000000e+00f);\n    float4 v__3 = (v__1) < (v__2) ? (v__1) : (v__2);\n    float4 v__4 = (float4)(-4.000000e+00f, -4.000000e+00f, -4.000000e+00f, -4.000000e+00f);\n    *(float4*)(((float*)T_fast_erf_1) + cse_var_1) = ((((v__3) > (v__4) ? (v__3) : (v__4)) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * ((float4)(-2.726142e-10f, -2.726142e-10f, -2.726142e-10f, -2.726142e-10f))) + ((float4)(2.770681e-08f, 2.770681e-08f, 2.770681e-08f, 2.770681e-08f)))) + ((float4)(-2.101024e-06f, -2.101024e-06f, -2.101024e-06f, -2.101024e-06f)))) + ((float4)(-5.692506e-05f, -5.692506e-05f, -5.692506e-05f, -5.692506e-05f)))) + ((float4)(-7.349906e-04f, -7.349906e-04f, -7.349906e-04f, -7.349906e-04f)))) + ((float4)(-2.954600e-03f, -2.954600e-03f, -2.954600e-03f, -2.954600e-03f)))) + ((float4)(-1.609603e-02f, -1.609603e-02f, -1.609603e-02f, -1.609603e-02f)))) / (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * ((float4)(-1.456607e-05f, -1.456607e-05f, -1.456607e-05f, -1.456607e-05f))) + ((float4)(-2.133740e-04f, -2.133740e-04f, -2.133740e-04f, -2.133740e-04f)))) + ((float4)(-1.682827e-03f, -1.682827e-03f, -1.682827e-03f, -1.682827e-03f)))) + ((float4)(-7.373329e-03f, -7.373329e-03f, -7.373329e-03f, -7.373329e-03f)))) + ((float4)(-1.426474e-02f, -1.426474e-02f, -1.426474e-02f, -1.426474e-02f))));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_erf"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 16, 20, 36), \"float32\"), T_fast_erf: T.Buffer((5, 16, 20, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(1600):\n            for ax3 in range(36):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 36 + ax3\n                T_fast_erf_1 = T.Buffer((57600,), data=T_fast_erf.data)\n                data_1 = T.Buffer((57600,), data=data.data)\n                T_fast_erf_1[cse_var_1] = T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_erf_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_erf = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_erf_1 = (((DLTensor*)T_fast_erf)[0].data);\n  void* default_function_T_fast_erf_shape = (((DLTensor*)T_fast_erf)[0].shape);\n  void* default_function_T_fast_erf_strides = (((DLTensor*)T_fast_erf)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_erf_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1600; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 36; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 36) + ax3);\n      float v_ = ((float*)data_1)[cse_var_1];\n      float v__1 = (v_) < (4.000000e+00f) ? (v_) : (4.000000e+00f);\n      ((float*)T_fast_erf_1)[cse_var_1] = ((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_erf"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 16, 8, 8), \"float32\"), T_fast_erf: T.Buffer((7, 16, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(112):\n            for ax2, ax3 in T.grid(8, 8):\n                cse_var_1: T.int32 = ax0_ax1_fused * 64 + ax2 * 8 + ax3\n                T_fast_erf_1 = T.Buffer((7168,), data=T_fast_erf.data)\n                data_1 = T.Buffer((7168,), data=data.data)\n                T_fast_erf_1[cse_var_1] = T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_erf_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_erf = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_erf_1 = (((DLTensor*)T_fast_erf)[0].data);\n  void* default_function_T_fast_erf_shape = (((DLTensor*)T_fast_erf)[0].shape);\n  void* default_function_T_fast_erf_strides = (((DLTensor*)T_fast_erf)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_erf_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 112; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3);\n        float v_ = ((float*)data_1)[cse_var_1];\n        float v__1 = (v_) < (4.000000e+00f) ? (v_) : (4.000000e+00f);\n        ((float*)T_fast_erf_1)[cse_var_1] = ((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_erf"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 40, 36, 12), \"float32\"), T_fast_erf: T.Buffer((8, 40, 36, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(11520):\n            cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 12\n            T_fast_erf_1 = T.Buffer((138240,), data=T_fast_erf.data)\n            data_1 = T.Buffer((138240,), data=data.data)\n            T_fast_erf_1[cse_var_1:cse_var_1 + 12] = T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * T.Broadcast(T.float32(-2.7261423674040941e-10), 12) + T.Broadcast(T.float32(2.7706814620387377e-08), 12)) + T.Broadcast(T.float32(-2.101023937939317e-06), 12)) + T.Broadcast(T.float32(-5.6925062381196767e-05), 12)) + T.Broadcast(T.float32(-0.00073499063728377223), 12)) + T.Broadcast(T.float32(-0.0029545999132096767), 12)) + T.Broadcast(T.float32(-0.016096033155918121), 12)) / (T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * (T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * T.max(T.min(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(4), 12)), T.Broadcast(T.float32(-4), 12)) * T.Broadcast(T.float32(-1.4566071513399947e-05), 12) + T.Broadcast(T.float32(-0.00021337404905352741), 12)) + T.Broadcast(T.float32(-0.001682827016338706), 12)) + T.Broadcast(T.float32(-0.0073733292520046234), 12)) + T.Broadcast(T.float32(-0.014264739118516445), 12))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_erf_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_erf = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_erf_1 = (((DLTensor*)T_fast_erf)[0].data);\n  void* default_function_T_fast_erf_shape = (((DLTensor*)T_fast_erf)[0].shape);\n  void* default_function_T_fast_erf_strides = (((DLTensor*)T_fast_erf)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_erf_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 11520; ++ax0_ax1_fused_ax2_fused) {\n    int32_t cse_var_1 = (ax0_ax1_fused_ax2_fused * 12);\n    int32_t12 v_ = int32_t12((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7), (cse_var_1)+(1*8), (cse_var_1)+(1*9), (cse_var_1)+(1*10), (cse_var_1)+(1*11));\n    float12 v__1 = float12(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb]);\n    float12 v__2 = (float12)(4.000000e+00f, 4.000000e+00f, 4.000000e+00f, 4.000000e+00f, 4.000000e+00f, 4.000000e+00f, 4.000000e+00f, 4.000000e+00f, 4.000000e+00f, 4.000000e+00f, 4.000000e+00f, 4.000000e+00f);\n    float12 v__3 = (v__1) < (v__2) ? (v__1) : (v__2);\n    float12 v__4 = (float12)(-4.000000e+00f, -4.000000e+00f, -4.000000e+00f, -4.000000e+00f, -4.000000e+00f, -4.000000e+00f, -4.000000e+00f, -4.000000e+00f, -4.000000e+00f, -4.000000e+00f, -4.000000e+00f, -4.000000e+00f);\n    *(float12*)(((float*)T_fast_erf_1) + cse_var_1) = ((((v__3) > (v__4) ? (v__3) : (v__4)) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * ((float12)(-2.726142e-10f, -2.726142e-10f, -2.726142e-10f, -2.726142e-10f, -2.726142e-10f, -2.726142e-10f, -2.726142e-10f, -2.726142e-10f, -2.726142e-10f, -2.726142e-10f, -2.726142e-10f, -2.726142e-10f))) + ((float12)(2.770681e-08f, 2.770681e-08f, 2.770681e-08f, 2.770681e-08f, 2.770681e-08f, 2.770681e-08f, 2.770681e-08f, 2.770681e-08f, 2.770681e-08f, 2.770681e-08f, 2.770681e-08f, 2.770681e-08f)))) + ((float12)(-2.101024e-06f, -2.101024e-06f, -2.101024e-06f, -2.101024e-06f, -2.101024e-06f, -2.101024e-06f, -2.101024e-06f, -2.101024e-06f, -2.101024e-06f, -2.101024e-06f, -2.101024e-06f, -2.101024e-06f)))) + ((float12)(-5.692506e-05f, -5.692506e-05f, -5.692506e-05f, -5.692506e-05f, -5.692506e-05f, -5.692506e-05f, -5.692506e-05f, -5.692506e-05f, -5.692506e-05f, -5.692506e-05f, -5.692506e-05f, -5.692506e-05f)))) + ((float12)(-7.349906e-04f, -7.349906e-04f, -7.349906e-04f, -7.349906e-04f, -7.349906e-04f, -7.349906e-04f, -7.349906e-04f, -7.349906e-04f, -7.349906e-04f, -7.349906e-04f, -7.349906e-04f, -7.349906e-04f)))) + ((float12)(-2.954600e-03f, -2.954600e-03f, -2.954600e-03f, -2.954600e-03f, -2.954600e-03f, -2.954600e-03f, -2.954600e-03f, -2.954600e-03f, -2.954600e-03f, -2.954600e-03f, -2.954600e-03f, -2.954600e-03f)))) + ((float12)(-1.609603e-02f, -1.609603e-02f, -1.609603e-02f, -1.609603e-02f, -1.609603e-02f, -1.609603e-02f, -1.609603e-02f, -1.609603e-02f, -1.609603e-02f, -1.609603e-02f, -1.609603e-02f, -1.609603e-02f)))) / (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * (((((v__3) > (v__4) ? (v__3) : (v__4)) * ((v__3) > (v__4) ? (v__3) : (v__4))) * ((float12)(-1.456607e-05f, -1.456607e-05f, -1.456607e-05f, -1.456607e-05f, -1.456607e-05f, -1.456607e-05f, -1.456607e-05f, -1.456607e-05f, -1.456607e-05f, -1.456607e-05f, -1.456607e-05f, -1.456607e-05f))) + ((float12)(-2.133740e-04f, -2.133740e-04f, -2.133740e-04f, -2.133740e-04f, -2.133740e-04f, -2.133740e-04f, -2.133740e-04f, -2.133740e-04f, -2.133740e-04f, -2.133740e-04f, -2.133740e-04f, -2.133740e-04f)))) + ((float12)(-1.682827e-03f, -1.682827e-03f, -1.682827e-03f, -1.682827e-03f, -1.682827e-03f, -1.682827e-03f, -1.682827e-03f, -1.682827e-03f, -1.682827e-03f, -1.682827e-03f, -1.682827e-03f, -1.682827e-03f)))) + ((float12)(-7.373329e-03f, -7.373329e-03f, -7.373329e-03f, -7.373329e-03f, -7.373329e-03f, -7.373329e-03f, -7.373329e-03f, -7.373329e-03f, -7.373329e-03f, -7.373329e-03f, -7.373329e-03f, -7.373329e-03f)))) + ((float12)(-1.426474e-02f, -1.426474e-02f, -1.426474e-02f, -1.426474e-02f, -1.426474e-02f, -1.426474e-02f, -1.426474e-02f, -1.426474e-02f, -1.426474e-02f, -1.426474e-02f, -1.426474e-02f, -1.426474e-02f))));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_erf"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 32, 40, 4), \"float32\"), T_fast_exp: T.Buffer((9, 32, 40, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(11520):\n            for ax3_s in range(4):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 4 + ax3_s\n                T_fast_exp_1 = T.Buffer((46080,), data=T_fast_exp.data)\n                data_1 = T.Buffer((46080,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_exp_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_exp = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_exp_1 = (((DLTensor*)T_fast_exp)[0].data);\n  void* default_function_T_fast_exp_shape = (((DLTensor*)T_fast_exp)[0].shape);\n  void* default_function_T_fast_exp_strides = (((DLTensor*)T_fast_exp)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_exp_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 11520; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3_s = 0; ax3_s < 4; ++ax3_s) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 4) + ax3_s);\n        float v_ = ((float*)data_1)[cse_var_1];\n        float v__1 = (v_) < (8.837627e+01f) ? (v_) : (8.837627e+01f);\n        int32_t v__2 = ((int32_t)(floorf(((((v__1) > (-8.837626e+01f) ? (v__1) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__3 = ((float*)data_1)[cse_var_1];\n      float v__4 = (v__3) < (8.837627e+01f) ? (v__3) : (8.837627e+01f);\n      float v__5 = ((float*)data_1)[cse_var_1];\n      float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n      float v__7 = ((float*)data_1)[cse_var_1];\n      float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n      float v__9 = ((float*)data_1)[cse_var_1];\n      float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n      float v__11 = ((float*)data_1)[cse_var_1];\n      float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n      float v__13 = ((float*)data_1)[cse_var_1];\n      float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n      float v__15 = ((float*)data_1)[cse_var_1];\n      float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n      float v__17 = ((float*)data_1)[cse_var_1];\n      float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n      float v__19 = ((float*)data_1)[cse_var_1];\n      float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n      float v__21 = ((float*)data_1)[cse_var_1];\n      float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n      float v__23 = ((float*)data_1)[cse_var_1];\n      float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n      float v__25 = ((float*)data_1)[cse_var_1];\n      float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n      float v__27 = ((float*)data_1)[cse_var_1];\n      float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n      float v__29 = ((float*)data_1)[cse_var_1];\n      float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n      float v__31 = ((float*)data_1)[cse_var_1];\n      float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n      float v__33 = ((float*)data_1)[cse_var_1];\n      float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n      float v__35 = (*(float *)(&(v__2))) * ((((((((((((((1.987569e-04f * (((v__4) > (-8.837626e+01f) ? (v__4) : (-8.837626e+01f)) - (floorf(((((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) - (floorf(((((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) - (floorf(((((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) - (floorf(((((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) - (floorf(((((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) - (floorf(((((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) - (floorf(((((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) - (floorf(((((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__36 = ((float*)data_1)[cse_var_1];\n      ((float*)T_fast_exp_1)[cse_var_1] = ((v__35) > (v__36) ? (v__35) : (v__36));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_exp"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 36, 32, 28), \"float32\"), T_fast_exp: T.Buffer((8, 36, 32, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(258048):\n            T_fast_exp_1 = T.Buffer((258048,), data=T_fast_exp.data)\n            data_1 = T.Buffer((258048,), data=data.data)\n            T_fast_exp_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_exp_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_exp = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_exp_1 = (((DLTensor*)T_fast_exp)[0].data);\n  void* default_function_T_fast_exp_shape = (((DLTensor*)T_fast_exp)[0].shape);\n  void* default_function_T_fast_exp_strides = (((DLTensor*)T_fast_exp)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_exp_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 258048; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n      float v_ = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n      float v__1 = (v_) < (8.837627e+01f) ? (v_) : (8.837627e+01f);\n      int32_t v__2 = ((int32_t)(floorf(((((v__1) > (-8.837626e+01f) ? (v__1) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    float v__3 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__4 = (v__3) < (8.837627e+01f) ? (v__3) : (8.837627e+01f);\n    float v__5 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n    float v__7 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n    float v__9 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n    float v__11 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n    float v__13 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n    float v__15 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n    float v__17 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n    float v__19 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n    float v__21 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n    float v__23 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n    float v__25 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n    float v__27 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n    float v__29 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n    float v__31 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n    float v__33 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n    float v__35 = (*(float *)(&(v__2))) * ((((((((((((((1.987569e-04f * (((v__4) > (-8.837626e+01f) ? (v__4) : (-8.837626e+01f)) - (floorf(((((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) - (floorf(((((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) - (floorf(((((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) - (floorf(((((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) - (floorf(((((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) - (floorf(((((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) - (floorf(((((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) - (floorf(((((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n    float v__36 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    ((float*)T_fast_exp_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = ((v__35) > (v__36) ? (v__35) : (v__36));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_exp"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 20, 36, 4), \"float32\"), T_fast_exp: T.Buffer((6, 20, 36, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(4320):\n            for ax3_s in range(4):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 4 + ax3_s\n                T_fast_exp_1 = T.Buffer((17280,), data=T_fast_exp.data)\n                data_1 = T.Buffer((17280,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_exp_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_exp = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_exp_1 = (((DLTensor*)T_fast_exp)[0].data);\n  void* default_function_T_fast_exp_shape = (((DLTensor*)T_fast_exp)[0].shape);\n  void* default_function_T_fast_exp_strides = (((DLTensor*)T_fast_exp)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_exp_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4320; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3_s = 0; ax3_s < 4; ++ax3_s) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 4) + ax3_s);\n        float v_ = ((float*)data_1)[cse_var_1];\n        float v__1 = (v_) < (8.837627e+01f) ? (v_) : (8.837627e+01f);\n        int32_t v__2 = ((int32_t)(floorf(((((v__1) > (-8.837626e+01f) ? (v__1) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__3 = ((float*)data_1)[cse_var_1];\n      float v__4 = (v__3) < (8.837627e+01f) ? (v__3) : (8.837627e+01f);\n      float v__5 = ((float*)data_1)[cse_var_1];\n      float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n      float v__7 = ((float*)data_1)[cse_var_1];\n      float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n      float v__9 = ((float*)data_1)[cse_var_1];\n      float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n      float v__11 = ((float*)data_1)[cse_var_1];\n      float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n      float v__13 = ((float*)data_1)[cse_var_1];\n      float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n      float v__15 = ((float*)data_1)[cse_var_1];\n      float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n      float v__17 = ((float*)data_1)[cse_var_1];\n      float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n      float v__19 = ((float*)data_1)[cse_var_1];\n      float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n      float v__21 = ((float*)data_1)[cse_var_1];\n      float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n      float v__23 = ((float*)data_1)[cse_var_1];\n      float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n      float v__25 = ((float*)data_1)[cse_var_1];\n      float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n      float v__27 = ((float*)data_1)[cse_var_1];\n      float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n      float v__29 = ((float*)data_1)[cse_var_1];\n      float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n      float v__31 = ((float*)data_1)[cse_var_1];\n      float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n      float v__33 = ((float*)data_1)[cse_var_1];\n      float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n      float v__35 = (*(float *)(&(v__2))) * ((((((((((((((1.987569e-04f * (((v__4) > (-8.837626e+01f) ? (v__4) : (-8.837626e+01f)) - (floorf(((((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) - (floorf(((((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) - (floorf(((((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) - (floorf(((((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) - (floorf(((((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) - (floorf(((((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) - (floorf(((((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) - (floorf(((((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__36 = ((float*)data_1)[cse_var_1];\n      ((float*)T_fast_exp_1)[cse_var_1] = ((v__35) > (v__36) ? (v__35) : (v__36));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_exp"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 40, 16, 24), \"float32\"), T_fast_exp: T.Buffer((2, 40, 16, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(30720):\n            T_fast_exp_1 = T.Buffer((30720,), data=T_fast_exp.data)\n            data_1 = T.Buffer((30720,), data=data.data)\n            T_fast_exp_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_exp_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_exp = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_exp_1 = (((DLTensor*)T_fast_exp)[0].data);\n  void* default_function_T_fast_exp_shape = (((DLTensor*)T_fast_exp)[0].shape);\n  void* default_function_T_fast_exp_strides = (((DLTensor*)T_fast_exp)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_exp_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 30720; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n      float v_ = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n      float v__1 = (v_) < (8.837627e+01f) ? (v_) : (8.837627e+01f);\n      int32_t v__2 = ((int32_t)(floorf(((((v__1) > (-8.837626e+01f) ? (v__1) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    float v__3 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__4 = (v__3) < (8.837627e+01f) ? (v__3) : (8.837627e+01f);\n    float v__5 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n    float v__7 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n    float v__9 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n    float v__11 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n    float v__13 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n    float v__15 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n    float v__17 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n    float v__19 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n    float v__21 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n    float v__23 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n    float v__25 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n    float v__27 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n    float v__29 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n    float v__31 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n    float v__33 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n    float v__35 = (*(float *)(&(v__2))) * ((((((((((((((1.987569e-04f * (((v__4) > (-8.837626e+01f) ? (v__4) : (-8.837626e+01f)) - (floorf(((((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) - (floorf(((((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) - (floorf(((((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) - (floorf(((((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) - (floorf(((((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) - (floorf(((((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) - (floorf(((((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) - (floorf(((((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n    float v__36 = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    ((float*)T_fast_exp_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = ((v__35) > (v__36) ? (v__35) : (v__36));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_exp"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 28, 28, 12), \"float32\"), T_fast_exp: T.Buffer((3, 28, 28, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(84):\n            for ax2, ax3_s in T.grid(28, 12):\n                cse_var_1: T.int32 = ax0_ax1_fused * 336 + ax2 * 12 + ax3_s\n                T_fast_exp_1 = T.Buffer((28224,), data=T_fast_exp.data)\n                data_1 = T.Buffer((28224,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_exp_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_exp = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_exp_1 = (((DLTensor*)T_fast_exp)[0].data);\n  void* default_function_T_fast_exp_shape = (((DLTensor*)T_fast_exp)[0].shape);\n  void* default_function_T_fast_exp_strides = (((DLTensor*)T_fast_exp)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_exp_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 84; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 28; ++ax2) {\n      for (int32_t ax3_s = 0; ax3_s < 12; ++ax3_s) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 336) + (ax2 * 12)) + ax3_s);\n          float v_ = ((float*)data_1)[cse_var_1];\n          float v__1 = (v_) < (8.837627e+01f) ? (v_) : (8.837627e+01f);\n          int32_t v__2 = ((int32_t)(floorf(((((v__1) > (-8.837626e+01f) ? (v__1) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__3 = ((float*)data_1)[cse_var_1];\n        float v__4 = (v__3) < (8.837627e+01f) ? (v__3) : (8.837627e+01f);\n        float v__5 = ((float*)data_1)[cse_var_1];\n        float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n        float v__7 = ((float*)data_1)[cse_var_1];\n        float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n        float v__9 = ((float*)data_1)[cse_var_1];\n        float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n        float v__11 = ((float*)data_1)[cse_var_1];\n        float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n        float v__13 = ((float*)data_1)[cse_var_1];\n        float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n        float v__15 = ((float*)data_1)[cse_var_1];\n        float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n        float v__17 = ((float*)data_1)[cse_var_1];\n        float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n        float v__19 = ((float*)data_1)[cse_var_1];\n        float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n        float v__21 = ((float*)data_1)[cse_var_1];\n        float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n        float v__23 = ((float*)data_1)[cse_var_1];\n        float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n        float v__25 = ((float*)data_1)[cse_var_1];\n        float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n        float v__27 = ((float*)data_1)[cse_var_1];\n        float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n        float v__29 = ((float*)data_1)[cse_var_1];\n        float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n        float v__31 = ((float*)data_1)[cse_var_1];\n        float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n        float v__33 = ((float*)data_1)[cse_var_1];\n        float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n        float v__35 = (*(float *)(&(v__2))) * ((((((((((((((1.987569e-04f * (((v__4) > (-8.837626e+01f) ? (v__4) : (-8.837626e+01f)) - (floorf(((((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) - (floorf(((((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) - (floorf(((((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) - (floorf(((((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) - (floorf(((((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) - (floorf(((((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) - (floorf(((((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) - (floorf(((((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__36 = ((float*)data_1)[cse_var_1];\n        ((float*)T_fast_exp_1)[cse_var_1] = ((v__35) > (v__36) ? (v__35) : (v__36));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_exp"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 32, 40, 8), \"float32\"), T_fast_exp: T.Buffer((7, 32, 40, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(224):\n            for ax2, ax3_s in T.grid(40, 8):\n                cse_var_1: T.int32 = ax0_ax1_fused * 320 + ax2 * 8 + ax3_s\n                T_fast_exp_1 = T.Buffer((71680,), data=T_fast_exp.data)\n                data_1 = T.Buffer((71680,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_exp_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_exp = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_exp_1 = (((DLTensor*)T_fast_exp)[0].data);\n  void* default_function_T_fast_exp_shape = (((DLTensor*)T_fast_exp)[0].shape);\n  void* default_function_T_fast_exp_strides = (((DLTensor*)T_fast_exp)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_exp_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 224; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 40; ++ax2) {\n      for (int32_t ax3_s = 0; ax3_s < 8; ++ax3_s) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 320) + (ax2 * 8)) + ax3_s);\n          float v_ = ((float*)data_1)[cse_var_1];\n          float v__1 = (v_) < (8.837627e+01f) ? (v_) : (8.837627e+01f);\n          int32_t v__2 = ((int32_t)(floorf(((((v__1) > (-8.837626e+01f) ? (v__1) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__3 = ((float*)data_1)[cse_var_1];\n        float v__4 = (v__3) < (8.837627e+01f) ? (v__3) : (8.837627e+01f);\n        float v__5 = ((float*)data_1)[cse_var_1];\n        float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n        float v__7 = ((float*)data_1)[cse_var_1];\n        float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n        float v__9 = ((float*)data_1)[cse_var_1];\n        float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n        float v__11 = ((float*)data_1)[cse_var_1];\n        float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n        float v__13 = ((float*)data_1)[cse_var_1];\n        float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n        float v__15 = ((float*)data_1)[cse_var_1];\n        float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n        float v__17 = ((float*)data_1)[cse_var_1];\n        float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n        float v__19 = ((float*)data_1)[cse_var_1];\n        float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n        float v__21 = ((float*)data_1)[cse_var_1];\n        float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n        float v__23 = ((float*)data_1)[cse_var_1];\n        float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n        float v__25 = ((float*)data_1)[cse_var_1];\n        float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n        float v__27 = ((float*)data_1)[cse_var_1];\n        float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n        float v__29 = ((float*)data_1)[cse_var_1];\n        float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n        float v__31 = ((float*)data_1)[cse_var_1];\n        float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n        float v__33 = ((float*)data_1)[cse_var_1];\n        float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n        float v__35 = (*(float *)(&(v__2))) * ((((((((((((((1.987569e-04f * (((v__4) > (-8.837626e+01f) ? (v__4) : (-8.837626e+01f)) - (floorf(((((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) - (floorf(((((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) - (floorf(((((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) - (floorf(((((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) - (floorf(((((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) - (floorf(((((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) - (floorf(((((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) - (floorf(((((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__36 = ((float*)data_1)[cse_var_1];\n        ((float*)T_fast_exp_1)[cse_var_1] = ((v__35) > (v__36) ? (v__35) : (v__36));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_exp"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 16, 12, 28), \"float32\"), T_fast_exp: T.Buffer((8, 16, 12, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(1536):\n            for ax3 in range(28):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 28 + ax3\n                T_fast_exp_1 = T.Buffer((43008,), data=T_fast_exp.data)\n                data_1 = T.Buffer((43008,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_exp_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_exp = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_exp_1 = (((DLTensor*)T_fast_exp)[0].data);\n  void* default_function_T_fast_exp_shape = (((DLTensor*)T_fast_exp)[0].shape);\n  void* default_function_T_fast_exp_strides = (((DLTensor*)T_fast_exp)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_exp_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1536; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 28; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 28) + ax3);\n        float v_ = ((float*)data_1)[cse_var_1];\n        float v__1 = (v_) < (8.837627e+01f) ? (v_) : (8.837627e+01f);\n        int32_t v__2 = ((int32_t)(floorf(((((v__1) > (-8.837626e+01f) ? (v__1) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__3 = ((float*)data_1)[cse_var_1];\n      float v__4 = (v__3) < (8.837627e+01f) ? (v__3) : (8.837627e+01f);\n      float v__5 = ((float*)data_1)[cse_var_1];\n      float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n      float v__7 = ((float*)data_1)[cse_var_1];\n      float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n      float v__9 = ((float*)data_1)[cse_var_1];\n      float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n      float v__11 = ((float*)data_1)[cse_var_1];\n      float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n      float v__13 = ((float*)data_1)[cse_var_1];\n      float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n      float v__15 = ((float*)data_1)[cse_var_1];\n      float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n      float v__17 = ((float*)data_1)[cse_var_1];\n      float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n      float v__19 = ((float*)data_1)[cse_var_1];\n      float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n      float v__21 = ((float*)data_1)[cse_var_1];\n      float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n      float v__23 = ((float*)data_1)[cse_var_1];\n      float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n      float v__25 = ((float*)data_1)[cse_var_1];\n      float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n      float v__27 = ((float*)data_1)[cse_var_1];\n      float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n      float v__29 = ((float*)data_1)[cse_var_1];\n      float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n      float v__31 = ((float*)data_1)[cse_var_1];\n      float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n      float v__33 = ((float*)data_1)[cse_var_1];\n      float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n      float v__35 = (*(float *)(&(v__2))) * ((((((((((((((1.987569e-04f * (((v__4) > (-8.837626e+01f) ? (v__4) : (-8.837626e+01f)) - (floorf(((((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) - (floorf(((((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) - (floorf(((((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) - (floorf(((((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) - (floorf(((((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) - (floorf(((((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) - (floorf(((((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) - (floorf(((((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__36 = ((float*)data_1)[cse_var_1];\n      ((float*)T_fast_exp_1)[cse_var_1] = ((v__35) > (v__36) ? (v__35) : (v__36));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_exp"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 36, 32, 20), \"float32\"), T_fast_exp: T.Buffer((7, 36, 32, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(252):\n            for ax2, ax3 in T.grid(32, 20):\n                cse_var_1: T.int32 = ax0_ax1_fused * 640 + ax2 * 20 + ax3\n                T_fast_exp_1 = T.Buffer((161280,), data=T_fast_exp.data)\n                data_1 = T.Buffer((161280,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_exp_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_exp = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_exp_1 = (((DLTensor*)T_fast_exp)[0].data);\n  void* default_function_T_fast_exp_shape = (((DLTensor*)T_fast_exp)[0].shape);\n  void* default_function_T_fast_exp_strides = (((DLTensor*)T_fast_exp)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_exp_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 252; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 32; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 640) + (ax2 * 20)) + ax3);\n          float v_ = ((float*)data_1)[cse_var_1];\n          float v__1 = (v_) < (8.837627e+01f) ? (v_) : (8.837627e+01f);\n          int32_t v__2 = ((int32_t)(floorf(((((v__1) > (-8.837626e+01f) ? (v__1) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__3 = ((float*)data_1)[cse_var_1];\n        float v__4 = (v__3) < (8.837627e+01f) ? (v__3) : (8.837627e+01f);\n        float v__5 = ((float*)data_1)[cse_var_1];\n        float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n        float v__7 = ((float*)data_1)[cse_var_1];\n        float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n        float v__9 = ((float*)data_1)[cse_var_1];\n        float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n        float v__11 = ((float*)data_1)[cse_var_1];\n        float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n        float v__13 = ((float*)data_1)[cse_var_1];\n        float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n        float v__15 = ((float*)data_1)[cse_var_1];\n        float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n        float v__17 = ((float*)data_1)[cse_var_1];\n        float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n        float v__19 = ((float*)data_1)[cse_var_1];\n        float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n        float v__21 = ((float*)data_1)[cse_var_1];\n        float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n        float v__23 = ((float*)data_1)[cse_var_1];\n        float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n        float v__25 = ((float*)data_1)[cse_var_1];\n        float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n        float v__27 = ((float*)data_1)[cse_var_1];\n        float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n        float v__29 = ((float*)data_1)[cse_var_1];\n        float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n        float v__31 = ((float*)data_1)[cse_var_1];\n        float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n        float v__33 = ((float*)data_1)[cse_var_1];\n        float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n        float v__35 = (*(float *)(&(v__2))) * ((((((((((((((1.987569e-04f * (((v__4) > (-8.837626e+01f) ? (v__4) : (-8.837626e+01f)) - (floorf(((((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) - (floorf(((((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) - (floorf(((((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) - (floorf(((((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) - (floorf(((((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) - (floorf(((((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) - (floorf(((((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) - (floorf(((((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__36 = ((float*)data_1)[cse_var_1];\n        ((float*)T_fast_exp_1)[cse_var_1] = ((v__35) > (v__36) ? (v__35) : (v__36));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_exp"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 12, 24, 36), \"float32\"), T_fast_exp: T.Buffer((3, 12, 24, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(864):\n            for ax3 in range(36):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 36 + ax3\n                T_fast_exp_1 = T.Buffer((31104,), data=T_fast_exp.data)\n                data_1 = T.Buffer((31104,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_exp_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_exp = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_exp_1 = (((DLTensor*)T_fast_exp)[0].data);\n  void* default_function_T_fast_exp_shape = (((DLTensor*)T_fast_exp)[0].shape);\n  void* default_function_T_fast_exp_strides = (((DLTensor*)T_fast_exp)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_exp_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 864; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 36; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 36) + ax3);\n        float v_ = ((float*)data_1)[cse_var_1];\n        float v__1 = (v_) < (8.837627e+01f) ? (v_) : (8.837627e+01f);\n        int32_t v__2 = ((int32_t)(floorf(((((v__1) > (-8.837626e+01f) ? (v__1) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__3 = ((float*)data_1)[cse_var_1];\n      float v__4 = (v__3) < (8.837627e+01f) ? (v__3) : (8.837627e+01f);\n      float v__5 = ((float*)data_1)[cse_var_1];\n      float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n      float v__7 = ((float*)data_1)[cse_var_1];\n      float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n      float v__9 = ((float*)data_1)[cse_var_1];\n      float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n      float v__11 = ((float*)data_1)[cse_var_1];\n      float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n      float v__13 = ((float*)data_1)[cse_var_1];\n      float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n      float v__15 = ((float*)data_1)[cse_var_1];\n      float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n      float v__17 = ((float*)data_1)[cse_var_1];\n      float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n      float v__19 = ((float*)data_1)[cse_var_1];\n      float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n      float v__21 = ((float*)data_1)[cse_var_1];\n      float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n      float v__23 = ((float*)data_1)[cse_var_1];\n      float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n      float v__25 = ((float*)data_1)[cse_var_1];\n      float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n      float v__27 = ((float*)data_1)[cse_var_1];\n      float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n      float v__29 = ((float*)data_1)[cse_var_1];\n      float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n      float v__31 = ((float*)data_1)[cse_var_1];\n      float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n      float v__33 = ((float*)data_1)[cse_var_1];\n      float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n      float v__35 = (*(float *)(&(v__2))) * ((((((((((((((1.987569e-04f * (((v__4) > (-8.837626e+01f) ? (v__4) : (-8.837626e+01f)) - (floorf(((((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) - (floorf(((((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) - (floorf(((((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) - (floorf(((((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) - (floorf(((((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) - (floorf(((((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) - (floorf(((((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) - (floorf(((((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__36 = ((float*)data_1)[cse_var_1];\n      ((float*)T_fast_exp_1)[cse_var_1] = ((v__35) > (v__36) ? (v__35) : (v__36));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_exp"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 16, 12, 24), \"float32\"), T_fast_exp: T.Buffer((8, 16, 12, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(1536):\n            for ax3 in range(24):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 24 + ax3\n                T_fast_exp_1 = T.Buffer((36864,), data=T_fast_exp.data)\n                data_1 = T.Buffer((36864,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_exp_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_exp = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_exp_1 = (((DLTensor*)T_fast_exp)[0].data);\n  void* default_function_T_fast_exp_shape = (((DLTensor*)T_fast_exp)[0].shape);\n  void* default_function_T_fast_exp_strides = (((DLTensor*)T_fast_exp)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_exp_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1536; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 24; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 24) + ax3);\n        float v_ = ((float*)data_1)[cse_var_1];\n        float v__1 = (v_) < (8.837627e+01f) ? (v_) : (8.837627e+01f);\n        int32_t v__2 = ((int32_t)(floorf(((((v__1) > (-8.837626e+01f) ? (v__1) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__3 = ((float*)data_1)[cse_var_1];\n      float v__4 = (v__3) < (8.837627e+01f) ? (v__3) : (8.837627e+01f);\n      float v__5 = ((float*)data_1)[cse_var_1];\n      float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n      float v__7 = ((float*)data_1)[cse_var_1];\n      float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n      float v__9 = ((float*)data_1)[cse_var_1];\n      float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n      float v__11 = ((float*)data_1)[cse_var_1];\n      float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n      float v__13 = ((float*)data_1)[cse_var_1];\n      float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n      float v__15 = ((float*)data_1)[cse_var_1];\n      float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n      float v__17 = ((float*)data_1)[cse_var_1];\n      float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n      float v__19 = ((float*)data_1)[cse_var_1];\n      float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n      float v__21 = ((float*)data_1)[cse_var_1];\n      float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n      float v__23 = ((float*)data_1)[cse_var_1];\n      float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n      float v__25 = ((float*)data_1)[cse_var_1];\n      float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n      float v__27 = ((float*)data_1)[cse_var_1];\n      float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n      float v__29 = ((float*)data_1)[cse_var_1];\n      float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n      float v__31 = ((float*)data_1)[cse_var_1];\n      float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n      float v__33 = ((float*)data_1)[cse_var_1];\n      float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n      float v__35 = (*(float *)(&(v__2))) * ((((((((((((((1.987569e-04f * (((v__4) > (-8.837626e+01f) ? (v__4) : (-8.837626e+01f)) - (floorf(((((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) - (floorf(((((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) - (floorf(((((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) - (floorf(((((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) - (floorf(((((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) - (floorf(((((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) - (floorf(((((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) - (floorf(((((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__36 = ((float*)data_1)[cse_var_1];\n      ((float*)T_fast_exp_1)[cse_var_1] = ((v__35) > (v__36) ? (v__35) : (v__36));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_exp"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 12, 24, 40), \"float32\"), T_fast_tanh: T.Buffer((7, 12, 24, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(84):\n            for ax2, ax3 in T.grid(24, 40):\n                cse_var_1: T.int32 = ax0_ax1_fused * 960 + ax2 * 40 + ax3\n                T_fast_tanh_1 = T.Buffer((80640,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((80640,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_tanh_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_tanh = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_tanh_1 = (((DLTensor*)T_fast_tanh)[0].data);\n  void* default_function_T_fast_tanh_shape = (((DLTensor*)T_fast_tanh)[0].shape);\n  void* default_function_T_fast_tanh_strides = (((DLTensor*)T_fast_tanh)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_tanh_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 84; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 24; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 40; ++ax3) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 960) + (ax2 * 40)) + ax3);\n        float v_ = ((float*)data_1)[cse_var_1];\n        float v__1 = (9.000000e+00f) < (v_) ? (9.000000e+00f) : (v_);\n        ((float*)T_fast_tanh_1)[cse_var_1] = ((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_tanh"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 40, 32, 12), \"float32\"), T_fast_tanh: T.Buffer((6, 40, 32, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(7680):\n            cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 12\n            T_fast_tanh_1 = T.Buffer((92160,), data=T_fast_tanh.data)\n            data_1 = T.Buffer((92160,), data=data.data)\n            T_fast_tanh_1[cse_var_1:cse_var_1 + 12] = T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * (T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * (T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * (T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * (T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * (T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * (T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * T.Broadcast(T.float32(-2.76076847742355e-16), 12) + T.Broadcast(T.float32(2.0001879048247699e-13), 12)) + T.Broadcast(T.float32(-8.60467152213735e-11), 12)) + T.Broadcast(T.float32(5.1222970903711401e-08), 12)) + T.Broadcast(T.float32(1.4857223571797901e-05), 12)) + T.Broadcast(T.float32(0.00063726192887543596), 12)) + T.Broadcast(T.float32(0.0048935245589178597), 12)) / (T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * (T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * (T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * T.max(T.Broadcast(T.float32(-9), 12), T.min(T.Broadcast(T.float32(9), 12), data_1[cse_var_1:cse_var_1 + 12])) * T.Broadcast(T.float32(1.1982583946670199e-06), 12) + T.Broadcast(T.float32(0.000118534705686654), 12)) + T.Broadcast(T.float32(0.0022684346324390002), 12)) + T.Broadcast(T.float32(0.0048935251855438504), 12))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_tanh_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_tanh = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_tanh_1 = (((DLTensor*)T_fast_tanh)[0].data);\n  void* default_function_T_fast_tanh_shape = (((DLTensor*)T_fast_tanh)[0].shape);\n  void* default_function_T_fast_tanh_strides = (((DLTensor*)T_fast_tanh)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_tanh_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 7680; ++ax0_ax1_fused_ax2_fused) {\n    int32_t cse_var_1 = (ax0_ax1_fused_ax2_fused * 12);\n    float12 v_ = (float12)(-9.000000e+00f, -9.000000e+00f, -9.000000e+00f, -9.000000e+00f, -9.000000e+00f, -9.000000e+00f, -9.000000e+00f, -9.000000e+00f, -9.000000e+00f, -9.000000e+00f, -9.000000e+00f, -9.000000e+00f);\n    float12 v__1 = (float12)(9.000000e+00f, 9.000000e+00f, 9.000000e+00f, 9.000000e+00f, 9.000000e+00f, 9.000000e+00f, 9.000000e+00f, 9.000000e+00f, 9.000000e+00f, 9.000000e+00f, 9.000000e+00f, 9.000000e+00f);\n    int32_t12 v__2 = int32_t12((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7), (cse_var_1)+(1*8), (cse_var_1)+(1*9), (cse_var_1)+(1*10), (cse_var_1)+(1*11));\n    float12 v__3 = float12(((float*)data_1)[v__2.s0],((float*)data_1)[v__2.s1],((float*)data_1)[v__2.s2],((float*)data_1)[v__2.s3],((float*)data_1)[v__2.s4],((float*)data_1)[v__2.s5],((float*)data_1)[v__2.s6],((float*)data_1)[v__2.s7],((float*)data_1)[v__2.s8],((float*)data_1)[v__2.s9],((float*)data_1)[v__2.sa],((float*)data_1)[v__2.sb]);\n    float12 v__4 = (v__1) < (v__3) ? (v__1) : (v__3);\n    *(float12*)(((float*)T_fast_tanh_1) + cse_var_1) = ((((v_) > (v__4) ? (v_) : (v__4)) * (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * ((float12)(-2.760768e-16f, -2.760768e-16f, -2.760768e-16f, -2.760768e-16f, -2.760768e-16f, -2.760768e-16f, -2.760768e-16f, -2.760768e-16f, -2.760768e-16f, -2.760768e-16f, -2.760768e-16f, -2.760768e-16f))) + ((float12)(2.000188e-13f, 2.000188e-13f, 2.000188e-13f, 2.000188e-13f, 2.000188e-13f, 2.000188e-13f, 2.000188e-13f, 2.000188e-13f, 2.000188e-13f, 2.000188e-13f, 2.000188e-13f, 2.000188e-13f)))) + ((float12)(-8.604672e-11f, -8.604672e-11f, -8.604672e-11f, -8.604672e-11f, -8.604672e-11f, -8.604672e-11f, -8.604672e-11f, -8.604672e-11f, -8.604672e-11f, -8.604672e-11f, -8.604672e-11f, -8.604672e-11f)))) + ((float12)(5.122297e-08f, 5.122297e-08f, 5.122297e-08f, 5.122297e-08f, 5.122297e-08f, 5.122297e-08f, 5.122297e-08f, 5.122297e-08f, 5.122297e-08f, 5.122297e-08f, 5.122297e-08f, 5.122297e-08f)))) + ((float12)(1.485722e-05f, 1.485722e-05f, 1.485722e-05f, 1.485722e-05f, 1.485722e-05f, 1.485722e-05f, 1.485722e-05f, 1.485722e-05f, 1.485722e-05f, 1.485722e-05f, 1.485722e-05f, 1.485722e-05f)))) + ((float12)(6.372619e-04f, 6.372619e-04f, 6.372619e-04f, 6.372619e-04f, 6.372619e-04f, 6.372619e-04f, 6.372619e-04f, 6.372619e-04f, 6.372619e-04f, 6.372619e-04f, 6.372619e-04f, 6.372619e-04f)))) + ((float12)(4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f)))) / (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * ((float12)(1.198258e-06f, 1.198258e-06f, 1.198258e-06f, 1.198258e-06f, 1.198258e-06f, 1.198258e-06f, 1.198258e-06f, 1.198258e-06f, 1.198258e-06f, 1.198258e-06f, 1.198258e-06f, 1.198258e-06f))) + ((float12)(1.185347e-04f, 1.185347e-04f, 1.185347e-04f, 1.185347e-04f, 1.185347e-04f, 1.185347e-04f, 1.185347e-04f, 1.185347e-04f, 1.185347e-04f, 1.185347e-04f, 1.185347e-04f, 1.185347e-04f)))) + ((float12)(2.268435e-03f, 2.268435e-03f, 2.268435e-03f, 2.268435e-03f, 2.268435e-03f, 2.268435e-03f, 2.268435e-03f, 2.268435e-03f, 2.268435e-03f, 2.268435e-03f, 2.268435e-03f, 2.268435e-03f)))) + ((float12)(4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f))));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_tanh"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 40, 20, 24), \"float32\"), T_fast_tanh: T.Buffer((5, 40, 20, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(4000):\n            for ax3 in range(24):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 24 + ax3\n                T_fast_tanh_1 = T.Buffer((96000,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((96000,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_tanh_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_tanh = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_tanh_1 = (((DLTensor*)T_fast_tanh)[0].data);\n  void* default_function_T_fast_tanh_shape = (((DLTensor*)T_fast_tanh)[0].shape);\n  void* default_function_T_fast_tanh_strides = (((DLTensor*)T_fast_tanh)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_tanh_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4000; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 24; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 24) + ax3);\n      float v_ = ((float*)data_1)[cse_var_1];\n      float v__1 = (9.000000e+00f) < (v_) ? (9.000000e+00f) : (v_);\n      ((float*)T_fast_tanh_1)[cse_var_1] = ((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_tanh"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 32, 12, 20), \"float32\"), T_fast_tanh: T.Buffer((8, 32, 12, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(3072):\n            for ax3 in range(20):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 20 + ax3\n                T_fast_tanh_1 = T.Buffer((61440,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((61440,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_tanh_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_tanh = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_tanh_1 = (((DLTensor*)T_fast_tanh)[0].data);\n  void* default_function_T_fast_tanh_shape = (((DLTensor*)T_fast_tanh)[0].shape);\n  void* default_function_T_fast_tanh_strides = (((DLTensor*)T_fast_tanh)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_tanh_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3072; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 20) + ax3);\n      float v_ = ((float*)data_1)[cse_var_1];\n      float v__1 = (9.000000e+00f) < (v_) ? (9.000000e+00f) : (v_);\n      ((float*)T_fast_tanh_1)[cse_var_1] = ((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_tanh"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 4, 36, 20), \"float32\"), T_fast_tanh: T.Buffer((5, 4, 36, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(20):\n            for ax2, ax3 in T.grid(36, 20):\n                cse_var_1: T.int32 = ax0_ax1_fused * 720 + ax2 * 20 + ax3\n                T_fast_tanh_1 = T.Buffer((14400,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((14400,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_tanh_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_tanh = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_tanh_1 = (((DLTensor*)T_fast_tanh)[0].data);\n  void* default_function_T_fast_tanh_shape = (((DLTensor*)T_fast_tanh)[0].shape);\n  void* default_function_T_fast_tanh_strides = (((DLTensor*)T_fast_tanh)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_tanh_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 20; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 36; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 720) + (ax2 * 20)) + ax3);\n        float v_ = ((float*)data_1)[cse_var_1];\n        float v__1 = (9.000000e+00f) < (v_) ? (9.000000e+00f) : (v_);\n        ((float*)T_fast_tanh_1)[cse_var_1] = ((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_tanh"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 32, 20, 32), \"float32\"), T_fast_tanh: T.Buffer((8, 32, 20, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(163840):\n            T_fast_tanh_1 = T.Buffer((163840,), data=T_fast_tanh.data)\n            data_1 = T.Buffer((163840,), data=data.data)\n            T_fast_tanh_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_tanh_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_tanh = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_tanh_1 = (((DLTensor*)T_fast_tanh)[0].data);\n  void* default_function_T_fast_tanh_shape = (((DLTensor*)T_fast_tanh)[0].shape);\n  void* default_function_T_fast_tanh_strides = (((DLTensor*)T_fast_tanh)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_tanh_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 163840; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    float v_ = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__1 = (9.000000e+00f) < (v_) ? (9.000000e+00f) : (v_);\n    ((float*)T_fast_tanh_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = ((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_tanh"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 32, 28, 32), \"float32\"), T_fast_tanh: T.Buffer((5, 32, 28, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(160):\n            for ax2, ax3 in T.grid(28, 32):\n                cse_var_1: T.int32 = ax0_ax1_fused * 896 + ax2 * 32 + ax3\n                T_fast_tanh_1 = T.Buffer((143360,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((143360,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_tanh_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_tanh = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_tanh_1 = (((DLTensor*)T_fast_tanh)[0].data);\n  void* default_function_T_fast_tanh_shape = (((DLTensor*)T_fast_tanh)[0].shape);\n  void* default_function_T_fast_tanh_strides = (((DLTensor*)T_fast_tanh)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_tanh_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 160; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 28; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 32; ++ax3) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 896) + (ax2 * 32)) + ax3);\n        float v_ = ((float*)data_1)[cse_var_1];\n        float v__1 = (9.000000e+00f) < (v_) ? (9.000000e+00f) : (v_);\n        ((float*)T_fast_tanh_1)[cse_var_1] = ((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_tanh"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 16, 20, 4), \"float32\"), T_fast_tanh: T.Buffer((5, 16, 20, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(5):\n            for ax1, ax2 in T.grid(16, 20):\n                cse_var_1: T.int32 = ax0 * 1280 + ax1 * 80 + ax2 * 4\n                T_fast_tanh_1 = T.Buffer((6400,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((6400,), data=data.data)\n                T_fast_tanh_1[cse_var_1:cse_var_1 + 4] = T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * (T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * (T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * (T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * (T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * (T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * (T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * T.Broadcast(T.float32(-2.76076847742355e-16), 4) + T.Broadcast(T.float32(2.0001879048247699e-13), 4)) + T.Broadcast(T.float32(-8.60467152213735e-11), 4)) + T.Broadcast(T.float32(5.1222970903711401e-08), 4)) + T.Broadcast(T.float32(1.4857223571797901e-05), 4)) + T.Broadcast(T.float32(0.00063726192887543596), 4)) + T.Broadcast(T.float32(0.0048935245589178597), 4)) / (T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * (T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * (T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * T.max(T.Broadcast(T.float32(-9), 4), T.min(T.Broadcast(T.float32(9), 4), data_1[cse_var_1:cse_var_1 + 4])) * T.Broadcast(T.float32(1.1982583946670199e-06), 4) + T.Broadcast(T.float32(0.000118534705686654), 4)) + T.Broadcast(T.float32(0.0022684346324390002), 4)) + T.Broadcast(T.float32(0.0048935251855438504), 4))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_tanh_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_tanh = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_tanh_1 = (((DLTensor*)T_fast_tanh)[0].data);\n  void* default_function_T_fast_tanh_shape = (((DLTensor*)T_fast_tanh)[0].shape);\n  void* default_function_T_fast_tanh_strides = (((DLTensor*)T_fast_tanh)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_tanh_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n        int32_t cse_var_1 = (((ax0 * 1280) + (ax1 * 80)) + (ax2 * 4));\n        float4 v_ = (float4)(-9.000000e+00f, -9.000000e+00f, -9.000000e+00f, -9.000000e+00f);\n        float4 v__1 = (float4)(9.000000e+00f, 9.000000e+00f, 9.000000e+00f, 9.000000e+00f);\n        int32_t4 v__2 = int32_t4((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3));\n        float4 v__3 = float4(((float*)data_1)[v__2.s0],((float*)data_1)[v__2.s1],((float*)data_1)[v__2.s2],((float*)data_1)[v__2.s3]);\n        float4 v__4 = (v__1) < (v__3) ? (v__1) : (v__3);\n        *(float4*)(((float*)T_fast_tanh_1) + cse_var_1) = ((((v_) > (v__4) ? (v_) : (v__4)) * (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * ((float4)(-2.760768e-16f, -2.760768e-16f, -2.760768e-16f, -2.760768e-16f))) + ((float4)(2.000188e-13f, 2.000188e-13f, 2.000188e-13f, 2.000188e-13f)))) + ((float4)(-8.604672e-11f, -8.604672e-11f, -8.604672e-11f, -8.604672e-11f)))) + ((float4)(5.122297e-08f, 5.122297e-08f, 5.122297e-08f, 5.122297e-08f)))) + ((float4)(1.485722e-05f, 1.485722e-05f, 1.485722e-05f, 1.485722e-05f)))) + ((float4)(6.372619e-04f, 6.372619e-04f, 6.372619e-04f, 6.372619e-04f)))) + ((float4)(4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f)))) / (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * (((((v_) > (v__4) ? (v_) : (v__4)) * ((v_) > (v__4) ? (v_) : (v__4))) * ((float4)(1.198258e-06f, 1.198258e-06f, 1.198258e-06f, 1.198258e-06f))) + ((float4)(1.185347e-04f, 1.185347e-04f, 1.185347e-04f, 1.185347e-04f)))) + ((float4)(2.268435e-03f, 2.268435e-03f, 2.268435e-03f, 2.268435e-03f)))) + ((float4)(4.893525e-03f, 4.893525e-03f, 4.893525e-03f, 4.893525e-03f))));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_tanh"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 4, 28, 28), \"float32\"), T_fast_tanh: T.Buffer((9, 4, 28, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(1008):\n            for ax3 in range(28):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 28 + ax3\n                T_fast_tanh_1 = T.Buffer((28224,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((28224,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_tanh_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_tanh = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_tanh_1 = (((DLTensor*)T_fast_tanh)[0].data);\n  void* default_function_T_fast_tanh_shape = (((DLTensor*)T_fast_tanh)[0].shape);\n  void* default_function_T_fast_tanh_strides = (((DLTensor*)T_fast_tanh)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_tanh_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1008; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 28; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 28) + ax3);\n      float v_ = ((float*)data_1)[cse_var_1];\n      float v__1 = (9.000000e+00f) < (v_) ? (9.000000e+00f) : (v_);\n      ((float*)T_fast_tanh_1)[cse_var_1] = ((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_tanh"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 8, 12, 20), \"float32\"), T_fast_tanh: T.Buffer((2, 8, 12, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(192):\n            for ax3 in range(20):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 20 + ax3\n                T_fast_tanh_1 = T.Buffer((3840,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((3840,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_tanh_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_tanh = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_tanh_1 = (((DLTensor*)T_fast_tanh)[0].data);\n  void* default_function_T_fast_tanh_shape = (((DLTensor*)T_fast_tanh)[0].shape);\n  void* default_function_T_fast_tanh_strides = (((DLTensor*)T_fast_tanh)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_tanh_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 192; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 20) + ax3);\n      float v_ = ((float*)data_1)[cse_var_1];\n      float v__1 = (9.000000e+00f) < (v_) ? (9.000000e+00f) : (v_);\n      ((float*)T_fast_tanh_1)[cse_var_1] = ((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fast_tanh"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 36, 36, 8), \"float32\"), T_reverse_sequence: T.Buffer((7, 36, 36, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(9072):\n            T_reverse_sequence_1 = T.Buffer((72576,), data=T_reverse_sequence.data)\n            data_1 = T.Buffer((72576,), data=data.data)\n            T_reverse_sequence_1[ax0_ax1_fused_ax2_fused * 8:ax0_ax1_fused_ax2_fused * 8 + 8] = data_1[ax0_ax1_fused_ax2_fused % 1296 * 8 + 62208 - ax0_ax1_fused_ax2_fused // 1296 * 10368:ax0_ax1_fused_ax2_fused % 1296 * 8 + 62208 - ax0_ax1_fused_ax2_fused // 1296 * 10368 + 8]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 162) * 64) + ((int)threadIdx.x)) + 62208) - ((((int)blockIdx.x) / 162) * 10368))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 9072; ++ax0_ax1_fused_ax2_fused) {\n    *(float8*)(((float*)T_reverse_sequence_1) + (ax0_ax1_fused_ax2_fused * 8)) = *(float8*)(((float*)data_1) + ((((ax0_ax1_fused_ax2_fused % 1296) * 8) + 62208) - ((ax0_ax1_fused_ax2_fused / 1296) * 10368)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 24, 8, 36), \"float32\"), T_reverse_sequence: T.Buffer((5, 24, 8, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(120):\n            for ax2, ax3 in T.grid(8, 36):\n                cse_var_1: T.int32 = ax2 * 36\n                T_reverse_sequence_1 = T.Buffer((34560,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((34560,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused * 288 + cse_var_1 + ax3] = data_1[ax0_ax1_fused % 24 * 288 + cse_var_1 + ax3 + 27648 - ax0_ax1_fused // 24 * 6912]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 108) * 64) + ((int)threadIdx.x)) + 27648) - ((((int)blockIdx.x) / 108) * 6912))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 120; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 36; ++ax3) {\n        int32_t cse_var_1 = (ax2 * 36);\n        ((float*)T_reverse_sequence_1)[(((ax0_ax1_fused * 288) + cse_var_1) + ax3)] = ((float*)data_1)[((((((ax0_ax1_fused % 24) * 288) + cse_var_1) + ax3) + 27648) - ((ax0_ax1_fused / 24) * 6912))];\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 20, 12, 40), \"float32\"), T_reverse_sequence: T.Buffer((5, 20, 12, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(1200):\n            for ax3 in range(40):\n                T_reverse_sequence_1 = T.Buffer((48000,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((48000,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused_ax2_fused * 40 + ax3] = data_1[ax0_ax1_fused_ax2_fused % 240 * 40 + ax3 + 38400 - ax0_ax1_fused_ax2_fused // 240 * 9600]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 160) * 60) + ((int)threadIdx.x)) + 38400) - ((((int)blockIdx.x) / 160) * 9600))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1200; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 40; ++ax3) {\n      ((float*)T_reverse_sequence_1)[((ax0_ax1_fused_ax2_fused * 40) + ax3)] = ((float*)data_1)[(((((ax0_ax1_fused_ax2_fused % 240) * 40) + ax3) + 38400) - ((ax0_ax1_fused_ax2_fused / 240) * 9600))];\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 28, 28, 36), \"float32\"), T_reverse_sequence: T.Buffer((6, 28, 28, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(4704):\n            for ax3 in range(36):\n                T_reverse_sequence_1 = T.Buffer((169344,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((169344,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused_ax2_fused * 36 + ax3] = data_1[ax0_ax1_fused_ax2_fused % 784 * 36 + ax3 + 141120 - ax0_ax1_fused_ax2_fused // 784 * 28224]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 588) * 48) + ((int)threadIdx.x)) + 141120) - ((((int)blockIdx.x) / 588) * 28224))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4704; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 36; ++ax3) {\n      ((float*)T_reverse_sequence_1)[((ax0_ax1_fused_ax2_fused * 36) + ax3)] = ((float*)data_1)[(((((ax0_ax1_fused_ax2_fused % 784) * 36) + ax3) + 141120) - ((ax0_ax1_fused_ax2_fused / 784) * 28224))];\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 20, 36, 12), \"float32\"), T_reverse_sequence: T.Buffer((2, 20, 36, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(2):\n            for ax1, ax2 in T.grid(20, 36):\n                cse_var_3: T.int32 = ax1 * 432\n                cse_var_2: T.int32 = ax2 * 12\n                cse_var_1: T.int32 = ax0 * 8640\n                T_reverse_sequence_1 = T.Buffer((17280,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((17280,), data=data.data)\n                T_reverse_sequence_1[cse_var_1 + cse_var_3 + cse_var_2:cse_var_1 + cse_var_3 + cse_var_2 + 12] = data_1[cse_var_3 + cse_var_2 + 8640 - cse_var_1:cse_var_3 + cse_var_2 + 8640 - cse_var_1 + 12]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 216) * 40) + ((int)threadIdx.x)) + 8640) - ((((int)blockIdx.x) / 216) * 8640))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 36; ++ax2) {\n        int32_t cse_var_3 = (ax1 * 432);\n        int32_t cse_var_2 = (ax2 * 12);\n        int32_t cse_var_1 = (ax0 * 8640);\n        int32_t12 v_ = int32_t12(((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*0), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*1), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*2), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*3), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*4), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*5), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*6), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*7), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*8), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*9), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*10), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*11));\n        *(float12*)(((float*)T_reverse_sequence_1) + ((cse_var_1 + cse_var_3) + cse_var_2)) = (float12(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb]));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 4, 24, 12), \"float32\"), T_reverse_sequence: T.Buffer((6, 4, 24, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(6912):\n            T_reverse_sequence_1 = T.Buffer((6912,), data=T_reverse_sequence.data)\n            data_1 = T.Buffer((6912,), data=data.data)\n            T_reverse_sequence_1[ax0_ax1_fused_ax2_fused_ax3_fused] = data_1[ax0_ax1_fused_ax2_fused_ax3_fused % 1152 + 5760 - ax0_ax1_fused_ax2_fused_ax3_fused // 1152 * 1152]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 24) * 48) + ((int)threadIdx.x)) + 5760) - ((((int)blockIdx.x) / 24) * 1152))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 6912; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    ((float*)T_reverse_sequence_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = ((float*)data_1)[(((ax0_ax1_fused_ax2_fused_ax3_fused % 1152) + 5760) - ((ax0_ax1_fused_ax2_fused_ax3_fused / 1152) * 1152))];\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 20, 16, 28), \"float32\"), T_reverse_sequence: T.Buffer((3, 20, 16, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(960):\n            for ax3 in range(28):\n                T_reverse_sequence_1 = T.Buffer((26880,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((26880,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused_ax2_fused * 28 + ax3] = data_1[ax0_ax1_fused_ax2_fused % 320 * 28 + ax3 + 17920 - ax0_ax1_fused_ax2_fused // 320 * 8960]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 320) * 28) + ((int)threadIdx.x)) + 17920) - ((((int)blockIdx.x) / 320) * 8960))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 960; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 28; ++ax3) {\n      ((float*)T_reverse_sequence_1)[((ax0_ax1_fused_ax2_fused * 28) + ax3)] = ((float*)data_1)[(((((ax0_ax1_fused_ax2_fused % 320) * 28) + ax3) + 17920) - ((ax0_ax1_fused_ax2_fused / 320) * 8960))];\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 28, 12, 28), \"float32\"), T_reverse_sequence: T.Buffer((2, 28, 12, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(672):\n            for ax3 in range(28):\n                T_reverse_sequence_1 = T.Buffer((18816,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((18816,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused_ax2_fused * 28 + ax3] = data_1[ax0_ax1_fused_ax2_fused % 336 * 28 + ax3 + 9408 - ax0_ax1_fused_ax2_fused // 336 * 9408]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 147) * 64) + ((int)threadIdx.x)) + 9408) - ((((int)blockIdx.x) / 147) * 9408))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 672; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 28; ++ax3) {\n      ((float*)T_reverse_sequence_1)[((ax0_ax1_fused_ax2_fused * 28) + ax3)] = ((float*)data_1)[(((((ax0_ax1_fused_ax2_fused % 336) * 28) + ax3) + 9408) - ((ax0_ax1_fused_ax2_fused / 336) * 9408))];\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 28, 12, 28), \"float32\"), T_reverse_sequence: T.Buffer((2, 28, 12, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(56):\n            for ax2, ax3 in T.grid(12, 28):\n                cse_var_1: T.int32 = ax2 * 28\n                T_reverse_sequence_1 = T.Buffer((18816,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((18816,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused * 336 + cse_var_1 + ax3] = data_1[ax0_ax1_fused % 28 * 336 + cse_var_1 + ax3 + 9408 - ax0_ax1_fused // 28 * 9408]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 294) * 32) + ((int)threadIdx.x)) + 9408) - ((((int)blockIdx.x) / 294) * 9408))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 56; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 28; ++ax3) {\n        int32_t cse_var_1 = (ax2 * 28);\n        ((float*)T_reverse_sequence_1)[(((ax0_ax1_fused * 336) + cse_var_1) + ax3)] = ((float*)data_1)[((((((ax0_ax1_fused % 28) * 336) + cse_var_1) + ax3) + 9408) - ((ax0_ax1_fused / 28) * 9408))];\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 16, 28, 16), \"float32\"), T_reverse_sequence: T.Buffer((7, 16, 28, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(3136):\n            T_reverse_sequence_1 = T.Buffer((50176,), data=T_reverse_sequence.data)\n            data_1 = T.Buffer((50176,), data=data.data)\n            T_reverse_sequence_1[ax0_ax1_fused_ax2_fused * 16:ax0_ax1_fused_ax2_fused * 16 + 16] = data_1[ax0_ax1_fused_ax2_fused % 448 * 16 + 43008 - ax0_ax1_fused_ax2_fused // 448 * 7168:ax0_ax1_fused_ax2_fused % 448 * 16 + 43008 - ax0_ax1_fused_ax2_fused // 448 * 7168 + 16]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 112) * 64) + ((int)threadIdx.x)) + 43008) - ((((int)blockIdx.x) / 112) * 7168))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3136; ++ax0_ax1_fused_ax2_fused) {\n    *(float16*)(((float*)T_reverse_sequence_1) + (ax0_ax1_fused_ax2_fused * 16)) = *(float16*)(((float*)data_1) + ((((ax0_ax1_fused_ax2_fused % 448) * 16) + 43008) - ((ax0_ax1_fused_ax2_fused / 448) * 7168)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 8, 8, 12), \"float32\"), compute: T.Buffer((9, 8, 8, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(6912):\n            compute_1 = T.Buffer((6912,), data=compute.data)\n            data_1 = T.Buffer((6912,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.floor(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 6912; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = floorf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "floor"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 36, 4, 8), \"float32\"), compute: T.Buffer((3, 36, 4, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(108):\n            for i2, i3 in T.grid(4, 8):\n                cse_var_1: T.int32 = i0_i1_fused * 32 + i2 * 8 + i3\n                compute_1 = T.Buffer((3456,), data=compute.data)\n                data_1 = T.Buffer((3456,), data=data.data)\n                compute_1[cse_var_1] = T.floor(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 108; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 8; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 32) + (i2 * 8)) + i3);\n        ((float*)compute_1)[cse_var_1] = floorf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "floor"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 24, 20, 20), \"float32\"), compute: T.Buffer((3, 24, 20, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(28800):\n            compute_1 = T.Buffer((28800,), data=compute.data)\n            data_1 = T.Buffer((28800,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.floor(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 28800; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = floorf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "floor"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 24, 36, 40), \"float32\"), compute: T.Buffer((6, 24, 36, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(207360):\n            compute_1 = T.Buffer((207360,), data=compute.data)\n            data_1 = T.Buffer((207360,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.floor(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 207360; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = floorf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "floor"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 32, 20, 28), \"float32\"), compute: T.Buffer((3, 32, 20, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(53760):\n            compute_1 = T.Buffer((53760,), data=compute.data)\n            data_1 = T.Buffer((53760,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.floor(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 53760; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = floorf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "floor"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 12, 8, 28), \"float32\"), compute: T.Buffer((4, 12, 8, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(48):\n            for i2, i3 in T.grid(8, 28):\n                cse_var_1: T.int32 = i0_i1_fused * 224 + i2 * 28 + i3\n                compute_1 = T.Buffer((10752,), data=compute.data)\n                data_1 = T.Buffer((10752,), data=data.data)\n                compute_1[cse_var_1] = T.floor(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      for (int32_t i3 = 0; i3 < 28; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 224) + (i2 * 28)) + i3);\n        ((float*)compute_1)[cse_var_1] = floorf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "floor"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 16, 40, 28), \"float32\"), compute: T.Buffer((7, 16, 40, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(4480):\n            for i3 in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                compute_1 = T.Buffer((125440,), data=compute.data)\n                data_1 = T.Buffer((125440,), data=data.data)\n                compute_1[cse_var_1] = T.floor(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4480; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 28) + i3);\n      ((float*)compute_1)[cse_var_1] = floorf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "floor"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 28, 16, 12), \"float32\"), compute: T.Buffer((2, 28, 16, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(896):\n            for i3 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 12 + i3\n                compute_1 = T.Buffer((10752,), data=compute.data)\n                data_1 = T.Buffer((10752,), data=data.data)\n                compute_1[cse_var_1] = T.floor(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 896; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 12; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 12) + i3);\n      ((float*)compute_1)[cse_var_1] = floorf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "floor"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 16, 28, 28), \"float32\"), compute: T.Buffer((10, 16, 28, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(4480):\n            for i3 in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                compute_1 = T.Buffer((125440,), data=compute.data)\n                data_1 = T.Buffer((125440,), data=data.data)\n                compute_1[cse_var_1] = T.floor(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4480; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 28) + i3);\n      ((float*)compute_1)[cse_var_1] = floorf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "floor"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 24, 20, 40), \"float32\"), compute: T.Buffer((1, 24, 20, 40), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(24, 20, 40):\n            cse_var_1: T.int32 = i1 * 800 + i2 * 40 + i3\n            compute_1 = T.Buffer((19200,), \"int8\", data=compute.data)\n            data_1 = T.Buffer((19200,), data=data.data)\n            compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i1 = 0; i1 < 24; ++i1) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      for (int32_t i3 = 0; i3 < 40; ++i3) {\n        int32_t cse_var_1 = (((i1 * 800) + (i2 * 40)) + i3);\n        ((int8_t*)compute_1)[cse_var_1] = ((int8_t)(((float*)data_1)[cse_var_1] != ((float*)data_1)[cse_var_1]));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "isnan"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 12, 40, 36), \"float32\"), compute: T.Buffer((5, 12, 40, 36), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2400):\n            for i3 in range(36):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 36 + i3\n                compute_1 = T.Buffer((86400,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((86400,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2400; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 36; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 36) + i3);\n      ((int8_t*)compute_1)[cse_var_1] = ((int8_t)(((float*)data_1)[cse_var_1] != ((float*)data_1)[cse_var_1]));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "isnan"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 20, 28, 28), \"float32\"), compute: T.Buffer((8, 20, 28, 28), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(160):\n            for i2, i3 in T.grid(28, 28):\n                cse_var_1: T.int32 = i0_i1_fused * 784 + i2 * 28 + i3\n                compute_1 = T.Buffer((125440,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((125440,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 160; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 28; ++i2) {\n      for (int32_t i3 = 0; i3 < 28; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 784) + (i2 * 28)) + i3);\n        ((int8_t*)compute_1)[cse_var_1] = ((int8_t)(((float*)data_1)[cse_var_1] != ((float*)data_1)[cse_var_1]));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "isnan"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 16, 16, 32), \"float32\"), compute: T.Buffer((3, 16, 16, 32), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(768):\n            for i3 in range(32):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 32 + i3\n                compute_1 = T.Buffer((24576,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((24576,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 768; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 32; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 32) + i3);\n      ((int8_t*)compute_1)[cse_var_1] = ((int8_t)(((float*)data_1)[cse_var_1] != ((float*)data_1)[cse_var_1]));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "isnan"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 36, 40, 40), \"float32\"), compute: T.Buffer((3, 36, 40, 40), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(108):\n            for i2, i3 in T.grid(40, 40):\n                cse_var_1: T.int32 = i0_i1_fused * 1600 + i2 * 40 + i3\n                compute_1 = T.Buffer((172800,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((172800,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 108; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 40; ++i2) {\n      for (int32_t i3 = 0; i3 < 40; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 1600) + (i2 * 40)) + i3);\n        ((int8_t*)compute_1)[cse_var_1] = ((int8_t)(((float*)data_1)[cse_var_1] != ((float*)data_1)[cse_var_1]));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "isnan"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 4, 16, 40), \"float32\"), compute: T.Buffer((10, 4, 16, 40), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(640):\n            for i3 in range(40):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 40 + i3\n                compute_1 = T.Buffer((25600,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((25600,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 640; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 40; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 40) + i3);\n      ((int8_t*)compute_1)[cse_var_1] = ((int8_t)(((float*)data_1)[cse_var_1] != ((float*)data_1)[cse_var_1]));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "isnan"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 16, 28, 12), \"float32\"), compute: T.Buffer((7, 16, 28, 12), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(112):\n            for i2, i3_s in T.grid(28, 12):\n                cse_var_1: T.int32 = i0_i1_fused * 336 + i2 * 12 + i3_s\n                compute_1 = T.Buffer((37632,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((37632,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 112; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 28; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 12; ++i3_s) {\n        int32_t cse_var_1 = (((i0_i1_fused * 336) + (i2 * 12)) + i3_s);\n        ((int8_t*)compute_1)[cse_var_1] = ((int8_t)(((float*)data_1)[cse_var_1] != ((float*)data_1)[cse_var_1]));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "isnan"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 40, 4, 8), \"float32\"), compute: T.Buffer((6, 40, 4, 8), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(240):\n            for i2, i3_s in T.grid(4, 8):\n                cse_var_1: T.int32 = i0_i1_fused * 32 + i2 * 8 + i3_s\n                compute_1 = T.Buffer((7680,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((7680,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 240; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n        int32_t cse_var_1 = (((i0_i1_fused * 32) + (i2 * 8)) + i3_s);\n        ((int8_t*)compute_1)[cse_var_1] = ((int8_t)(((float*)data_1)[cse_var_1] != ((float*)data_1)[cse_var_1]));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "isnan"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 12, 24, 36), \"float32\"), compute: T.Buffer((9, 12, 24, 36), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(108):\n            for i2, i3 in T.grid(24, 36):\n                cse_var_1: T.int32 = i0_i1_fused * 864 + i2 * 36 + i3\n                compute_1 = T.Buffer((93312,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((93312,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 108; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 24; ++i2) {\n      for (int32_t i3 = 0; i3 < 36; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 864) + (i2 * 36)) + i3);\n        ((int8_t*)compute_1)[cse_var_1] = ((int8_t)(((float*)data_1)[cse_var_1] != ((float*)data_1)[cse_var_1]));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "isnan"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 12, 40, 12), \"float32\"), compute: T.Buffer((5, 12, 40, 12), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(60):\n            for i2, i3_s in T.grid(40, 12):\n                cse_var_1: T.int32 = i0_i1_fused * 480 + i2 * 12 + i3_s\n                compute_1 = T.Buffer((28800,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((28800,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 60; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 40; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 12; ++i3_s) {\n        int32_t cse_var_1 = (((i0_i1_fused * 480) + (i2 * 12)) + i3_s);\n        ((int8_t*)compute_1)[cse_var_1] = ((int8_t)(((float*)data_1)[cse_var_1] != ((float*)data_1)[cse_var_1]));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "isnan"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 40, 40, 20), \"float32\"), compute: T.Buffer((5, 40, 40, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(8000):\n            for i3 in range(20):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 20 + i3\n                compute_1 = T.Buffer((160000,), data=compute.data)\n                data_1 = T.Buffer((160000,), data=data.data)\n                compute_1[cse_var_1] = T.log(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __logf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float logf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 8000; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 20; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 20) + i3);\n      ((float*)compute_1)[cse_var_1] = logf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 28, 4, 20), \"float32\"), compute: T.Buffer((1, 28, 4, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(28, 4, 20):\n            cse_var_1: T.int32 = i1 * 80 + i2 * 20 + i3\n            compute_1 = T.Buffer((2240,), data=compute.data)\n            data_1 = T.Buffer((2240,), data=data.data)\n            compute_1[cse_var_1] = T.log(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = __logf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float logf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i1 = 0; i1 < 28; ++i1) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        int32_t cse_var_1 = (((i1 * 80) + (i2 * 20)) + i3);\n        ((float*)compute_1)[cse_var_1] = logf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 32, 12, 24), \"float32\"), compute: T.Buffer((8, 32, 12, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(256):\n            for i2, i3 in T.grid(12, 24):\n                cse_var_1: T.int32 = i0_i1_fused * 288 + i2 * 24 + i3\n                compute_1 = T.Buffer((73728,), data=compute.data)\n                data_1 = T.Buffer((73728,), data=data.data)\n                compute_1[cse_var_1] = T.log(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = __logf(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float logf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 256; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 288) + (i2 * 24)) + i3);\n        ((float*)compute_1)[cse_var_1] = logf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 24, 4, 40), \"float32\"), compute: T.Buffer((10, 24, 4, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(960):\n            for i3 in range(40):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 40 + i3\n                compute_1 = T.Buffer((38400,), data=compute.data)\n                data_1 = T.Buffer((38400,), data=data.data)\n                compute_1[cse_var_1] = T.log(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = __logf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float logf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 960; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 40; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 40) + i3);\n      ((float*)compute_1)[cse_var_1] = logf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 28, 8, 40), \"float32\"), compute: T.Buffer((2, 28, 8, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(17920):\n            compute_1 = T.Buffer((17920,), data=compute.data)\n            data_1 = T.Buffer((17920,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.log(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __logf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float logf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 17920; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = logf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 4, 20, 16), \"float32\"), compute: T.Buffer((5, 4, 20, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(400):\n            for i3 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 16 + i3\n                compute_1 = T.Buffer((6400,), data=compute.data)\n                data_1 = T.Buffer((6400,), data=data.data)\n                compute_1[cse_var_1] = T.log(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(25) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(25) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))] = __logf(data[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float logf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 400; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 16; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 16) + i3);\n      ((float*)compute_1)[cse_var_1] = logf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 28, 28, 40), \"float32\"), compute: T.Buffer((4, 28, 28, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(3136):\n            for i3 in range(40):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 40 + i3\n                compute_1 = T.Buffer((125440,), data=compute.data)\n                data_1 = T.Buffer((125440,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log10f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3136; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 40; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 40) + i3);\n      ((float*)compute_1)[cse_var_1] = log10f(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log10"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 32, 28, 28), \"float32\"), compute: T.Buffer((7, 32, 28, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(6272):\n            for i3 in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                compute_1 = T.Buffer((175616,), data=compute.data)\n                data_1 = T.Buffer((175616,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log10f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 6272; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 28) + i3);\n      ((float*)compute_1)[cse_var_1] = log10f(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log10"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 28, 8, 20), \"float32\"), compute: T.Buffer((10, 28, 8, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2240):\n            for i3 in range(20):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 20 + i3\n                compute_1 = T.Buffer((44800,), data=compute.data)\n                data_1 = T.Buffer((44800,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log10f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2240; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 20; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 20) + i3);\n      ((float*)compute_1)[cse_var_1] = log10f(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log10"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 8, 16, 32), \"float32\"), compute: T.Buffer((10, 8, 16, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(40960):\n            compute_1 = T.Buffer((40960,), data=compute.data)\n            data_1 = T.Buffer((40960,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.log10(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log10f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 40960; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = log10f(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log10"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 12, 32, 28), \"float32\"), compute: T.Buffer((8, 12, 32, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(3072):\n            for i3 in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                compute_1 = T.Buffer((86016,), data=compute.data)\n                data_1 = T.Buffer((86016,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log10f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3072; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 28) + i3);\n      ((float*)compute_1)[cse_var_1] = log10f(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log10"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 12, 28, 36), \"float32\"), compute: T.Buffer((6, 12, 28, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2016):\n            for i3 in range(36):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 36 + i3\n                compute_1 = T.Buffer((72576,), data=compute.data)\n                data_1 = T.Buffer((72576,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log10f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2016; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 36; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 36) + i3);\n      ((float*)compute_1)[cse_var_1] = log10f(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log10"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 4, 12, 4), \"float32\"), compute: T.Buffer((2, 4, 12, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(8):\n            for i2, i3 in T.grid(12, 4):\n                cse_var_1: T.int32 = i0_i1_fused * 48 + i2 * 4 + i3\n                compute_1 = T.Buffer((384,), data=compute.data)\n                data_1 = T.Buffer((384,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log10f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 8; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3 = 0; i3 < 4; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 48) + (i2 * 4)) + i3);\n        ((float*)compute_1)[cse_var_1] = log10f(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log10"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 40, 12, 32), \"float32\"), compute: T.Buffer((4, 40, 12, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1920):\n            for i3 in range(32):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 32 + i3\n                compute_1 = T.Buffer((61440,), data=compute.data)\n                data_1 = T.Buffer((61440,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log10f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1920; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 32; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 32) + i3);\n      ((float*)compute_1)[cse_var_1] = log10f(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log10"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 24, 12, 24), \"float32\"), compute: T.Buffer((10, 24, 12, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(240):\n            for i2, i3 in T.grid(12, 24):\n                cse_var_1: T.int32 = i0_i1_fused * 288 + i2 * 24 + i3\n                compute_1 = T.Buffer((69120,), data=compute.data)\n                data_1 = T.Buffer((69120,), data=data.data)\n                compute_1[cse_var_1] = T.log2(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log2f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 240; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 288) + (i2 * 24)) + i3);\n        ((float*)compute_1)[cse_var_1] = log2f(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log2"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 4, 4, 36), \"float32\"), compute: T.Buffer((1, 4, 4, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(4, 4, 36):\n            cse_var_1: T.int32 = i1 * 144 + i2 * 36 + i3\n            compute_1 = T.Buffer((576,), data=compute.data)\n            data_1 = T.Buffer((576,), data=data.data)\n            compute_1[cse_var_1] = T.log2(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log2f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i1 = 0; i1 < 4; ++i1) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 36; ++i3) {\n        int32_t cse_var_1 = (((i1 * 144) + (i2 * 36)) + i3);\n        ((float*)compute_1)[cse_var_1] = log2f(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log2"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 16, 36, 24), \"float32\"), compute: T.Buffer((1, 16, 36, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(16):\n            for i2, i3 in T.grid(36, 24):\n                cse_var_1: T.int32 = i0_i1_fused * 864 + i2 * 24 + i3\n                compute_1 = T.Buffer((13824,), data=compute.data)\n                data_1 = T.Buffer((13824,), data=data.data)\n                compute_1[cse_var_1] = T.log2(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log2f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 16; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 36; ++i2) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 864) + (i2 * 24)) + i3);\n        ((float*)compute_1)[cse_var_1] = log2f(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log2"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 12, 12, 20), \"float32\"), compute: T.Buffer((2, 12, 12, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(24):\n            for i2, i3 in T.grid(12, 20):\n                cse_var_1: T.int32 = i0_i1_fused * 240 + i2 * 20 + i3\n                compute_1 = T.Buffer((5760,), data=compute.data)\n                data_1 = T.Buffer((5760,), data=data.data)\n                compute_1[cse_var_1] = T.log2(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log2f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 24; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 240) + (i2 * 20)) + i3);\n        ((float*)compute_1)[cse_var_1] = log2f(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log2"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 12, 24, 24), \"float32\"), compute: T.Buffer((1, 12, 24, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(288):\n            for i3 in range(24):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 24 + i3\n                compute_1 = T.Buffer((6912,), data=compute.data)\n                data_1 = T.Buffer((6912,), data=data.data)\n                compute_1[cse_var_1] = T.log2(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log2f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 288; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 24; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 24) + i3);\n      ((float*)compute_1)[cse_var_1] = log2f(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log2"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 8, 36, 40), \"float32\"), compute: T.Buffer((9, 8, 36, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(72):\n            for i2, i3 in T.grid(36, 40):\n                cse_var_1: T.int32 = i0_i1_fused * 1440 + i2 * 40 + i3\n                compute_1 = T.Buffer((103680,), data=compute.data)\n                data_1 = T.Buffer((103680,), data=data.data)\n                compute_1[cse_var_1] = T.log2(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log2f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 72; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 36; ++i2) {\n      for (int32_t i3 = 0; i3 < 40; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 1440) + (i2 * 40)) + i3);\n        ((float*)compute_1)[cse_var_1] = log2f(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log2"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 12, 16, 12), \"float32\"), compute: T.Buffer((3, 12, 16, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(36):\n            for i2, i3 in T.grid(16, 12):\n                cse_var_1: T.int32 = i0_i1_fused * 192 + i2 * 12 + i3\n                compute_1 = T.Buffer((6912,), data=compute.data)\n                data_1 = T.Buffer((6912,), data=data.data)\n                compute_1[cse_var_1] = T.log2(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log2f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 36; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      for (int32_t i3 = 0; i3 < 12; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 192) + (i2 * 12)) + i3);\n        ((float*)compute_1)[cse_var_1] = log2f(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log2"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 4, 4, 16), \"float32\"), compute: T.Buffer((6, 4, 4, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(24):\n            for i2, i3 in T.grid(4, 16):\n                cse_var_1: T.int32 = i0_i1_fused * 64 + i2 * 16 + i3\n                compute_1 = T.Buffer((1536,), data=compute.data)\n                data_1 = T.Buffer((1536,), data=data.data)\n                compute_1[cse_var_1] = T.log2(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log2f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 24; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 16; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 64) + (i2 * 16)) + i3);\n        ((float*)compute_1)[cse_var_1] = log2f(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "log2"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 16, 32, 28), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([48], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((48,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(48):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(896):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 48\n            data_1 = T.Buffer((43008,), data=data.data)\n            data_red_rf_1[0] = T.max(data_red_rf_1[0], data_1[cse_var_1])\n            data_red_rf_1[1] = T.max(data_red_rf_1[1], data_1[cse_var_1 + 1])\n            data_red_rf_1[2] = T.max(data_red_rf_1[2], data_1[cse_var_1 + 2])\n            data_red_rf_1[3] = T.max(data_red_rf_1[3], data_1[cse_var_1 + 3])\n            data_red_rf_1[4] = T.max(data_red_rf_1[4], data_1[cse_var_1 + 4])\n            data_red_rf_1[5] = T.max(data_red_rf_1[5], data_1[cse_var_1 + 5])\n            data_red_rf_1[6] = T.max(data_red_rf_1[6], data_1[cse_var_1 + 6])\n            data_red_rf_1[7] = T.max(data_red_rf_1[7], data_1[cse_var_1 + 7])\n            data_red_rf_1[8] = T.max(data_red_rf_1[8], data_1[cse_var_1 + 8])\n            data_red_rf_1[9] = T.max(data_red_rf_1[9], data_1[cse_var_1 + 9])\n            data_red_rf_1[10] = T.max(data_red_rf_1[10], data_1[cse_var_1 + 10])\n            data_red_rf_1[11] = T.max(data_red_rf_1[11], data_1[cse_var_1 + 11])\n            data_red_rf_1[12] = T.max(data_red_rf_1[12], data_1[cse_var_1 + 12])\n            data_red_rf_1[13] = T.max(data_red_rf_1[13], data_1[cse_var_1 + 13])\n            data_red_rf_1[14] = T.max(data_red_rf_1[14], data_1[cse_var_1 + 14])\n            data_red_rf_1[15] = T.max(data_red_rf_1[15], data_1[cse_var_1 + 15])\n            data_red_rf_1[16] = T.max(data_red_rf_1[16], data_1[cse_var_1 + 16])\n            data_red_rf_1[17] = T.max(data_red_rf_1[17], data_1[cse_var_1 + 17])\n            data_red_rf_1[18] = T.max(data_red_rf_1[18], data_1[cse_var_1 + 18])\n            data_red_rf_1[19] = T.max(data_red_rf_1[19], data_1[cse_var_1 + 19])\n            data_red_rf_1[20] = T.max(data_red_rf_1[20], data_1[cse_var_1 + 20])\n            data_red_rf_1[21] = T.max(data_red_rf_1[21], data_1[cse_var_1 + 21])\n            data_red_rf_1[22] = T.max(data_red_rf_1[22], data_1[cse_var_1 + 22])\n            data_red_rf_1[23] = T.max(data_red_rf_1[23], data_1[cse_var_1 + 23])\n            data_red_rf_1[24] = T.max(data_red_rf_1[24], data_1[cse_var_1 + 24])\n            data_red_rf_1[25] = T.max(data_red_rf_1[25], data_1[cse_var_1 + 25])\n            data_red_rf_1[26] = T.max(data_red_rf_1[26], data_1[cse_var_1 + 26])\n            data_red_rf_1[27] = T.max(data_red_rf_1[27], data_1[cse_var_1 + 27])\n            data_red_rf_1[28] = T.max(data_red_rf_1[28], data_1[cse_var_1 + 28])\n            data_red_rf_1[29] = T.max(data_red_rf_1[29], data_1[cse_var_1 + 29])\n            data_red_rf_1[30] = T.max(data_red_rf_1[30], data_1[cse_var_1 + 30])\n            data_red_rf_1[31] = T.max(data_red_rf_1[31], data_1[cse_var_1 + 31])\n            data_red_rf_1[32] = T.max(data_red_rf_1[32], data_1[cse_var_1 + 32])\n            data_red_rf_1[33] = T.max(data_red_rf_1[33], data_1[cse_var_1 + 33])\n            data_red_rf_1[34] = T.max(data_red_rf_1[34], data_1[cse_var_1 + 34])\n            data_red_rf_1[35] = T.max(data_red_rf_1[35], data_1[cse_var_1 + 35])\n            data_red_rf_1[36] = T.max(data_red_rf_1[36], data_1[cse_var_1 + 36])\n            data_red_rf_1[37] = T.max(data_red_rf_1[37], data_1[cse_var_1 + 37])\n            data_red_rf_1[38] = T.max(data_red_rf_1[38], data_1[cse_var_1 + 38])\n            data_red_rf_1[39] = T.max(data_red_rf_1[39], data_1[cse_var_1 + 39])\n            data_red_rf_1[40] = T.max(data_red_rf_1[40], data_1[cse_var_1 + 40])\n            data_red_rf_1[41] = T.max(data_red_rf_1[41], data_1[cse_var_1 + 41])\n            data_red_rf_1[42] = T.max(data_red_rf_1[42], data_1[cse_var_1 + 42])\n            data_red_rf_1[43] = T.max(data_red_rf_1[43], data_1[cse_var_1 + 43])\n            data_red_rf_1[44] = T.max(data_red_rf_1[44], data_1[cse_var_1 + 44])\n            data_red_rf_1[45] = T.max(data_red_rf_1[45], data_1[cse_var_1 + 45])\n            data_red_rf_1[46] = T.max(data_red_rf_1[46], data_1[cse_var_1 + 46])\n            data_red_rf_1[47] = T.max(data_red_rf_1[47], data_1[cse_var_1 + 47])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[0])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[1])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[2])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[3])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[4])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[5])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[6])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[7])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[8])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[9])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[10])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[11])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[12])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[13])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[14])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[15])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[16])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[17])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[18])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[19])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[20])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[21])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[22])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[23])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[24])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[25])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[26])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[27])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[28])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[29])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[30])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[31])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[32])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[33])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[34])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[35])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[36])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[37])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[38])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[39])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[40])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[41])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[42])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[43])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[44])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[45])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[46])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[47])\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1344; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[48];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 48; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 896; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 48);\n    float v_ = data_red_rf[0];\n    float v__1 = ((float*)data_1)[cse_var_1];\n    data_red_rf[0] = ((v_) > (v__1) ? (v_) : (v__1));\n    float v__2 = data_red_rf[1];\n    float v__3 = ((float*)data_1)[(cse_var_1 + 1)];\n    data_red_rf[1] = ((v__2) > (v__3) ? (v__2) : (v__3));\n    float v__4 = data_red_rf[2];\n    float v__5 = ((float*)data_1)[(cse_var_1 + 2)];\n    data_red_rf[2] = ((v__4) > (v__5) ? (v__4) : (v__5));\n    float v__6 = data_red_rf[3];\n    float v__7 = ((float*)data_1)[(cse_var_1 + 3)];\n    data_red_rf[3] = ((v__6) > (v__7) ? (v__6) : (v__7));\n    float v__8 = data_red_rf[4];\n    float v__9 = ((float*)data_1)[(cse_var_1 + 4)];\n    data_red_rf[4] = ((v__8) > (v__9) ? (v__8) : (v__9));\n    float v__10 = data_red_rf[5];\n    float v__11 = ((float*)data_1)[(cse_var_1 + 5)];\n    data_red_rf[5] = ((v__10) > (v__11) ? (v__10) : (v__11));\n    float v__12 = data_red_rf[6];\n    float v__13 = ((float*)data_1)[(cse_var_1 + 6)];\n    data_red_rf[6] = ((v__12) > (v__13) ? (v__12) : (v__13));\n    float v__14 = data_red_rf[7];\n    float v__15 = ((float*)data_1)[(cse_var_1 + 7)];\n    data_red_rf[7] = ((v__14) > (v__15) ? (v__14) : (v__15));\n    float v__16 = data_red_rf[8];\n    float v__17 = ((float*)data_1)[(cse_var_1 + 8)];\n    data_red_rf[8] = ((v__16) > (v__17) ? (v__16) : (v__17));\n    float v__18 = data_red_rf[9];\n    float v__19 = ((float*)data_1)[(cse_var_1 + 9)];\n    data_red_rf[9] = ((v__18) > (v__19) ? (v__18) : (v__19));\n    float v__20 = data_red_rf[10];\n    float v__21 = ((float*)data_1)[(cse_var_1 + 10)];\n    data_red_rf[10] = ((v__20) > (v__21) ? (v__20) : (v__21));\n    float v__22 = data_red_rf[11];\n    float v__23 = ((float*)data_1)[(cse_var_1 + 11)];\n    data_red_rf[11] = ((v__22) > (v__23) ? (v__22) : (v__23));\n    float v__24 = data_red_rf[12];\n    float v__25 = ((float*)data_1)[(cse_var_1 + 12)];\n    data_red_rf[12] = ((v__24) > (v__25) ? (v__24) : (v__25));\n    float v__26 = data_red_rf[13];\n    float v__27 = ((float*)data_1)[(cse_var_1 + 13)];\n    data_red_rf[13] = ((v__26) > (v__27) ? (v__26) : (v__27));\n    float v__28 = data_red_rf[14];\n    float v__29 = ((float*)data_1)[(cse_var_1 + 14)];\n    data_red_rf[14] = ((v__28) > (v__29) ? (v__28) : (v__29));\n    float v__30 = data_red_rf[15];\n    float v__31 = ((float*)data_1)[(cse_var_1 + 15)];\n    data_red_rf[15] = ((v__30) > (v__31) ? (v__30) : (v__31));\n    float v__32 = data_red_rf[16];\n    float v__33 = ((float*)data_1)[(cse_var_1 + 16)];\n    data_red_rf[16] = ((v__32) > (v__33) ? (v__32) : (v__33));\n    float v__34 = data_red_rf[17];\n    float v__35 = ((float*)data_1)[(cse_var_1 + 17)];\n    data_red_rf[17] = ((v__34) > (v__35) ? (v__34) : (v__35));\n    float v__36 = data_red_rf[18];\n    float v__37 = ((float*)data_1)[(cse_var_1 + 18)];\n    data_red_rf[18] = ((v__36) > (v__37) ? (v__36) : (v__37));\n    float v__38 = data_red_rf[19];\n    float v__39 = ((float*)data_1)[(cse_var_1 + 19)];\n    data_red_rf[19] = ((v__38) > (v__39) ? (v__38) : (v__39));\n    float v__40 = data_red_rf[20];\n    float v__41 = ((float*)data_1)[(cse_var_1 + 20)];\n    data_red_rf[20] = ((v__40) > (v__41) ? (v__40) : (v__41));\n    float v__42 = data_red_rf[21];\n    float v__43 = ((float*)data_1)[(cse_var_1 + 21)];\n    data_red_rf[21] = ((v__42) > (v__43) ? (v__42) : (v__43));\n    float v__44 = data_red_rf[22];\n    float v__45 = ((float*)data_1)[(cse_var_1 + 22)];\n    data_red_rf[22] = ((v__44) > (v__45) ? (v__44) : (v__45));\n    float v__46 = data_red_rf[23];\n    float v__47 = ((float*)data_1)[(cse_var_1 + 23)];\n    data_red_rf[23] = ((v__46) > (v__47) ? (v__46) : (v__47));\n    float v__48 = data_red_rf[24];\n    float v__49 = ((float*)data_1)[(cse_var_1 + 24)];\n    data_red_rf[24] = ((v__48) > (v__49) ? (v__48) : (v__49));\n    float v__50 = data_red_rf[25];\n    float v__51 = ((float*)data_1)[(cse_var_1 + 25)];\n    data_red_rf[25] = ((v__50) > (v__51) ? (v__50) : (v__51));\n    float v__52 = data_red_rf[26];\n    float v__53 = ((float*)data_1)[(cse_var_1 + 26)];\n    data_red_rf[26] = ((v__52) > (v__53) ? (v__52) : (v__53));\n    float v__54 = data_red_rf[27];\n    float v__55 = ((float*)data_1)[(cse_var_1 + 27)];\n    data_red_rf[27] = ((v__54) > (v__55) ? (v__54) : (v__55));\n    float v__56 = data_red_rf[28];\n    float v__57 = ((float*)data_1)[(cse_var_1 + 28)];\n    data_red_rf[28] = ((v__56) > (v__57) ? (v__56) : (v__57));\n    float v__58 = data_red_rf[29];\n    float v__59 = ((float*)data_1)[(cse_var_1 + 29)];\n    data_red_rf[29] = ((v__58) > (v__59) ? (v__58) : (v__59));\n    float v__60 = data_red_rf[30];\n    float v__61 = ((float*)data_1)[(cse_var_1 + 30)];\n    data_red_rf[30] = ((v__60) > (v__61) ? (v__60) : (v__61));\n    float v__62 = data_red_rf[31];\n    float v__63 = ((float*)data_1)[(cse_var_1 + 31)];\n    data_red_rf[31] = ((v__62) > (v__63) ? (v__62) : (v__63));\n    float v__64 = data_red_rf[32];\n    float v__65 = ((float*)data_1)[(cse_var_1 + 32)];\n    data_red_rf[32] = ((v__64) > (v__65) ? (v__64) : (v__65));\n    float v__66 = data_red_rf[33];\n    float v__67 = ((float*)data_1)[(cse_var_1 + 33)];\n    data_red_rf[33] = ((v__66) > (v__67) ? (v__66) : (v__67));\n    float v__68 = data_red_rf[34];\n    float v__69 = ((float*)data_1)[(cse_var_1 + 34)];\n    data_red_rf[34] = ((v__68) > (v__69) ? (v__68) : (v__69));\n    float v__70 = data_red_rf[35];\n    float v__71 = ((float*)data_1)[(cse_var_1 + 35)];\n    data_red_rf[35] = ((v__70) > (v__71) ? (v__70) : (v__71));\n    float v__72 = data_red_rf[36];\n    float v__73 = ((float*)data_1)[(cse_var_1 + 36)];\n    data_red_rf[36] = ((v__72) > (v__73) ? (v__72) : (v__73));\n    float v__74 = data_red_rf[37];\n    float v__75 = ((float*)data_1)[(cse_var_1 + 37)];\n    data_red_rf[37] = ((v__74) > (v__75) ? (v__74) : (v__75));\n    float v__76 = data_red_rf[38];\n    float v__77 = ((float*)data_1)[(cse_var_1 + 38)];\n    data_red_rf[38] = ((v__76) > (v__77) ? (v__76) : (v__77));\n    float v__78 = data_red_rf[39];\n    float v__79 = ((float*)data_1)[(cse_var_1 + 39)];\n    data_red_rf[39] = ((v__78) > (v__79) ? (v__78) : (v__79));\n    float v__80 = data_red_rf[40];\n    float v__81 = ((float*)data_1)[(cse_var_1 + 40)];\n    data_red_rf[40] = ((v__80) > (v__81) ? (v__80) : (v__81));\n    float v__82 = data_red_rf[41];\n    float v__83 = ((float*)data_1)[(cse_var_1 + 41)];\n    data_red_rf[41] = ((v__82) > (v__83) ? (v__82) : (v__83));\n    float v__84 = data_red_rf[42];\n    float v__85 = ((float*)data_1)[(cse_var_1 + 42)];\n    data_red_rf[42] = ((v__84) > (v__85) ? (v__84) : (v__85));\n    float v__86 = data_red_rf[43];\n    float v__87 = ((float*)data_1)[(cse_var_1 + 43)];\n    data_red_rf[43] = ((v__86) > (v__87) ? (v__86) : (v__87));\n    float v__88 = data_red_rf[44];\n    float v__89 = ((float*)data_1)[(cse_var_1 + 44)];\n    data_red_rf[44] = ((v__88) > (v__89) ? (v__88) : (v__89));\n    float v__90 = data_red_rf[45];\n    float v__91 = ((float*)data_1)[(cse_var_1 + 45)];\n    data_red_rf[45] = ((v__90) > (v__91) ? (v__90) : (v__91));\n    float v__92 = data_red_rf[46];\n    float v__93 = ((float*)data_1)[(cse_var_1 + 46)];\n    data_red_rf[46] = ((v__92) > (v__93) ? (v__92) : (v__93));\n    float v__94 = data_red_rf[47];\n    float v__95 = ((float*)data_1)[(cse_var_1 + 47)];\n    data_red_rf[47] = ((v__94) > (v__95) ? (v__94) : (v__95));\n  }\n  ((float*)data_red_1)[0] = -3.402823e+38f;\n  float v__96 = ((float*)data_red_1)[0];\n  float v__97 = data_red_rf[0];\n  ((float*)data_red_1)[0] = ((v__96) > (v__97) ? (v__96) : (v__97));\n  float v__98 = ((float*)data_red_1)[0];\n  float v__99 = data_red_rf[1];\n  ((float*)data_red_1)[0] = ((v__98) > (v__99) ? (v__98) : (v__99));\n  float v__100 = ((float*)data_red_1)[0];\n  float v__101 = data_red_rf[2];\n  ((float*)data_red_1)[0] = ((v__100) > (v__101) ? (v__100) : (v__101));\n  float v__102 = ((float*)data_red_1)[0];\n  float v__103 = data_red_rf[3];\n  ((float*)data_red_1)[0] = ((v__102) > (v__103) ? (v__102) : (v__103));\n  float v__104 = ((float*)data_red_1)[0];\n  float v__105 = data_red_rf[4];\n  ((float*)data_red_1)[0] = ((v__104) > (v__105) ? (v__104) : (v__105));\n  float v__106 = ((float*)data_red_1)[0];\n  float v__107 = data_red_rf[5];\n  ((float*)data_red_1)[0] = ((v__106) > (v__107) ? (v__106) : (v__107));\n  float v__108 = ((float*)data_red_1)[0];\n  float v__109 = data_red_rf[6];\n  ((float*)data_red_1)[0] = ((v__108) > (v__109) ? (v__108) : (v__109));\n  float v__110 = ((float*)data_red_1)[0];\n  float v__111 = data_red_rf[7];\n  ((float*)data_red_1)[0] = ((v__110) > (v__111) ? (v__110) : (v__111));\n  float v__112 = ((float*)data_red_1)[0];\n  float v__113 = data_red_rf[8];\n  ((float*)data_red_1)[0] = ((v__112) > (v__113) ? (v__112) : (v__113));\n  float v__114 = ((float*)data_red_1)[0];\n  float v__115 = data_red_rf[9];\n  ((float*)data_red_1)[0] = ((v__114) > (v__115) ? (v__114) : (v__115));\n  float v__116 = ((float*)data_red_1)[0];\n  float v__117 = data_red_rf[10];\n  ((float*)data_red_1)[0] = ((v__116) > (v__117) ? (v__116) : (v__117));\n  float v__118 = ((float*)data_red_1)[0];\n  float v__119 = data_red_rf[11];\n  ((float*)data_red_1)[0] = ((v__118) > (v__119) ? (v__118) : (v__119));\n  float v__120 = ((float*)data_red_1)[0];\n  float v__121 = data_red_rf[12];\n  ((float*)data_red_1)[0] = ((v__120) > (v__121) ? (v__120) : (v__121));\n  float v__122 = ((float*)data_red_1)[0];\n  float v__123 = data_red_rf[13];\n  ((float*)data_red_1)[0] = ((v__122) > (v__123) ? (v__122) : (v__123));\n  float v__124 = ((float*)data_red_1)[0];\n  float v__125 = data_red_rf[14];\n  ((float*)data_red_1)[0] = ((v__124) > (v__125) ? (v__124) : (v__125));\n  float v__126 = ((float*)data_red_1)[0];\n  float v__127 = data_red_rf[15];\n  ((float*)data_red_1)[0] = ((v__126) > (v__127) ? (v__126) : (v__127));\n  float v__128 = ((float*)data_red_1)[0];\n  float v__129 = data_red_rf[16];\n  ((float*)data_red_1)[0] = ((v__128) > (v__129) ? (v__128) : (v__129));\n  float v__130 = ((float*)data_red_1)[0];\n  float v__131 = data_red_rf[17];\n  ((float*)data_red_1)[0] = ((v__130) > (v__131) ? (v__130) : (v__131));\n  float v__132 = ((float*)data_red_1)[0];\n  float v__133 = data_red_rf[18];\n  ((float*)data_red_1)[0] = ((v__132) > (v__133) ? (v__132) : (v__133));\n  float v__134 = ((float*)data_red_1)[0];\n  float v__135 = data_red_rf[19];\n  ((float*)data_red_1)[0] = ((v__134) > (v__135) ? (v__134) : (v__135));\n  float v__136 = ((float*)data_red_1)[0];\n  float v__137 = data_red_rf[20];\n  ((float*)data_red_1)[0] = ((v__136) > (v__137) ? (v__136) : (v__137));\n  float v__138 = ((float*)data_red_1)[0];\n  float v__139 = data_red_rf[21];\n  ((float*)data_red_1)[0] = ((v__138) > (v__139) ? (v__138) : (v__139));\n  float v__140 = ((float*)data_red_1)[0];\n  float v__141 = data_red_rf[22];\n  ((float*)data_red_1)[0] = ((v__140) > (v__141) ? (v__140) : (v__141));\n  float v__142 = ((float*)data_red_1)[0];\n  float v__143 = data_red_rf[23];\n  ((float*)data_red_1)[0] = ((v__142) > (v__143) ? (v__142) : (v__143));\n  float v__144 = ((float*)data_red_1)[0];\n  float v__145 = data_red_rf[24];\n  ((float*)data_red_1)[0] = ((v__144) > (v__145) ? (v__144) : (v__145));\n  float v__146 = ((float*)data_red_1)[0];\n  float v__147 = data_red_rf[25];\n  ((float*)data_red_1)[0] = ((v__146) > (v__147) ? (v__146) : (v__147));\n  float v__148 = ((float*)data_red_1)[0];\n  float v__149 = data_red_rf[26];\n  ((float*)data_red_1)[0] = ((v__148) > (v__149) ? (v__148) : (v__149));\n  float v__150 = ((float*)data_red_1)[0];\n  float v__151 = data_red_rf[27];\n  ((float*)data_red_1)[0] = ((v__150) > (v__151) ? (v__150) : (v__151));\n  float v__152 = ((float*)data_red_1)[0];\n  float v__153 = data_red_rf[28];\n  ((float*)data_red_1)[0] = ((v__152) > (v__153) ? (v__152) : (v__153));\n  float v__154 = ((float*)data_red_1)[0];\n  float v__155 = data_red_rf[29];\n  ((float*)data_red_1)[0] = ((v__154) > (v__155) ? (v__154) : (v__155));\n  float v__156 = ((float*)data_red_1)[0];\n  float v__157 = data_red_rf[30];\n  ((float*)data_red_1)[0] = ((v__156) > (v__157) ? (v__156) : (v__157));\n  float v__158 = ((float*)data_red_1)[0];\n  float v__159 = data_red_rf[31];\n  ((float*)data_red_1)[0] = ((v__158) > (v__159) ? (v__158) : (v__159));\n  float v__160 = ((float*)data_red_1)[0];\n  float v__161 = data_red_rf[32];\n  ((float*)data_red_1)[0] = ((v__160) > (v__161) ? (v__160) : (v__161));\n  float v__162 = ((float*)data_red_1)[0];\n  float v__163 = data_red_rf[33];\n  ((float*)data_red_1)[0] = ((v__162) > (v__163) ? (v__162) : (v__163));\n  float v__164 = ((float*)data_red_1)[0];\n  float v__165 = data_red_rf[34];\n  ((float*)data_red_1)[0] = ((v__164) > (v__165) ? (v__164) : (v__165));\n  float v__166 = ((float*)data_red_1)[0];\n  float v__167 = data_red_rf[35];\n  ((float*)data_red_1)[0] = ((v__166) > (v__167) ? (v__166) : (v__167));\n  float v__168 = ((float*)data_red_1)[0];\n  float v__169 = data_red_rf[36];\n  ((float*)data_red_1)[0] = ((v__168) > (v__169) ? (v__168) : (v__169));\n  float v__170 = ((float*)data_red_1)[0];\n  float v__171 = data_red_rf[37];\n  ((float*)data_red_1)[0] = ((v__170) > (v__171) ? (v__170) : (v__171));\n  float v__172 = ((float*)data_red_1)[0];\n  float v__173 = data_red_rf[38];\n  ((float*)data_red_1)[0] = ((v__172) > (v__173) ? (v__172) : (v__173));\n  float v__174 = ((float*)data_red_1)[0];\n  float v__175 = data_red_rf[39];\n  ((float*)data_red_1)[0] = ((v__174) > (v__175) ? (v__174) : (v__175));\n  float v__176 = ((float*)data_red_1)[0];\n  float v__177 = data_red_rf[40];\n  ((float*)data_red_1)[0] = ((v__176) > (v__177) ? (v__176) : (v__177));\n  float v__178 = ((float*)data_red_1)[0];\n  float v__179 = data_red_rf[41];\n  ((float*)data_red_1)[0] = ((v__178) > (v__179) ? (v__178) : (v__179));\n  float v__180 = ((float*)data_red_1)[0];\n  float v__181 = data_red_rf[42];\n  ((float*)data_red_1)[0] = ((v__180) > (v__181) ? (v__180) : (v__181));\n  float v__182 = ((float*)data_red_1)[0];\n  float v__183 = data_red_rf[43];\n  ((float*)data_red_1)[0] = ((v__182) > (v__183) ? (v__182) : (v__183));\n  float v__184 = ((float*)data_red_1)[0];\n  float v__185 = data_red_rf[44];\n  ((float*)data_red_1)[0] = ((v__184) > (v__185) ? (v__184) : (v__185));\n  float v__186 = ((float*)data_red_1)[0];\n  float v__187 = data_red_rf[45];\n  ((float*)data_red_1)[0] = ((v__186) > (v__187) ? (v__186) : (v__187));\n  float v__188 = ((float*)data_red_1)[0];\n  float v__189 = data_red_rf[46];\n  ((float*)data_red_1)[0] = ((v__188) > (v__189) ? (v__188) : (v__189));\n  float v__190 = ((float*)data_red_1)[0];\n  float v__191 = data_red_rf[47];\n  ((float*)data_red_1)[0] = ((v__190) > (v__191) ? (v__190) : (v__191));\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "max"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 16, 24, 36), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([45], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((45,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(45):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(3072, 45):\n            data_1 = T.Buffer((138240,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 45 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(45):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 4320; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[45];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 45; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 3072; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 45; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      float v_ = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner];\n      float v__1 = ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 45) + k0_k1_fused_k2_fused_k3_fused_inner)];\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = ((v_) > (v__1) ? (v_) : (v__1));\n    }\n  }\n  ((float*)data_red_1)[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 45; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    float v__2 = ((float*)data_red_1)[0];\n    float v__3 = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v];\n    ((float*)data_red_1)[0] = ((v__2) > (v__3) ? (v__2) : (v__3));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "max"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 16, 4, 40), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([15], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((15,), data=data_red_rf, align=32)\n        data_red_rf_1[0:15] = T.Broadcast(T.float32(-3.4028234663852886e+38), 15)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(1536):\n            data_1 = T.Buffer((23040,), data=data.data)\n            data_red_rf_1[0:15] = T.max(data_red_rf_1[0:15], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 15:k0_k1_fused_k2_fused_k3_fused_outer * 15 + 15])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[0])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[1])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[2])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[3])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[4])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[5])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[6])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[7])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[8])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[9])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[10])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[11])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[12])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[13])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[14])\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 720; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[15];\n  *(float15*)(data_red_rf + 0) = ((float15)(-3.402823e+38f, -3.402823e+38f, -3.402823e+38f, -3.402823e+38f, -3.402823e+38f, -3.402823e+38f, -3.402823e+38f, -3.402823e+38f, -3.402823e+38f, -3.402823e+38f, -3.402823e+38f, -3.402823e+38f, -3.402823e+38f, -3.402823e+38f, -3.402823e+38f));\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1536; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    float15 v_ = *(float15*)(data_red_rf + 0);\n    float15 v__1 = *(float15*)(((float*)data_1) + (k0_k1_fused_k2_fused_k3_fused_outer * 15));\n    *(float15*)(data_red_rf + 0) = ((v_) > (v__1) ? (v_) : (v__1));\n  }\n  ((float*)data_red_1)[0] = -3.402823e+38f;\n  float v__2 = ((float*)data_red_1)[0];\n  float v__3 = data_red_rf[0];\n  ((float*)data_red_1)[0] = ((v__2) > (v__3) ? (v__2) : (v__3));\n  float v__4 = ((float*)data_red_1)[0];\n  float v__5 = data_red_rf[1];\n  ((float*)data_red_1)[0] = ((v__4) > (v__5) ? (v__4) : (v__5));\n  float v__6 = ((float*)data_red_1)[0];\n  float v__7 = data_red_rf[2];\n  ((float*)data_red_1)[0] = ((v__6) > (v__7) ? (v__6) : (v__7));\n  float v__8 = ((float*)data_red_1)[0];\n  float v__9 = data_red_rf[3];\n  ((float*)data_red_1)[0] = ((v__8) > (v__9) ? (v__8) : (v__9));\n  float v__10 = ((float*)data_red_1)[0];\n  float v__11 = data_red_rf[4];\n  ((float*)data_red_1)[0] = ((v__10) > (v__11) ? (v__10) : (v__11));\n  float v__12 = ((float*)data_red_1)[0];\n  float v__13 = data_red_rf[5];\n  ((float*)data_red_1)[0] = ((v__12) > (v__13) ? (v__12) : (v__13));\n  float v__14 = ((float*)data_red_1)[0];\n  float v__15 = data_red_rf[6];\n  ((float*)data_red_1)[0] = ((v__14) > (v__15) ? (v__14) : (v__15));\n  float v__16 = ((float*)data_red_1)[0];\n  float v__17 = data_red_rf[7];\n  ((float*)data_red_1)[0] = ((v__16) > (v__17) ? (v__16) : (v__17));\n  float v__18 = ((float*)data_red_1)[0];\n  float v__19 = data_red_rf[8];\n  ((float*)data_red_1)[0] = ((v__18) > (v__19) ? (v__18) : (v__19));\n  float v__20 = ((float*)data_red_1)[0];\n  float v__21 = data_red_rf[9];\n  ((float*)data_red_1)[0] = ((v__20) > (v__21) ? (v__20) : (v__21));\n  float v__22 = ((float*)data_red_1)[0];\n  float v__23 = data_red_rf[10];\n  ((float*)data_red_1)[0] = ((v__22) > (v__23) ? (v__22) : (v__23));\n  float v__24 = ((float*)data_red_1)[0];\n  float v__25 = data_red_rf[11];\n  ((float*)data_red_1)[0] = ((v__24) > (v__25) ? (v__24) : (v__25));\n  float v__26 = ((float*)data_red_1)[0];\n  float v__27 = data_red_rf[12];\n  ((float*)data_red_1)[0] = ((v__26) > (v__27) ? (v__26) : (v__27));\n  float v__28 = ((float*)data_red_1)[0];\n  float v__29 = data_red_rf[13];\n  ((float*)data_red_1)[0] = ((v__28) > (v__29) ? (v__28) : (v__29));\n  float v__30 = ((float*)data_red_1)[0];\n  float v__31 = data_red_rf[14];\n  ((float*)data_red_1)[0] = ((v__30) > (v__31) ? (v__30) : (v__31));\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "max"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 12, 32, 36), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([32], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((32,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(32):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(864, 32):\n            data_1 = T.Buffer((27648,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 32 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(32):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 864; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[32];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 32; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 864; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 32; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      float v_ = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner];\n      float v__1 = ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + k0_k1_fused_k2_fused_k3_fused_inner)];\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = ((v_) > (v__1) ? (v_) : (v__1));\n    }\n  }\n  ((float*)data_red_1)[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 32; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    float v__2 = ((float*)data_red_1)[0];\n    float v__3 = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v];\n    ((float*)data_red_1)[0] = ((v__2) > (v__3) ? (v__2) : (v__3));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "max"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 8, 40, 16), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([36], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((36,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(36):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(1280):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 36\n            data_1 = T.Buffer((46080,), data=data.data)\n            data_red_rf_1[0] = T.max(data_red_rf_1[0], data_1[cse_var_1])\n            data_red_rf_1[1] = T.max(data_red_rf_1[1], data_1[cse_var_1 + 1])\n            data_red_rf_1[2] = T.max(data_red_rf_1[2], data_1[cse_var_1 + 2])\n            data_red_rf_1[3] = T.max(data_red_rf_1[3], data_1[cse_var_1 + 3])\n            data_red_rf_1[4] = T.max(data_red_rf_1[4], data_1[cse_var_1 + 4])\n            data_red_rf_1[5] = T.max(data_red_rf_1[5], data_1[cse_var_1 + 5])\n            data_red_rf_1[6] = T.max(data_red_rf_1[6], data_1[cse_var_1 + 6])\n            data_red_rf_1[7] = T.max(data_red_rf_1[7], data_1[cse_var_1 + 7])\n            data_red_rf_1[8] = T.max(data_red_rf_1[8], data_1[cse_var_1 + 8])\n            data_red_rf_1[9] = T.max(data_red_rf_1[9], data_1[cse_var_1 + 9])\n            data_red_rf_1[10] = T.max(data_red_rf_1[10], data_1[cse_var_1 + 10])\n            data_red_rf_1[11] = T.max(data_red_rf_1[11], data_1[cse_var_1 + 11])\n            data_red_rf_1[12] = T.max(data_red_rf_1[12], data_1[cse_var_1 + 12])\n            data_red_rf_1[13] = T.max(data_red_rf_1[13], data_1[cse_var_1 + 13])\n            data_red_rf_1[14] = T.max(data_red_rf_1[14], data_1[cse_var_1 + 14])\n            data_red_rf_1[15] = T.max(data_red_rf_1[15], data_1[cse_var_1 + 15])\n            data_red_rf_1[16] = T.max(data_red_rf_1[16], data_1[cse_var_1 + 16])\n            data_red_rf_1[17] = T.max(data_red_rf_1[17], data_1[cse_var_1 + 17])\n            data_red_rf_1[18] = T.max(data_red_rf_1[18], data_1[cse_var_1 + 18])\n            data_red_rf_1[19] = T.max(data_red_rf_1[19], data_1[cse_var_1 + 19])\n            data_red_rf_1[20] = T.max(data_red_rf_1[20], data_1[cse_var_1 + 20])\n            data_red_rf_1[21] = T.max(data_red_rf_1[21], data_1[cse_var_1 + 21])\n            data_red_rf_1[22] = T.max(data_red_rf_1[22], data_1[cse_var_1 + 22])\n            data_red_rf_1[23] = T.max(data_red_rf_1[23], data_1[cse_var_1 + 23])\n            data_red_rf_1[24] = T.max(data_red_rf_1[24], data_1[cse_var_1 + 24])\n            data_red_rf_1[25] = T.max(data_red_rf_1[25], data_1[cse_var_1 + 25])\n            data_red_rf_1[26] = T.max(data_red_rf_1[26], data_1[cse_var_1 + 26])\n            data_red_rf_1[27] = T.max(data_red_rf_1[27], data_1[cse_var_1 + 27])\n            data_red_rf_1[28] = T.max(data_red_rf_1[28], data_1[cse_var_1 + 28])\n            data_red_rf_1[29] = T.max(data_red_rf_1[29], data_1[cse_var_1 + 29])\n            data_red_rf_1[30] = T.max(data_red_rf_1[30], data_1[cse_var_1 + 30])\n            data_red_rf_1[31] = T.max(data_red_rf_1[31], data_1[cse_var_1 + 31])\n            data_red_rf_1[32] = T.max(data_red_rf_1[32], data_1[cse_var_1 + 32])\n            data_red_rf_1[33] = T.max(data_red_rf_1[33], data_1[cse_var_1 + 33])\n            data_red_rf_1[34] = T.max(data_red_rf_1[34], data_1[cse_var_1 + 34])\n            data_red_rf_1[35] = T.max(data_red_rf_1[35], data_1[cse_var_1 + 35])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[0])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[1])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[2])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[3])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[4])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[5])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[6])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[7])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[8])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[9])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[10])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[11])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[12])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[13])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[14])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[15])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[16])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[17])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[18])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[19])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[20])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[21])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[22])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[23])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[24])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[25])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[26])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[27])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[28])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[29])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[30])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[31])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[32])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[33])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[34])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[35])\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1440; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[36];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 36; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1280; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 36);\n    float v_ = data_red_rf[0];\n    float v__1 = ((float*)data_1)[cse_var_1];\n    data_red_rf[0] = ((v_) > (v__1) ? (v_) : (v__1));\n    float v__2 = data_red_rf[1];\n    float v__3 = ((float*)data_1)[(cse_var_1 + 1)];\n    data_red_rf[1] = ((v__2) > (v__3) ? (v__2) : (v__3));\n    float v__4 = data_red_rf[2];\n    float v__5 = ((float*)data_1)[(cse_var_1 + 2)];\n    data_red_rf[2] = ((v__4) > (v__5) ? (v__4) : (v__5));\n    float v__6 = data_red_rf[3];\n    float v__7 = ((float*)data_1)[(cse_var_1 + 3)];\n    data_red_rf[3] = ((v__6) > (v__7) ? (v__6) : (v__7));\n    float v__8 = data_red_rf[4];\n    float v__9 = ((float*)data_1)[(cse_var_1 + 4)];\n    data_red_rf[4] = ((v__8) > (v__9) ? (v__8) : (v__9));\n    float v__10 = data_red_rf[5];\n    float v__11 = ((float*)data_1)[(cse_var_1 + 5)];\n    data_red_rf[5] = ((v__10) > (v__11) ? (v__10) : (v__11));\n    float v__12 = data_red_rf[6];\n    float v__13 = ((float*)data_1)[(cse_var_1 + 6)];\n    data_red_rf[6] = ((v__12) > (v__13) ? (v__12) : (v__13));\n    float v__14 = data_red_rf[7];\n    float v__15 = ((float*)data_1)[(cse_var_1 + 7)];\n    data_red_rf[7] = ((v__14) > (v__15) ? (v__14) : (v__15));\n    float v__16 = data_red_rf[8];\n    float v__17 = ((float*)data_1)[(cse_var_1 + 8)];\n    data_red_rf[8] = ((v__16) > (v__17) ? (v__16) : (v__17));\n    float v__18 = data_red_rf[9];\n    float v__19 = ((float*)data_1)[(cse_var_1 + 9)];\n    data_red_rf[9] = ((v__18) > (v__19) ? (v__18) : (v__19));\n    float v__20 = data_red_rf[10];\n    float v__21 = ((float*)data_1)[(cse_var_1 + 10)];\n    data_red_rf[10] = ((v__20) > (v__21) ? (v__20) : (v__21));\n    float v__22 = data_red_rf[11];\n    float v__23 = ((float*)data_1)[(cse_var_1 + 11)];\n    data_red_rf[11] = ((v__22) > (v__23) ? (v__22) : (v__23));\n    float v__24 = data_red_rf[12];\n    float v__25 = ((float*)data_1)[(cse_var_1 + 12)];\n    data_red_rf[12] = ((v__24) > (v__25) ? (v__24) : (v__25));\n    float v__26 = data_red_rf[13];\n    float v__27 = ((float*)data_1)[(cse_var_1 + 13)];\n    data_red_rf[13] = ((v__26) > (v__27) ? (v__26) : (v__27));\n    float v__28 = data_red_rf[14];\n    float v__29 = ((float*)data_1)[(cse_var_1 + 14)];\n    data_red_rf[14] = ((v__28) > (v__29) ? (v__28) : (v__29));\n    float v__30 = data_red_rf[15];\n    float v__31 = ((float*)data_1)[(cse_var_1 + 15)];\n    data_red_rf[15] = ((v__30) > (v__31) ? (v__30) : (v__31));\n    float v__32 = data_red_rf[16];\n    float v__33 = ((float*)data_1)[(cse_var_1 + 16)];\n    data_red_rf[16] = ((v__32) > (v__33) ? (v__32) : (v__33));\n    float v__34 = data_red_rf[17];\n    float v__35 = ((float*)data_1)[(cse_var_1 + 17)];\n    data_red_rf[17] = ((v__34) > (v__35) ? (v__34) : (v__35));\n    float v__36 = data_red_rf[18];\n    float v__37 = ((float*)data_1)[(cse_var_1 + 18)];\n    data_red_rf[18] = ((v__36) > (v__37) ? (v__36) : (v__37));\n    float v__38 = data_red_rf[19];\n    float v__39 = ((float*)data_1)[(cse_var_1 + 19)];\n    data_red_rf[19] = ((v__38) > (v__39) ? (v__38) : (v__39));\n    float v__40 = data_red_rf[20];\n    float v__41 = ((float*)data_1)[(cse_var_1 + 20)];\n    data_red_rf[20] = ((v__40) > (v__41) ? (v__40) : (v__41));\n    float v__42 = data_red_rf[21];\n    float v__43 = ((float*)data_1)[(cse_var_1 + 21)];\n    data_red_rf[21] = ((v__42) > (v__43) ? (v__42) : (v__43));\n    float v__44 = data_red_rf[22];\n    float v__45 = ((float*)data_1)[(cse_var_1 + 22)];\n    data_red_rf[22] = ((v__44) > (v__45) ? (v__44) : (v__45));\n    float v__46 = data_red_rf[23];\n    float v__47 = ((float*)data_1)[(cse_var_1 + 23)];\n    data_red_rf[23] = ((v__46) > (v__47) ? (v__46) : (v__47));\n    float v__48 = data_red_rf[24];\n    float v__49 = ((float*)data_1)[(cse_var_1 + 24)];\n    data_red_rf[24] = ((v__48) > (v__49) ? (v__48) : (v__49));\n    float v__50 = data_red_rf[25];\n    float v__51 = ((float*)data_1)[(cse_var_1 + 25)];\n    data_red_rf[25] = ((v__50) > (v__51) ? (v__50) : (v__51));\n    float v__52 = data_red_rf[26];\n    float v__53 = ((float*)data_1)[(cse_var_1 + 26)];\n    data_red_rf[26] = ((v__52) > (v__53) ? (v__52) : (v__53));\n    float v__54 = data_red_rf[27];\n    float v__55 = ((float*)data_1)[(cse_var_1 + 27)];\n    data_red_rf[27] = ((v__54) > (v__55) ? (v__54) : (v__55));\n    float v__56 = data_red_rf[28];\n    float v__57 = ((float*)data_1)[(cse_var_1 + 28)];\n    data_red_rf[28] = ((v__56) > (v__57) ? (v__56) : (v__57));\n    float v__58 = data_red_rf[29];\n    float v__59 = ((float*)data_1)[(cse_var_1 + 29)];\n    data_red_rf[29] = ((v__58) > (v__59) ? (v__58) : (v__59));\n    float v__60 = data_red_rf[30];\n    float v__61 = ((float*)data_1)[(cse_var_1 + 30)];\n    data_red_rf[30] = ((v__60) > (v__61) ? (v__60) : (v__61));\n    float v__62 = data_red_rf[31];\n    float v__63 = ((float*)data_1)[(cse_var_1 + 31)];\n    data_red_rf[31] = ((v__62) > (v__63) ? (v__62) : (v__63));\n    float v__64 = data_red_rf[32];\n    float v__65 = ((float*)data_1)[(cse_var_1 + 32)];\n    data_red_rf[32] = ((v__64) > (v__65) ? (v__64) : (v__65));\n    float v__66 = data_red_rf[33];\n    float v__67 = ((float*)data_1)[(cse_var_1 + 33)];\n    data_red_rf[33] = ((v__66) > (v__67) ? (v__66) : (v__67));\n    float v__68 = data_red_rf[34];\n    float v__69 = ((float*)data_1)[(cse_var_1 + 34)];\n    data_red_rf[34] = ((v__68) > (v__69) ? (v__68) : (v__69));\n    float v__70 = data_red_rf[35];\n    float v__71 = ((float*)data_1)[(cse_var_1 + 35)];\n    data_red_rf[35] = ((v__70) > (v__71) ? (v__70) : (v__71));\n  }\n  ((float*)data_red_1)[0] = -3.402823e+38f;\n  float v__72 = ((float*)data_red_1)[0];\n  float v__73 = data_red_rf[0];\n  ((float*)data_red_1)[0] = ((v__72) > (v__73) ? (v__72) : (v__73));\n  float v__74 = ((float*)data_red_1)[0];\n  float v__75 = data_red_rf[1];\n  ((float*)data_red_1)[0] = ((v__74) > (v__75) ? (v__74) : (v__75));\n  float v__76 = ((float*)data_red_1)[0];\n  float v__77 = data_red_rf[2];\n  ((float*)data_red_1)[0] = ((v__76) > (v__77) ? (v__76) : (v__77));\n  float v__78 = ((float*)data_red_1)[0];\n  float v__79 = data_red_rf[3];\n  ((float*)data_red_1)[0] = ((v__78) > (v__79) ? (v__78) : (v__79));\n  float v__80 = ((float*)data_red_1)[0];\n  float v__81 = data_red_rf[4];\n  ((float*)data_red_1)[0] = ((v__80) > (v__81) ? (v__80) : (v__81));\n  float v__82 = ((float*)data_red_1)[0];\n  float v__83 = data_red_rf[5];\n  ((float*)data_red_1)[0] = ((v__82) > (v__83) ? (v__82) : (v__83));\n  float v__84 = ((float*)data_red_1)[0];\n  float v__85 = data_red_rf[6];\n  ((float*)data_red_1)[0] = ((v__84) > (v__85) ? (v__84) : (v__85));\n  float v__86 = ((float*)data_red_1)[0];\n  float v__87 = data_red_rf[7];\n  ((float*)data_red_1)[0] = ((v__86) > (v__87) ? (v__86) : (v__87));\n  float v__88 = ((float*)data_red_1)[0];\n  float v__89 = data_red_rf[8];\n  ((float*)data_red_1)[0] = ((v__88) > (v__89) ? (v__88) : (v__89));\n  float v__90 = ((float*)data_red_1)[0];\n  float v__91 = data_red_rf[9];\n  ((float*)data_red_1)[0] = ((v__90) > (v__91) ? (v__90) : (v__91));\n  float v__92 = ((float*)data_red_1)[0];\n  float v__93 = data_red_rf[10];\n  ((float*)data_red_1)[0] = ((v__92) > (v__93) ? (v__92) : (v__93));\n  float v__94 = ((float*)data_red_1)[0];\n  float v__95 = data_red_rf[11];\n  ((float*)data_red_1)[0] = ((v__94) > (v__95) ? (v__94) : (v__95));\n  float v__96 = ((float*)data_red_1)[0];\n  float v__97 = data_red_rf[12];\n  ((float*)data_red_1)[0] = ((v__96) > (v__97) ? (v__96) : (v__97));\n  float v__98 = ((float*)data_red_1)[0];\n  float v__99 = data_red_rf[13];\n  ((float*)data_red_1)[0] = ((v__98) > (v__99) ? (v__98) : (v__99));\n  float v__100 = ((float*)data_red_1)[0];\n  float v__101 = data_red_rf[14];\n  ((float*)data_red_1)[0] = ((v__100) > (v__101) ? (v__100) : (v__101));\n  float v__102 = ((float*)data_red_1)[0];\n  float v__103 = data_red_rf[15];\n  ((float*)data_red_1)[0] = ((v__102) > (v__103) ? (v__102) : (v__103));\n  float v__104 = ((float*)data_red_1)[0];\n  float v__105 = data_red_rf[16];\n  ((float*)data_red_1)[0] = ((v__104) > (v__105) ? (v__104) : (v__105));\n  float v__106 = ((float*)data_red_1)[0];\n  float v__107 = data_red_rf[17];\n  ((float*)data_red_1)[0] = ((v__106) > (v__107) ? (v__106) : (v__107));\n  float v__108 = ((float*)data_red_1)[0];\n  float v__109 = data_red_rf[18];\n  ((float*)data_red_1)[0] = ((v__108) > (v__109) ? (v__108) : (v__109));\n  float v__110 = ((float*)data_red_1)[0];\n  float v__111 = data_red_rf[19];\n  ((float*)data_red_1)[0] = ((v__110) > (v__111) ? (v__110) : (v__111));\n  float v__112 = ((float*)data_red_1)[0];\n  float v__113 = data_red_rf[20];\n  ((float*)data_red_1)[0] = ((v__112) > (v__113) ? (v__112) : (v__113));\n  float v__114 = ((float*)data_red_1)[0];\n  float v__115 = data_red_rf[21];\n  ((float*)data_red_1)[0] = ((v__114) > (v__115) ? (v__114) : (v__115));\n  float v__116 = ((float*)data_red_1)[0];\n  float v__117 = data_red_rf[22];\n  ((float*)data_red_1)[0] = ((v__116) > (v__117) ? (v__116) : (v__117));\n  float v__118 = ((float*)data_red_1)[0];\n  float v__119 = data_red_rf[23];\n  ((float*)data_red_1)[0] = ((v__118) > (v__119) ? (v__118) : (v__119));\n  float v__120 = ((float*)data_red_1)[0];\n  float v__121 = data_red_rf[24];\n  ((float*)data_red_1)[0] = ((v__120) > (v__121) ? (v__120) : (v__121));\n  float v__122 = ((float*)data_red_1)[0];\n  float v__123 = data_red_rf[25];\n  ((float*)data_red_1)[0] = ((v__122) > (v__123) ? (v__122) : (v__123));\n  float v__124 = ((float*)data_red_1)[0];\n  float v__125 = data_red_rf[26];\n  ((float*)data_red_1)[0] = ((v__124) > (v__125) ? (v__124) : (v__125));\n  float v__126 = ((float*)data_red_1)[0];\n  float v__127 = data_red_rf[27];\n  ((float*)data_red_1)[0] = ((v__126) > (v__127) ? (v__126) : (v__127));\n  float v__128 = ((float*)data_red_1)[0];\n  float v__129 = data_red_rf[28];\n  ((float*)data_red_1)[0] = ((v__128) > (v__129) ? (v__128) : (v__129));\n  float v__130 = ((float*)data_red_1)[0];\n  float v__131 = data_red_rf[29];\n  ((float*)data_red_1)[0] = ((v__130) > (v__131) ? (v__130) : (v__131));\n  float v__132 = ((float*)data_red_1)[0];\n  float v__133 = data_red_rf[30];\n  ((float*)data_red_1)[0] = ((v__132) > (v__133) ? (v__132) : (v__133));\n  float v__134 = ((float*)data_red_1)[0];\n  float v__135 = data_red_rf[31];\n  ((float*)data_red_1)[0] = ((v__134) > (v__135) ? (v__134) : (v__135));\n  float v__136 = ((float*)data_red_1)[0];\n  float v__137 = data_red_rf[32];\n  ((float*)data_red_1)[0] = ((v__136) > (v__137) ? (v__136) : (v__137));\n  float v__138 = ((float*)data_red_1)[0];\n  float v__139 = data_red_rf[33];\n  ((float*)data_red_1)[0] = ((v__138) > (v__139) ? (v__138) : (v__139));\n  float v__140 = ((float*)data_red_1)[0];\n  float v__141 = data_red_rf[34];\n  ((float*)data_red_1)[0] = ((v__140) > (v__141) ? (v__140) : (v__141));\n  float v__142 = ((float*)data_red_1)[0];\n  float v__143 = data_red_rf[35];\n  ((float*)data_red_1)[0] = ((v__142) > (v__143) ? (v__142) : (v__143));\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "max"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 8, 24, 12), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([64], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((64,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(64):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(180):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 64\n            data_1 = T.Buffer((11520,), data=data.data)\n            data_red_rf_1[0] = T.max(data_red_rf_1[0], data_1[cse_var_1])\n            data_red_rf_1[1] = T.max(data_red_rf_1[1], data_1[cse_var_1 + 1])\n            data_red_rf_1[2] = T.max(data_red_rf_1[2], data_1[cse_var_1 + 2])\n            data_red_rf_1[3] = T.max(data_red_rf_1[3], data_1[cse_var_1 + 3])\n            data_red_rf_1[4] = T.max(data_red_rf_1[4], data_1[cse_var_1 + 4])\n            data_red_rf_1[5] = T.max(data_red_rf_1[5], data_1[cse_var_1 + 5])\n            data_red_rf_1[6] = T.max(data_red_rf_1[6], data_1[cse_var_1 + 6])\n            data_red_rf_1[7] = T.max(data_red_rf_1[7], data_1[cse_var_1 + 7])\n            data_red_rf_1[8] = T.max(data_red_rf_1[8], data_1[cse_var_1 + 8])\n            data_red_rf_1[9] = T.max(data_red_rf_1[9], data_1[cse_var_1 + 9])\n            data_red_rf_1[10] = T.max(data_red_rf_1[10], data_1[cse_var_1 + 10])\n            data_red_rf_1[11] = T.max(data_red_rf_1[11], data_1[cse_var_1 + 11])\n            data_red_rf_1[12] = T.max(data_red_rf_1[12], data_1[cse_var_1 + 12])\n            data_red_rf_1[13] = T.max(data_red_rf_1[13], data_1[cse_var_1 + 13])\n            data_red_rf_1[14] = T.max(data_red_rf_1[14], data_1[cse_var_1 + 14])\n            data_red_rf_1[15] = T.max(data_red_rf_1[15], data_1[cse_var_1 + 15])\n            data_red_rf_1[16] = T.max(data_red_rf_1[16], data_1[cse_var_1 + 16])\n            data_red_rf_1[17] = T.max(data_red_rf_1[17], data_1[cse_var_1 + 17])\n            data_red_rf_1[18] = T.max(data_red_rf_1[18], data_1[cse_var_1 + 18])\n            data_red_rf_1[19] = T.max(data_red_rf_1[19], data_1[cse_var_1 + 19])\n            data_red_rf_1[20] = T.max(data_red_rf_1[20], data_1[cse_var_1 + 20])\n            data_red_rf_1[21] = T.max(data_red_rf_1[21], data_1[cse_var_1 + 21])\n            data_red_rf_1[22] = T.max(data_red_rf_1[22], data_1[cse_var_1 + 22])\n            data_red_rf_1[23] = T.max(data_red_rf_1[23], data_1[cse_var_1 + 23])\n            data_red_rf_1[24] = T.max(data_red_rf_1[24], data_1[cse_var_1 + 24])\n            data_red_rf_1[25] = T.max(data_red_rf_1[25], data_1[cse_var_1 + 25])\n            data_red_rf_1[26] = T.max(data_red_rf_1[26], data_1[cse_var_1 + 26])\n            data_red_rf_1[27] = T.max(data_red_rf_1[27], data_1[cse_var_1 + 27])\n            data_red_rf_1[28] = T.max(data_red_rf_1[28], data_1[cse_var_1 + 28])\n            data_red_rf_1[29] = T.max(data_red_rf_1[29], data_1[cse_var_1 + 29])\n            data_red_rf_1[30] = T.max(data_red_rf_1[30], data_1[cse_var_1 + 30])\n            data_red_rf_1[31] = T.max(data_red_rf_1[31], data_1[cse_var_1 + 31])\n            data_red_rf_1[32] = T.max(data_red_rf_1[32], data_1[cse_var_1 + 32])\n            data_red_rf_1[33] = T.max(data_red_rf_1[33], data_1[cse_var_1 + 33])\n            data_red_rf_1[34] = T.max(data_red_rf_1[34], data_1[cse_var_1 + 34])\n            data_red_rf_1[35] = T.max(data_red_rf_1[35], data_1[cse_var_1 + 35])\n            data_red_rf_1[36] = T.max(data_red_rf_1[36], data_1[cse_var_1 + 36])\n            data_red_rf_1[37] = T.max(data_red_rf_1[37], data_1[cse_var_1 + 37])\n            data_red_rf_1[38] = T.max(data_red_rf_1[38], data_1[cse_var_1 + 38])\n            data_red_rf_1[39] = T.max(data_red_rf_1[39], data_1[cse_var_1 + 39])\n            data_red_rf_1[40] = T.max(data_red_rf_1[40], data_1[cse_var_1 + 40])\n            data_red_rf_1[41] = T.max(data_red_rf_1[41], data_1[cse_var_1 + 41])\n            data_red_rf_1[42] = T.max(data_red_rf_1[42], data_1[cse_var_1 + 42])\n            data_red_rf_1[43] = T.max(data_red_rf_1[43], data_1[cse_var_1 + 43])\n            data_red_rf_1[44] = T.max(data_red_rf_1[44], data_1[cse_var_1 + 44])\n            data_red_rf_1[45] = T.max(data_red_rf_1[45], data_1[cse_var_1 + 45])\n            data_red_rf_1[46] = T.max(data_red_rf_1[46], data_1[cse_var_1 + 46])\n            data_red_rf_1[47] = T.max(data_red_rf_1[47], data_1[cse_var_1 + 47])\n            data_red_rf_1[48] = T.max(data_red_rf_1[48], data_1[cse_var_1 + 48])\n            data_red_rf_1[49] = T.max(data_red_rf_1[49], data_1[cse_var_1 + 49])\n            data_red_rf_1[50] = T.max(data_red_rf_1[50], data_1[cse_var_1 + 50])\n            data_red_rf_1[51] = T.max(data_red_rf_1[51], data_1[cse_var_1 + 51])\n            data_red_rf_1[52] = T.max(data_red_rf_1[52], data_1[cse_var_1 + 52])\n            data_red_rf_1[53] = T.max(data_red_rf_1[53], data_1[cse_var_1 + 53])\n            data_red_rf_1[54] = T.max(data_red_rf_1[54], data_1[cse_var_1 + 54])\n            data_red_rf_1[55] = T.max(data_red_rf_1[55], data_1[cse_var_1 + 55])\n            data_red_rf_1[56] = T.max(data_red_rf_1[56], data_1[cse_var_1 + 56])\n            data_red_rf_1[57] = T.max(data_red_rf_1[57], data_1[cse_var_1 + 57])\n            data_red_rf_1[58] = T.max(data_red_rf_1[58], data_1[cse_var_1 + 58])\n            data_red_rf_1[59] = T.max(data_red_rf_1[59], data_1[cse_var_1 + 59])\n            data_red_rf_1[60] = T.max(data_red_rf_1[60], data_1[cse_var_1 + 60])\n            data_red_rf_1[61] = T.max(data_red_rf_1[61], data_1[cse_var_1 + 61])\n            data_red_rf_1[62] = T.max(data_red_rf_1[62], data_1[cse_var_1 + 62])\n            data_red_rf_1[63] = T.max(data_red_rf_1[63], data_1[cse_var_1 + 63])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[0])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[1])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[2])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[3])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[4])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[5])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[6])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[7])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[8])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[9])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[10])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[11])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[12])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[13])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[14])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[15])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[16])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[17])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[18])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[19])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[20])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[21])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[22])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[23])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[24])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[25])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[26])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[27])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[28])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[29])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[30])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[31])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[32])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[33])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[34])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[35])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[36])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[37])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[38])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[39])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[40])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[41])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[42])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[43])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[44])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[45])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[46])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[47])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[48])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[49])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[50])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[51])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[52])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[53])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[54])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[55])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[56])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[57])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[58])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[59])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[60])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[61])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[62])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[63])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 5; ++k0) {\n    for (int k1 = 0; k1 < 8; ++k1) {\n      for (int k2 = 0; k2 < 24; ++k2) {\n        for (int k3 = 0; k3 < 12; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 2304) + (k1 * 288)) + (k2 * 12)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[64];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 180; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 64);\n    float v_ = data_red_rf[0];\n    float v__1 = ((float*)data_1)[cse_var_1];\n    data_red_rf[0] = ((v_) > (v__1) ? (v_) : (v__1));\n    float v__2 = data_red_rf[1];\n    float v__3 = ((float*)data_1)[(cse_var_1 + 1)];\n    data_red_rf[1] = ((v__2) > (v__3) ? (v__2) : (v__3));\n    float v__4 = data_red_rf[2];\n    float v__5 = ((float*)data_1)[(cse_var_1 + 2)];\n    data_red_rf[2] = ((v__4) > (v__5) ? (v__4) : (v__5));\n    float v__6 = data_red_rf[3];\n    float v__7 = ((float*)data_1)[(cse_var_1 + 3)];\n    data_red_rf[3] = ((v__6) > (v__7) ? (v__6) : (v__7));\n    float v__8 = data_red_rf[4];\n    float v__9 = ((float*)data_1)[(cse_var_1 + 4)];\n    data_red_rf[4] = ((v__8) > (v__9) ? (v__8) : (v__9));\n    float v__10 = data_red_rf[5];\n    float v__11 = ((float*)data_1)[(cse_var_1 + 5)];\n    data_red_rf[5] = ((v__10) > (v__11) ? (v__10) : (v__11));\n    float v__12 = data_red_rf[6];\n    float v__13 = ((float*)data_1)[(cse_var_1 + 6)];\n    data_red_rf[6] = ((v__12) > (v__13) ? (v__12) : (v__13));\n    float v__14 = data_red_rf[7];\n    float v__15 = ((float*)data_1)[(cse_var_1 + 7)];\n    data_red_rf[7] = ((v__14) > (v__15) ? (v__14) : (v__15));\n    float v__16 = data_red_rf[8];\n    float v__17 = ((float*)data_1)[(cse_var_1 + 8)];\n    data_red_rf[8] = ((v__16) > (v__17) ? (v__16) : (v__17));\n    float v__18 = data_red_rf[9];\n    float v__19 = ((float*)data_1)[(cse_var_1 + 9)];\n    data_red_rf[9] = ((v__18) > (v__19) ? (v__18) : (v__19));\n    float v__20 = data_red_rf[10];\n    float v__21 = ((float*)data_1)[(cse_var_1 + 10)];\n    data_red_rf[10] = ((v__20) > (v__21) ? (v__20) : (v__21));\n    float v__22 = data_red_rf[11];\n    float v__23 = ((float*)data_1)[(cse_var_1 + 11)];\n    data_red_rf[11] = ((v__22) > (v__23) ? (v__22) : (v__23));\n    float v__24 = data_red_rf[12];\n    float v__25 = ((float*)data_1)[(cse_var_1 + 12)];\n    data_red_rf[12] = ((v__24) > (v__25) ? (v__24) : (v__25));\n    float v__26 = data_red_rf[13];\n    float v__27 = ((float*)data_1)[(cse_var_1 + 13)];\n    data_red_rf[13] = ((v__26) > (v__27) ? (v__26) : (v__27));\n    float v__28 = data_red_rf[14];\n    float v__29 = ((float*)data_1)[(cse_var_1 + 14)];\n    data_red_rf[14] = ((v__28) > (v__29) ? (v__28) : (v__29));\n    float v__30 = data_red_rf[15];\n    float v__31 = ((float*)data_1)[(cse_var_1 + 15)];\n    data_red_rf[15] = ((v__30) > (v__31) ? (v__30) : (v__31));\n    float v__32 = data_red_rf[16];\n    float v__33 = ((float*)data_1)[(cse_var_1 + 16)];\n    data_red_rf[16] = ((v__32) > (v__33) ? (v__32) : (v__33));\n    float v__34 = data_red_rf[17];\n    float v__35 = ((float*)data_1)[(cse_var_1 + 17)];\n    data_red_rf[17] = ((v__34) > (v__35) ? (v__34) : (v__35));\n    float v__36 = data_red_rf[18];\n    float v__37 = ((float*)data_1)[(cse_var_1 + 18)];\n    data_red_rf[18] = ((v__36) > (v__37) ? (v__36) : (v__37));\n    float v__38 = data_red_rf[19];\n    float v__39 = ((float*)data_1)[(cse_var_1 + 19)];\n    data_red_rf[19] = ((v__38) > (v__39) ? (v__38) : (v__39));\n    float v__40 = data_red_rf[20];\n    float v__41 = ((float*)data_1)[(cse_var_1 + 20)];\n    data_red_rf[20] = ((v__40) > (v__41) ? (v__40) : (v__41));\n    float v__42 = data_red_rf[21];\n    float v__43 = ((float*)data_1)[(cse_var_1 + 21)];\n    data_red_rf[21] = ((v__42) > (v__43) ? (v__42) : (v__43));\n    float v__44 = data_red_rf[22];\n    float v__45 = ((float*)data_1)[(cse_var_1 + 22)];\n    data_red_rf[22] = ((v__44) > (v__45) ? (v__44) : (v__45));\n    float v__46 = data_red_rf[23];\n    float v__47 = ((float*)data_1)[(cse_var_1 + 23)];\n    data_red_rf[23] = ((v__46) > (v__47) ? (v__46) : (v__47));\n    float v__48 = data_red_rf[24];\n    float v__49 = ((float*)data_1)[(cse_var_1 + 24)];\n    data_red_rf[24] = ((v__48) > (v__49) ? (v__48) : (v__49));\n    float v__50 = data_red_rf[25];\n    float v__51 = ((float*)data_1)[(cse_var_1 + 25)];\n    data_red_rf[25] = ((v__50) > (v__51) ? (v__50) : (v__51));\n    float v__52 = data_red_rf[26];\n    float v__53 = ((float*)data_1)[(cse_var_1 + 26)];\n    data_red_rf[26] = ((v__52) > (v__53) ? (v__52) : (v__53));\n    float v__54 = data_red_rf[27];\n    float v__55 = ((float*)data_1)[(cse_var_1 + 27)];\n    data_red_rf[27] = ((v__54) > (v__55) ? (v__54) : (v__55));\n    float v__56 = data_red_rf[28];\n    float v__57 = ((float*)data_1)[(cse_var_1 + 28)];\n    data_red_rf[28] = ((v__56) > (v__57) ? (v__56) : (v__57));\n    float v__58 = data_red_rf[29];\n    float v__59 = ((float*)data_1)[(cse_var_1 + 29)];\n    data_red_rf[29] = ((v__58) > (v__59) ? (v__58) : (v__59));\n    float v__60 = data_red_rf[30];\n    float v__61 = ((float*)data_1)[(cse_var_1 + 30)];\n    data_red_rf[30] = ((v__60) > (v__61) ? (v__60) : (v__61));\n    float v__62 = data_red_rf[31];\n    float v__63 = ((float*)data_1)[(cse_var_1 + 31)];\n    data_red_rf[31] = ((v__62) > (v__63) ? (v__62) : (v__63));\n    float v__64 = data_red_rf[32];\n    float v__65 = ((float*)data_1)[(cse_var_1 + 32)];\n    data_red_rf[32] = ((v__64) > (v__65) ? (v__64) : (v__65));\n    float v__66 = data_red_rf[33];\n    float v__67 = ((float*)data_1)[(cse_var_1 + 33)];\n    data_red_rf[33] = ((v__66) > (v__67) ? (v__66) : (v__67));\n    float v__68 = data_red_rf[34];\n    float v__69 = ((float*)data_1)[(cse_var_1 + 34)];\n    data_red_rf[34] = ((v__68) > (v__69) ? (v__68) : (v__69));\n    float v__70 = data_red_rf[35];\n    float v__71 = ((float*)data_1)[(cse_var_1 + 35)];\n    data_red_rf[35] = ((v__70) > (v__71) ? (v__70) : (v__71));\n    float v__72 = data_red_rf[36];\n    float v__73 = ((float*)data_1)[(cse_var_1 + 36)];\n    data_red_rf[36] = ((v__72) > (v__73) ? (v__72) : (v__73));\n    float v__74 = data_red_rf[37];\n    float v__75 = ((float*)data_1)[(cse_var_1 + 37)];\n    data_red_rf[37] = ((v__74) > (v__75) ? (v__74) : (v__75));\n    float v__76 = data_red_rf[38];\n    float v__77 = ((float*)data_1)[(cse_var_1 + 38)];\n    data_red_rf[38] = ((v__76) > (v__77) ? (v__76) : (v__77));\n    float v__78 = data_red_rf[39];\n    float v__79 = ((float*)data_1)[(cse_var_1 + 39)];\n    data_red_rf[39] = ((v__78) > (v__79) ? (v__78) : (v__79));\n    float v__80 = data_red_rf[40];\n    float v__81 = ((float*)data_1)[(cse_var_1 + 40)];\n    data_red_rf[40] = ((v__80) > (v__81) ? (v__80) : (v__81));\n    float v__82 = data_red_rf[41];\n    float v__83 = ((float*)data_1)[(cse_var_1 + 41)];\n    data_red_rf[41] = ((v__82) > (v__83) ? (v__82) : (v__83));\n    float v__84 = data_red_rf[42];\n    float v__85 = ((float*)data_1)[(cse_var_1 + 42)];\n    data_red_rf[42] = ((v__84) > (v__85) ? (v__84) : (v__85));\n    float v__86 = data_red_rf[43];\n    float v__87 = ((float*)data_1)[(cse_var_1 + 43)];\n    data_red_rf[43] = ((v__86) > (v__87) ? (v__86) : (v__87));\n    float v__88 = data_red_rf[44];\n    float v__89 = ((float*)data_1)[(cse_var_1 + 44)];\n    data_red_rf[44] = ((v__88) > (v__89) ? (v__88) : (v__89));\n    float v__90 = data_red_rf[45];\n    float v__91 = ((float*)data_1)[(cse_var_1 + 45)];\n    data_red_rf[45] = ((v__90) > (v__91) ? (v__90) : (v__91));\n    float v__92 = data_red_rf[46];\n    float v__93 = ((float*)data_1)[(cse_var_1 + 46)];\n    data_red_rf[46] = ((v__92) > (v__93) ? (v__92) : (v__93));\n    float v__94 = data_red_rf[47];\n    float v__95 = ((float*)data_1)[(cse_var_1 + 47)];\n    data_red_rf[47] = ((v__94) > (v__95) ? (v__94) : (v__95));\n    float v__96 = data_red_rf[48];\n    float v__97 = ((float*)data_1)[(cse_var_1 + 48)];\n    data_red_rf[48] = ((v__96) > (v__97) ? (v__96) : (v__97));\n    float v__98 = data_red_rf[49];\n    float v__99 = ((float*)data_1)[(cse_var_1 + 49)];\n    data_red_rf[49] = ((v__98) > (v__99) ? (v__98) : (v__99));\n    float v__100 = data_red_rf[50];\n    float v__101 = ((float*)data_1)[(cse_var_1 + 50)];\n    data_red_rf[50] = ((v__100) > (v__101) ? (v__100) : (v__101));\n    float v__102 = data_red_rf[51];\n    float v__103 = ((float*)data_1)[(cse_var_1 + 51)];\n    data_red_rf[51] = ((v__102) > (v__103) ? (v__102) : (v__103));\n    float v__104 = data_red_rf[52];\n    float v__105 = ((float*)data_1)[(cse_var_1 + 52)];\n    data_red_rf[52] = ((v__104) > (v__105) ? (v__104) : (v__105));\n    float v__106 = data_red_rf[53];\n    float v__107 = ((float*)data_1)[(cse_var_1 + 53)];\n    data_red_rf[53] = ((v__106) > (v__107) ? (v__106) : (v__107));\n    float v__108 = data_red_rf[54];\n    float v__109 = ((float*)data_1)[(cse_var_1 + 54)];\n    data_red_rf[54] = ((v__108) > (v__109) ? (v__108) : (v__109));\n    float v__110 = data_red_rf[55];\n    float v__111 = ((float*)data_1)[(cse_var_1 + 55)];\n    data_red_rf[55] = ((v__110) > (v__111) ? (v__110) : (v__111));\n    float v__112 = data_red_rf[56];\n    float v__113 = ((float*)data_1)[(cse_var_1 + 56)];\n    data_red_rf[56] = ((v__112) > (v__113) ? (v__112) : (v__113));\n    float v__114 = data_red_rf[57];\n    float v__115 = ((float*)data_1)[(cse_var_1 + 57)];\n    data_red_rf[57] = ((v__114) > (v__115) ? (v__114) : (v__115));\n    float v__116 = data_red_rf[58];\n    float v__117 = ((float*)data_1)[(cse_var_1 + 58)];\n    data_red_rf[58] = ((v__116) > (v__117) ? (v__116) : (v__117));\n    float v__118 = data_red_rf[59];\n    float v__119 = ((float*)data_1)[(cse_var_1 + 59)];\n    data_red_rf[59] = ((v__118) > (v__119) ? (v__118) : (v__119));\n    float v__120 = data_red_rf[60];\n    float v__121 = ((float*)data_1)[(cse_var_1 + 60)];\n    data_red_rf[60] = ((v__120) > (v__121) ? (v__120) : (v__121));\n    float v__122 = data_red_rf[61];\n    float v__123 = ((float*)data_1)[(cse_var_1 + 61)];\n    data_red_rf[61] = ((v__122) > (v__123) ? (v__122) : (v__123));\n    float v__124 = data_red_rf[62];\n    float v__125 = ((float*)data_1)[(cse_var_1 + 62)];\n    data_red_rf[62] = ((v__124) > (v__125) ? (v__124) : (v__125));\n    float v__126 = data_red_rf[63];\n    float v__127 = ((float*)data_1)[(cse_var_1 + 63)];\n    data_red_rf[63] = ((v__126) > (v__127) ? (v__126) : (v__127));\n  }\n  ((float*)data_red_1)[0] = -3.402823e+38f;\n  float v__128 = ((float*)data_red_1)[0];\n  float v__129 = data_red_rf[0];\n  ((float*)data_red_1)[0] = ((v__128) > (v__129) ? (v__128) : (v__129));\n  float v__130 = ((float*)data_red_1)[0];\n  float v__131 = data_red_rf[1];\n  ((float*)data_red_1)[0] = ((v__130) > (v__131) ? (v__130) : (v__131));\n  float v__132 = ((float*)data_red_1)[0];\n  float v__133 = data_red_rf[2];\n  ((float*)data_red_1)[0] = ((v__132) > (v__133) ? (v__132) : (v__133));\n  float v__134 = ((float*)data_red_1)[0];\n  float v__135 = data_red_rf[3];\n  ((float*)data_red_1)[0] = ((v__134) > (v__135) ? (v__134) : (v__135));\n  float v__136 = ((float*)data_red_1)[0];\n  float v__137 = data_red_rf[4];\n  ((float*)data_red_1)[0] = ((v__136) > (v__137) ? (v__136) : (v__137));\n  float v__138 = ((float*)data_red_1)[0];\n  float v__139 = data_red_rf[5];\n  ((float*)data_red_1)[0] = ((v__138) > (v__139) ? (v__138) : (v__139));\n  float v__140 = ((float*)data_red_1)[0];\n  float v__141 = data_red_rf[6];\n  ((float*)data_red_1)[0] = ((v__140) > (v__141) ? (v__140) : (v__141));\n  float v__142 = ((float*)data_red_1)[0];\n  float v__143 = data_red_rf[7];\n  ((float*)data_red_1)[0] = ((v__142) > (v__143) ? (v__142) : (v__143));\n  float v__144 = ((float*)data_red_1)[0];\n  float v__145 = data_red_rf[8];\n  ((float*)data_red_1)[0] = ((v__144) > (v__145) ? (v__144) : (v__145));\n  float v__146 = ((float*)data_red_1)[0];\n  float v__147 = data_red_rf[9];\n  ((float*)data_red_1)[0] = ((v__146) > (v__147) ? (v__146) : (v__147));\n  float v__148 = ((float*)data_red_1)[0];\n  float v__149 = data_red_rf[10];\n  ((float*)data_red_1)[0] = ((v__148) > (v__149) ? (v__148) : (v__149));\n  float v__150 = ((float*)data_red_1)[0];\n  float v__151 = data_red_rf[11];\n  ((float*)data_red_1)[0] = ((v__150) > (v__151) ? (v__150) : (v__151));\n  float v__152 = ((float*)data_red_1)[0];\n  float v__153 = data_red_rf[12];\n  ((float*)data_red_1)[0] = ((v__152) > (v__153) ? (v__152) : (v__153));\n  float v__154 = ((float*)data_red_1)[0];\n  float v__155 = data_red_rf[13];\n  ((float*)data_red_1)[0] = ((v__154) > (v__155) ? (v__154) : (v__155));\n  float v__156 = ((float*)data_red_1)[0];\n  float v__157 = data_red_rf[14];\n  ((float*)data_red_1)[0] = ((v__156) > (v__157) ? (v__156) : (v__157));\n  float v__158 = ((float*)data_red_1)[0];\n  float v__159 = data_red_rf[15];\n  ((float*)data_red_1)[0] = ((v__158) > (v__159) ? (v__158) : (v__159));\n  float v__160 = ((float*)data_red_1)[0];\n  float v__161 = data_red_rf[16];\n  ((float*)data_red_1)[0] = ((v__160) > (v__161) ? (v__160) : (v__161));\n  float v__162 = ((float*)data_red_1)[0];\n  float v__163 = data_red_rf[17];\n  ((float*)data_red_1)[0] = ((v__162) > (v__163) ? (v__162) : (v__163));\n  float v__164 = ((float*)data_red_1)[0];\n  float v__165 = data_red_rf[18];\n  ((float*)data_red_1)[0] = ((v__164) > (v__165) ? (v__164) : (v__165));\n  float v__166 = ((float*)data_red_1)[0];\n  float v__167 = data_red_rf[19];\n  ((float*)data_red_1)[0] = ((v__166) > (v__167) ? (v__166) : (v__167));\n  float v__168 = ((float*)data_red_1)[0];\n  float v__169 = data_red_rf[20];\n  ((float*)data_red_1)[0] = ((v__168) > (v__169) ? (v__168) : (v__169));\n  float v__170 = ((float*)data_red_1)[0];\n  float v__171 = data_red_rf[21];\n  ((float*)data_red_1)[0] = ((v__170) > (v__171) ? (v__170) : (v__171));\n  float v__172 = ((float*)data_red_1)[0];\n  float v__173 = data_red_rf[22];\n  ((float*)data_red_1)[0] = ((v__172) > (v__173) ? (v__172) : (v__173));\n  float v__174 = ((float*)data_red_1)[0];\n  float v__175 = data_red_rf[23];\n  ((float*)data_red_1)[0] = ((v__174) > (v__175) ? (v__174) : (v__175));\n  float v__176 = ((float*)data_red_1)[0];\n  float v__177 = data_red_rf[24];\n  ((float*)data_red_1)[0] = ((v__176) > (v__177) ? (v__176) : (v__177));\n  float v__178 = ((float*)data_red_1)[0];\n  float v__179 = data_red_rf[25];\n  ((float*)data_red_1)[0] = ((v__178) > (v__179) ? (v__178) : (v__179));\n  float v__180 = ((float*)data_red_1)[0];\n  float v__181 = data_red_rf[26];\n  ((float*)data_red_1)[0] = ((v__180) > (v__181) ? (v__180) : (v__181));\n  float v__182 = ((float*)data_red_1)[0];\n  float v__183 = data_red_rf[27];\n  ((float*)data_red_1)[0] = ((v__182) > (v__183) ? (v__182) : (v__183));\n  float v__184 = ((float*)data_red_1)[0];\n  float v__185 = data_red_rf[28];\n  ((float*)data_red_1)[0] = ((v__184) > (v__185) ? (v__184) : (v__185));\n  float v__186 = ((float*)data_red_1)[0];\n  float v__187 = data_red_rf[29];\n  ((float*)data_red_1)[0] = ((v__186) > (v__187) ? (v__186) : (v__187));\n  float v__188 = ((float*)data_red_1)[0];\n  float v__189 = data_red_rf[30];\n  ((float*)data_red_1)[0] = ((v__188) > (v__189) ? (v__188) : (v__189));\n  float v__190 = ((float*)data_red_1)[0];\n  float v__191 = data_red_rf[31];\n  ((float*)data_red_1)[0] = ((v__190) > (v__191) ? (v__190) : (v__191));\n  float v__192 = ((float*)data_red_1)[0];\n  float v__193 = data_red_rf[32];\n  ((float*)data_red_1)[0] = ((v__192) > (v__193) ? (v__192) : (v__193));\n  float v__194 = ((float*)data_red_1)[0];\n  float v__195 = data_red_rf[33];\n  ((float*)data_red_1)[0] = ((v__194) > (v__195) ? (v__194) : (v__195));\n  float v__196 = ((float*)data_red_1)[0];\n  float v__197 = data_red_rf[34];\n  ((float*)data_red_1)[0] = ((v__196) > (v__197) ? (v__196) : (v__197));\n  float v__198 = ((float*)data_red_1)[0];\n  float v__199 = data_red_rf[35];\n  ((float*)data_red_1)[0] = ((v__198) > (v__199) ? (v__198) : (v__199));\n  float v__200 = ((float*)data_red_1)[0];\n  float v__201 = data_red_rf[36];\n  ((float*)data_red_1)[0] = ((v__200) > (v__201) ? (v__200) : (v__201));\n  float v__202 = ((float*)data_red_1)[0];\n  float v__203 = data_red_rf[37];\n  ((float*)data_red_1)[0] = ((v__202) > (v__203) ? (v__202) : (v__203));\n  float v__204 = ((float*)data_red_1)[0];\n  float v__205 = data_red_rf[38];\n  ((float*)data_red_1)[0] = ((v__204) > (v__205) ? (v__204) : (v__205));\n  float v__206 = ((float*)data_red_1)[0];\n  float v__207 = data_red_rf[39];\n  ((float*)data_red_1)[0] = ((v__206) > (v__207) ? (v__206) : (v__207));\n  float v__208 = ((float*)data_red_1)[0];\n  float v__209 = data_red_rf[40];\n  ((float*)data_red_1)[0] = ((v__208) > (v__209) ? (v__208) : (v__209));\n  float v__210 = ((float*)data_red_1)[0];\n  float v__211 = data_red_rf[41];\n  ((float*)data_red_1)[0] = ((v__210) > (v__211) ? (v__210) : (v__211));\n  float v__212 = ((float*)data_red_1)[0];\n  float v__213 = data_red_rf[42];\n  ((float*)data_red_1)[0] = ((v__212) > (v__213) ? (v__212) : (v__213));\n  float v__214 = ((float*)data_red_1)[0];\n  float v__215 = data_red_rf[43];\n  ((float*)data_red_1)[0] = ((v__214) > (v__215) ? (v__214) : (v__215));\n  float v__216 = ((float*)data_red_1)[0];\n  float v__217 = data_red_rf[44];\n  ((float*)data_red_1)[0] = ((v__216) > (v__217) ? (v__216) : (v__217));\n  float v__218 = ((float*)data_red_1)[0];\n  float v__219 = data_red_rf[45];\n  ((float*)data_red_1)[0] = ((v__218) > (v__219) ? (v__218) : (v__219));\n  float v__220 = ((float*)data_red_1)[0];\n  float v__221 = data_red_rf[46];\n  ((float*)data_red_1)[0] = ((v__220) > (v__221) ? (v__220) : (v__221));\n  float v__222 = ((float*)data_red_1)[0];\n  float v__223 = data_red_rf[47];\n  ((float*)data_red_1)[0] = ((v__222) > (v__223) ? (v__222) : (v__223));\n  float v__224 = ((float*)data_red_1)[0];\n  float v__225 = data_red_rf[48];\n  ((float*)data_red_1)[0] = ((v__224) > (v__225) ? (v__224) : (v__225));\n  float v__226 = ((float*)data_red_1)[0];\n  float v__227 = data_red_rf[49];\n  ((float*)data_red_1)[0] = ((v__226) > (v__227) ? (v__226) : (v__227));\n  float v__228 = ((float*)data_red_1)[0];\n  float v__229 = data_red_rf[50];\n  ((float*)data_red_1)[0] = ((v__228) > (v__229) ? (v__228) : (v__229));\n  float v__230 = ((float*)data_red_1)[0];\n  float v__231 = data_red_rf[51];\n  ((float*)data_red_1)[0] = ((v__230) > (v__231) ? (v__230) : (v__231));\n  float v__232 = ((float*)data_red_1)[0];\n  float v__233 = data_red_rf[52];\n  ((float*)data_red_1)[0] = ((v__232) > (v__233) ? (v__232) : (v__233));\n  float v__234 = ((float*)data_red_1)[0];\n  float v__235 = data_red_rf[53];\n  ((float*)data_red_1)[0] = ((v__234) > (v__235) ? (v__234) : (v__235));\n  float v__236 = ((float*)data_red_1)[0];\n  float v__237 = data_red_rf[54];\n  ((float*)data_red_1)[0] = ((v__236) > (v__237) ? (v__236) : (v__237));\n  float v__238 = ((float*)data_red_1)[0];\n  float v__239 = data_red_rf[55];\n  ((float*)data_red_1)[0] = ((v__238) > (v__239) ? (v__238) : (v__239));\n  float v__240 = ((float*)data_red_1)[0];\n  float v__241 = data_red_rf[56];\n  ((float*)data_red_1)[0] = ((v__240) > (v__241) ? (v__240) : (v__241));\n  float v__242 = ((float*)data_red_1)[0];\n  float v__243 = data_red_rf[57];\n  ((float*)data_red_1)[0] = ((v__242) > (v__243) ? (v__242) : (v__243));\n  float v__244 = ((float*)data_red_1)[0];\n  float v__245 = data_red_rf[58];\n  ((float*)data_red_1)[0] = ((v__244) > (v__245) ? (v__244) : (v__245));\n  float v__246 = ((float*)data_red_1)[0];\n  float v__247 = data_red_rf[59];\n  ((float*)data_red_1)[0] = ((v__246) > (v__247) ? (v__246) : (v__247));\n  float v__248 = ((float*)data_red_1)[0];\n  float v__249 = data_red_rf[60];\n  ((float*)data_red_1)[0] = ((v__248) > (v__249) ? (v__248) : (v__249));\n  float v__250 = ((float*)data_red_1)[0];\n  float v__251 = data_red_rf[61];\n  ((float*)data_red_1)[0] = ((v__250) > (v__251) ? (v__250) : (v__251));\n  float v__252 = ((float*)data_red_1)[0];\n  float v__253 = data_red_rf[62];\n  ((float*)data_red_1)[0] = ((v__252) > (v__253) ? (v__252) : (v__253));\n  float v__254 = ((float*)data_red_1)[0];\n  float v__255 = data_red_rf[63];\n  ((float*)data_red_1)[0] = ((v__254) > (v__255) ? (v__254) : (v__255));\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "max"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 28, 36, 8), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([16], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((16,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(16):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(2520):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 16\n            data_1 = T.Buffer((40320,), data=data.data)\n            data_red_rf_1[0] = T.max(data_red_rf_1[0], data_1[cse_var_1])\n            data_red_rf_1[1] = T.max(data_red_rf_1[1], data_1[cse_var_1 + 1])\n            data_red_rf_1[2] = T.max(data_red_rf_1[2], data_1[cse_var_1 + 2])\n            data_red_rf_1[3] = T.max(data_red_rf_1[3], data_1[cse_var_1 + 3])\n            data_red_rf_1[4] = T.max(data_red_rf_1[4], data_1[cse_var_1 + 4])\n            data_red_rf_1[5] = T.max(data_red_rf_1[5], data_1[cse_var_1 + 5])\n            data_red_rf_1[6] = T.max(data_red_rf_1[6], data_1[cse_var_1 + 6])\n            data_red_rf_1[7] = T.max(data_red_rf_1[7], data_1[cse_var_1 + 7])\n            data_red_rf_1[8] = T.max(data_red_rf_1[8], data_1[cse_var_1 + 8])\n            data_red_rf_1[9] = T.max(data_red_rf_1[9], data_1[cse_var_1 + 9])\n            data_red_rf_1[10] = T.max(data_red_rf_1[10], data_1[cse_var_1 + 10])\n            data_red_rf_1[11] = T.max(data_red_rf_1[11], data_1[cse_var_1 + 11])\n            data_red_rf_1[12] = T.max(data_red_rf_1[12], data_1[cse_var_1 + 12])\n            data_red_rf_1[13] = T.max(data_red_rf_1[13], data_1[cse_var_1 + 13])\n            data_red_rf_1[14] = T.max(data_red_rf_1[14], data_1[cse_var_1 + 14])\n            data_red_rf_1[15] = T.max(data_red_rf_1[15], data_1[cse_var_1 + 15])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[0])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[1])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[2])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[3])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[4])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[5])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[6])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[7])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[8])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[9])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[10])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[11])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[12])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[13])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[14])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[15])\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1260; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[16];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 16; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 2520; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 16);\n    float v_ = data_red_rf[0];\n    float v__1 = ((float*)data_1)[cse_var_1];\n    data_red_rf[0] = ((v_) > (v__1) ? (v_) : (v__1));\n    float v__2 = data_red_rf[1];\n    float v__3 = ((float*)data_1)[(cse_var_1 + 1)];\n    data_red_rf[1] = ((v__2) > (v__3) ? (v__2) : (v__3));\n    float v__4 = data_red_rf[2];\n    float v__5 = ((float*)data_1)[(cse_var_1 + 2)];\n    data_red_rf[2] = ((v__4) > (v__5) ? (v__4) : (v__5));\n    float v__6 = data_red_rf[3];\n    float v__7 = ((float*)data_1)[(cse_var_1 + 3)];\n    data_red_rf[3] = ((v__6) > (v__7) ? (v__6) : (v__7));\n    float v__8 = data_red_rf[4];\n    float v__9 = ((float*)data_1)[(cse_var_1 + 4)];\n    data_red_rf[4] = ((v__8) > (v__9) ? (v__8) : (v__9));\n    float v__10 = data_red_rf[5];\n    float v__11 = ((float*)data_1)[(cse_var_1 + 5)];\n    data_red_rf[5] = ((v__10) > (v__11) ? (v__10) : (v__11));\n    float v__12 = data_red_rf[6];\n    float v__13 = ((float*)data_1)[(cse_var_1 + 6)];\n    data_red_rf[6] = ((v__12) > (v__13) ? (v__12) : (v__13));\n    float v__14 = data_red_rf[7];\n    float v__15 = ((float*)data_1)[(cse_var_1 + 7)];\n    data_red_rf[7] = ((v__14) > (v__15) ? (v__14) : (v__15));\n    float v__16 = data_red_rf[8];\n    float v__17 = ((float*)data_1)[(cse_var_1 + 8)];\n    data_red_rf[8] = ((v__16) > (v__17) ? (v__16) : (v__17));\n    float v__18 = data_red_rf[9];\n    float v__19 = ((float*)data_1)[(cse_var_1 + 9)];\n    data_red_rf[9] = ((v__18) > (v__19) ? (v__18) : (v__19));\n    float v__20 = data_red_rf[10];\n    float v__21 = ((float*)data_1)[(cse_var_1 + 10)];\n    data_red_rf[10] = ((v__20) > (v__21) ? (v__20) : (v__21));\n    float v__22 = data_red_rf[11];\n    float v__23 = ((float*)data_1)[(cse_var_1 + 11)];\n    data_red_rf[11] = ((v__22) > (v__23) ? (v__22) : (v__23));\n    float v__24 = data_red_rf[12];\n    float v__25 = ((float*)data_1)[(cse_var_1 + 12)];\n    data_red_rf[12] = ((v__24) > (v__25) ? (v__24) : (v__25));\n    float v__26 = data_red_rf[13];\n    float v__27 = ((float*)data_1)[(cse_var_1 + 13)];\n    data_red_rf[13] = ((v__26) > (v__27) ? (v__26) : (v__27));\n    float v__28 = data_red_rf[14];\n    float v__29 = ((float*)data_1)[(cse_var_1 + 14)];\n    data_red_rf[14] = ((v__28) > (v__29) ? (v__28) : (v__29));\n    float v__30 = data_red_rf[15];\n    float v__31 = ((float*)data_1)[(cse_var_1 + 15)];\n    data_red_rf[15] = ((v__30) > (v__31) ? (v__30) : (v__31));\n  }\n  ((float*)data_red_1)[0] = -3.402823e+38f;\n  float v__32 = ((float*)data_red_1)[0];\n  float v__33 = data_red_rf[0];\n  ((float*)data_red_1)[0] = ((v__32) > (v__33) ? (v__32) : (v__33));\n  float v__34 = ((float*)data_red_1)[0];\n  float v__35 = data_red_rf[1];\n  ((float*)data_red_1)[0] = ((v__34) > (v__35) ? (v__34) : (v__35));\n  float v__36 = ((float*)data_red_1)[0];\n  float v__37 = data_red_rf[2];\n  ((float*)data_red_1)[0] = ((v__36) > (v__37) ? (v__36) : (v__37));\n  float v__38 = ((float*)data_red_1)[0];\n  float v__39 = data_red_rf[3];\n  ((float*)data_red_1)[0] = ((v__38) > (v__39) ? (v__38) : (v__39));\n  float v__40 = ((float*)data_red_1)[0];\n  float v__41 = data_red_rf[4];\n  ((float*)data_red_1)[0] = ((v__40) > (v__41) ? (v__40) : (v__41));\n  float v__42 = ((float*)data_red_1)[0];\n  float v__43 = data_red_rf[5];\n  ((float*)data_red_1)[0] = ((v__42) > (v__43) ? (v__42) : (v__43));\n  float v__44 = ((float*)data_red_1)[0];\n  float v__45 = data_red_rf[6];\n  ((float*)data_red_1)[0] = ((v__44) > (v__45) ? (v__44) : (v__45));\n  float v__46 = ((float*)data_red_1)[0];\n  float v__47 = data_red_rf[7];\n  ((float*)data_red_1)[0] = ((v__46) > (v__47) ? (v__46) : (v__47));\n  float v__48 = ((float*)data_red_1)[0];\n  float v__49 = data_red_rf[8];\n  ((float*)data_red_1)[0] = ((v__48) > (v__49) ? (v__48) : (v__49));\n  float v__50 = ((float*)data_red_1)[0];\n  float v__51 = data_red_rf[9];\n  ((float*)data_red_1)[0] = ((v__50) > (v__51) ? (v__50) : (v__51));\n  float v__52 = ((float*)data_red_1)[0];\n  float v__53 = data_red_rf[10];\n  ((float*)data_red_1)[0] = ((v__52) > (v__53) ? (v__52) : (v__53));\n  float v__54 = ((float*)data_red_1)[0];\n  float v__55 = data_red_rf[11];\n  ((float*)data_red_1)[0] = ((v__54) > (v__55) ? (v__54) : (v__55));\n  float v__56 = ((float*)data_red_1)[0];\n  float v__57 = data_red_rf[12];\n  ((float*)data_red_1)[0] = ((v__56) > (v__57) ? (v__56) : (v__57));\n  float v__58 = ((float*)data_red_1)[0];\n  float v__59 = data_red_rf[13];\n  ((float*)data_red_1)[0] = ((v__58) > (v__59) ? (v__58) : (v__59));\n  float v__60 = ((float*)data_red_1)[0];\n  float v__61 = data_red_rf[14];\n  ((float*)data_red_1)[0] = ((v__60) > (v__61) ? (v__60) : (v__61));\n  float v__62 = ((float*)data_red_1)[0];\n  float v__63 = data_red_rf[15];\n  ((float*)data_red_1)[0] = ((v__62) > (v__63) ? (v__62) : (v__63));\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "max"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 32, 12, 20), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([60], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((60,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(60):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(256, 60):\n            data_1 = T.Buffer((15360,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 60 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[0])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[1])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[2])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[3])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[4])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[5])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[6])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[7])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[8])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[9])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[10])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[11])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[12])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[13])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[14])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[15])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[16])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[17])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[18])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[19])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[20])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[21])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[22])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[23])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[24])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[25])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[26])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[27])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[28])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[29])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[30])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[31])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[32])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[33])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[34])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[35])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[36])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[37])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[38])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[39])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[40])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[41])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[42])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[43])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[44])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[45])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[46])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[47])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[48])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[49])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[50])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[51])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[52])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[53])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[54])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[55])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[56])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[57])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[58])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[59])\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 480; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[60];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 60; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 256; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 60; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      float v_ = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner];\n      float v__1 = ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 60) + k0_k1_fused_k2_fused_k3_fused_inner)];\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = ((v_) > (v__1) ? (v_) : (v__1));\n    }\n  }\n  ((float*)data_red_1)[0] = -3.402823e+38f;\n  float v__2 = ((float*)data_red_1)[0];\n  float v__3 = data_red_rf[0];\n  ((float*)data_red_1)[0] = ((v__2) > (v__3) ? (v__2) : (v__3));\n  float v__4 = ((float*)data_red_1)[0];\n  float v__5 = data_red_rf[1];\n  ((float*)data_red_1)[0] = ((v__4) > (v__5) ? (v__4) : (v__5));\n  float v__6 = ((float*)data_red_1)[0];\n  float v__7 = data_red_rf[2];\n  ((float*)data_red_1)[0] = ((v__6) > (v__7) ? (v__6) : (v__7));\n  float v__8 = ((float*)data_red_1)[0];\n  float v__9 = data_red_rf[3];\n  ((float*)data_red_1)[0] = ((v__8) > (v__9) ? (v__8) : (v__9));\n  float v__10 = ((float*)data_red_1)[0];\n  float v__11 = data_red_rf[4];\n  ((float*)data_red_1)[0] = ((v__10) > (v__11) ? (v__10) : (v__11));\n  float v__12 = ((float*)data_red_1)[0];\n  float v__13 = data_red_rf[5];\n  ((float*)data_red_1)[0] = ((v__12) > (v__13) ? (v__12) : (v__13));\n  float v__14 = ((float*)data_red_1)[0];\n  float v__15 = data_red_rf[6];\n  ((float*)data_red_1)[0] = ((v__14) > (v__15) ? (v__14) : (v__15));\n  float v__16 = ((float*)data_red_1)[0];\n  float v__17 = data_red_rf[7];\n  ((float*)data_red_1)[0] = ((v__16) > (v__17) ? (v__16) : (v__17));\n  float v__18 = ((float*)data_red_1)[0];\n  float v__19 = data_red_rf[8];\n  ((float*)data_red_1)[0] = ((v__18) > (v__19) ? (v__18) : (v__19));\n  float v__20 = ((float*)data_red_1)[0];\n  float v__21 = data_red_rf[9];\n  ((float*)data_red_1)[0] = ((v__20) > (v__21) ? (v__20) : (v__21));\n  float v__22 = ((float*)data_red_1)[0];\n  float v__23 = data_red_rf[10];\n  ((float*)data_red_1)[0] = ((v__22) > (v__23) ? (v__22) : (v__23));\n  float v__24 = ((float*)data_red_1)[0];\n  float v__25 = data_red_rf[11];\n  ((float*)data_red_1)[0] = ((v__24) > (v__25) ? (v__24) : (v__25));\n  float v__26 = ((float*)data_red_1)[0];\n  float v__27 = data_red_rf[12];\n  ((float*)data_red_1)[0] = ((v__26) > (v__27) ? (v__26) : (v__27));\n  float v__28 = ((float*)data_red_1)[0];\n  float v__29 = data_red_rf[13];\n  ((float*)data_red_1)[0] = ((v__28) > (v__29) ? (v__28) : (v__29));\n  float v__30 = ((float*)data_red_1)[0];\n  float v__31 = data_red_rf[14];\n  ((float*)data_red_1)[0] = ((v__30) > (v__31) ? (v__30) : (v__31));\n  float v__32 = ((float*)data_red_1)[0];\n  float v__33 = data_red_rf[15];\n  ((float*)data_red_1)[0] = ((v__32) > (v__33) ? (v__32) : (v__33));\n  float v__34 = ((float*)data_red_1)[0];\n  float v__35 = data_red_rf[16];\n  ((float*)data_red_1)[0] = ((v__34) > (v__35) ? (v__34) : (v__35));\n  float v__36 = ((float*)data_red_1)[0];\n  float v__37 = data_red_rf[17];\n  ((float*)data_red_1)[0] = ((v__36) > (v__37) ? (v__36) : (v__37));\n  float v__38 = ((float*)data_red_1)[0];\n  float v__39 = data_red_rf[18];\n  ((float*)data_red_1)[0] = ((v__38) > (v__39) ? (v__38) : (v__39));\n  float v__40 = ((float*)data_red_1)[0];\n  float v__41 = data_red_rf[19];\n  ((float*)data_red_1)[0] = ((v__40) > (v__41) ? (v__40) : (v__41));\n  float v__42 = ((float*)data_red_1)[0];\n  float v__43 = data_red_rf[20];\n  ((float*)data_red_1)[0] = ((v__42) > (v__43) ? (v__42) : (v__43));\n  float v__44 = ((float*)data_red_1)[0];\n  float v__45 = data_red_rf[21];\n  ((float*)data_red_1)[0] = ((v__44) > (v__45) ? (v__44) : (v__45));\n  float v__46 = ((float*)data_red_1)[0];\n  float v__47 = data_red_rf[22];\n  ((float*)data_red_1)[0] = ((v__46) > (v__47) ? (v__46) : (v__47));\n  float v__48 = ((float*)data_red_1)[0];\n  float v__49 = data_red_rf[23];\n  ((float*)data_red_1)[0] = ((v__48) > (v__49) ? (v__48) : (v__49));\n  float v__50 = ((float*)data_red_1)[0];\n  float v__51 = data_red_rf[24];\n  ((float*)data_red_1)[0] = ((v__50) > (v__51) ? (v__50) : (v__51));\n  float v__52 = ((float*)data_red_1)[0];\n  float v__53 = data_red_rf[25];\n  ((float*)data_red_1)[0] = ((v__52) > (v__53) ? (v__52) : (v__53));\n  float v__54 = ((float*)data_red_1)[0];\n  float v__55 = data_red_rf[26];\n  ((float*)data_red_1)[0] = ((v__54) > (v__55) ? (v__54) : (v__55));\n  float v__56 = ((float*)data_red_1)[0];\n  float v__57 = data_red_rf[27];\n  ((float*)data_red_1)[0] = ((v__56) > (v__57) ? (v__56) : (v__57));\n  float v__58 = ((float*)data_red_1)[0];\n  float v__59 = data_red_rf[28];\n  ((float*)data_red_1)[0] = ((v__58) > (v__59) ? (v__58) : (v__59));\n  float v__60 = ((float*)data_red_1)[0];\n  float v__61 = data_red_rf[29];\n  ((float*)data_red_1)[0] = ((v__60) > (v__61) ? (v__60) : (v__61));\n  float v__62 = ((float*)data_red_1)[0];\n  float v__63 = data_red_rf[30];\n  ((float*)data_red_1)[0] = ((v__62) > (v__63) ? (v__62) : (v__63));\n  float v__64 = ((float*)data_red_1)[0];\n  float v__65 = data_red_rf[31];\n  ((float*)data_red_1)[0] = ((v__64) > (v__65) ? (v__64) : (v__65));\n  float v__66 = ((float*)data_red_1)[0];\n  float v__67 = data_red_rf[32];\n  ((float*)data_red_1)[0] = ((v__66) > (v__67) ? (v__66) : (v__67));\n  float v__68 = ((float*)data_red_1)[0];\n  float v__69 = data_red_rf[33];\n  ((float*)data_red_1)[0] = ((v__68) > (v__69) ? (v__68) : (v__69));\n  float v__70 = ((float*)data_red_1)[0];\n  float v__71 = data_red_rf[34];\n  ((float*)data_red_1)[0] = ((v__70) > (v__71) ? (v__70) : (v__71));\n  float v__72 = ((float*)data_red_1)[0];\n  float v__73 = data_red_rf[35];\n  ((float*)data_red_1)[0] = ((v__72) > (v__73) ? (v__72) : (v__73));\n  float v__74 = ((float*)data_red_1)[0];\n  float v__75 = data_red_rf[36];\n  ((float*)data_red_1)[0] = ((v__74) > (v__75) ? (v__74) : (v__75));\n  float v__76 = ((float*)data_red_1)[0];\n  float v__77 = data_red_rf[37];\n  ((float*)data_red_1)[0] = ((v__76) > (v__77) ? (v__76) : (v__77));\n  float v__78 = ((float*)data_red_1)[0];\n  float v__79 = data_red_rf[38];\n  ((float*)data_red_1)[0] = ((v__78) > (v__79) ? (v__78) : (v__79));\n  float v__80 = ((float*)data_red_1)[0];\n  float v__81 = data_red_rf[39];\n  ((float*)data_red_1)[0] = ((v__80) > (v__81) ? (v__80) : (v__81));\n  float v__82 = ((float*)data_red_1)[0];\n  float v__83 = data_red_rf[40];\n  ((float*)data_red_1)[0] = ((v__82) > (v__83) ? (v__82) : (v__83));\n  float v__84 = ((float*)data_red_1)[0];\n  float v__85 = data_red_rf[41];\n  ((float*)data_red_1)[0] = ((v__84) > (v__85) ? (v__84) : (v__85));\n  float v__86 = ((float*)data_red_1)[0];\n  float v__87 = data_red_rf[42];\n  ((float*)data_red_1)[0] = ((v__86) > (v__87) ? (v__86) : (v__87));\n  float v__88 = ((float*)data_red_1)[0];\n  float v__89 = data_red_rf[43];\n  ((float*)data_red_1)[0] = ((v__88) > (v__89) ? (v__88) : (v__89));\n  float v__90 = ((float*)data_red_1)[0];\n  float v__91 = data_red_rf[44];\n  ((float*)data_red_1)[0] = ((v__90) > (v__91) ? (v__90) : (v__91));\n  float v__92 = ((float*)data_red_1)[0];\n  float v__93 = data_red_rf[45];\n  ((float*)data_red_1)[0] = ((v__92) > (v__93) ? (v__92) : (v__93));\n  float v__94 = ((float*)data_red_1)[0];\n  float v__95 = data_red_rf[46];\n  ((float*)data_red_1)[0] = ((v__94) > (v__95) ? (v__94) : (v__95));\n  float v__96 = ((float*)data_red_1)[0];\n  float v__97 = data_red_rf[47];\n  ((float*)data_red_1)[0] = ((v__96) > (v__97) ? (v__96) : (v__97));\n  float v__98 = ((float*)data_red_1)[0];\n  float v__99 = data_red_rf[48];\n  ((float*)data_red_1)[0] = ((v__98) > (v__99) ? (v__98) : (v__99));\n  float v__100 = ((float*)data_red_1)[0];\n  float v__101 = data_red_rf[49];\n  ((float*)data_red_1)[0] = ((v__100) > (v__101) ? (v__100) : (v__101));\n  float v__102 = ((float*)data_red_1)[0];\n  float v__103 = data_red_rf[50];\n  ((float*)data_red_1)[0] = ((v__102) > (v__103) ? (v__102) : (v__103));\n  float v__104 = ((float*)data_red_1)[0];\n  float v__105 = data_red_rf[51];\n  ((float*)data_red_1)[0] = ((v__104) > (v__105) ? (v__104) : (v__105));\n  float v__106 = ((float*)data_red_1)[0];\n  float v__107 = data_red_rf[52];\n  ((float*)data_red_1)[0] = ((v__106) > (v__107) ? (v__106) : (v__107));\n  float v__108 = ((float*)data_red_1)[0];\n  float v__109 = data_red_rf[53];\n  ((float*)data_red_1)[0] = ((v__108) > (v__109) ? (v__108) : (v__109));\n  float v__110 = ((float*)data_red_1)[0];\n  float v__111 = data_red_rf[54];\n  ((float*)data_red_1)[0] = ((v__110) > (v__111) ? (v__110) : (v__111));\n  float v__112 = ((float*)data_red_1)[0];\n  float v__113 = data_red_rf[55];\n  ((float*)data_red_1)[0] = ((v__112) > (v__113) ? (v__112) : (v__113));\n  float v__114 = ((float*)data_red_1)[0];\n  float v__115 = data_red_rf[56];\n  ((float*)data_red_1)[0] = ((v__114) > (v__115) ? (v__114) : (v__115));\n  float v__116 = ((float*)data_red_1)[0];\n  float v__117 = data_red_rf[57];\n  ((float*)data_red_1)[0] = ((v__116) > (v__117) ? (v__116) : (v__117));\n  float v__118 = ((float*)data_red_1)[0];\n  float v__119 = data_red_rf[58];\n  ((float*)data_red_1)[0] = ((v__118) > (v__119) ? (v__118) : (v__119));\n  float v__120 = ((float*)data_red_1)[0];\n  float v__121 = data_red_rf[59];\n  ((float*)data_red_1)[0] = ((v__120) > (v__121) ? (v__120) : (v__121));\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "max"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 12, 12, 20), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([60], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((60,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(60):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(48, 60):\n            data_1 = T.Buffer((2880,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 60 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[0])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[1])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[2])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[3])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[4])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[5])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[6])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[7])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[8])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[9])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[10])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[11])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[12])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[13])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[14])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[15])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[16])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[17])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[18])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[19])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[20])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[21])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[22])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[23])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[24])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[25])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[26])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[27])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[28])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[29])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[30])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[31])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[32])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[33])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[34])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[35])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[36])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[37])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[38])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[39])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[40])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[41])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[42])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[43])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[44])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[45])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[46])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[47])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[48])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[49])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[50])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[51])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[52])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[53])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[54])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[55])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[56])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[57])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[58])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[59])\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 90; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[60];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 60; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 48; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 60; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      float v_ = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner];\n      float v__1 = ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 60) + k0_k1_fused_k2_fused_k3_fused_inner)];\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = ((v_) < (v__1) ? (v_) : (v__1));\n    }\n  }\n  ((float*)data_red_1)[0] = 3.402823e+38f;\n  float v__2 = ((float*)data_red_1)[0];\n  float v__3 = data_red_rf[0];\n  ((float*)data_red_1)[0] = ((v__2) < (v__3) ? (v__2) : (v__3));\n  float v__4 = ((float*)data_red_1)[0];\n  float v__5 = data_red_rf[1];\n  ((float*)data_red_1)[0] = ((v__4) < (v__5) ? (v__4) : (v__5));\n  float v__6 = ((float*)data_red_1)[0];\n  float v__7 = data_red_rf[2];\n  ((float*)data_red_1)[0] = ((v__6) < (v__7) ? (v__6) : (v__7));\n  float v__8 = ((float*)data_red_1)[0];\n  float v__9 = data_red_rf[3];\n  ((float*)data_red_1)[0] = ((v__8) < (v__9) ? (v__8) : (v__9));\n  float v__10 = ((float*)data_red_1)[0];\n  float v__11 = data_red_rf[4];\n  ((float*)data_red_1)[0] = ((v__10) < (v__11) ? (v__10) : (v__11));\n  float v__12 = ((float*)data_red_1)[0];\n  float v__13 = data_red_rf[5];\n  ((float*)data_red_1)[0] = ((v__12) < (v__13) ? (v__12) : (v__13));\n  float v__14 = ((float*)data_red_1)[0];\n  float v__15 = data_red_rf[6];\n  ((float*)data_red_1)[0] = ((v__14) < (v__15) ? (v__14) : (v__15));\n  float v__16 = ((float*)data_red_1)[0];\n  float v__17 = data_red_rf[7];\n  ((float*)data_red_1)[0] = ((v__16) < (v__17) ? (v__16) : (v__17));\n  float v__18 = ((float*)data_red_1)[0];\n  float v__19 = data_red_rf[8];\n  ((float*)data_red_1)[0] = ((v__18) < (v__19) ? (v__18) : (v__19));\n  float v__20 = ((float*)data_red_1)[0];\n  float v__21 = data_red_rf[9];\n  ((float*)data_red_1)[0] = ((v__20) < (v__21) ? (v__20) : (v__21));\n  float v__22 = ((float*)data_red_1)[0];\n  float v__23 = data_red_rf[10];\n  ((float*)data_red_1)[0] = ((v__22) < (v__23) ? (v__22) : (v__23));\n  float v__24 = ((float*)data_red_1)[0];\n  float v__25 = data_red_rf[11];\n  ((float*)data_red_1)[0] = ((v__24) < (v__25) ? (v__24) : (v__25));\n  float v__26 = ((float*)data_red_1)[0];\n  float v__27 = data_red_rf[12];\n  ((float*)data_red_1)[0] = ((v__26) < (v__27) ? (v__26) : (v__27));\n  float v__28 = ((float*)data_red_1)[0];\n  float v__29 = data_red_rf[13];\n  ((float*)data_red_1)[0] = ((v__28) < (v__29) ? (v__28) : (v__29));\n  float v__30 = ((float*)data_red_1)[0];\n  float v__31 = data_red_rf[14];\n  ((float*)data_red_1)[0] = ((v__30) < (v__31) ? (v__30) : (v__31));\n  float v__32 = ((float*)data_red_1)[0];\n  float v__33 = data_red_rf[15];\n  ((float*)data_red_1)[0] = ((v__32) < (v__33) ? (v__32) : (v__33));\n  float v__34 = ((float*)data_red_1)[0];\n  float v__35 = data_red_rf[16];\n  ((float*)data_red_1)[0] = ((v__34) < (v__35) ? (v__34) : (v__35));\n  float v__36 = ((float*)data_red_1)[0];\n  float v__37 = data_red_rf[17];\n  ((float*)data_red_1)[0] = ((v__36) < (v__37) ? (v__36) : (v__37));\n  float v__38 = ((float*)data_red_1)[0];\n  float v__39 = data_red_rf[18];\n  ((float*)data_red_1)[0] = ((v__38) < (v__39) ? (v__38) : (v__39));\n  float v__40 = ((float*)data_red_1)[0];\n  float v__41 = data_red_rf[19];\n  ((float*)data_red_1)[0] = ((v__40) < (v__41) ? (v__40) : (v__41));\n  float v__42 = ((float*)data_red_1)[0];\n  float v__43 = data_red_rf[20];\n  ((float*)data_red_1)[0] = ((v__42) < (v__43) ? (v__42) : (v__43));\n  float v__44 = ((float*)data_red_1)[0];\n  float v__45 = data_red_rf[21];\n  ((float*)data_red_1)[0] = ((v__44) < (v__45) ? (v__44) : (v__45));\n  float v__46 = ((float*)data_red_1)[0];\n  float v__47 = data_red_rf[22];\n  ((float*)data_red_1)[0] = ((v__46) < (v__47) ? (v__46) : (v__47));\n  float v__48 = ((float*)data_red_1)[0];\n  float v__49 = data_red_rf[23];\n  ((float*)data_red_1)[0] = ((v__48) < (v__49) ? (v__48) : (v__49));\n  float v__50 = ((float*)data_red_1)[0];\n  float v__51 = data_red_rf[24];\n  ((float*)data_red_1)[0] = ((v__50) < (v__51) ? (v__50) : (v__51));\n  float v__52 = ((float*)data_red_1)[0];\n  float v__53 = data_red_rf[25];\n  ((float*)data_red_1)[0] = ((v__52) < (v__53) ? (v__52) : (v__53));\n  float v__54 = ((float*)data_red_1)[0];\n  float v__55 = data_red_rf[26];\n  ((float*)data_red_1)[0] = ((v__54) < (v__55) ? (v__54) : (v__55));\n  float v__56 = ((float*)data_red_1)[0];\n  float v__57 = data_red_rf[27];\n  ((float*)data_red_1)[0] = ((v__56) < (v__57) ? (v__56) : (v__57));\n  float v__58 = ((float*)data_red_1)[0];\n  float v__59 = data_red_rf[28];\n  ((float*)data_red_1)[0] = ((v__58) < (v__59) ? (v__58) : (v__59));\n  float v__60 = ((float*)data_red_1)[0];\n  float v__61 = data_red_rf[29];\n  ((float*)data_red_1)[0] = ((v__60) < (v__61) ? (v__60) : (v__61));\n  float v__62 = ((float*)data_red_1)[0];\n  float v__63 = data_red_rf[30];\n  ((float*)data_red_1)[0] = ((v__62) < (v__63) ? (v__62) : (v__63));\n  float v__64 = ((float*)data_red_1)[0];\n  float v__65 = data_red_rf[31];\n  ((float*)data_red_1)[0] = ((v__64) < (v__65) ? (v__64) : (v__65));\n  float v__66 = ((float*)data_red_1)[0];\n  float v__67 = data_red_rf[32];\n  ((float*)data_red_1)[0] = ((v__66) < (v__67) ? (v__66) : (v__67));\n  float v__68 = ((float*)data_red_1)[0];\n  float v__69 = data_red_rf[33];\n  ((float*)data_red_1)[0] = ((v__68) < (v__69) ? (v__68) : (v__69));\n  float v__70 = ((float*)data_red_1)[0];\n  float v__71 = data_red_rf[34];\n  ((float*)data_red_1)[0] = ((v__70) < (v__71) ? (v__70) : (v__71));\n  float v__72 = ((float*)data_red_1)[0];\n  float v__73 = data_red_rf[35];\n  ((float*)data_red_1)[0] = ((v__72) < (v__73) ? (v__72) : (v__73));\n  float v__74 = ((float*)data_red_1)[0];\n  float v__75 = data_red_rf[36];\n  ((float*)data_red_1)[0] = ((v__74) < (v__75) ? (v__74) : (v__75));\n  float v__76 = ((float*)data_red_1)[0];\n  float v__77 = data_red_rf[37];\n  ((float*)data_red_1)[0] = ((v__76) < (v__77) ? (v__76) : (v__77));\n  float v__78 = ((float*)data_red_1)[0];\n  float v__79 = data_red_rf[38];\n  ((float*)data_red_1)[0] = ((v__78) < (v__79) ? (v__78) : (v__79));\n  float v__80 = ((float*)data_red_1)[0];\n  float v__81 = data_red_rf[39];\n  ((float*)data_red_1)[0] = ((v__80) < (v__81) ? (v__80) : (v__81));\n  float v__82 = ((float*)data_red_1)[0];\n  float v__83 = data_red_rf[40];\n  ((float*)data_red_1)[0] = ((v__82) < (v__83) ? (v__82) : (v__83));\n  float v__84 = ((float*)data_red_1)[0];\n  float v__85 = data_red_rf[41];\n  ((float*)data_red_1)[0] = ((v__84) < (v__85) ? (v__84) : (v__85));\n  float v__86 = ((float*)data_red_1)[0];\n  float v__87 = data_red_rf[42];\n  ((float*)data_red_1)[0] = ((v__86) < (v__87) ? (v__86) : (v__87));\n  float v__88 = ((float*)data_red_1)[0];\n  float v__89 = data_red_rf[43];\n  ((float*)data_red_1)[0] = ((v__88) < (v__89) ? (v__88) : (v__89));\n  float v__90 = ((float*)data_red_1)[0];\n  float v__91 = data_red_rf[44];\n  ((float*)data_red_1)[0] = ((v__90) < (v__91) ? (v__90) : (v__91));\n  float v__92 = ((float*)data_red_1)[0];\n  float v__93 = data_red_rf[45];\n  ((float*)data_red_1)[0] = ((v__92) < (v__93) ? (v__92) : (v__93));\n  float v__94 = ((float*)data_red_1)[0];\n  float v__95 = data_red_rf[46];\n  ((float*)data_red_1)[0] = ((v__94) < (v__95) ? (v__94) : (v__95));\n  float v__96 = ((float*)data_red_1)[0];\n  float v__97 = data_red_rf[47];\n  ((float*)data_red_1)[0] = ((v__96) < (v__97) ? (v__96) : (v__97));\n  float v__98 = ((float*)data_red_1)[0];\n  float v__99 = data_red_rf[48];\n  ((float*)data_red_1)[0] = ((v__98) < (v__99) ? (v__98) : (v__99));\n  float v__100 = ((float*)data_red_1)[0];\n  float v__101 = data_red_rf[49];\n  ((float*)data_red_1)[0] = ((v__100) < (v__101) ? (v__100) : (v__101));\n  float v__102 = ((float*)data_red_1)[0];\n  float v__103 = data_red_rf[50];\n  ((float*)data_red_1)[0] = ((v__102) < (v__103) ? (v__102) : (v__103));\n  float v__104 = ((float*)data_red_1)[0];\n  float v__105 = data_red_rf[51];\n  ((float*)data_red_1)[0] = ((v__104) < (v__105) ? (v__104) : (v__105));\n  float v__106 = ((float*)data_red_1)[0];\n  float v__107 = data_red_rf[52];\n  ((float*)data_red_1)[0] = ((v__106) < (v__107) ? (v__106) : (v__107));\n  float v__108 = ((float*)data_red_1)[0];\n  float v__109 = data_red_rf[53];\n  ((float*)data_red_1)[0] = ((v__108) < (v__109) ? (v__108) : (v__109));\n  float v__110 = ((float*)data_red_1)[0];\n  float v__111 = data_red_rf[54];\n  ((float*)data_red_1)[0] = ((v__110) < (v__111) ? (v__110) : (v__111));\n  float v__112 = ((float*)data_red_1)[0];\n  float v__113 = data_red_rf[55];\n  ((float*)data_red_1)[0] = ((v__112) < (v__113) ? (v__112) : (v__113));\n  float v__114 = ((float*)data_red_1)[0];\n  float v__115 = data_red_rf[56];\n  ((float*)data_red_1)[0] = ((v__114) < (v__115) ? (v__114) : (v__115));\n  float v__116 = ((float*)data_red_1)[0];\n  float v__117 = data_red_rf[57];\n  ((float*)data_red_1)[0] = ((v__116) < (v__117) ? (v__116) : (v__117));\n  float v__118 = ((float*)data_red_1)[0];\n  float v__119 = data_red_rf[58];\n  ((float*)data_red_1)[0] = ((v__118) < (v__119) ? (v__118) : (v__119));\n  float v__120 = ((float*)data_red_1)[0];\n  float v__121 = data_red_rf[59];\n  ((float*)data_red_1)[0] = ((v__120) < (v__121) ? (v__120) : (v__121));\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "min"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 12, 8, 24), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([36], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((36,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(36):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(192, 36):\n            data_1 = T.Buffer((6912,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 36 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[0])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[1])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[2])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[3])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[4])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[5])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[6])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[7])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[8])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[9])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[10])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[11])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[12])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[13])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[14])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[15])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[16])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[17])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[18])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[19])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[20])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[21])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[22])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[23])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[24])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[25])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[26])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[27])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[28])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[29])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[30])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[31])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[32])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[33])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[34])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[35])\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 216; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[36];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 36; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 192; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 36; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      float v_ = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner];\n      float v__1 = ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 36) + k0_k1_fused_k2_fused_k3_fused_inner)];\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = ((v_) < (v__1) ? (v_) : (v__1));\n    }\n  }\n  ((float*)data_red_1)[0] = 3.402823e+38f;\n  float v__2 = ((float*)data_red_1)[0];\n  float v__3 = data_red_rf[0];\n  ((float*)data_red_1)[0] = ((v__2) < (v__3) ? (v__2) : (v__3));\n  float v__4 = ((float*)data_red_1)[0];\n  float v__5 = data_red_rf[1];\n  ((float*)data_red_1)[0] = ((v__4) < (v__5) ? (v__4) : (v__5));\n  float v__6 = ((float*)data_red_1)[0];\n  float v__7 = data_red_rf[2];\n  ((float*)data_red_1)[0] = ((v__6) < (v__7) ? (v__6) : (v__7));\n  float v__8 = ((float*)data_red_1)[0];\n  float v__9 = data_red_rf[3];\n  ((float*)data_red_1)[0] = ((v__8) < (v__9) ? (v__8) : (v__9));\n  float v__10 = ((float*)data_red_1)[0];\n  float v__11 = data_red_rf[4];\n  ((float*)data_red_1)[0] = ((v__10) < (v__11) ? (v__10) : (v__11));\n  float v__12 = ((float*)data_red_1)[0];\n  float v__13 = data_red_rf[5];\n  ((float*)data_red_1)[0] = ((v__12) < (v__13) ? (v__12) : (v__13));\n  float v__14 = ((float*)data_red_1)[0];\n  float v__15 = data_red_rf[6];\n  ((float*)data_red_1)[0] = ((v__14) < (v__15) ? (v__14) : (v__15));\n  float v__16 = ((float*)data_red_1)[0];\n  float v__17 = data_red_rf[7];\n  ((float*)data_red_1)[0] = ((v__16) < (v__17) ? (v__16) : (v__17));\n  float v__18 = ((float*)data_red_1)[0];\n  float v__19 = data_red_rf[8];\n  ((float*)data_red_1)[0] = ((v__18) < (v__19) ? (v__18) : (v__19));\n  float v__20 = ((float*)data_red_1)[0];\n  float v__21 = data_red_rf[9];\n  ((float*)data_red_1)[0] = ((v__20) < (v__21) ? (v__20) : (v__21));\n  float v__22 = ((float*)data_red_1)[0];\n  float v__23 = data_red_rf[10];\n  ((float*)data_red_1)[0] = ((v__22) < (v__23) ? (v__22) : (v__23));\n  float v__24 = ((float*)data_red_1)[0];\n  float v__25 = data_red_rf[11];\n  ((float*)data_red_1)[0] = ((v__24) < (v__25) ? (v__24) : (v__25));\n  float v__26 = ((float*)data_red_1)[0];\n  float v__27 = data_red_rf[12];\n  ((float*)data_red_1)[0] = ((v__26) < (v__27) ? (v__26) : (v__27));\n  float v__28 = ((float*)data_red_1)[0];\n  float v__29 = data_red_rf[13];\n  ((float*)data_red_1)[0] = ((v__28) < (v__29) ? (v__28) : (v__29));\n  float v__30 = ((float*)data_red_1)[0];\n  float v__31 = data_red_rf[14];\n  ((float*)data_red_1)[0] = ((v__30) < (v__31) ? (v__30) : (v__31));\n  float v__32 = ((float*)data_red_1)[0];\n  float v__33 = data_red_rf[15];\n  ((float*)data_red_1)[0] = ((v__32) < (v__33) ? (v__32) : (v__33));\n  float v__34 = ((float*)data_red_1)[0];\n  float v__35 = data_red_rf[16];\n  ((float*)data_red_1)[0] = ((v__34) < (v__35) ? (v__34) : (v__35));\n  float v__36 = ((float*)data_red_1)[0];\n  float v__37 = data_red_rf[17];\n  ((float*)data_red_1)[0] = ((v__36) < (v__37) ? (v__36) : (v__37));\n  float v__38 = ((float*)data_red_1)[0];\n  float v__39 = data_red_rf[18];\n  ((float*)data_red_1)[0] = ((v__38) < (v__39) ? (v__38) : (v__39));\n  float v__40 = ((float*)data_red_1)[0];\n  float v__41 = data_red_rf[19];\n  ((float*)data_red_1)[0] = ((v__40) < (v__41) ? (v__40) : (v__41));\n  float v__42 = ((float*)data_red_1)[0];\n  float v__43 = data_red_rf[20];\n  ((float*)data_red_1)[0] = ((v__42) < (v__43) ? (v__42) : (v__43));\n  float v__44 = ((float*)data_red_1)[0];\n  float v__45 = data_red_rf[21];\n  ((float*)data_red_1)[0] = ((v__44) < (v__45) ? (v__44) : (v__45));\n  float v__46 = ((float*)data_red_1)[0];\n  float v__47 = data_red_rf[22];\n  ((float*)data_red_1)[0] = ((v__46) < (v__47) ? (v__46) : (v__47));\n  float v__48 = ((float*)data_red_1)[0];\n  float v__49 = data_red_rf[23];\n  ((float*)data_red_1)[0] = ((v__48) < (v__49) ? (v__48) : (v__49));\n  float v__50 = ((float*)data_red_1)[0];\n  float v__51 = data_red_rf[24];\n  ((float*)data_red_1)[0] = ((v__50) < (v__51) ? (v__50) : (v__51));\n  float v__52 = ((float*)data_red_1)[0];\n  float v__53 = data_red_rf[25];\n  ((float*)data_red_1)[0] = ((v__52) < (v__53) ? (v__52) : (v__53));\n  float v__54 = ((float*)data_red_1)[0];\n  float v__55 = data_red_rf[26];\n  ((float*)data_red_1)[0] = ((v__54) < (v__55) ? (v__54) : (v__55));\n  float v__56 = ((float*)data_red_1)[0];\n  float v__57 = data_red_rf[27];\n  ((float*)data_red_1)[0] = ((v__56) < (v__57) ? (v__56) : (v__57));\n  float v__58 = ((float*)data_red_1)[0];\n  float v__59 = data_red_rf[28];\n  ((float*)data_red_1)[0] = ((v__58) < (v__59) ? (v__58) : (v__59));\n  float v__60 = ((float*)data_red_1)[0];\n  float v__61 = data_red_rf[29];\n  ((float*)data_red_1)[0] = ((v__60) < (v__61) ? (v__60) : (v__61));\n  float v__62 = ((float*)data_red_1)[0];\n  float v__63 = data_red_rf[30];\n  ((float*)data_red_1)[0] = ((v__62) < (v__63) ? (v__62) : (v__63));\n  float v__64 = ((float*)data_red_1)[0];\n  float v__65 = data_red_rf[31];\n  ((float*)data_red_1)[0] = ((v__64) < (v__65) ? (v__64) : (v__65));\n  float v__66 = ((float*)data_red_1)[0];\n  float v__67 = data_red_rf[32];\n  ((float*)data_red_1)[0] = ((v__66) < (v__67) ? (v__66) : (v__67));\n  float v__68 = ((float*)data_red_1)[0];\n  float v__69 = data_red_rf[33];\n  ((float*)data_red_1)[0] = ((v__68) < (v__69) ? (v__68) : (v__69));\n  float v__70 = ((float*)data_red_1)[0];\n  float v__71 = data_red_rf[34];\n  ((float*)data_red_1)[0] = ((v__70) < (v__71) ? (v__70) : (v__71));\n  float v__72 = ((float*)data_red_1)[0];\n  float v__73 = data_red_rf[35];\n  ((float*)data_red_1)[0] = ((v__72) < (v__73) ? (v__72) : (v__73));\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "min"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 24, 40, 8), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([12], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((12,), data=data_red_rf, align=32)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(12):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(3840):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 12\n            data_1 = T.Buffer((46080,), data=data.data)\n            data_red_rf_1[0] = T.min(data_red_rf_1[0], data_1[cse_var_1])\n            data_red_rf_1[1] = T.min(data_red_rf_1[1], data_1[cse_var_1 + 1])\n            data_red_rf_1[2] = T.min(data_red_rf_1[2], data_1[cse_var_1 + 2])\n            data_red_rf_1[3] = T.min(data_red_rf_1[3], data_1[cse_var_1 + 3])\n            data_red_rf_1[4] = T.min(data_red_rf_1[4], data_1[cse_var_1 + 4])\n            data_red_rf_1[5] = T.min(data_red_rf_1[5], data_1[cse_var_1 + 5])\n            data_red_rf_1[6] = T.min(data_red_rf_1[6], data_1[cse_var_1 + 6])\n            data_red_rf_1[7] = T.min(data_red_rf_1[7], data_1[cse_var_1 + 7])\n            data_red_rf_1[8] = T.min(data_red_rf_1[8], data_1[cse_var_1 + 8])\n            data_red_rf_1[9] = T.min(data_red_rf_1[9], data_1[cse_var_1 + 9])\n            data_red_rf_1[10] = T.min(data_red_rf_1[10], data_1[cse_var_1 + 10])\n            data_red_rf_1[11] = T.min(data_red_rf_1[11], data_1[cse_var_1 + 11])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(12):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1440; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[12];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 12; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 3840; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 12);\n    float v_ = data_red_rf[0];\n    float v__1 = ((float*)data_1)[cse_var_1];\n    data_red_rf[0] = ((v_) < (v__1) ? (v_) : (v__1));\n    float v__2 = data_red_rf[1];\n    float v__3 = ((float*)data_1)[(cse_var_1 + 1)];\n    data_red_rf[1] = ((v__2) < (v__3) ? (v__2) : (v__3));\n    float v__4 = data_red_rf[2];\n    float v__5 = ((float*)data_1)[(cse_var_1 + 2)];\n    data_red_rf[2] = ((v__4) < (v__5) ? (v__4) : (v__5));\n    float v__6 = data_red_rf[3];\n    float v__7 = ((float*)data_1)[(cse_var_1 + 3)];\n    data_red_rf[3] = ((v__6) < (v__7) ? (v__6) : (v__7));\n    float v__8 = data_red_rf[4];\n    float v__9 = ((float*)data_1)[(cse_var_1 + 4)];\n    data_red_rf[4] = ((v__8) < (v__9) ? (v__8) : (v__9));\n    float v__10 = data_red_rf[5];\n    float v__11 = ((float*)data_1)[(cse_var_1 + 5)];\n    data_red_rf[5] = ((v__10) < (v__11) ? (v__10) : (v__11));\n    float v__12 = data_red_rf[6];\n    float v__13 = ((float*)data_1)[(cse_var_1 + 6)];\n    data_red_rf[6] = ((v__12) < (v__13) ? (v__12) : (v__13));\n    float v__14 = data_red_rf[7];\n    float v__15 = ((float*)data_1)[(cse_var_1 + 7)];\n    data_red_rf[7] = ((v__14) < (v__15) ? (v__14) : (v__15));\n    float v__16 = data_red_rf[8];\n    float v__17 = ((float*)data_1)[(cse_var_1 + 8)];\n    data_red_rf[8] = ((v__16) < (v__17) ? (v__16) : (v__17));\n    float v__18 = data_red_rf[9];\n    float v__19 = ((float*)data_1)[(cse_var_1 + 9)];\n    data_red_rf[9] = ((v__18) < (v__19) ? (v__18) : (v__19));\n    float v__20 = data_red_rf[10];\n    float v__21 = ((float*)data_1)[(cse_var_1 + 10)];\n    data_red_rf[10] = ((v__20) < (v__21) ? (v__20) : (v__21));\n    float v__22 = data_red_rf[11];\n    float v__23 = ((float*)data_1)[(cse_var_1 + 11)];\n    data_red_rf[11] = ((v__22) < (v__23) ? (v__22) : (v__23));\n  }\n  ((float*)data_red_1)[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 12; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    float v__24 = ((float*)data_red_1)[0];\n    float v__25 = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v];\n    ((float*)data_red_1)[0] = ((v__24) < (v__25) ? (v__24) : (v__25));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "min"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 36, 40, 12), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([20], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((20,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(20):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(1728, 20):\n            data_1 = T.Buffer((34560,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 20 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(20):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1080; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[20];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 20; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1728; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 20; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      float v_ = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner];\n      float v__1 = ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 20) + k0_k1_fused_k2_fused_k3_fused_inner)];\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = ((v_) < (v__1) ? (v_) : (v__1));\n    }\n  }\n  ((float*)data_red_1)[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 20; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    float v__2 = ((float*)data_red_1)[0];\n    float v__3 = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v];\n    ((float*)data_red_1)[0] = ((v__2) < (v__3) ? (v__2) : (v__3));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "min"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 16, 32, 8), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([64], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((64,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(64):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(448, 64):\n            data_1 = T.Buffer((28672,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 64 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(64):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 896; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[64];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 448; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 64; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      float v_ = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner];\n      float v__1 = ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 64) + k0_k1_fused_k2_fused_k3_fused_inner)];\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = ((v_) < (v__1) ? (v_) : (v__1));\n    }\n  }\n  ((float*)data_red_1)[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    float v__2 = ((float*)data_red_1)[0];\n    float v__3 = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v];\n    ((float*)data_red_1)[0] = ((v__2) < (v__3) ? (v__2) : (v__3));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "min"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 32, 28, 16), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([42], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((42,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(42):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(2048):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 42\n            data_1 = T.Buffer((86016,), data=data.data)\n            data_red_rf_1[0] = T.min(data_red_rf_1[0], data_1[cse_var_1])\n            data_red_rf_1[1] = T.min(data_red_rf_1[1], data_1[cse_var_1 + 1])\n            data_red_rf_1[2] = T.min(data_red_rf_1[2], data_1[cse_var_1 + 2])\n            data_red_rf_1[3] = T.min(data_red_rf_1[3], data_1[cse_var_1 + 3])\n            data_red_rf_1[4] = T.min(data_red_rf_1[4], data_1[cse_var_1 + 4])\n            data_red_rf_1[5] = T.min(data_red_rf_1[5], data_1[cse_var_1 + 5])\n            data_red_rf_1[6] = T.min(data_red_rf_1[6], data_1[cse_var_1 + 6])\n            data_red_rf_1[7] = T.min(data_red_rf_1[7], data_1[cse_var_1 + 7])\n            data_red_rf_1[8] = T.min(data_red_rf_1[8], data_1[cse_var_1 + 8])\n            data_red_rf_1[9] = T.min(data_red_rf_1[9], data_1[cse_var_1 + 9])\n            data_red_rf_1[10] = T.min(data_red_rf_1[10], data_1[cse_var_1 + 10])\n            data_red_rf_1[11] = T.min(data_red_rf_1[11], data_1[cse_var_1 + 11])\n            data_red_rf_1[12] = T.min(data_red_rf_1[12], data_1[cse_var_1 + 12])\n            data_red_rf_1[13] = T.min(data_red_rf_1[13], data_1[cse_var_1 + 13])\n            data_red_rf_1[14] = T.min(data_red_rf_1[14], data_1[cse_var_1 + 14])\n            data_red_rf_1[15] = T.min(data_red_rf_1[15], data_1[cse_var_1 + 15])\n            data_red_rf_1[16] = T.min(data_red_rf_1[16], data_1[cse_var_1 + 16])\n            data_red_rf_1[17] = T.min(data_red_rf_1[17], data_1[cse_var_1 + 17])\n            data_red_rf_1[18] = T.min(data_red_rf_1[18], data_1[cse_var_1 + 18])\n            data_red_rf_1[19] = T.min(data_red_rf_1[19], data_1[cse_var_1 + 19])\n            data_red_rf_1[20] = T.min(data_red_rf_1[20], data_1[cse_var_1 + 20])\n            data_red_rf_1[21] = T.min(data_red_rf_1[21], data_1[cse_var_1 + 21])\n            data_red_rf_1[22] = T.min(data_red_rf_1[22], data_1[cse_var_1 + 22])\n            data_red_rf_1[23] = T.min(data_red_rf_1[23], data_1[cse_var_1 + 23])\n            data_red_rf_1[24] = T.min(data_red_rf_1[24], data_1[cse_var_1 + 24])\n            data_red_rf_1[25] = T.min(data_red_rf_1[25], data_1[cse_var_1 + 25])\n            data_red_rf_1[26] = T.min(data_red_rf_1[26], data_1[cse_var_1 + 26])\n            data_red_rf_1[27] = T.min(data_red_rf_1[27], data_1[cse_var_1 + 27])\n            data_red_rf_1[28] = T.min(data_red_rf_1[28], data_1[cse_var_1 + 28])\n            data_red_rf_1[29] = T.min(data_red_rf_1[29], data_1[cse_var_1 + 29])\n            data_red_rf_1[30] = T.min(data_red_rf_1[30], data_1[cse_var_1 + 30])\n            data_red_rf_1[31] = T.min(data_red_rf_1[31], data_1[cse_var_1 + 31])\n            data_red_rf_1[32] = T.min(data_red_rf_1[32], data_1[cse_var_1 + 32])\n            data_red_rf_1[33] = T.min(data_red_rf_1[33], data_1[cse_var_1 + 33])\n            data_red_rf_1[34] = T.min(data_red_rf_1[34], data_1[cse_var_1 + 34])\n            data_red_rf_1[35] = T.min(data_red_rf_1[35], data_1[cse_var_1 + 35])\n            data_red_rf_1[36] = T.min(data_red_rf_1[36], data_1[cse_var_1 + 36])\n            data_red_rf_1[37] = T.min(data_red_rf_1[37], data_1[cse_var_1 + 37])\n            data_red_rf_1[38] = T.min(data_red_rf_1[38], data_1[cse_var_1 + 38])\n            data_red_rf_1[39] = T.min(data_red_rf_1[39], data_1[cse_var_1 + 39])\n            data_red_rf_1[40] = T.min(data_red_rf_1[40], data_1[cse_var_1 + 40])\n            data_red_rf_1[41] = T.min(data_red_rf_1[41], data_1[cse_var_1 + 41])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[0])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[1])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[2])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[3])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[4])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[5])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[6])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[7])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[8])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[9])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[10])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[11])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[12])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[13])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[14])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[15])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[16])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[17])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[18])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[19])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[20])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[21])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[22])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[23])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[24])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[25])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[26])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[27])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[28])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[29])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[30])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[31])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[32])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[33])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[34])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[35])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[36])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[37])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[38])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[39])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[40])\n        data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[41])\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 2688; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[42];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 42; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 2048; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 42);\n    float v_ = data_red_rf[0];\n    float v__1 = ((float*)data_1)[cse_var_1];\n    data_red_rf[0] = ((v_) < (v__1) ? (v_) : (v__1));\n    float v__2 = data_red_rf[1];\n    float v__3 = ((float*)data_1)[(cse_var_1 + 1)];\n    data_red_rf[1] = ((v__2) < (v__3) ? (v__2) : (v__3));\n    float v__4 = data_red_rf[2];\n    float v__5 = ((float*)data_1)[(cse_var_1 + 2)];\n    data_red_rf[2] = ((v__4) < (v__5) ? (v__4) : (v__5));\n    float v__6 = data_red_rf[3];\n    float v__7 = ((float*)data_1)[(cse_var_1 + 3)];\n    data_red_rf[3] = ((v__6) < (v__7) ? (v__6) : (v__7));\n    float v__8 = data_red_rf[4];\n    float v__9 = ((float*)data_1)[(cse_var_1 + 4)];\n    data_red_rf[4] = ((v__8) < (v__9) ? (v__8) : (v__9));\n    float v__10 = data_red_rf[5];\n    float v__11 = ((float*)data_1)[(cse_var_1 + 5)];\n    data_red_rf[5] = ((v__10) < (v__11) ? (v__10) : (v__11));\n    float v__12 = data_red_rf[6];\n    float v__13 = ((float*)data_1)[(cse_var_1 + 6)];\n    data_red_rf[6] = ((v__12) < (v__13) ? (v__12) : (v__13));\n    float v__14 = data_red_rf[7];\n    float v__15 = ((float*)data_1)[(cse_var_1 + 7)];\n    data_red_rf[7] = ((v__14) < (v__15) ? (v__14) : (v__15));\n    float v__16 = data_red_rf[8];\n    float v__17 = ((float*)data_1)[(cse_var_1 + 8)];\n    data_red_rf[8] = ((v__16) < (v__17) ? (v__16) : (v__17));\n    float v__18 = data_red_rf[9];\n    float v__19 = ((float*)data_1)[(cse_var_1 + 9)];\n    data_red_rf[9] = ((v__18) < (v__19) ? (v__18) : (v__19));\n    float v__20 = data_red_rf[10];\n    float v__21 = ((float*)data_1)[(cse_var_1 + 10)];\n    data_red_rf[10] = ((v__20) < (v__21) ? (v__20) : (v__21));\n    float v__22 = data_red_rf[11];\n    float v__23 = ((float*)data_1)[(cse_var_1 + 11)];\n    data_red_rf[11] = ((v__22) < (v__23) ? (v__22) : (v__23));\n    float v__24 = data_red_rf[12];\n    float v__25 = ((float*)data_1)[(cse_var_1 + 12)];\n    data_red_rf[12] = ((v__24) < (v__25) ? (v__24) : (v__25));\n    float v__26 = data_red_rf[13];\n    float v__27 = ((float*)data_1)[(cse_var_1 + 13)];\n    data_red_rf[13] = ((v__26) < (v__27) ? (v__26) : (v__27));\n    float v__28 = data_red_rf[14];\n    float v__29 = ((float*)data_1)[(cse_var_1 + 14)];\n    data_red_rf[14] = ((v__28) < (v__29) ? (v__28) : (v__29));\n    float v__30 = data_red_rf[15];\n    float v__31 = ((float*)data_1)[(cse_var_1 + 15)];\n    data_red_rf[15] = ((v__30) < (v__31) ? (v__30) : (v__31));\n    float v__32 = data_red_rf[16];\n    float v__33 = ((float*)data_1)[(cse_var_1 + 16)];\n    data_red_rf[16] = ((v__32) < (v__33) ? (v__32) : (v__33));\n    float v__34 = data_red_rf[17];\n    float v__35 = ((float*)data_1)[(cse_var_1 + 17)];\n    data_red_rf[17] = ((v__34) < (v__35) ? (v__34) : (v__35));\n    float v__36 = data_red_rf[18];\n    float v__37 = ((float*)data_1)[(cse_var_1 + 18)];\n    data_red_rf[18] = ((v__36) < (v__37) ? (v__36) : (v__37));\n    float v__38 = data_red_rf[19];\n    float v__39 = ((float*)data_1)[(cse_var_1 + 19)];\n    data_red_rf[19] = ((v__38) < (v__39) ? (v__38) : (v__39));\n    float v__40 = data_red_rf[20];\n    float v__41 = ((float*)data_1)[(cse_var_1 + 20)];\n    data_red_rf[20] = ((v__40) < (v__41) ? (v__40) : (v__41));\n    float v__42 = data_red_rf[21];\n    float v__43 = ((float*)data_1)[(cse_var_1 + 21)];\n    data_red_rf[21] = ((v__42) < (v__43) ? (v__42) : (v__43));\n    float v__44 = data_red_rf[22];\n    float v__45 = ((float*)data_1)[(cse_var_1 + 22)];\n    data_red_rf[22] = ((v__44) < (v__45) ? (v__44) : (v__45));\n    float v__46 = data_red_rf[23];\n    float v__47 = ((float*)data_1)[(cse_var_1 + 23)];\n    data_red_rf[23] = ((v__46) < (v__47) ? (v__46) : (v__47));\n    float v__48 = data_red_rf[24];\n    float v__49 = ((float*)data_1)[(cse_var_1 + 24)];\n    data_red_rf[24] = ((v__48) < (v__49) ? (v__48) : (v__49));\n    float v__50 = data_red_rf[25];\n    float v__51 = ((float*)data_1)[(cse_var_1 + 25)];\n    data_red_rf[25] = ((v__50) < (v__51) ? (v__50) : (v__51));\n    float v__52 = data_red_rf[26];\n    float v__53 = ((float*)data_1)[(cse_var_1 + 26)];\n    data_red_rf[26] = ((v__52) < (v__53) ? (v__52) : (v__53));\n    float v__54 = data_red_rf[27];\n    float v__55 = ((float*)data_1)[(cse_var_1 + 27)];\n    data_red_rf[27] = ((v__54) < (v__55) ? (v__54) : (v__55));\n    float v__56 = data_red_rf[28];\n    float v__57 = ((float*)data_1)[(cse_var_1 + 28)];\n    data_red_rf[28] = ((v__56) < (v__57) ? (v__56) : (v__57));\n    float v__58 = data_red_rf[29];\n    float v__59 = ((float*)data_1)[(cse_var_1 + 29)];\n    data_red_rf[29] = ((v__58) < (v__59) ? (v__58) : (v__59));\n    float v__60 = data_red_rf[30];\n    float v__61 = ((float*)data_1)[(cse_var_1 + 30)];\n    data_red_rf[30] = ((v__60) < (v__61) ? (v__60) : (v__61));\n    float v__62 = data_red_rf[31];\n    float v__63 = ((float*)data_1)[(cse_var_1 + 31)];\n    data_red_rf[31] = ((v__62) < (v__63) ? (v__62) : (v__63));\n    float v__64 = data_red_rf[32];\n    float v__65 = ((float*)data_1)[(cse_var_1 + 32)];\n    data_red_rf[32] = ((v__64) < (v__65) ? (v__64) : (v__65));\n    float v__66 = data_red_rf[33];\n    float v__67 = ((float*)data_1)[(cse_var_1 + 33)];\n    data_red_rf[33] = ((v__66) < (v__67) ? (v__66) : (v__67));\n    float v__68 = data_red_rf[34];\n    float v__69 = ((float*)data_1)[(cse_var_1 + 34)];\n    data_red_rf[34] = ((v__68) < (v__69) ? (v__68) : (v__69));\n    float v__70 = data_red_rf[35];\n    float v__71 = ((float*)data_1)[(cse_var_1 + 35)];\n    data_red_rf[35] = ((v__70) < (v__71) ? (v__70) : (v__71));\n    float v__72 = data_red_rf[36];\n    float v__73 = ((float*)data_1)[(cse_var_1 + 36)];\n    data_red_rf[36] = ((v__72) < (v__73) ? (v__72) : (v__73));\n    float v__74 = data_red_rf[37];\n    float v__75 = ((float*)data_1)[(cse_var_1 + 37)];\n    data_red_rf[37] = ((v__74) < (v__75) ? (v__74) : (v__75));\n    float v__76 = data_red_rf[38];\n    float v__77 = ((float*)data_1)[(cse_var_1 + 38)];\n    data_red_rf[38] = ((v__76) < (v__77) ? (v__76) : (v__77));\n    float v__78 = data_red_rf[39];\n    float v__79 = ((float*)data_1)[(cse_var_1 + 39)];\n    data_red_rf[39] = ((v__78) < (v__79) ? (v__78) : (v__79));\n    float v__80 = data_red_rf[40];\n    float v__81 = ((float*)data_1)[(cse_var_1 + 40)];\n    data_red_rf[40] = ((v__80) < (v__81) ? (v__80) : (v__81));\n    float v__82 = data_red_rf[41];\n    float v__83 = ((float*)data_1)[(cse_var_1 + 41)];\n    data_red_rf[41] = ((v__82) < (v__83) ? (v__82) : (v__83));\n  }\n  ((float*)data_red_1)[0] = 3.402823e+38f;\n  float v__84 = ((float*)data_red_1)[0];\n  float v__85 = data_red_rf[0];\n  ((float*)data_red_1)[0] = ((v__84) < (v__85) ? (v__84) : (v__85));\n  float v__86 = ((float*)data_red_1)[0];\n  float v__87 = data_red_rf[1];\n  ((float*)data_red_1)[0] = ((v__86) < (v__87) ? (v__86) : (v__87));\n  float v__88 = ((float*)data_red_1)[0];\n  float v__89 = data_red_rf[2];\n  ((float*)data_red_1)[0] = ((v__88) < (v__89) ? (v__88) : (v__89));\n  float v__90 = ((float*)data_red_1)[0];\n  float v__91 = data_red_rf[3];\n  ((float*)data_red_1)[0] = ((v__90) < (v__91) ? (v__90) : (v__91));\n  float v__92 = ((float*)data_red_1)[0];\n  float v__93 = data_red_rf[4];\n  ((float*)data_red_1)[0] = ((v__92) < (v__93) ? (v__92) : (v__93));\n  float v__94 = ((float*)data_red_1)[0];\n  float v__95 = data_red_rf[5];\n  ((float*)data_red_1)[0] = ((v__94) < (v__95) ? (v__94) : (v__95));\n  float v__96 = ((float*)data_red_1)[0];\n  float v__97 = data_red_rf[6];\n  ((float*)data_red_1)[0] = ((v__96) < (v__97) ? (v__96) : (v__97));\n  float v__98 = ((float*)data_red_1)[0];\n  float v__99 = data_red_rf[7];\n  ((float*)data_red_1)[0] = ((v__98) < (v__99) ? (v__98) : (v__99));\n  float v__100 = ((float*)data_red_1)[0];\n  float v__101 = data_red_rf[8];\n  ((float*)data_red_1)[0] = ((v__100) < (v__101) ? (v__100) : (v__101));\n  float v__102 = ((float*)data_red_1)[0];\n  float v__103 = data_red_rf[9];\n  ((float*)data_red_1)[0] = ((v__102) < (v__103) ? (v__102) : (v__103));\n  float v__104 = ((float*)data_red_1)[0];\n  float v__105 = data_red_rf[10];\n  ((float*)data_red_1)[0] = ((v__104) < (v__105) ? (v__104) : (v__105));\n  float v__106 = ((float*)data_red_1)[0];\n  float v__107 = data_red_rf[11];\n  ((float*)data_red_1)[0] = ((v__106) < (v__107) ? (v__106) : (v__107));\n  float v__108 = ((float*)data_red_1)[0];\n  float v__109 = data_red_rf[12];\n  ((float*)data_red_1)[0] = ((v__108) < (v__109) ? (v__108) : (v__109));\n  float v__110 = ((float*)data_red_1)[0];\n  float v__111 = data_red_rf[13];\n  ((float*)data_red_1)[0] = ((v__110) < (v__111) ? (v__110) : (v__111));\n  float v__112 = ((float*)data_red_1)[0];\n  float v__113 = data_red_rf[14];\n  ((float*)data_red_1)[0] = ((v__112) < (v__113) ? (v__112) : (v__113));\n  float v__114 = ((float*)data_red_1)[0];\n  float v__115 = data_red_rf[15];\n  ((float*)data_red_1)[0] = ((v__114) < (v__115) ? (v__114) : (v__115));\n  float v__116 = ((float*)data_red_1)[0];\n  float v__117 = data_red_rf[16];\n  ((float*)data_red_1)[0] = ((v__116) < (v__117) ? (v__116) : (v__117));\n  float v__118 = ((float*)data_red_1)[0];\n  float v__119 = data_red_rf[17];\n  ((float*)data_red_1)[0] = ((v__118) < (v__119) ? (v__118) : (v__119));\n  float v__120 = ((float*)data_red_1)[0];\n  float v__121 = data_red_rf[18];\n  ((float*)data_red_1)[0] = ((v__120) < (v__121) ? (v__120) : (v__121));\n  float v__122 = ((float*)data_red_1)[0];\n  float v__123 = data_red_rf[19];\n  ((float*)data_red_1)[0] = ((v__122) < (v__123) ? (v__122) : (v__123));\n  float v__124 = ((float*)data_red_1)[0];\n  float v__125 = data_red_rf[20];\n  ((float*)data_red_1)[0] = ((v__124) < (v__125) ? (v__124) : (v__125));\n  float v__126 = ((float*)data_red_1)[0];\n  float v__127 = data_red_rf[21];\n  ((float*)data_red_1)[0] = ((v__126) < (v__127) ? (v__126) : (v__127));\n  float v__128 = ((float*)data_red_1)[0];\n  float v__129 = data_red_rf[22];\n  ((float*)data_red_1)[0] = ((v__128) < (v__129) ? (v__128) : (v__129));\n  float v__130 = ((float*)data_red_1)[0];\n  float v__131 = data_red_rf[23];\n  ((float*)data_red_1)[0] = ((v__130) < (v__131) ? (v__130) : (v__131));\n  float v__132 = ((float*)data_red_1)[0];\n  float v__133 = data_red_rf[24];\n  ((float*)data_red_1)[0] = ((v__132) < (v__133) ? (v__132) : (v__133));\n  float v__134 = ((float*)data_red_1)[0];\n  float v__135 = data_red_rf[25];\n  ((float*)data_red_1)[0] = ((v__134) < (v__135) ? (v__134) : (v__135));\n  float v__136 = ((float*)data_red_1)[0];\n  float v__137 = data_red_rf[26];\n  ((float*)data_red_1)[0] = ((v__136) < (v__137) ? (v__136) : (v__137));\n  float v__138 = ((float*)data_red_1)[0];\n  float v__139 = data_red_rf[27];\n  ((float*)data_red_1)[0] = ((v__138) < (v__139) ? (v__138) : (v__139));\n  float v__140 = ((float*)data_red_1)[0];\n  float v__141 = data_red_rf[28];\n  ((float*)data_red_1)[0] = ((v__140) < (v__141) ? (v__140) : (v__141));\n  float v__142 = ((float*)data_red_1)[0];\n  float v__143 = data_red_rf[29];\n  ((float*)data_red_1)[0] = ((v__142) < (v__143) ? (v__142) : (v__143));\n  float v__144 = ((float*)data_red_1)[0];\n  float v__145 = data_red_rf[30];\n  ((float*)data_red_1)[0] = ((v__144) < (v__145) ? (v__144) : (v__145));\n  float v__146 = ((float*)data_red_1)[0];\n  float v__147 = data_red_rf[31];\n  ((float*)data_red_1)[0] = ((v__146) < (v__147) ? (v__146) : (v__147));\n  float v__148 = ((float*)data_red_1)[0];\n  float v__149 = data_red_rf[32];\n  ((float*)data_red_1)[0] = ((v__148) < (v__149) ? (v__148) : (v__149));\n  float v__150 = ((float*)data_red_1)[0];\n  float v__151 = data_red_rf[33];\n  ((float*)data_red_1)[0] = ((v__150) < (v__151) ? (v__150) : (v__151));\n  float v__152 = ((float*)data_red_1)[0];\n  float v__153 = data_red_rf[34];\n  ((float*)data_red_1)[0] = ((v__152) < (v__153) ? (v__152) : (v__153));\n  float v__154 = ((float*)data_red_1)[0];\n  float v__155 = data_red_rf[35];\n  ((float*)data_red_1)[0] = ((v__154) < (v__155) ? (v__154) : (v__155));\n  float v__156 = ((float*)data_red_1)[0];\n  float v__157 = data_red_rf[36];\n  ((float*)data_red_1)[0] = ((v__156) < (v__157) ? (v__156) : (v__157));\n  float v__158 = ((float*)data_red_1)[0];\n  float v__159 = data_red_rf[37];\n  ((float*)data_red_1)[0] = ((v__158) < (v__159) ? (v__158) : (v__159));\n  float v__160 = ((float*)data_red_1)[0];\n  float v__161 = data_red_rf[38];\n  ((float*)data_red_1)[0] = ((v__160) < (v__161) ? (v__160) : (v__161));\n  float v__162 = ((float*)data_red_1)[0];\n  float v__163 = data_red_rf[39];\n  ((float*)data_red_1)[0] = ((v__162) < (v__163) ? (v__162) : (v__163));\n  float v__164 = ((float*)data_red_1)[0];\n  float v__165 = data_red_rf[40];\n  ((float*)data_red_1)[0] = ((v__164) < (v__165) ? (v__164) : (v__165));\n  float v__166 = ((float*)data_red_1)[0];\n  float v__167 = data_red_rf[41];\n  ((float*)data_red_1)[0] = ((v__166) < (v__167) ? (v__166) : (v__167));\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "min"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 4, 20, 8), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([20], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((20,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(20):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(128, 20):\n            data_1 = T.Buffer((2560,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 20 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(20):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 80; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[20];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 20; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 128; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 20; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      float v_ = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner];\n      float v__1 = ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 20) + k0_k1_fused_k2_fused_k3_fused_inner)];\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = ((v_) < (v__1) ? (v_) : (v__1));\n    }\n  }\n  ((float*)data_red_1)[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 20; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    float v__2 = ((float*)data_red_1)[0];\n    float v__3 = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v];\n    ((float*)data_red_1)[0] = ((v__2) < (v__3) ? (v__2) : (v__3));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "min"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 28, 12, 28), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([32], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((32,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(32):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(1176):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 32\n            data_1 = T.Buffer((37632,), data=data.data)\n            data_red_rf_1[0] = T.min(data_red_rf_1[0], data_1[cse_var_1])\n            data_red_rf_1[1] = T.min(data_red_rf_1[1], data_1[cse_var_1 + 1])\n            data_red_rf_1[2] = T.min(data_red_rf_1[2], data_1[cse_var_1 + 2])\n            data_red_rf_1[3] = T.min(data_red_rf_1[3], data_1[cse_var_1 + 3])\n            data_red_rf_1[4] = T.min(data_red_rf_1[4], data_1[cse_var_1 + 4])\n            data_red_rf_1[5] = T.min(data_red_rf_1[5], data_1[cse_var_1 + 5])\n            data_red_rf_1[6] = T.min(data_red_rf_1[6], data_1[cse_var_1 + 6])\n            data_red_rf_1[7] = T.min(data_red_rf_1[7], data_1[cse_var_1 + 7])\n            data_red_rf_1[8] = T.min(data_red_rf_1[8], data_1[cse_var_1 + 8])\n            data_red_rf_1[9] = T.min(data_red_rf_1[9], data_1[cse_var_1 + 9])\n            data_red_rf_1[10] = T.min(data_red_rf_1[10], data_1[cse_var_1 + 10])\n            data_red_rf_1[11] = T.min(data_red_rf_1[11], data_1[cse_var_1 + 11])\n            data_red_rf_1[12] = T.min(data_red_rf_1[12], data_1[cse_var_1 + 12])\n            data_red_rf_1[13] = T.min(data_red_rf_1[13], data_1[cse_var_1 + 13])\n            data_red_rf_1[14] = T.min(data_red_rf_1[14], data_1[cse_var_1 + 14])\n            data_red_rf_1[15] = T.min(data_red_rf_1[15], data_1[cse_var_1 + 15])\n            data_red_rf_1[16] = T.min(data_red_rf_1[16], data_1[cse_var_1 + 16])\n            data_red_rf_1[17] = T.min(data_red_rf_1[17], data_1[cse_var_1 + 17])\n            data_red_rf_1[18] = T.min(data_red_rf_1[18], data_1[cse_var_1 + 18])\n            data_red_rf_1[19] = T.min(data_red_rf_1[19], data_1[cse_var_1 + 19])\n            data_red_rf_1[20] = T.min(data_red_rf_1[20], data_1[cse_var_1 + 20])\n            data_red_rf_1[21] = T.min(data_red_rf_1[21], data_1[cse_var_1 + 21])\n            data_red_rf_1[22] = T.min(data_red_rf_1[22], data_1[cse_var_1 + 22])\n            data_red_rf_1[23] = T.min(data_red_rf_1[23], data_1[cse_var_1 + 23])\n            data_red_rf_1[24] = T.min(data_red_rf_1[24], data_1[cse_var_1 + 24])\n            data_red_rf_1[25] = T.min(data_red_rf_1[25], data_1[cse_var_1 + 25])\n            data_red_rf_1[26] = T.min(data_red_rf_1[26], data_1[cse_var_1 + 26])\n            data_red_rf_1[27] = T.min(data_red_rf_1[27], data_1[cse_var_1 + 27])\n            data_red_rf_1[28] = T.min(data_red_rf_1[28], data_1[cse_var_1 + 28])\n            data_red_rf_1[29] = T.min(data_red_rf_1[29], data_1[cse_var_1 + 29])\n            data_red_rf_1[30] = T.min(data_red_rf_1[30], data_1[cse_var_1 + 30])\n            data_red_rf_1[31] = T.min(data_red_rf_1[31], data_1[cse_var_1 + 31])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(32):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1176; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[32];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 32; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1176; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 32);\n    float v_ = data_red_rf[0];\n    float v__1 = ((float*)data_1)[cse_var_1];\n    data_red_rf[0] = ((v_) < (v__1) ? (v_) : (v__1));\n    float v__2 = data_red_rf[1];\n    float v__3 = ((float*)data_1)[(cse_var_1 + 1)];\n    data_red_rf[1] = ((v__2) < (v__3) ? (v__2) : (v__3));\n    float v__4 = data_red_rf[2];\n    float v__5 = ((float*)data_1)[(cse_var_1 + 2)];\n    data_red_rf[2] = ((v__4) < (v__5) ? (v__4) : (v__5));\n    float v__6 = data_red_rf[3];\n    float v__7 = ((float*)data_1)[(cse_var_1 + 3)];\n    data_red_rf[3] = ((v__6) < (v__7) ? (v__6) : (v__7));\n    float v__8 = data_red_rf[4];\n    float v__9 = ((float*)data_1)[(cse_var_1 + 4)];\n    data_red_rf[4] = ((v__8) < (v__9) ? (v__8) : (v__9));\n    float v__10 = data_red_rf[5];\n    float v__11 = ((float*)data_1)[(cse_var_1 + 5)];\n    data_red_rf[5] = ((v__10) < (v__11) ? (v__10) : (v__11));\n    float v__12 = data_red_rf[6];\n    float v__13 = ((float*)data_1)[(cse_var_1 + 6)];\n    data_red_rf[6] = ((v__12) < (v__13) ? (v__12) : (v__13));\n    float v__14 = data_red_rf[7];\n    float v__15 = ((float*)data_1)[(cse_var_1 + 7)];\n    data_red_rf[7] = ((v__14) < (v__15) ? (v__14) : (v__15));\n    float v__16 = data_red_rf[8];\n    float v__17 = ((float*)data_1)[(cse_var_1 + 8)];\n    data_red_rf[8] = ((v__16) < (v__17) ? (v__16) : (v__17));\n    float v__18 = data_red_rf[9];\n    float v__19 = ((float*)data_1)[(cse_var_1 + 9)];\n    data_red_rf[9] = ((v__18) < (v__19) ? (v__18) : (v__19));\n    float v__20 = data_red_rf[10];\n    float v__21 = ((float*)data_1)[(cse_var_1 + 10)];\n    data_red_rf[10] = ((v__20) < (v__21) ? (v__20) : (v__21));\n    float v__22 = data_red_rf[11];\n    float v__23 = ((float*)data_1)[(cse_var_1 + 11)];\n    data_red_rf[11] = ((v__22) < (v__23) ? (v__22) : (v__23));\n    float v__24 = data_red_rf[12];\n    float v__25 = ((float*)data_1)[(cse_var_1 + 12)];\n    data_red_rf[12] = ((v__24) < (v__25) ? (v__24) : (v__25));\n    float v__26 = data_red_rf[13];\n    float v__27 = ((float*)data_1)[(cse_var_1 + 13)];\n    data_red_rf[13] = ((v__26) < (v__27) ? (v__26) : (v__27));\n    float v__28 = data_red_rf[14];\n    float v__29 = ((float*)data_1)[(cse_var_1 + 14)];\n    data_red_rf[14] = ((v__28) < (v__29) ? (v__28) : (v__29));\n    float v__30 = data_red_rf[15];\n    float v__31 = ((float*)data_1)[(cse_var_1 + 15)];\n    data_red_rf[15] = ((v__30) < (v__31) ? (v__30) : (v__31));\n    float v__32 = data_red_rf[16];\n    float v__33 = ((float*)data_1)[(cse_var_1 + 16)];\n    data_red_rf[16] = ((v__32) < (v__33) ? (v__32) : (v__33));\n    float v__34 = data_red_rf[17];\n    float v__35 = ((float*)data_1)[(cse_var_1 + 17)];\n    data_red_rf[17] = ((v__34) < (v__35) ? (v__34) : (v__35));\n    float v__36 = data_red_rf[18];\n    float v__37 = ((float*)data_1)[(cse_var_1 + 18)];\n    data_red_rf[18] = ((v__36) < (v__37) ? (v__36) : (v__37));\n    float v__38 = data_red_rf[19];\n    float v__39 = ((float*)data_1)[(cse_var_1 + 19)];\n    data_red_rf[19] = ((v__38) < (v__39) ? (v__38) : (v__39));\n    float v__40 = data_red_rf[20];\n    float v__41 = ((float*)data_1)[(cse_var_1 + 20)];\n    data_red_rf[20] = ((v__40) < (v__41) ? (v__40) : (v__41));\n    float v__42 = data_red_rf[21];\n    float v__43 = ((float*)data_1)[(cse_var_1 + 21)];\n    data_red_rf[21] = ((v__42) < (v__43) ? (v__42) : (v__43));\n    float v__44 = data_red_rf[22];\n    float v__45 = ((float*)data_1)[(cse_var_1 + 22)];\n    data_red_rf[22] = ((v__44) < (v__45) ? (v__44) : (v__45));\n    float v__46 = data_red_rf[23];\n    float v__47 = ((float*)data_1)[(cse_var_1 + 23)];\n    data_red_rf[23] = ((v__46) < (v__47) ? (v__46) : (v__47));\n    float v__48 = data_red_rf[24];\n    float v__49 = ((float*)data_1)[(cse_var_1 + 24)];\n    data_red_rf[24] = ((v__48) < (v__49) ? (v__48) : (v__49));\n    float v__50 = data_red_rf[25];\n    float v__51 = ((float*)data_1)[(cse_var_1 + 25)];\n    data_red_rf[25] = ((v__50) < (v__51) ? (v__50) : (v__51));\n    float v__52 = data_red_rf[26];\n    float v__53 = ((float*)data_1)[(cse_var_1 + 26)];\n    data_red_rf[26] = ((v__52) < (v__53) ? (v__52) : (v__53));\n    float v__54 = data_red_rf[27];\n    float v__55 = ((float*)data_1)[(cse_var_1 + 27)];\n    data_red_rf[27] = ((v__54) < (v__55) ? (v__54) : (v__55));\n    float v__56 = data_red_rf[28];\n    float v__57 = ((float*)data_1)[(cse_var_1 + 28)];\n    data_red_rf[28] = ((v__56) < (v__57) ? (v__56) : (v__57));\n    float v__58 = data_red_rf[29];\n    float v__59 = ((float*)data_1)[(cse_var_1 + 29)];\n    data_red_rf[29] = ((v__58) < (v__59) ? (v__58) : (v__59));\n    float v__60 = data_red_rf[30];\n    float v__61 = ((float*)data_1)[(cse_var_1 + 30)];\n    data_red_rf[30] = ((v__60) < (v__61) ? (v__60) : (v__61));\n    float v__62 = data_red_rf[31];\n    float v__63 = ((float*)data_1)[(cse_var_1 + 31)];\n    data_red_rf[31] = ((v__62) < (v__63) ? (v__62) : (v__63));\n  }\n  ((float*)data_red_1)[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 32; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    float v__64 = ((float*)data_red_1)[0];\n    float v__65 = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v];\n    ((float*)data_red_1)[0] = ((v__64) < (v__65) ? (v__64) : (v__65));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "min"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 20, 24, 4), \"float32\"), compute: T.Buffer((4, 20, 24, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(20, 24):\n                cse_var_1: T.int32 = i0 * 1920 + i1 * 96 + i2 * 4\n                compute_1 = T.Buffer((7680,), data=compute.data)\n                data_1 = T.Buffer((7680,), data=data.data)\n                compute_1[cse_var_1:cse_var_1 + 4] = data_1[cse_var_1:cse_var_1 + 4] * T.Broadcast(T.float32(-1), 4)\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 24; ++i2) {\n        int32_t cse_var_1 = (((i0 * 1920) + (i1 * 96)) + (i2 * 4));\n        int32_t4 v_ = int32_t4((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3));\n        *(float4*)(((float*)compute_1) + cse_var_1) = ((float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3])) * ((float4)(-1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f)));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "negative"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 8, 36, 16), \"float32\"), compute: T.Buffer((10, 8, 36, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(8, 36):\n                cse_var_1: T.int32 = i0 * 4608 + i1 * 576 + i2 * 16\n                compute_1 = T.Buffer((46080,), data=compute.data)\n                data_1 = T.Buffer((46080,), data=data.data)\n                compute_1[cse_var_1:cse_var_1 + 16] = data_1[cse_var_1:cse_var_1 + 16] * T.Broadcast(T.float32(-1), 16)\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 36; ++i2) {\n        int32_t cse_var_1 = (((i0 * 4608) + (i1 * 576)) + (i2 * 16));\n        int32_t16 v_ = int32_t16((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7), (cse_var_1)+(1*8), (cse_var_1)+(1*9), (cse_var_1)+(1*10), (cse_var_1)+(1*11), (cse_var_1)+(1*12), (cse_var_1)+(1*13), (cse_var_1)+(1*14), (cse_var_1)+(1*15));\n        *(float16*)(((float*)compute_1) + cse_var_1) = ((float16(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb],((float*)data_1)[v_.sc],((float*)data_1)[v_.sd],((float*)data_1)[v_.se],((float*)data_1)[v_.sf])) * ((float16)(-1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f)));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "negative"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 40, 16, 8), \"float32\"), compute: T.Buffer((9, 40, 16, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(40, 16):\n                cse_var_1: T.int32 = i0 * 5120 + i1 * 128 + i2 * 8\n                compute_1 = T.Buffer((46080,), data=compute.data)\n                data_1 = T.Buffer((46080,), data=data.data)\n                compute_1[cse_var_1:cse_var_1 + 8] = data_1[cse_var_1:cse_var_1 + 8] * T.Broadcast(T.float32(-1), 8)\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 40; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        int32_t cse_var_1 = (((i0 * 5120) + (i1 * 128)) + (i2 * 8));\n        int32_t8 v_ = int32_t8((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7));\n        *(float8*)(((float*)compute_1) + cse_var_1) = ((float8(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7])) * ((float8)(-1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f)));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "negative"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 8, 12, 36), \"float32\"), compute: T.Buffer((7, 8, 12, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(7):\n            for i1, i2, i3 in T.grid(8, 12, 36):\n                cse_var_1: T.int32 = i0 * 3456 + i1 * 432 + i2 * 36 + i3\n                compute_1 = T.Buffer((24192,), data=compute.data)\n                data_1 = T.Buffer((24192,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        for (int32_t i3 = 0; i3 < 36; ++i3) {\n          int32_t cse_var_1 = ((((i0 * 3456) + (i1 * 432)) + (i2 * 36)) + i3);\n          ((float*)compute_1)[cse_var_1] = (((float*)data_1)[cse_var_1] * -1.000000e+00f);\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "negative"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 16, 32, 36), \"float32\"), compute: T.Buffer((9, 16, 32, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(144):\n            for i2, i3 in T.grid(32, 36):\n                cse_var_1: T.int32 = i0_i1_fused * 1152 + i2 * 36 + i3\n                compute_1 = T.Buffer((165888,), data=compute.data)\n                data_1 = T.Buffer((165888,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 144; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 32; ++i2) {\n      for (int32_t i3 = 0; i3 < 36; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 1152) + (i2 * 36)) + i3);\n        ((float*)compute_1)[cse_var_1] = (((float*)data_1)[cse_var_1] * -1.000000e+00f);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "negative"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 12, 20, 20), \"float32\"), compute: T.Buffer((9, 12, 20, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(108):\n            for i2, i3 in T.grid(20, 20):\n                cse_var_1: T.int32 = i0_i1_fused * 400 + i2 * 20 + i3\n                compute_1 = T.Buffer((43200,), data=compute.data)\n                data_1 = T.Buffer((43200,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 108; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 400) + (i2 * 20)) + i3);\n        ((float*)compute_1)[cse_var_1] = (((float*)data_1)[cse_var_1] * -1.000000e+00f);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "negative"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 24, 4, 28), \"float32\"), compute: T.Buffer((6, 24, 4, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(576):\n            for i3 in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                compute_1 = T.Buffer((16128,), data=compute.data)\n                data_1 = T.Buffer((16128,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 576; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 28) + i3);\n      ((float*)compute_1)[cse_var_1] = (((float*)data_1)[cse_var_1] * -1.000000e+00f);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "negative"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 20, 28, 8), \"float32\"), compute: T.Buffer((2, 20, 28, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(20, 28):\n                cse_var_1: T.int32 = i0 * 4480 + i1 * 224 + i2 * 8\n                compute_1 = T.Buffer((8960,), data=compute.data)\n                data_1 = T.Buffer((8960,), data=data.data)\n                compute_1[cse_var_1:cse_var_1 + 8] = data_1[cse_var_1:cse_var_1 + 8] * T.Broadcast(T.float32(-1), 8)\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 28; ++i2) {\n        int32_t cse_var_1 = (((i0 * 4480) + (i1 * 224)) + (i2 * 8));\n        int32_t8 v_ = int32_t8((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7));\n        *(float8*)(((float*)compute_1) + cse_var_1) = ((float8(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7])) * ((float8)(-1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f)));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "negative"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 4, 36, 24), \"float32\"), compute: T.Buffer((8, 4, 36, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(8):\n            for i1, i2, i3 in T.grid(4, 36, 24):\n                cse_var_1: T.int32 = i0 * 3456 + i1 * 864 + i2 * 24 + i3\n                compute_1 = T.Buffer((27648,), data=compute.data)\n                data_1 = T.Buffer((27648,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 36; ++i2) {\n        for (int32_t i3 = 0; i3 < 24; ++i3) {\n          int32_t cse_var_1 = ((((i0 * 3456) + (i1 * 864)) + (i2 * 24)) + i3);\n          ((float*)compute_1)[cse_var_1] = (((float*)data_1)[cse_var_1] * -1.000000e+00f);\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "negative"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 40, 8, 28), \"float32\"), compute: T.Buffer((3, 40, 8, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(26880):\n            compute_1 = T.Buffer((26880,), data=compute.data)\n            data_1 = T.Buffer((26880,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused] * T.float32(-1)\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 26880; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = (((float*)data_1)[i0_i1_fused_i2_fused_i3_fused] * -1.000000e+00f);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "negative"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 16, 32, 40), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([32], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((32,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(32):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(3200):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 32\n            data_1 = T.Buffer((102400,), data=data.data)\n            data_red_rf_1[0] = data_red_rf_1[0] * data_1[cse_var_1]\n            data_red_rf_1[1] = data_red_rf_1[1] * data_1[cse_var_1 + 1]\n            data_red_rf_1[2] = data_red_rf_1[2] * data_1[cse_var_1 + 2]\n            data_red_rf_1[3] = data_red_rf_1[3] * data_1[cse_var_1 + 3]\n            data_red_rf_1[4] = data_red_rf_1[4] * data_1[cse_var_1 + 4]\n            data_red_rf_1[5] = data_red_rf_1[5] * data_1[cse_var_1 + 5]\n            data_red_rf_1[6] = data_red_rf_1[6] * data_1[cse_var_1 + 6]\n            data_red_rf_1[7] = data_red_rf_1[7] * data_1[cse_var_1 + 7]\n            data_red_rf_1[8] = data_red_rf_1[8] * data_1[cse_var_1 + 8]\n            data_red_rf_1[9] = data_red_rf_1[9] * data_1[cse_var_1 + 9]\n            data_red_rf_1[10] = data_red_rf_1[10] * data_1[cse_var_1 + 10]\n            data_red_rf_1[11] = data_red_rf_1[11] * data_1[cse_var_1 + 11]\n            data_red_rf_1[12] = data_red_rf_1[12] * data_1[cse_var_1 + 12]\n            data_red_rf_1[13] = data_red_rf_1[13] * data_1[cse_var_1 + 13]\n            data_red_rf_1[14] = data_red_rf_1[14] * data_1[cse_var_1 + 14]\n            data_red_rf_1[15] = data_red_rf_1[15] * data_1[cse_var_1 + 15]\n            data_red_rf_1[16] = data_red_rf_1[16] * data_1[cse_var_1 + 16]\n            data_red_rf_1[17] = data_red_rf_1[17] * data_1[cse_var_1 + 17]\n            data_red_rf_1[18] = data_red_rf_1[18] * data_1[cse_var_1 + 18]\n            data_red_rf_1[19] = data_red_rf_1[19] * data_1[cse_var_1 + 19]\n            data_red_rf_1[20] = data_red_rf_1[20] * data_1[cse_var_1 + 20]\n            data_red_rf_1[21] = data_red_rf_1[21] * data_1[cse_var_1 + 21]\n            data_red_rf_1[22] = data_red_rf_1[22] * data_1[cse_var_1 + 22]\n            data_red_rf_1[23] = data_red_rf_1[23] * data_1[cse_var_1 + 23]\n            data_red_rf_1[24] = data_red_rf_1[24] * data_1[cse_var_1 + 24]\n            data_red_rf_1[25] = data_red_rf_1[25] * data_1[cse_var_1 + 25]\n            data_red_rf_1[26] = data_red_rf_1[26] * data_1[cse_var_1 + 26]\n            data_red_rf_1[27] = data_red_rf_1[27] * data_1[cse_var_1 + 27]\n            data_red_rf_1[28] = data_red_rf_1[28] * data_1[cse_var_1 + 28]\n            data_red_rf_1[29] = data_red_rf_1[29] * data_1[cse_var_1 + 29]\n            data_red_rf_1[30] = data_red_rf_1[30] * data_1[cse_var_1 + 30]\n            data_red_rf_1[31] = data_red_rf_1[31] * data_1[cse_var_1 + 31]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[0]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[1]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[2]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[3]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[4]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[5]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[6]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[7]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[8]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[9]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[10]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[11]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[12]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[13]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[14]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[15]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[16]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[17]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[18]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[19]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[20]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[21]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[22]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[23]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[24]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[25]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[26]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[27]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[28]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[29]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[30]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[31]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 1.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 3200; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[32];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 32; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 1.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 3200; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 32);\n    data_red_rf[0] = (data_red_rf[0] * ((float*)data_1)[cse_var_1]);\n    data_red_rf[1] = (data_red_rf[1] * ((float*)data_1)[(cse_var_1 + 1)]);\n    data_red_rf[2] = (data_red_rf[2] * ((float*)data_1)[(cse_var_1 + 2)]);\n    data_red_rf[3] = (data_red_rf[3] * ((float*)data_1)[(cse_var_1 + 3)]);\n    data_red_rf[4] = (data_red_rf[4] * ((float*)data_1)[(cse_var_1 + 4)]);\n    data_red_rf[5] = (data_red_rf[5] * ((float*)data_1)[(cse_var_1 + 5)]);\n    data_red_rf[6] = (data_red_rf[6] * ((float*)data_1)[(cse_var_1 + 6)]);\n    data_red_rf[7] = (data_red_rf[7] * ((float*)data_1)[(cse_var_1 + 7)]);\n    data_red_rf[8] = (data_red_rf[8] * ((float*)data_1)[(cse_var_1 + 8)]);\n    data_red_rf[9] = (data_red_rf[9] * ((float*)data_1)[(cse_var_1 + 9)]);\n    data_red_rf[10] = (data_red_rf[10] * ((float*)data_1)[(cse_var_1 + 10)]);\n    data_red_rf[11] = (data_red_rf[11] * ((float*)data_1)[(cse_var_1 + 11)]);\n    data_red_rf[12] = (data_red_rf[12] * ((float*)data_1)[(cse_var_1 + 12)]);\n    data_red_rf[13] = (data_red_rf[13] * ((float*)data_1)[(cse_var_1 + 13)]);\n    data_red_rf[14] = (data_red_rf[14] * ((float*)data_1)[(cse_var_1 + 14)]);\n    data_red_rf[15] = (data_red_rf[15] * ((float*)data_1)[(cse_var_1 + 15)]);\n    data_red_rf[16] = (data_red_rf[16] * ((float*)data_1)[(cse_var_1 + 16)]);\n    data_red_rf[17] = (data_red_rf[17] * ((float*)data_1)[(cse_var_1 + 17)]);\n    data_red_rf[18] = (data_red_rf[18] * ((float*)data_1)[(cse_var_1 + 18)]);\n    data_red_rf[19] = (data_red_rf[19] * ((float*)data_1)[(cse_var_1 + 19)]);\n    data_red_rf[20] = (data_red_rf[20] * ((float*)data_1)[(cse_var_1 + 20)]);\n    data_red_rf[21] = (data_red_rf[21] * ((float*)data_1)[(cse_var_1 + 21)]);\n    data_red_rf[22] = (data_red_rf[22] * ((float*)data_1)[(cse_var_1 + 22)]);\n    data_red_rf[23] = (data_red_rf[23] * ((float*)data_1)[(cse_var_1 + 23)]);\n    data_red_rf[24] = (data_red_rf[24] * ((float*)data_1)[(cse_var_1 + 24)]);\n    data_red_rf[25] = (data_red_rf[25] * ((float*)data_1)[(cse_var_1 + 25)]);\n    data_red_rf[26] = (data_red_rf[26] * ((float*)data_1)[(cse_var_1 + 26)]);\n    data_red_rf[27] = (data_red_rf[27] * ((float*)data_1)[(cse_var_1 + 27)]);\n    data_red_rf[28] = (data_red_rf[28] * ((float*)data_1)[(cse_var_1 + 28)]);\n    data_red_rf[29] = (data_red_rf[29] * ((float*)data_1)[(cse_var_1 + 29)]);\n    data_red_rf[30] = (data_red_rf[30] * ((float*)data_1)[(cse_var_1 + 30)]);\n    data_red_rf[31] = (data_red_rf[31] * ((float*)data_1)[(cse_var_1 + 31)]);\n  }\n  ((float*)data_red_1)[0] = 1.000000e+00f;\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[0]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[1]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[2]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[3]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[4]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[5]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[6]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[7]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[8]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[9]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[10]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[11]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[12]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[13]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[14]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[15]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[16]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[17]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[18]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[19]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[20]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[21]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[22]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[23]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[24]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[25]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[26]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[27]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[28]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[29]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[30]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[31]);\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "prod"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 24, 24, 4), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([18], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((18,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(18):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(1152, 18):\n            data_1 = T.Buffer((20736,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] * data_1[k0_k1_fused_k2_fused_k3_fused_outer * 18 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[0]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[1]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[2]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[3]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[4]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[5]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[6]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[7]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[8]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[9]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[10]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[11]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[12]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[13]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[14]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[15]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[16]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[17]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 1.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 648; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[18];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 18; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 1.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1152; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 18; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] * ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 18) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  ((float*)data_red_1)[0] = 1.000000e+00f;\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[0]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[1]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[2]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[3]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[4]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[5]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[6]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[7]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[8]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[9]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[10]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[11]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[12]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[13]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[14]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[15]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[16]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[17]);\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "prod"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 8, 12, 12), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([32], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((32,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(32):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(72, 32):\n            data_1 = T.Buffer((2304,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] * data_1[k0_k1_fused_k2_fused_k3_fused_outer * 32 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[0]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[1]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[2]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[3]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[4]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[5]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[6]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[7]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[8]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[9]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[10]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[11]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[12]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[13]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[14]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[15]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[16]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[17]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[18]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[19]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[20]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[21]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[22]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[23]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[24]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[25]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[26]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[27]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[28]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[29]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[30]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[31]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 1.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 72; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[32];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 32; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 1.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 72; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 32; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] * ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  ((float*)data_red_1)[0] = 1.000000e+00f;\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[0]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[1]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[2]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[3]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[4]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[5]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[6]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[7]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[8]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[9]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[10]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[11]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[12]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[13]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[14]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[15]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[16]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[17]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[18]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[19]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[20]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[21]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[22]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[23]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[24]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[25]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[26]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[27]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[28]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[29]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[30]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[31]);\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "prod"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 12, 36, 28), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([3888], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((3888,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_outer in T.parallel(3888):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer] = T.float32(1)\n            for k0_k1_fused_k2_fused_k3_fused_inner in range(28):\n                data_1 = T.Buffer((108864,), data=data.data)\n                data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer] * data_1[k0_k1_fused_k2_fused_k3_fused_outer * 28 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer_v in range(3888):\n            data_red_1[0] = data_red_1[0] * data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer_v]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 1.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 3402; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  void* data_red_rf = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)15552, 2, 32);\n  if (data_red_rf == NULL) {\n    return -1;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 3888; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    ((float*)data_red_rf)[k0_k1_fused_k2_fused_k3_fused_outer] = 1.000000e+00f;\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 28; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      ((float*)data_red_rf)[k0_k1_fused_k2_fused_k3_fused_outer] = (((float*)data_red_rf)[k0_k1_fused_k2_fused_k3_fused_outer] * ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 28) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  ((float*)data_red_1)[0] = 1.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer_v = 0; k0_k1_fused_k2_fused_k3_fused_outer_v < 3888; ++k0_k1_fused_k2_fused_k3_fused_outer_v) {\n    ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * ((float*)data_red_rf)[k0_k1_fused_k2_fused_k3_fused_outer_v]);\n  }\n  if (TVMBackendFreeWorkspace(1, dev_id, data_red_rf) != 0) {\n    return -1;\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "prod"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 16, 40, 28), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([64], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((64,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(64):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(2800):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 64\n            data_1 = T.Buffer((179200,), data=data.data)\n            data_red_rf_1[0] = data_red_rf_1[0] * data_1[cse_var_1]\n            data_red_rf_1[1] = data_red_rf_1[1] * data_1[cse_var_1 + 1]\n            data_red_rf_1[2] = data_red_rf_1[2] * data_1[cse_var_1 + 2]\n            data_red_rf_1[3] = data_red_rf_1[3] * data_1[cse_var_1 + 3]\n            data_red_rf_1[4] = data_red_rf_1[4] * data_1[cse_var_1 + 4]\n            data_red_rf_1[5] = data_red_rf_1[5] * data_1[cse_var_1 + 5]\n            data_red_rf_1[6] = data_red_rf_1[6] * data_1[cse_var_1 + 6]\n            data_red_rf_1[7] = data_red_rf_1[7] * data_1[cse_var_1 + 7]\n            data_red_rf_1[8] = data_red_rf_1[8] * data_1[cse_var_1 + 8]\n            data_red_rf_1[9] = data_red_rf_1[9] * data_1[cse_var_1 + 9]\n            data_red_rf_1[10] = data_red_rf_1[10] * data_1[cse_var_1 + 10]\n            data_red_rf_1[11] = data_red_rf_1[11] * data_1[cse_var_1 + 11]\n            data_red_rf_1[12] = data_red_rf_1[12] * data_1[cse_var_1 + 12]\n            data_red_rf_1[13] = data_red_rf_1[13] * data_1[cse_var_1 + 13]\n            data_red_rf_1[14] = data_red_rf_1[14] * data_1[cse_var_1 + 14]\n            data_red_rf_1[15] = data_red_rf_1[15] * data_1[cse_var_1 + 15]\n            data_red_rf_1[16] = data_red_rf_1[16] * data_1[cse_var_1 + 16]\n            data_red_rf_1[17] = data_red_rf_1[17] * data_1[cse_var_1 + 17]\n            data_red_rf_1[18] = data_red_rf_1[18] * data_1[cse_var_1 + 18]\n            data_red_rf_1[19] = data_red_rf_1[19] * data_1[cse_var_1 + 19]\n            data_red_rf_1[20] = data_red_rf_1[20] * data_1[cse_var_1 + 20]\n            data_red_rf_1[21] = data_red_rf_1[21] * data_1[cse_var_1 + 21]\n            data_red_rf_1[22] = data_red_rf_1[22] * data_1[cse_var_1 + 22]\n            data_red_rf_1[23] = data_red_rf_1[23] * data_1[cse_var_1 + 23]\n            data_red_rf_1[24] = data_red_rf_1[24] * data_1[cse_var_1 + 24]\n            data_red_rf_1[25] = data_red_rf_1[25] * data_1[cse_var_1 + 25]\n            data_red_rf_1[26] = data_red_rf_1[26] * data_1[cse_var_1 + 26]\n            data_red_rf_1[27] = data_red_rf_1[27] * data_1[cse_var_1 + 27]\n            data_red_rf_1[28] = data_red_rf_1[28] * data_1[cse_var_1 + 28]\n            data_red_rf_1[29] = data_red_rf_1[29] * data_1[cse_var_1 + 29]\n            data_red_rf_1[30] = data_red_rf_1[30] * data_1[cse_var_1 + 30]\n            data_red_rf_1[31] = data_red_rf_1[31] * data_1[cse_var_1 + 31]\n            data_red_rf_1[32] = data_red_rf_1[32] * data_1[cse_var_1 + 32]\n            data_red_rf_1[33] = data_red_rf_1[33] * data_1[cse_var_1 + 33]\n            data_red_rf_1[34] = data_red_rf_1[34] * data_1[cse_var_1 + 34]\n            data_red_rf_1[35] = data_red_rf_1[35] * data_1[cse_var_1 + 35]\n            data_red_rf_1[36] = data_red_rf_1[36] * data_1[cse_var_1 + 36]\n            data_red_rf_1[37] = data_red_rf_1[37] * data_1[cse_var_1 + 37]\n            data_red_rf_1[38] = data_red_rf_1[38] * data_1[cse_var_1 + 38]\n            data_red_rf_1[39] = data_red_rf_1[39] * data_1[cse_var_1 + 39]\n            data_red_rf_1[40] = data_red_rf_1[40] * data_1[cse_var_1 + 40]\n            data_red_rf_1[41] = data_red_rf_1[41] * data_1[cse_var_1 + 41]\n            data_red_rf_1[42] = data_red_rf_1[42] * data_1[cse_var_1 + 42]\n            data_red_rf_1[43] = data_red_rf_1[43] * data_1[cse_var_1 + 43]\n            data_red_rf_1[44] = data_red_rf_1[44] * data_1[cse_var_1 + 44]\n            data_red_rf_1[45] = data_red_rf_1[45] * data_1[cse_var_1 + 45]\n            data_red_rf_1[46] = data_red_rf_1[46] * data_1[cse_var_1 + 46]\n            data_red_rf_1[47] = data_red_rf_1[47] * data_1[cse_var_1 + 47]\n            data_red_rf_1[48] = data_red_rf_1[48] * data_1[cse_var_1 + 48]\n            data_red_rf_1[49] = data_red_rf_1[49] * data_1[cse_var_1 + 49]\n            data_red_rf_1[50] = data_red_rf_1[50] * data_1[cse_var_1 + 50]\n            data_red_rf_1[51] = data_red_rf_1[51] * data_1[cse_var_1 + 51]\n            data_red_rf_1[52] = data_red_rf_1[52] * data_1[cse_var_1 + 52]\n            data_red_rf_1[53] = data_red_rf_1[53] * data_1[cse_var_1 + 53]\n            data_red_rf_1[54] = data_red_rf_1[54] * data_1[cse_var_1 + 54]\n            data_red_rf_1[55] = data_red_rf_1[55] * data_1[cse_var_1 + 55]\n            data_red_rf_1[56] = data_red_rf_1[56] * data_1[cse_var_1 + 56]\n            data_red_rf_1[57] = data_red_rf_1[57] * data_1[cse_var_1 + 57]\n            data_red_rf_1[58] = data_red_rf_1[58] * data_1[cse_var_1 + 58]\n            data_red_rf_1[59] = data_red_rf_1[59] * data_1[cse_var_1 + 59]\n            data_red_rf_1[60] = data_red_rf_1[60] * data_1[cse_var_1 + 60]\n            data_red_rf_1[61] = data_red_rf_1[61] * data_1[cse_var_1 + 61]\n            data_red_rf_1[62] = data_red_rf_1[62] * data_1[cse_var_1 + 62]\n            data_red_rf_1[63] = data_red_rf_1[63] * data_1[cse_var_1 + 63]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[0]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[1]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[2]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[3]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[4]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[5]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[6]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[7]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[8]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[9]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[10]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[11]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[12]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[13]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[14]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[15]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[16]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[17]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[18]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[19]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[20]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[21]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[22]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[23]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[24]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[25]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[26]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[27]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[28]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[29]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[30]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[31]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[32]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[33]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[34]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[35]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[36]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[37]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[38]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[39]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[40]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[41]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[42]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[43]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[44]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[45]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[46]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[47]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[48]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[49]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[50]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[51]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[52]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[53]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[54]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[55]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[56]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[57]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[58]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[59]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[60]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[61]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[62]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[63]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 1.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 5600; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[64];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 1.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 2800; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 64);\n    data_red_rf[0] = (data_red_rf[0] * ((float*)data_1)[cse_var_1]);\n    data_red_rf[1] = (data_red_rf[1] * ((float*)data_1)[(cse_var_1 + 1)]);\n    data_red_rf[2] = (data_red_rf[2] * ((float*)data_1)[(cse_var_1 + 2)]);\n    data_red_rf[3] = (data_red_rf[3] * ((float*)data_1)[(cse_var_1 + 3)]);\n    data_red_rf[4] = (data_red_rf[4] * ((float*)data_1)[(cse_var_1 + 4)]);\n    data_red_rf[5] = (data_red_rf[5] * ((float*)data_1)[(cse_var_1 + 5)]);\n    data_red_rf[6] = (data_red_rf[6] * ((float*)data_1)[(cse_var_1 + 6)]);\n    data_red_rf[7] = (data_red_rf[7] * ((float*)data_1)[(cse_var_1 + 7)]);\n    data_red_rf[8] = (data_red_rf[8] * ((float*)data_1)[(cse_var_1 + 8)]);\n    data_red_rf[9] = (data_red_rf[9] * ((float*)data_1)[(cse_var_1 + 9)]);\n    data_red_rf[10] = (data_red_rf[10] * ((float*)data_1)[(cse_var_1 + 10)]);\n    data_red_rf[11] = (data_red_rf[11] * ((float*)data_1)[(cse_var_1 + 11)]);\n    data_red_rf[12] = (data_red_rf[12] * ((float*)data_1)[(cse_var_1 + 12)]);\n    data_red_rf[13] = (data_red_rf[13] * ((float*)data_1)[(cse_var_1 + 13)]);\n    data_red_rf[14] = (data_red_rf[14] * ((float*)data_1)[(cse_var_1 + 14)]);\n    data_red_rf[15] = (data_red_rf[15] * ((float*)data_1)[(cse_var_1 + 15)]);\n    data_red_rf[16] = (data_red_rf[16] * ((float*)data_1)[(cse_var_1 + 16)]);\n    data_red_rf[17] = (data_red_rf[17] * ((float*)data_1)[(cse_var_1 + 17)]);\n    data_red_rf[18] = (data_red_rf[18] * ((float*)data_1)[(cse_var_1 + 18)]);\n    data_red_rf[19] = (data_red_rf[19] * ((float*)data_1)[(cse_var_1 + 19)]);\n    data_red_rf[20] = (data_red_rf[20] * ((float*)data_1)[(cse_var_1 + 20)]);\n    data_red_rf[21] = (data_red_rf[21] * ((float*)data_1)[(cse_var_1 + 21)]);\n    data_red_rf[22] = (data_red_rf[22] * ((float*)data_1)[(cse_var_1 + 22)]);\n    data_red_rf[23] = (data_red_rf[23] * ((float*)data_1)[(cse_var_1 + 23)]);\n    data_red_rf[24] = (data_red_rf[24] * ((float*)data_1)[(cse_var_1 + 24)]);\n    data_red_rf[25] = (data_red_rf[25] * ((float*)data_1)[(cse_var_1 + 25)]);\n    data_red_rf[26] = (data_red_rf[26] * ((float*)data_1)[(cse_var_1 + 26)]);\n    data_red_rf[27] = (data_red_rf[27] * ((float*)data_1)[(cse_var_1 + 27)]);\n    data_red_rf[28] = (data_red_rf[28] * ((float*)data_1)[(cse_var_1 + 28)]);\n    data_red_rf[29] = (data_red_rf[29] * ((float*)data_1)[(cse_var_1 + 29)]);\n    data_red_rf[30] = (data_red_rf[30] * ((float*)data_1)[(cse_var_1 + 30)]);\n    data_red_rf[31] = (data_red_rf[31] * ((float*)data_1)[(cse_var_1 + 31)]);\n    data_red_rf[32] = (data_red_rf[32] * ((float*)data_1)[(cse_var_1 + 32)]);\n    data_red_rf[33] = (data_red_rf[33] * ((float*)data_1)[(cse_var_1 + 33)]);\n    data_red_rf[34] = (data_red_rf[34] * ((float*)data_1)[(cse_var_1 + 34)]);\n    data_red_rf[35] = (data_red_rf[35] * ((float*)data_1)[(cse_var_1 + 35)]);\n    data_red_rf[36] = (data_red_rf[36] * ((float*)data_1)[(cse_var_1 + 36)]);\n    data_red_rf[37] = (data_red_rf[37] * ((float*)data_1)[(cse_var_1 + 37)]);\n    data_red_rf[38] = (data_red_rf[38] * ((float*)data_1)[(cse_var_1 + 38)]);\n    data_red_rf[39] = (data_red_rf[39] * ((float*)data_1)[(cse_var_1 + 39)]);\n    data_red_rf[40] = (data_red_rf[40] * ((float*)data_1)[(cse_var_1 + 40)]);\n    data_red_rf[41] = (data_red_rf[41] * ((float*)data_1)[(cse_var_1 + 41)]);\n    data_red_rf[42] = (data_red_rf[42] * ((float*)data_1)[(cse_var_1 + 42)]);\n    data_red_rf[43] = (data_red_rf[43] * ((float*)data_1)[(cse_var_1 + 43)]);\n    data_red_rf[44] = (data_red_rf[44] * ((float*)data_1)[(cse_var_1 + 44)]);\n    data_red_rf[45] = (data_red_rf[45] * ((float*)data_1)[(cse_var_1 + 45)]);\n    data_red_rf[46] = (data_red_rf[46] * ((float*)data_1)[(cse_var_1 + 46)]);\n    data_red_rf[47] = (data_red_rf[47] * ((float*)data_1)[(cse_var_1 + 47)]);\n    data_red_rf[48] = (data_red_rf[48] * ((float*)data_1)[(cse_var_1 + 48)]);\n    data_red_rf[49] = (data_red_rf[49] * ((float*)data_1)[(cse_var_1 + 49)]);\n    data_red_rf[50] = (data_red_rf[50] * ((float*)data_1)[(cse_var_1 + 50)]);\n    data_red_rf[51] = (data_red_rf[51] * ((float*)data_1)[(cse_var_1 + 51)]);\n    data_red_rf[52] = (data_red_rf[52] * ((float*)data_1)[(cse_var_1 + 52)]);\n    data_red_rf[53] = (data_red_rf[53] * ((float*)data_1)[(cse_var_1 + 53)]);\n    data_red_rf[54] = (data_red_rf[54] * ((float*)data_1)[(cse_var_1 + 54)]);\n    data_red_rf[55] = (data_red_rf[55] * ((float*)data_1)[(cse_var_1 + 55)]);\n    data_red_rf[56] = (data_red_rf[56] * ((float*)data_1)[(cse_var_1 + 56)]);\n    data_red_rf[57] = (data_red_rf[57] * ((float*)data_1)[(cse_var_1 + 57)]);\n    data_red_rf[58] = (data_red_rf[58] * ((float*)data_1)[(cse_var_1 + 58)]);\n    data_red_rf[59] = (data_red_rf[59] * ((float*)data_1)[(cse_var_1 + 59)]);\n    data_red_rf[60] = (data_red_rf[60] * ((float*)data_1)[(cse_var_1 + 60)]);\n    data_red_rf[61] = (data_red_rf[61] * ((float*)data_1)[(cse_var_1 + 61)]);\n    data_red_rf[62] = (data_red_rf[62] * ((float*)data_1)[(cse_var_1 + 62)]);\n    data_red_rf[63] = (data_red_rf[63] * ((float*)data_1)[(cse_var_1 + 63)]);\n  }\n  ((float*)data_red_1)[0] = 1.000000e+00f;\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[0]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[1]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[2]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[3]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[4]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[5]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[6]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[7]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[8]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[9]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[10]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[11]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[12]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[13]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[14]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[15]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[16]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[17]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[18]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[19]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[20]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[21]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[22]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[23]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[24]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[25]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[26]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[27]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[28]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[29]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[30]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[31]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[32]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[33]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[34]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[35]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[36]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[37]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[38]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[39]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[40]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[41]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[42]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[43]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[44]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[45]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[46]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[47]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[48]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[49]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[50]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[51]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[52]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[53]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[54]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[55]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[56]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[57]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[58]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[59]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[60]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[61]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[62]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[63]);\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "prod"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 24, 28, 28), \"float32\"), compute: T.Buffer((10, 24, 28, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(6720):\n            for i3 in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                compute_1 = T.Buffer((188160,), data=compute.data)\n                data_1 = T.Buffer((188160,), data=data.data)\n                compute_1[cse_var_1] = T.round(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float roundf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 6720; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 28) + i3);\n      ((float*)compute_1)[cse_var_1] = roundf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "round"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 4, 32, 8), \"float32\"), compute: T.Buffer((3, 4, 32, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3072):\n            compute_1 = T.Buffer((3072,), data=compute.data)\n            data_1 = T.Buffer((3072,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.round(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float roundf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3072; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = roundf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "round"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 24, 4, 16), \"float32\"), compute: T.Buffer((5, 24, 4, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            for i3 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 16 + i3\n                compute_1 = T.Buffer((7680,), data=compute.data)\n                data_1 = T.Buffer((7680,), data=data.data)\n                compute_1[cse_var_1] = T.round(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float roundf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 16; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 16) + i3);\n      ((float*)compute_1)[cse_var_1] = roundf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "round"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 8, 12, 28), \"float32\"), compute: T.Buffer((7, 8, 12, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(18816):\n            compute_1 = T.Buffer((18816,), data=compute.data)\n            data_1 = T.Buffer((18816,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.round(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float roundf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 18816; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = roundf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "round"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 36, 8, 20), \"float32\"), compute: T.Buffer((8, 36, 8, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(46080):\n            compute_1 = T.Buffer((46080,), data=compute.data)\n            data_1 = T.Buffer((46080,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.round(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float roundf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 46080; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = roundf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "round"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 24, 36, 28), \"float32\"), compute: T.Buffer((10, 24, 36, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(8640):\n            for i3 in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                compute_1 = T.Buffer((241920,), data=compute.data)\n                data_1 = T.Buffer((241920,), data=data.data)\n                compute_1[cse_var_1] = T.round(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float roundf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 8640; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 28) + i3);\n      ((float*)compute_1)[cse_var_1] = roundf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "round"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 32, 16, 28), \"float32\"), compute: T.Buffer((4, 32, 16, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(128):\n            for i2, i3 in T.grid(16, 28):\n                cse_var_1: T.int32 = i0_i1_fused * 448 + i2 * 28 + i3\n                compute_1 = T.Buffer((57344,), data=compute.data)\n                data_1 = T.Buffer((57344,), data=data.data)\n                compute_1[cse_var_1] = T.round(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float roundf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 128; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      for (int32_t i3 = 0; i3 < 28; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 448) + (i2 * 28)) + i3);\n        ((float*)compute_1)[cse_var_1] = roundf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "round"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 8, 16, 12), \"float32\"), compute: T.Buffer((3, 8, 16, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(24):\n            for i2, i3 in T.grid(16, 12):\n                cse_var_1: T.int32 = i0_i1_fused * 192 + i2 * 12 + i3\n                compute_1 = T.Buffer((4608,), data=compute.data)\n                data_1 = T.Buffer((4608,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 24; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      for (int32_t i3 = 0; i3 < 12; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 192) + (i2 * 12)) + i3);\n        ((float*)compute_1)[cse_var_1] = (1.000000e+00f / sqrtf(((float*)data_1)[cse_var_1]));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 16, 24, 28), \"float32\"), compute: T.Buffer((4, 16, 24, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(43008):\n            compute_1 = T.Buffer((43008,), data=compute.data)\n            data_1 = T.Buffer((43008,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.rsqrt(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 43008; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = (1.000000e+00f / sqrtf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 8, 28, 12), \"float32\"), compute: T.Buffer((1, 8, 28, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(8, 28, 12):\n            cse_var_1: T.int32 = i1 * 336 + i2 * 12 + i3\n            compute_1 = T.Buffer((2688,), data=compute.data)\n            data_1 = T.Buffer((2688,), data=data.data)\n            compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i1 = 0; i1 < 8; ++i1) {\n    for (int32_t i2 = 0; i2 < 28; ++i2) {\n      for (int32_t i3 = 0; i3 < 12; ++i3) {\n        int32_t cse_var_1 = (((i1 * 336) + (i2 * 12)) + i3);\n        ((float*)compute_1)[cse_var_1] = (1.000000e+00f / sqrtf(((float*)data_1)[cse_var_1]));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 4, 32, 4), \"float32\"), compute: T.Buffer((4, 4, 32, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(16):\n            for i2, i3 in T.grid(32, 4):\n                cse_var_1: T.int32 = i0_i1_fused * 128 + i2 * 4 + i3\n                compute_1 = T.Buffer((2048,), data=compute.data)\n                data_1 = T.Buffer((2048,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 16; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 32; ++i2) {\n      for (int32_t i3 = 0; i3 < 4; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 128) + (i2 * 4)) + i3);\n        ((float*)compute_1)[cse_var_1] = (1.000000e+00f / sqrtf(((float*)data_1)[cse_var_1]));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 24, 4, 4), \"float32\"), compute: T.Buffer((7, 24, 4, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(7):\n            for i1, i2, i3 in T.grid(24, 4, 4):\n                cse_var_1: T.int32 = i0 * 384 + i1 * 16 + i2 * 4 + i3\n                compute_1 = T.Buffer((2688,), data=compute.data)\n                data_1 = T.Buffer((2688,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 24; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        for (int32_t i3 = 0; i3 < 4; ++i3) {\n          int32_t cse_var_1 = ((((i0 * 384) + (i1 * 16)) + (i2 * 4)) + i3);\n          ((float*)compute_1)[cse_var_1] = (1.000000e+00f / sqrtf(((float*)data_1)[cse_var_1]));\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 20, 40, 36), \"float32\"), compute: T.Buffer((10, 20, 40, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(200):\n            for i2, i3 in T.grid(40, 36):\n                cse_var_1: T.int32 = i0_i1_fused * 1440 + i2 * 36 + i3\n                compute_1 = T.Buffer((288000,), data=compute.data)\n                data_1 = T.Buffer((288000,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 200; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 40; ++i2) {\n      for (int32_t i3 = 0; i3 < 36; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 1440) + (i2 * 36)) + i3);\n        ((float*)compute_1)[cse_var_1] = (1.000000e+00f / sqrtf(((float*)data_1)[cse_var_1]));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 36, 28, 16), \"float32\"), compute: T.Buffer((3, 36, 28, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(108):\n            for i2, i3_s in T.grid(28, 16):\n                cse_var_1: T.int32 = i0_i1_fused * 448 + i2 * 16 + i3_s\n                compute_1 = T.Buffer((48384,), data=compute.data)\n                data_1 = T.Buffer((48384,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 108; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 28; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 16; ++i3_s) {\n        int32_t cse_var_1 = (((i0_i1_fused * 448) + (i2 * 16)) + i3_s);\n        ((float*)compute_1)[cse_var_1] = (1.000000e+00f / sqrtf(((float*)data_1)[cse_var_1]));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 24, 20, 8), \"float32\"), compute: T.Buffer((9, 24, 20, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(4320):\n            for i3_s in range(8):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 8 + i3_s\n                compute_1 = T.Buffer((34560,), data=compute.data)\n                data_1 = T.Buffer((34560,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4320; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 8) + i3_s);\n      ((float*)compute_1)[cse_var_1] = (1.000000e+00f / sqrtf(((float*)data_1)[cse_var_1]));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 24, 8, 8), \"float32\"), compute: T.Buffer((5, 24, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(960):\n            for i3_s in range(8):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 8 + i3_s\n                compute_1 = T.Buffer((7680,), data=compute.data)\n                data_1 = T.Buffer((7680,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 960; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 8) + i3_s);\n      ((float*)compute_1)[cse_var_1] = (1.000000e+00f / sqrtf(((float*)data_1)[cse_var_1]));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 36, 40, 32), \"float32\"), compute: T.Buffer((5, 36, 40, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(180):\n            for i2, i3 in T.grid(40, 32):\n                cse_var_1: T.int32 = i0_i1_fused * 1280 + i2 * 32 + i3\n                compute_1 = T.Buffer((230400,), data=compute.data)\n                data_1 = T.Buffer((230400,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 180; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 40; ++i2) {\n      for (int32_t i3 = 0; i3 < 32; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 1280) + (i2 * 32)) + i3);\n        ((float*)compute_1)[cse_var_1] = (1.000000e+00f / sqrtf(((float*)data_1)[cse_var_1]));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "rsqrt"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 24, 12, 16), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 16, T.if_then_else(ax0 == 2, 12, T.if_then_else(ax0 == 1, 24, 1)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 16 : ((((int)threadIdx.x) == 2) ? 12 : ((((int)threadIdx.x) == 1) ? 24 : 1)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 16 : ((ax0 == 2) ? 12 : ((ax0 == 1) ? 24 : 1)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 4, 24, 4), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 4, T.if_then_else(ax0 == 2, 24, T.if_then_else(ax0 == 1, 4, 4)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 4 : ((((int)threadIdx.x) == 2) ? 24 : ((((int)threadIdx.x) == 1) ? 4 : 4)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 4 : ((ax0 == 2) ? 24 : ((ax0 == 1) ? 4 : 4)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 20, 40, 4), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 4, T.if_then_else(ax0 == 2, 40, T.if_then_else(ax0 == 1, 20, 8)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 4 : ((((int)threadIdx.x) == 2) ? 40 : ((((int)threadIdx.x) == 1) ? 20 : 8)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 4 : ((ax0 == 2) ? 40 : ((ax0 == 1) ? 20 : 8)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 20, 4, 24), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 24, T.if_then_else(ax0 == 2, 4, T.if_then_else(ax0 == 1, 20, 8)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 24 : ((((int)threadIdx.x) == 2) ? 4 : ((((int)threadIdx.x) == 1) ? 20 : 8)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 24 : ((ax0 == 2) ? 4 : ((ax0 == 1) ? 20 : 8)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 4, 20, 28), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 28, T.if_then_else(ax0 == 2, 20, T.if_then_else(ax0 == 1, 4, 1)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 28 : ((((int)threadIdx.x) == 2) ? 20 : ((((int)threadIdx.x) == 1) ? 4 : 1)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 28 : ((ax0 == 2) ? 20 : ((ax0 == 1) ? 4 : 1)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 40, 36, 28), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 28, T.if_then_else(ax0 == 2, 36, T.if_then_else(ax0 == 1, 40, 5)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 28 : ((((int)threadIdx.x) == 2) ? 36 : ((((int)threadIdx.x) == 1) ? 40 : 5)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 28 : ((ax0 == 2) ? 36 : ((ax0 == 1) ? 40 : 5)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 24, 40, 28), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 28, T.if_then_else(ax0 == 2, 40, T.if_then_else(ax0 == 1, 24, 6)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 28 : ((((int)threadIdx.x) == 2) ? 40 : ((((int)threadIdx.x) == 1) ? 24 : 6)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 28 : ((ax0 == 2) ? 40 : ((ax0 == 1) ? 24 : 6)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 24, 40, 24), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 24, T.if_then_else(ax0 == 2, 40, T.if_then_else(ax0 == 1, 24, 6)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 24 : ((((int)threadIdx.x) == 2) ? 40 : ((((int)threadIdx.x) == 1) ? 24 : 6)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 24 : ((ax0 == 2) ? 40 : ((ax0 == 1) ? 24 : 6)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 4, 20, 28), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 28, T.if_then_else(ax0 == 2, 20, T.if_then_else(ax0 == 1, 4, 1)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 28 : ((((int)threadIdx.x) == 2) ? 20 : ((((int)threadIdx.x) == 1) ? 4 : 1)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 28 : ((ax0 == 2) ? 20 : ((ax0 == 1) ? 4 : 1)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 12, 32, 40), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 40, T.if_then_else(ax0 == 2, 32, T.if_then_else(ax0 == 1, 12, 8)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 40 : ((((int)threadIdx.x) == 2) ? 32 : ((((int)threadIdx.x) == 1) ? 12 : 8)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 40 : ((ax0 == 2) ? 32 : ((ax0 == 1) ? 12 : 8)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 36, 4, 40), \"float32\"), compute: T.Buffer((6, 36, 4, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(864):\n            for i3 in range(40):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 40 + i3\n                compute_1 = T.Buffer((34560,), data=compute.data)\n                data_1 = T.Buffer((34560,), data=data.data)\n                compute_1[cse_var_1] = T.sigmoid(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]))));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 864; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 40; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 40) + i3);\n      ((float*)compute_1)[cse_var_1] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - ((float*)data_1)[cse_var_1]))));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sigmoid"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 8, 20, 28), \"float32\"), compute: T.Buffer((7, 8, 20, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(56):\n            for i2, i3 in T.grid(20, 28):\n                cse_var_1: T.int32 = i0_i1_fused * 560 + i2 * 28 + i3\n                compute_1 = T.Buffer((31360,), data=compute.data)\n                data_1 = T.Buffer((31360,), data=data.data)\n                compute_1[cse_var_1] = T.sigmoid(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]))));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 56; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      for (int32_t i3 = 0; i3 < 28; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 560) + (i2 * 28)) + i3);\n        ((float*)compute_1)[cse_var_1] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - ((float*)data_1)[cse_var_1]))));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sigmoid"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 16, 4, 20), \"float32\"), compute: T.Buffer((6, 16, 4, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(7680):\n            compute_1 = T.Buffer((7680,), data=compute.data)\n            data_1 = T.Buffer((7680,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sigmoid(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]))));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 7680; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - ((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]))));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sigmoid"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 8, 36, 4), \"float32\"), compute: T.Buffer((2, 8, 36, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(576):\n            for i3 in range(4):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 4 + i3\n                compute_1 = T.Buffer((2304,), data=compute.data)\n                data_1 = T.Buffer((2304,), data=data.data)\n                compute_1[cse_var_1] = T.sigmoid(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 576; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 4; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 4) + i3);\n      ((float*)compute_1)[cse_var_1] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - ((float*)data_1)[cse_var_1]))));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sigmoid"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 32, 40, 20), \"float32\"), compute: T.Buffer((4, 32, 40, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(5120):\n            for i3 in range(20):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 20 + i3\n                compute_1 = T.Buffer((102400,), data=compute.data)\n                data_1 = T.Buffer((102400,), data=data.data)\n                compute_1[cse_var_1] = T.sigmoid(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]))));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5120; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 20; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 20) + i3);\n      ((float*)compute_1)[cse_var_1] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - ((float*)data_1)[cse_var_1]))));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sigmoid"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 12, 28, 32), \"float32\"), compute: T.Buffer((5, 12, 28, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1680):\n            for i3 in range(32):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 32 + i3\n                compute_1 = T.Buffer((53760,), data=compute.data)\n                data_1 = T.Buffer((53760,), data=data.data)\n                compute_1[cse_var_1] = T.sigmoid(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]))));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1680; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 32; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 32) + i3);\n      ((float*)compute_1)[cse_var_1] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - ((float*)data_1)[cse_var_1]))));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sigmoid"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 20, 36, 32), \"float32\"), compute: T.Buffer((7, 20, 36, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(161280):\n            compute_1 = T.Buffer((161280,), data=compute.data)\n            data_1 = T.Buffer((161280,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sigmoid(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]))));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 161280; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - ((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]))));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sigmoid"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 20, 36, 20), \"float32\"), T_sign: T.Buffer((8, 20, 36, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(160):\n            for ax2, ax3 in T.grid(36, 20):\n                cse_var_1: T.int32 = ax0_ax1_fused * 720 + ax2 * 20 + ax3\n                T_sign_1 = T.Buffer((115200,), data=T_sign.data)\n                data_1 = T.Buffer((115200,), data=data.data)\n                T_sign_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], T.float32(1), T.Select(data_1[cse_var_1] < T.float32(0), T.float32(-1), T.float32(0)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_sign_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_sign = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_sign_1 = (((DLTensor*)T_sign)[0].data);\n  void* default_function_T_sign_shape = (((DLTensor*)T_sign)[0].shape);\n  void* default_function_T_sign_strides = (((DLTensor*)T_sign)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_sign_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 160; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 36; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 720) + (ax2 * 20)) + ax3);\n        ((float*)T_sign_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? 1.000000e+00f : ((((float*)data_1)[cse_var_1] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sign"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 12, 28, 24), \"float32\"), T_sign: T.Buffer((9, 12, 28, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(108):\n            for ax2, ax3 in T.grid(28, 24):\n                cse_var_1: T.int32 = ax0_ax1_fused * 672 + ax2 * 24 + ax3\n                T_sign_1 = T.Buffer((72576,), data=T_sign.data)\n                data_1 = T.Buffer((72576,), data=data.data)\n                T_sign_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], T.float32(1), T.Select(data_1[cse_var_1] < T.float32(0), T.float32(-1), T.float32(0)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_sign_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_sign = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_sign_1 = (((DLTensor*)T_sign)[0].data);\n  void* default_function_T_sign_shape = (((DLTensor*)T_sign)[0].shape);\n  void* default_function_T_sign_strides = (((DLTensor*)T_sign)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_sign_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 108; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 28; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 24; ++ax3) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 672) + (ax2 * 24)) + ax3);\n        ((float*)T_sign_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? 1.000000e+00f : ((((float*)data_1)[cse_var_1] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sign"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 40, 8, 32), \"float32\"), T_sign: T.Buffer((9, 40, 8, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(92160):\n            T_sign_1 = T.Buffer((92160,), data=T_sign.data)\n            data_1 = T.Buffer((92160,), data=data.data)\n            T_sign_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.if_then_else(T.float32(0) < data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(1), T.Select(data_1[ax0_ax1_fused_ax2_fused_ax3_fused] < T.float32(0), T.float32(-1), T.float32(0)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_sign_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_sign = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_sign_1 = (((DLTensor*)T_sign)[0].data);\n  void* default_function_T_sign_shape = (((DLTensor*)T_sign)[0].shape);\n  void* default_function_T_sign_strides = (((DLTensor*)T_sign)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_sign_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 92160; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    ((float*)T_sign_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = ((0.000000e+00f < ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused]) ? 1.000000e+00f : ((((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sign"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 16, 8, 32), \"float32\"), T_sign: T.Buffer((9, 16, 8, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(36864):\n            T_sign_1 = T.Buffer((36864,), data=T_sign.data)\n            data_1 = T.Buffer((36864,), data=data.data)\n            T_sign_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.if_then_else(T.float32(0) < data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(1), T.Select(data_1[ax0_ax1_fused_ax2_fused_ax3_fused] < T.float32(0), T.float32(-1), T.float32(0)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_sign_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_sign = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_sign_1 = (((DLTensor*)T_sign)[0].data);\n  void* default_function_T_sign_shape = (((DLTensor*)T_sign)[0].shape);\n  void* default_function_T_sign_strides = (((DLTensor*)T_sign)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_sign_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 36864; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    ((float*)T_sign_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = ((0.000000e+00f < ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused]) ? 1.000000e+00f : ((((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sign"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 8, 4, 4), \"float32\"), T_sign: T.Buffer((8, 8, 4, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(1024):\n            T_sign_1 = T.Buffer((1024,), data=T_sign.data)\n            data_1 = T.Buffer((1024,), data=data.data)\n            T_sign_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.if_then_else(T.float32(0) < data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(1), T.Select(data_1[ax0_ax1_fused_ax2_fused_ax3_fused] < T.float32(0), T.float32(-1), T.float32(0)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_sign_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_sign = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_sign_1 = (((DLTensor*)T_sign)[0].data);\n  void* default_function_T_sign_shape = (((DLTensor*)T_sign)[0].shape);\n  void* default_function_T_sign_strides = (((DLTensor*)T_sign)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_sign_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 1024; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    ((float*)T_sign_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = ((0.000000e+00f < ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused]) ? 1.000000e+00f : ((((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sign"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 20, 24, 12), \"float32\"), T_sign: T.Buffer((4, 20, 24, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(80):\n            for ax2 in range(24):\n                cse_var_1: T.int32 = ax0_ax1_fused * 288 + ax2 * 12\n                T_sign_1 = T.Buffer((23040,), data=T_sign.data)\n                data_1 = T.Buffer((23040,), data=data.data)\n                T_sign_1[cse_var_1:cse_var_1 + 12] = T.Select(T.Broadcast(T.float32(0), 12) < data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(1), 12), T.Select(data_1[cse_var_1:cse_var_1 + 12] < T.Broadcast(T.float32(0), 12), T.Broadcast(T.float32(-1), 12), T.Broadcast(T.float32(0), 12)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_sign_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_sign = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_sign_1 = (((DLTensor*)T_sign)[0].data);\n  void* default_function_T_sign_shape = (((DLTensor*)T_sign)[0].shape);\n  void* default_function_T_sign_strides = (((DLTensor*)T_sign)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_sign_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 80; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 24; ++ax2) {\n      int32_t cse_var_1 = ((ax0_ax1_fused * 288) + (ax2 * 12));\n      int32_t12 v_ = int32_t12((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7), (cse_var_1)+(1*8), (cse_var_1)+(1*9), (cse_var_1)+(1*10), (cse_var_1)+(1*11));\n      *(float12*)(((float*)T_sign_1) + cse_var_1) = ((((float12)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f)) < (float12(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb]))) ? ((float12)(1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f)) : (((float12(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb])) < ((float12)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f))) ? ((float12)(-1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f)) : ((float12)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f))));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sign"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 28, 24, 16), \"float32\"), T_sign: T.Buffer((6, 28, 24, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(6):\n            for ax1, ax2 in T.grid(28, 24):\n                cse_var_1: T.int32 = ax0 * 10752 + ax1 * 384 + ax2 * 16\n                T_sign_1 = T.Buffer((64512,), data=T_sign.data)\n                data_1 = T.Buffer((64512,), data=data.data)\n                T_sign_1[cse_var_1:cse_var_1 + 16] = T.Select(T.Broadcast(T.float32(0), 16) < data_1[cse_var_1:cse_var_1 + 16], T.Broadcast(T.float32(1), 16), T.Select(data_1[cse_var_1:cse_var_1 + 16] < T.Broadcast(T.float32(0), 16), T.Broadcast(T.float32(-1), 16), T.Broadcast(T.float32(0), 16)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_sign_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_sign = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_sign_1 = (((DLTensor*)T_sign)[0].data);\n  void* default_function_T_sign_shape = (((DLTensor*)T_sign)[0].shape);\n  void* default_function_T_sign_strides = (((DLTensor*)T_sign)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_sign_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 28; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 24; ++ax2) {\n        int32_t cse_var_1 = (((ax0 * 10752) + (ax1 * 384)) + (ax2 * 16));\n        int32_t16 v_ = int32_t16((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7), (cse_var_1)+(1*8), (cse_var_1)+(1*9), (cse_var_1)+(1*10), (cse_var_1)+(1*11), (cse_var_1)+(1*12), (cse_var_1)+(1*13), (cse_var_1)+(1*14), (cse_var_1)+(1*15));\n        *(float16*)(((float*)T_sign_1) + cse_var_1) = ((((float16)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f)) < (float16(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb],((float*)data_1)[v_.sc],((float*)data_1)[v_.sd],((float*)data_1)[v_.se],((float*)data_1)[v_.sf]))) ? ((float16)(1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f)) : (((float16(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb],((float*)data_1)[v_.sc],((float*)data_1)[v_.sd],((float*)data_1)[v_.se],((float*)data_1)[v_.sf])) < ((float16)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f))) ? ((float16)(-1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f)) : ((float16)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f))));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sign"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 12, 4, 8), \"float32\"), T_sign: T.Buffer((8, 12, 4, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(8):\n            for ax1, ax2, ax3 in T.grid(12, 4, 8):\n                cse_var_1: T.int32 = ax0 * 384 + ax1 * 32 + ax2 * 8 + ax3\n                T_sign_1 = T.Buffer((3072,), data=T_sign.data)\n                data_1 = T.Buffer((3072,), data=data.data)\n                T_sign_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], T.float32(1), T.Select(data_1[cse_var_1] < T.float32(0), T.float32(-1), T.float32(0)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_sign_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_sign = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_sign_1 = (((DLTensor*)T_sign)[0].data);\n  void* default_function_T_sign_shape = (((DLTensor*)T_sign)[0].shape);\n  void* default_function_T_sign_strides = (((DLTensor*)T_sign)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_sign_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n          int32_t cse_var_1 = ((((ax0 * 384) + (ax1 * 32)) + (ax2 * 8)) + ax3);\n          ((float*)T_sign_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? 1.000000e+00f : ((((float*)data_1)[cse_var_1] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sign"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 12, 36, 8), \"float32\"), T_sign: T.Buffer((8, 12, 36, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(96):\n            for ax2 in range(36):\n                cse_var_1: T.int32 = ax0_ax1_fused * 288 + ax2 * 8\n                T_sign_1 = T.Buffer((27648,), data=T_sign.data)\n                data_1 = T.Buffer((27648,), data=data.data)\n                T_sign_1[cse_var_1:cse_var_1 + 8] = T.Select(T.Broadcast(T.float32(0), 8) < data_1[cse_var_1:cse_var_1 + 8], T.Broadcast(T.float32(1), 8), T.Select(data_1[cse_var_1:cse_var_1 + 8] < T.Broadcast(T.float32(0), 8), T.Broadcast(T.float32(-1), 8), T.Broadcast(T.float32(0), 8)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_sign_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_sign = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_sign_1 = (((DLTensor*)T_sign)[0].data);\n  void* default_function_T_sign_shape = (((DLTensor*)T_sign)[0].shape);\n  void* default_function_T_sign_strides = (((DLTensor*)T_sign)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_sign_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 96; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 36; ++ax2) {\n      int32_t cse_var_1 = ((ax0_ax1_fused * 288) + (ax2 * 8));\n      int32_t8 v_ = int32_t8((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7));\n      *(float8*)(((float*)T_sign_1) + cse_var_1) = ((((float8)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f)) < (float8(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7]))) ? ((float8)(1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f)) : (((float8(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7])) < ((float8)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f))) ? ((float8)(-1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f)) : ((float8)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f))));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sign"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 20, 8, 8), \"float32\"), T_sign: T.Buffer((10, 20, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(1600):\n            cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 8\n            T_sign_1 = T.Buffer((12800,), data=T_sign.data)\n            data_1 = T.Buffer((12800,), data=data.data)\n            T_sign_1[cse_var_1:cse_var_1 + 8] = T.Select(T.Broadcast(T.float32(0), 8) < data_1[cse_var_1:cse_var_1 + 8], T.Broadcast(T.float32(1), 8), T.Select(data_1[cse_var_1:cse_var_1 + 8] < T.Broadcast(T.float32(0), 8), T.Broadcast(T.float32(-1), 8), T.Broadcast(T.float32(0), 8)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(25) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(25) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_sign_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_sign = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_sign_1 = (((DLTensor*)T_sign)[0].data);\n  void* default_function_T_sign_shape = (((DLTensor*)T_sign)[0].shape);\n  void* default_function_T_sign_strides = (((DLTensor*)T_sign)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_sign_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1600; ++ax0_ax1_fused_ax2_fused) {\n    int32_t cse_var_1 = (ax0_ax1_fused_ax2_fused * 8);\n    int32_t8 v_ = int32_t8((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7));\n    *(float8*)(((float*)T_sign_1) + cse_var_1) = ((((float8)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f)) < (float8(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7]))) ? ((float8)(1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f)) : (((float8(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7])) < ((float8)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f))) ? ((float8)(-1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f)) : ((float8)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f))));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sign"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 12, 8, 24), \"float32\"), compute: T.Buffer((3, 12, 8, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(6912):\n            compute_1 = T.Buffer((6912,), data=compute.data)\n            data_1 = T.Buffer((6912,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sin(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 6912; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = sinf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sin"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 28, 16, 20), \"float32\"), compute: T.Buffer((9, 28, 16, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(80640):\n            compute_1 = T.Buffer((80640,), data=compute.data)\n            data_1 = T.Buffer((80640,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sin(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 80640; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = sinf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sin"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 4, 32, 20), \"float32\"), compute: T.Buffer((4, 4, 32, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(10240):\n            compute_1 = T.Buffer((10240,), data=compute.data)\n            data_1 = T.Buffer((10240,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sin(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 10240; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = sinf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sin"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 32, 4, 4), \"float32\"), compute: T.Buffer((2, 32, 4, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(256):\n            for i3 in range(4):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 4 + i3\n                compute_1 = T.Buffer((1024,), data=compute.data)\n                data_1 = T.Buffer((1024,), data=data.data)\n                compute_1[cse_var_1] = T.sin(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 256; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 4; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 4) + i3);\n      ((float*)compute_1)[cse_var_1] = sinf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sin"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 24, 24, 28), \"float32\"), compute: T.Buffer((5, 24, 24, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(80640):\n            compute_1 = T.Buffer((80640,), data=compute.data)\n            data_1 = T.Buffer((80640,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sin(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 80640; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = sinf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sin"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 24, 28, 28), \"float32\"), compute: T.Buffer((8, 24, 28, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(150528):\n            compute_1 = T.Buffer((150528,), data=compute.data)\n            data_1 = T.Buffer((150528,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sin(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 150528; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = sinf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sin"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 36, 12, 32), \"float32\"), compute: T.Buffer((7, 36, 12, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(252):\n            for i2, i3 in T.grid(12, 32):\n                cse_var_1: T.int32 = i0_i1_fused * 384 + i2 * 32 + i3\n                compute_1 = T.Buffer((96768,), data=compute.data)\n                data_1 = T.Buffer((96768,), data=data.data)\n                compute_1[cse_var_1] = T.sin(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 252; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3 = 0; i3 < 32; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 384) + (i2 * 32)) + i3);\n        ((float*)compute_1)[cse_var_1] = sinf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sin"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 24, 36, 24), \"float32\"), compute: T.Buffer((2, 24, 36, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(48):\n            for i2, i3 in T.grid(36, 24):\n                cse_var_1: T.int32 = i0_i1_fused * 864 + i2 * 24 + i3\n                compute_1 = T.Buffer((41472,), data=compute.data)\n                data_1 = T.Buffer((41472,), data=data.data)\n                compute_1[cse_var_1] = T.sin(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 36; ++i2) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 864) + (i2 * 24)) + i3);\n        ((float*)compute_1)[cse_var_1] = sinf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sin"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 40, 28, 36), \"float32\"), compute: T.Buffer((9, 40, 28, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(362880):\n            compute_1 = T.Buffer((362880,), data=compute.data)\n            data_1 = T.Buffer((362880,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sinh(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 362880; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = sinhf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sinh"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 32, 24, 16), \"float32\"), compute: T.Buffer((1, 32, 24, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(32):\n            for i2, i3 in T.grid(24, 16):\n                cse_var_1: T.int32 = i0_i1_fused * 384 + i2 * 16 + i3\n                compute_1 = T.Buffer((12288,), data=compute.data)\n                data_1 = T.Buffer((12288,), data=data.data)\n                compute_1[cse_var_1] = T.sinh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 32; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 24; ++i2) {\n      for (int32_t i3 = 0; i3 < 16; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 384) + (i2 * 16)) + i3);\n        ((float*)compute_1)[cse_var_1] = sinhf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sinh"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 20, 24, 24), \"float32\"), compute: T.Buffer((4, 20, 24, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1920):\n            for i3 in range(24):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 24 + i3\n                compute_1 = T.Buffer((46080,), data=compute.data)\n                data_1 = T.Buffer((46080,), data=data.data)\n                compute_1[cse_var_1] = T.sinh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1920; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 24; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 24) + i3);\n      ((float*)compute_1)[cse_var_1] = sinhf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sinh"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 8, 4, 12), \"float32\"), compute: T.Buffer((9, 8, 4, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3456):\n            compute_1 = T.Buffer((3456,), data=compute.data)\n            data_1 = T.Buffer((3456,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sinh(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3456; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = sinhf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sinh"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 16, 40, 20), \"float32\"), compute: T.Buffer((7, 16, 40, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(89600):\n            compute_1 = T.Buffer((89600,), data=compute.data)\n            data_1 = T.Buffer((89600,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sinh(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 89600; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = sinhf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sinh"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 32, 40, 40), \"float32\"), compute: T.Buffer((2, 32, 40, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2560):\n            for i3 in range(40):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 40 + i3\n                compute_1 = T.Buffer((102400,), data=compute.data)\n                data_1 = T.Buffer((102400,), data=data.data)\n                compute_1[cse_var_1] = T.sinh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2560; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 40; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 40) + i3);\n      ((float*)compute_1)[cse_var_1] = sinhf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sinh"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 12, 12, 36), \"float32\"), compute: T.Buffer((5, 12, 12, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(25920):\n            compute_1 = T.Buffer((25920,), data=compute.data)\n            data_1 = T.Buffer((25920,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sinh(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 25920; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = sinhf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sinh"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 20, 4, 20), \"float32\"), compute: T.Buffer((4, 20, 4, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(320):\n            for i3 in range(20):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 20 + i3\n                compute_1 = T.Buffer((6400,), data=compute.data)\n                data_1 = T.Buffer((6400,), data=data.data)\n                compute_1[cse_var_1] = T.sinh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 320; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 20; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 20) + i3);\n      ((float*)compute_1)[cse_var_1] = sinhf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sinh"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 36, 4, 28), \"float32\"), compute: T.Buffer((5, 36, 4, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(20160):\n            compute_1 = T.Buffer((20160,), data=compute.data)\n            data_1 = T.Buffer((20160,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sinh(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 20160; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = sinhf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sinh"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 12, 8, 16), \"float32\"), compute: T.Buffer((5, 12, 8, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(60):\n            for i2, i3 in T.grid(8, 16):\n                cse_var_1: T.int32 = i0_i1_fused * 128 + i2 * 16 + i3\n                compute_1 = T.Buffer((7680,), data=compute.data)\n                data_1 = T.Buffer((7680,), data=data.data)\n                compute_1[cse_var_1] = T.sqrt(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 60; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      for (int32_t i3 = 0; i3 < 16; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 128) + (i2 * 16)) + i3);\n        ((float*)compute_1)[cse_var_1] = sqrtf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 40, 40, 20), \"float32\"), compute: T.Buffer((9, 40, 40, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(288000):\n            compute_1 = T.Buffer((288000,), data=compute.data)\n            data_1 = T.Buffer((288000,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sqrt(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 288000; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = sqrtf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 40, 36, 32), \"float32\"), compute: T.Buffer((9, 40, 36, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(12960):\n            for i3 in range(32):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 32 + i3\n                compute_1 = T.Buffer((414720,), data=compute.data)\n                data_1 = T.Buffer((414720,), data=data.data)\n                compute_1[cse_var_1] = T.sqrt(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 12960; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 32; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 32) + i3);\n      ((float*)compute_1)[cse_var_1] = sqrtf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 4, 8, 40), \"float32\"), compute: T.Buffer((10, 4, 8, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(10):\n            for i1, i2, i3 in T.grid(4, 8, 40):\n                cse_var_1: T.int32 = i0 * 1280 + i1 * 320 + i2 * 40 + i3\n                compute_1 = T.Buffer((12800,), data=compute.data)\n                data_1 = T.Buffer((12800,), data=data.data)\n                compute_1[cse_var_1] = T.sqrt(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        for (int32_t i3 = 0; i3 < 40; ++i3) {\n          int32_t cse_var_1 = ((((i0 * 1280) + (i1 * 320)) + (i2 * 40)) + i3);\n          ((float*)compute_1)[cse_var_1] = sqrtf(((float*)data_1)[cse_var_1]);\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 12, 4, 24), \"float32\"), compute: T.Buffer((1, 12, 4, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(12, 4, 24):\n            cse_var_1: T.int32 = i1 * 96 + i2 * 24 + i3\n            compute_1 = T.Buffer((1152,), data=compute.data)\n            data_1 = T.Buffer((1152,), data=data.data)\n            compute_1[cse_var_1] = T.sqrt(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i1 = 0; i1 < 12; ++i1) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_1 = (((i1 * 96) + (i2 * 24)) + i3);\n        ((float*)compute_1)[cse_var_1] = sqrtf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 24, 20, 24), \"float32\"), compute: T.Buffer((10, 24, 20, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(115200):\n            compute_1 = T.Buffer((115200,), data=compute.data)\n            data_1 = T.Buffer((115200,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sqrt(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 115200; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = sqrtf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 40, 28, 32), \"float32\"), compute: T.Buffer((8, 40, 28, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(286720):\n            compute_1 = T.Buffer((286720,), data=compute.data)\n            data_1 = T.Buffer((286720,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sqrt(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 286720; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = sqrtf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 4, 12, 20), \"float32\"), compute: T.Buffer((8, 4, 12, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(7680):\n            compute_1 = T.Buffer((7680,), data=compute.data)\n            data_1 = T.Buffer((7680,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sqrt(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 7680; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = sqrtf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 36, 8, 20), \"float32\"), compute: T.Buffer((8, 36, 8, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(288):\n            for i2, i3 in T.grid(8, 20):\n                cse_var_1: T.int32 = i0_i1_fused * 160 + i2 * 20 + i3\n                compute_1 = T.Buffer((46080,), data=compute.data)\n                data_1 = T.Buffer((46080,), data=data.data)\n                compute_1[cse_var_1] = T.sqrt(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 288; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 160) + (i2 * 20)) + i3);\n        ((float*)compute_1)[cse_var_1] = sqrtf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 4, 12, 28), \"float32\"), compute: T.Buffer((2, 4, 12, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(96):\n            for i3 in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                compute_1 = T.Buffer((2688,), data=compute.data)\n                data_1 = T.Buffer((2688,), data=data.data)\n                compute_1[cse_var_1] = T.tan(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 96; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 28) + i3);\n      ((float*)compute_1)[cse_var_1] = tanf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "tan"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 28, 32, 32), \"float32\"), compute: T.Buffer((9, 28, 32, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(8064):\n            for i3 in range(32):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 32 + i3\n                compute_1 = T.Buffer((258048,), data=compute.data)\n                data_1 = T.Buffer((258048,), data=data.data)\n                compute_1[cse_var_1] = T.tan(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 8064; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 32; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 32) + i3);\n      ((float*)compute_1)[cse_var_1] = tanf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "tan"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 4, 24, 8), \"float32\"), compute: T.Buffer((7, 4, 24, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(672):\n            for i3 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 8 + i3\n                compute_1 = T.Buffer((5376,), data=compute.data)\n                data_1 = T.Buffer((5376,), data=data.data)\n                compute_1[cse_var_1] = T.tan(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 672; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 8; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 8) + i3);\n      ((float*)compute_1)[cse_var_1] = tanf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "tan"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 4, 20, 36), \"float32\"), compute: T.Buffer((8, 4, 20, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(23040):\n            compute_1 = T.Buffer((23040,), data=compute.data)\n            data_1 = T.Buffer((23040,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.tan(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 23040; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = tanf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "tan"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 20, 8, 8), \"float32\"), compute: T.Buffer((1, 20, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(20, 8, 8):\n            cse_var_1: T.int32 = i1 * 64 + i2 * 8 + i3\n            compute_1 = T.Buffer((1280,), data=compute.data)\n            data_1 = T.Buffer((1280,), data=data.data)\n            compute_1[cse_var_1] = T.tan(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i1 = 0; i1 < 20; ++i1) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      for (int32_t i3 = 0; i3 < 8; ++i3) {\n        int32_t cse_var_1 = (((i1 * 64) + (i2 * 8)) + i3);\n        ((float*)compute_1)[cse_var_1] = tanf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "tan"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 4, 32, 8), \"float32\"), compute: T.Buffer((3, 4, 32, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3072):\n            compute_1 = T.Buffer((3072,), data=compute.data)\n            data_1 = T.Buffer((3072,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.tan(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3072; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = tanf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "tan"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 24, 16, 36), \"float32\"), compute: T.Buffer((6, 24, 16, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2304):\n            for i3 in range(36):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 36 + i3\n                compute_1 = T.Buffer((82944,), data=compute.data)\n                data_1 = T.Buffer((82944,), data=data.data)\n                compute_1[cse_var_1] = T.tan(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2304; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 36; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 36) + i3);\n      ((float*)compute_1)[cse_var_1] = tanf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "tan"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 36, 20, 32), \"float32\"), compute: T.Buffer((7, 36, 20, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(5040):\n            for i3 in range(32):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 32 + i3\n                compute_1 = T.Buffer((161280,), data=compute.data)\n                data_1 = T.Buffer((161280,), data=data.data)\n                compute_1[cse_var_1] = T.tanh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = tanhf(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float tanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5040; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 32; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 32) + i3);\n      ((float*)compute_1)[cse_var_1] = tanhf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "tanh"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 4, 20, 16), \"float32\"), compute: T.Buffer((3, 4, 20, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3840):\n            compute_1 = T.Buffer((3840,), data=compute.data)\n            data_1 = T.Buffer((3840,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.tanh(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = tanhf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float tanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3840; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = tanhf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "tanh"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 32, 28, 28), \"float32\"), compute: T.Buffer((1, 32, 28, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(896):\n            for i3 in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                compute_1 = T.Buffer((25088,), data=compute.data)\n                data_1 = T.Buffer((25088,), data=data.data)\n                compute_1[cse_var_1] = T.tanh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = tanhf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float tanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 896; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 28) + i3);\n      ((float*)compute_1)[cse_var_1] = tanhf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "tanh"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 24, 28, 24), \"float32\"), compute: T.Buffer((3, 24, 28, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(48384):\n            compute_1 = T.Buffer((48384,), data=compute.data)\n            data_1 = T.Buffer((48384,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.tanh(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = tanhf(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float tanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 48384; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = tanhf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "tanh"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 8, 12, 16), \"float32\"), compute: T.Buffer((3, 8, 12, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(4608):\n            compute_1 = T.Buffer((4608,), data=compute.data)\n            data_1 = T.Buffer((4608,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.tanh(data_1[i0_i1_fused_i2_fused_i3_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = tanhf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float tanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 4608; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = tanhf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "tanh"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 12, 28, 20), \"float32\"), compute: T.Buffer((3, 12, 28, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(36):\n            for i2, i3 in T.grid(28, 20):\n                cse_var_1: T.int32 = i0_i1_fused * 560 + i2 * 20 + i3\n                compute_1 = T.Buffer((20160,), data=compute.data)\n                data_1 = T.Buffer((20160,), data=data.data)\n                compute_1[cse_var_1] = T.tanh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = tanhf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float tanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 36; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 28; ++i2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 560) + (i2 * 20)) + i3);\n        ((float*)compute_1)[cse_var_1] = tanhf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "tanh"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 4, 16, 28), \"float32\"), compute: T.Buffer((1, 4, 16, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(64):\n            for i3 in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                compute_1 = T.Buffer((1792,), data=compute.data)\n                data_1 = T.Buffer((1792,), data=data.data)\n                compute_1[cse_var_1] = T.tanh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = tanhf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float tanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 64; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 28) + i3);\n      ((float*)compute_1)[cse_var_1] = tanhf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "tanh"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 36, 28, 32), \"float32\"), compute: T.Buffer((5, 36, 28, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(5040):\n            for i3 in range(32):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 32 + i3\n                compute_1 = T.Buffer((161280,), data=compute.data)\n                data_1 = T.Buffer((161280,), data=data.data)\n                compute_1[cse_var_1] = T.tanh(data_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = tanhf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float tanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5040; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 32; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 32) + i3);\n      ((float*)compute_1)[cse_var_1] = tanhf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "tanh"
  }
]