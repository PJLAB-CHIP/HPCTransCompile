[
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 12, 8, 16), \"float32\"), compute: T.Buffer((5, 1536), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i in T.parallel(5):\n            for j in range(1536):\n                cse_var_1: T.int32 = i * 1536 + j\n                compute_1 = T.Buffer((7680,), data=compute.data)\n                data_1 = T.Buffer((7680,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i = 0; i < 5; ++i) {\n    for (int32_t j = 0; j < 1536; ++j) {\n      int32_t cse_var_1 = ((i * 1536) + j);\n      ((float*)compute_1)[cse_var_1] = ((float*)data_1)[cse_var_1];\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flatten[5__12__8__16]"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 20, 24, 28), \"float32\"), adaptive_pool_avg: T.Buffer((5, 20, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(100):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n            adaptive_pool_sum_1[0] = T.float32(0)\n            for rv0 in range(24):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 672 + rv0 * 28\n                data_1 = T.Buffer((67200,), data=data.data)\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 1]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 2]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 3]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 4]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 5]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 6]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 7]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 8]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 9]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 10]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 11]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 12]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 13]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 14]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 15]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 16]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 17]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 18]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 19]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 20]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 21]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 22]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 23]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 24]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 25]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 26]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 27]\n            adaptive_pool_avg_1 = T.Buffer((100,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax0_ax1_fused_ax2_fused] = adaptive_pool_sum_1[0] * T.float32(0.001488095238095238)\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((int)blockIdx.x)] = (adaptive_pool_sum[((int)blockIdx.x)] * 1.488095e-03f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 24; ++rv0) {\n    for (int rv1 = 0; rv1 < 28; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + data[((((((int)blockIdx.x) * 1344) + (((int)threadIdx.x) * 672)) + (rv0 * 28)) + rv1)]);\n    }\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 100; ++ax0_ax1_fused_ax2_fused) {\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < 24; ++rv0) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 672) + (rv0 * 28));\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[cse_var_1]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 1)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 2)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 3)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 4)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 5)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 6)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 7)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 8)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 9)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 10)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 11)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 12)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 13)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 14)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 15)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 16)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 17)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 18)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 19)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 20)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 21)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 22)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 23)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 24)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 25)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 26)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 27)]);\n    }\n    ((float*)adaptive_pool_avg_1)[ax0_ax1_fused_ax2_fused] = (adaptive_pool_sum[0] * 1.488095e-03f);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "global_pool_avg[5__20__24__28]"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 36, 36, 8), \"float32\"), T_reverse_sequence: T.Buffer((7, 36, 36, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(9072):\n            T_reverse_sequence_1 = T.Buffer((72576,), data=T_reverse_sequence.data)\n            data_1 = T.Buffer((72576,), data=data.data)\n            T_reverse_sequence_1[ax0_ax1_fused_ax2_fused * 8:ax0_ax1_fused_ax2_fused * 8 + 8] = data_1[ax0_ax1_fused_ax2_fused % 1296 * 8 + 62208 - ax0_ax1_fused_ax2_fused // 1296 * 10368:ax0_ax1_fused_ax2_fused % 1296 * 8 + 62208 - ax0_ax1_fused_ax2_fused // 1296 * 10368 + 8]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 162) * 64) + ((int)threadIdx.x)) + 62208) - ((((int)blockIdx.x) / 162) * 10368))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 9072; ++ax0_ax1_fused_ax2_fused) {\n    *(float8*)(((float*)T_reverse_sequence_1) + (ax0_ax1_fused_ax2_fused * 8)) = *(float8*)(((float*)data_1) + ((((ax0_ax1_fused_ax2_fused % 1296) * 8) + 62208) - ((ax0_ax1_fused_ax2_fused / 1296) * 10368)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 24, 8, 36), \"float32\"), T_reverse_sequence: T.Buffer((5, 24, 8, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(120):\n            for ax2, ax3 in T.grid(8, 36):\n                cse_var_1: T.int32 = ax2 * 36\n                T_reverse_sequence_1 = T.Buffer((34560,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((34560,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused * 288 + cse_var_1 + ax3] = data_1[ax0_ax1_fused % 24 * 288 + cse_var_1 + ax3 + 27648 - ax0_ax1_fused // 24 * 6912]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 108) * 64) + ((int)threadIdx.x)) + 27648) - ((((int)blockIdx.x) / 108) * 6912))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 120; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 36; ++ax3) {\n        int32_t cse_var_1 = (ax2 * 36);\n        ((float*)T_reverse_sequence_1)[(((ax0_ax1_fused * 288) + cse_var_1) + ax3)] = ((float*)data_1)[((((((ax0_ax1_fused % 24) * 288) + cse_var_1) + ax3) + 27648) - ((ax0_ax1_fused / 24) * 6912))];\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 20, 12, 40), \"float32\"), T_reverse_sequence: T.Buffer((5, 20, 12, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(1200):\n            for ax3 in range(40):\n                T_reverse_sequence_1 = T.Buffer((48000,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((48000,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused_ax2_fused * 40 + ax3] = data_1[ax0_ax1_fused_ax2_fused % 240 * 40 + ax3 + 38400 - ax0_ax1_fused_ax2_fused // 240 * 9600]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 160) * 60) + ((int)threadIdx.x)) + 38400) - ((((int)blockIdx.x) / 160) * 9600))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1200; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 40; ++ax3) {\n      ((float*)T_reverse_sequence_1)[((ax0_ax1_fused_ax2_fused * 40) + ax3)] = ((float*)data_1)[(((((ax0_ax1_fused_ax2_fused % 240) * 40) + ax3) + 38400) - ((ax0_ax1_fused_ax2_fused / 240) * 9600))];\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 28, 28, 36), \"float32\"), T_reverse_sequence: T.Buffer((6, 28, 28, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(4704):\n            for ax3 in range(36):\n                T_reverse_sequence_1 = T.Buffer((169344,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((169344,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused_ax2_fused * 36 + ax3] = data_1[ax0_ax1_fused_ax2_fused % 784 * 36 + ax3 + 141120 - ax0_ax1_fused_ax2_fused // 784 * 28224]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 588) * 48) + ((int)threadIdx.x)) + 141120) - ((((int)blockIdx.x) / 588) * 28224))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4704; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 36; ++ax3) {\n      ((float*)T_reverse_sequence_1)[((ax0_ax1_fused_ax2_fused * 36) + ax3)] = ((float*)data_1)[(((((ax0_ax1_fused_ax2_fused % 784) * 36) + ax3) + 141120) - ((ax0_ax1_fused_ax2_fused / 784) * 28224))];\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 20, 36, 12), \"float32\"), T_reverse_sequence: T.Buffer((2, 20, 36, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(2):\n            for ax1, ax2 in T.grid(20, 36):\n                cse_var_3: T.int32 = ax1 * 432\n                cse_var_2: T.int32 = ax2 * 12\n                cse_var_1: T.int32 = ax0 * 8640\n                T_reverse_sequence_1 = T.Buffer((17280,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((17280,), data=data.data)\n                T_reverse_sequence_1[cse_var_1 + cse_var_3 + cse_var_2:cse_var_1 + cse_var_3 + cse_var_2 + 12] = data_1[cse_var_3 + cse_var_2 + 8640 - cse_var_1:cse_var_3 + cse_var_2 + 8640 - cse_var_1 + 12]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 216) * 40) + ((int)threadIdx.x)) + 8640) - ((((int)blockIdx.x) / 216) * 8640))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 36; ++ax2) {\n        int32_t cse_var_3 = (ax1 * 432);\n        int32_t cse_var_2 = (ax2 * 12);\n        int32_t cse_var_1 = (ax0 * 8640);\n        int32_t12 v_ = int32_t12(((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*0), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*1), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*2), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*3), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*4), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*5), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*6), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*7), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*8), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*9), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*10), ((((cse_var_3 + cse_var_2) + 8640) - cse_var_1))+(1*11));\n        *(float12*)(((float*)T_reverse_sequence_1) + ((cse_var_1 + cse_var_3) + cse_var_2)) = (float12(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb]));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 4, 24, 12), \"float32\"), T_reverse_sequence: T.Buffer((6, 4, 24, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(6912):\n            T_reverse_sequence_1 = T.Buffer((6912,), data=T_reverse_sequence.data)\n            data_1 = T.Buffer((6912,), data=data.data)\n            T_reverse_sequence_1[ax0_ax1_fused_ax2_fused_ax3_fused] = data_1[ax0_ax1_fused_ax2_fused_ax3_fused % 1152 + 5760 - ax0_ax1_fused_ax2_fused_ax3_fused // 1152 * 1152]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 24) * 48) + ((int)threadIdx.x)) + 5760) - ((((int)blockIdx.x) / 24) * 1152))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 6912; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    ((float*)T_reverse_sequence_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = ((float*)data_1)[(((ax0_ax1_fused_ax2_fused_ax3_fused % 1152) + 5760) - ((ax0_ax1_fused_ax2_fused_ax3_fused / 1152) * 1152))];\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 20, 16, 28), \"float32\"), T_reverse_sequence: T.Buffer((3, 20, 16, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(960):\n            for ax3 in range(28):\n                T_reverse_sequence_1 = T.Buffer((26880,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((26880,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused_ax2_fused * 28 + ax3] = data_1[ax0_ax1_fused_ax2_fused % 320 * 28 + ax3 + 17920 - ax0_ax1_fused_ax2_fused // 320 * 8960]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 320) * 28) + ((int)threadIdx.x)) + 17920) - ((((int)blockIdx.x) / 320) * 8960))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 960; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 28; ++ax3) {\n      ((float*)T_reverse_sequence_1)[((ax0_ax1_fused_ax2_fused * 28) + ax3)] = ((float*)data_1)[(((((ax0_ax1_fused_ax2_fused % 320) * 28) + ax3) + 17920) - ((ax0_ax1_fused_ax2_fused / 320) * 8960))];\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 28, 12, 28), \"float32\"), T_reverse_sequence: T.Buffer((2, 28, 12, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(672):\n            for ax3 in range(28):\n                T_reverse_sequence_1 = T.Buffer((18816,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((18816,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused_ax2_fused * 28 + ax3] = data_1[ax0_ax1_fused_ax2_fused % 336 * 28 + ax3 + 9408 - ax0_ax1_fused_ax2_fused // 336 * 9408]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 147) * 64) + ((int)threadIdx.x)) + 9408) - ((((int)blockIdx.x) / 147) * 9408))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 672; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 28; ++ax3) {\n      ((float*)T_reverse_sequence_1)[((ax0_ax1_fused_ax2_fused * 28) + ax3)] = ((float*)data_1)[(((((ax0_ax1_fused_ax2_fused % 336) * 28) + ax3) + 9408) - ((ax0_ax1_fused_ax2_fused / 336) * 9408))];\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 28, 12, 28), \"float32\"), T_reverse_sequence: T.Buffer((2, 28, 12, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(56):\n            for ax2, ax3 in T.grid(12, 28):\n                cse_var_1: T.int32 = ax2 * 28\n                T_reverse_sequence_1 = T.Buffer((18816,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((18816,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused * 336 + cse_var_1 + ax3] = data_1[ax0_ax1_fused % 28 * 336 + cse_var_1 + ax3 + 9408 - ax0_ax1_fused // 28 * 9408]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 294) * 32) + ((int)threadIdx.x)) + 9408) - ((((int)blockIdx.x) / 294) * 9408))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 56; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 28; ++ax3) {\n        int32_t cse_var_1 = (ax2 * 28);\n        ((float*)T_reverse_sequence_1)[(((ax0_ax1_fused * 336) + cse_var_1) + ax3)] = ((float*)data_1)[((((((ax0_ax1_fused % 28) * 336) + cse_var_1) + ax3) + 9408) - ((ax0_ax1_fused / 28) * 9408))];\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 16, 28, 16), \"float32\"), T_reverse_sequence: T.Buffer((7, 16, 28, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(3136):\n            T_reverse_sequence_1 = T.Buffer((50176,), data=T_reverse_sequence.data)\n            data_1 = T.Buffer((50176,), data=data.data)\n            T_reverse_sequence_1[ax0_ax1_fused_ax2_fused * 16:ax0_ax1_fused_ax2_fused * 16 + 16] = data_1[ax0_ax1_fused_ax2_fused % 448 * 16 + 43008 - ax0_ax1_fused_ax2_fused // 448 * 7168:ax0_ax1_fused_ax2_fused % 448 * 16 + 43008 - ax0_ax1_fused_ax2_fused // 448 * 7168 + 16]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 112) * 64) + ((int)threadIdx.x)) + 43008) - ((((int)blockIdx.x) / 112) * 7168))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3136; ++ax0_ax1_fused_ax2_fused) {\n    *(float16*)(((float*)T_reverse_sequence_1) + (ax0_ax1_fused_ax2_fused * 16)) = *(float16*)(((float*)data_1) + ((((ax0_ax1_fused_ax2_fused % 448) * 16) + 43008) - ((ax0_ax1_fused_ax2_fused / 448) * 7168)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flip"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 8, 20, 12), \"float32\"), Scale: T.Buffer((8,), \"float32\"), Shift: T.Buffer((8,), \"float32\"), ScaleShift: T.Buffer((4, 8, 20, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_outer_outer_outer, j_outer_outer_outer, b_outer_outer_inner, c_outer_outer_inner, j_outer_outer_inner, c_outer_inner, i_inner in T.grid(10, 3, 4, 2, 4, 4, 2):\n            cse_var_2: T.int32 = c_outer_outer_inner * 4 + c_outer_inner\n            cse_var_1: T.int32 = b_outer_outer_inner * 1920 + c_outer_outer_inner * 960 + c_outer_inner * 240 + i_outer_outer_outer * 24 + i_inner * 12 + j_outer_outer_outer * 4 + j_outer_outer_inner\n            ScaleShift_1 = T.Buffer((7680,), data=ScaleShift.data)\n            data_1 = T.Buffer((7680,), data=data.data)\n            ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale[cse_var_2] + Shift[cse_var_2]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] * Scale[((((int)blockIdx.x) % 80) / 10)]) + Shift[((((int)blockIdx.x) % 80) / 10)]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_Scale_strides == NULL)) {\n  }\n  if (!(default_function_Shift_strides == NULL)) {\n  }\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  }\n  for (int32_t i_outer_outer_outer = 0; i_outer_outer_outer < 10; ++i_outer_outer_outer) {\n    for (int32_t j_outer_outer_outer = 0; j_outer_outer_outer < 3; ++j_outer_outer_outer) {\n      for (int32_t b_outer_outer_inner = 0; b_outer_outer_inner < 4; ++b_outer_outer_inner) {\n        for (int32_t c_outer_outer_inner = 0; c_outer_outer_inner < 2; ++c_outer_outer_inner) {\n          for (int32_t j_outer_outer_inner = 0; j_outer_outer_inner < 4; ++j_outer_outer_inner) {\n            for (int32_t c_outer_inner = 0; c_outer_inner < 4; ++c_outer_inner) {\n              for (int32_t i_inner = 0; i_inner < 2; ++i_inner) {\n                int32_t cse_var_2 = ((c_outer_outer_inner * 4) + c_outer_inner);\n                int32_t cse_var_1 = (((((((b_outer_outer_inner * 1920) + (c_outer_outer_inner * 960)) + (c_outer_inner * 240)) + (i_outer_outer_outer * 24)) + (i_inner * 12)) + (j_outer_outer_outer * 4)) + j_outer_outer_inner);\n                ((float*)ScaleShift_1)[cse_var_1] = ((((float*)data_1)[cse_var_1] * ((float*)Scale_1)[cse_var_2]) + ((float*)Shift_1)[cse_var_2]);\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "scale_shift_nchw[4__8__20__12]"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 24, 12, 16), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 16, T.if_then_else(ax0 == 2, 12, T.if_then_else(ax0 == 1, 24, 1)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 16 : ((((int)threadIdx.x) == 2) ? 12 : ((((int)threadIdx.x) == 1) ? 24 : 1)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 16 : ((ax0 == 2) ? 12 : ((ax0 == 1) ? 24 : 1)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 4, 24, 4), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 4, T.if_then_else(ax0 == 2, 24, T.if_then_else(ax0 == 1, 4, 4)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 4 : ((((int)threadIdx.x) == 2) ? 24 : ((((int)threadIdx.x) == 1) ? 4 : 4)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 4 : ((ax0 == 2) ? 24 : ((ax0 == 1) ? 4 : 4)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 20, 40, 4), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 4, T.if_then_else(ax0 == 2, 40, T.if_then_else(ax0 == 1, 20, 8)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 4 : ((((int)threadIdx.x) == 2) ? 40 : ((((int)threadIdx.x) == 1) ? 20 : 8)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 4 : ((ax0 == 2) ? 40 : ((ax0 == 1) ? 20 : 8)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 20, 4, 24), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 24, T.if_then_else(ax0 == 2, 4, T.if_then_else(ax0 == 1, 20, 8)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 24 : ((((int)threadIdx.x) == 2) ? 4 : ((((int)threadIdx.x) == 1) ? 20 : 8)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 24 : ((ax0 == 2) ? 4 : ((ax0 == 1) ? 20 : 8)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 4, 20, 28), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 28, T.if_then_else(ax0 == 2, 20, T.if_then_else(ax0 == 1, 4, 1)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 28 : ((((int)threadIdx.x) == 2) ? 20 : ((((int)threadIdx.x) == 1) ? 4 : 1)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 28 : ((ax0 == 2) ? 20 : ((ax0 == 1) ? 4 : 1)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 40, 36, 28), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 28, T.if_then_else(ax0 == 2, 36, T.if_then_else(ax0 == 1, 40, 5)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 28 : ((((int)threadIdx.x) == 2) ? 36 : ((((int)threadIdx.x) == 1) ? 40 : 5)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 28 : ((ax0 == 2) ? 36 : ((ax0 == 1) ? 40 : 5)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 24, 40, 28), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 28, T.if_then_else(ax0 == 2, 40, T.if_then_else(ax0 == 1, 24, 6)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 28 : ((((int)threadIdx.x) == 2) ? 40 : ((((int)threadIdx.x) == 1) ? 24 : 6)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 28 : ((ax0 == 2) ? 40 : ((ax0 == 1) ? 24 : 6)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 24, 40, 24), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 24, T.if_then_else(ax0 == 2, 40, T.if_then_else(ax0 == 1, 24, 6)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 24 : ((((int)threadIdx.x) == 2) ? 40 : ((((int)threadIdx.x) == 1) ? 24 : 6)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 24 : ((ax0 == 2) ? 40 : ((ax0 == 1) ? 24 : 6)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 4, 20, 28), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 28, T.if_then_else(ax0 == 2, 20, T.if_then_else(ax0 == 1, 4, 1)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 28 : ((((int)threadIdx.x) == 2) ? 20 : ((((int)threadIdx.x) == 1) ? 4 : 1)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 28 : ((ax0 == 2) ? 20 : ((ax0 == 1) ? 4 : 1)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 12, 32, 40), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 40, T.if_then_else(ax0 == 2, 32, T.if_then_else(ax0 == 1, 12, 8)))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 40 : ((((int)threadIdx.x) == 2) ? 32 : ((((int)threadIdx.x) == 1) ? 12 : 8)));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 40 : ((ax0 == 2) ? 32 : ((ax0 == 1) ? 12 : 8)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "shape"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 24, 8, 36), \"float32\"), adaptive_pool_avg: T.Buffer((8, 24, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(192):\n            cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused_ax3_fused * 288\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n            adaptive_pool_sum_1[0] = T.float32(0)\n            data_1 = T.Buffer((55296,), data=data.data)\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 1]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 2]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 3]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 4]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 5]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 6]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 7]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 8]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 9]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 10]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 11]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 12]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 13]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 14]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 15]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 16]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 17]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 18]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 19]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 20]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 21]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 22]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 23]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 24]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 25]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 26]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 27]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 28]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 29]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 30]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 31]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 32]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 33]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 34]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 35]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 36]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 37]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 38]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 39]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 40]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 41]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 42]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 43]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 44]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 45]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 46]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 47]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 48]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 49]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 50]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 51]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 52]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 53]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 54]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 55]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 56]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 57]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 58]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 59]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 60]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 61]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 62]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 63]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 64]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 65]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 66]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 67]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 68]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 69]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 70]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 71]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 72]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 73]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 74]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 75]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 76]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 77]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 78]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 79]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 80]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 81]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 82]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 83]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 84]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 85]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 86]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 87]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 88]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 89]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 90]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 91]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 92]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 93]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 94]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 95]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 96]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 97]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 98]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 99]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 100]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 101]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 102]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 103]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 104]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 105]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 106]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 107]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 108]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 109]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 110]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 111]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 112]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 113]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 114]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 115]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 116]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 117]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 118]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 119]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 120]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 121]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 122]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 123]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 124]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 125]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 126]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 127]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 128]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 129]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 130]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 131]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 132]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 133]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 134]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 135]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 136]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 137]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 138]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 139]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 140]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 141]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 142]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 143]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 144]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 145]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 146]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 147]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 148]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 149]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 150]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 151]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 152]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 153]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 154]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 155]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 156]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 157]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 158]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 159]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 160]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 161]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 162]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 163]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 164]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 165]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 166]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 167]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 168]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 169]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 170]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 171]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 172]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 173]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 174]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 175]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 176]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 177]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 178]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 179]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 180]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 181]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 182]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 183]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 184]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 185]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 186]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 187]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 188]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 189]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 190]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 191]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 192]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 193]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 194]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 195]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 196]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 197]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 198]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 199]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 200]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 201]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 202]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 203]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 204]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 205]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 206]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 207]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 208]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 209]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 210]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 211]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 212]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 213]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 214]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 215]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 216]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 217]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 218]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 219]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 220]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 221]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 222]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 223]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 224]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 225]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 226]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 227]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 228]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 229]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 230]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 231]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 232]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 233]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 234]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 235]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 236]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 237]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 238]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 239]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 240]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 241]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 242]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 243]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 244]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 245]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 246]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 247]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 248]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 249]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 250]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 251]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 252]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 253]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 254]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 255]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 256]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 257]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 258]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 259]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 260]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 261]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 262]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 263]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 264]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 265]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 266]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 267]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 268]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 269]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 270]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 271]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 272]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 273]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 274]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 275]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 276]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 277]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 278]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 279]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 280]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 281]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 282]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 283]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 284]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 285]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 286]\n            adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 287]\n            adaptive_pool_avg_1 = T.Buffer((192,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax0_ax1_fused_ax2_fused_ax3_fused] = adaptive_pool_sum_1[0] * T.float32(0.003472222222222222)\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * 3.472222e-03f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 8; ++rv0) {\n    for (int rv1 = 0; rv1 < 36; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + data[((((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 288)) + (rv0 * 36)) + rv1)]);\n    }\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 192; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    int32_t cse_var_1 = (ax0_ax1_fused_ax2_fused_ax3_fused * 288);\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[cse_var_1]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 1)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 2)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 3)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 4)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 5)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 6)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 7)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 8)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 9)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 10)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 11)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 12)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 13)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 14)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 15)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 16)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 17)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 18)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 19)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 20)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 21)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 22)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 23)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 24)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 25)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 26)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 27)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 28)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 29)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 30)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 31)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 32)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 33)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 34)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 35)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 36)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 37)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 38)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 39)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 40)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 41)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 42)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 43)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 44)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 45)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 46)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 47)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 48)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 49)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 50)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 51)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 52)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 53)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 54)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 55)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 56)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 57)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 58)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 59)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 60)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 61)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 62)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 63)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 64)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 65)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 66)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 67)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 68)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 69)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 70)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 71)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 72)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 73)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 74)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 75)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 76)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 77)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 78)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 79)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 80)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 81)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 82)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 83)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 84)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 85)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 86)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 87)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 88)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 89)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 90)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 91)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 92)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 93)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 94)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 95)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 96)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 97)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 98)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 99)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 100)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 101)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 102)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 103)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 104)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 105)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 106)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 107)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 108)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 109)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 110)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 111)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 112)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 113)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 114)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 115)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 116)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 117)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 118)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 119)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 120)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 121)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 122)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 123)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 124)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 125)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 126)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 127)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 128)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 129)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 130)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 131)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 132)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 133)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 134)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 135)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 136)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 137)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 138)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 139)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 140)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 141)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 142)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 143)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 144)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 145)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 146)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 147)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 148)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 149)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 150)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 151)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 152)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 153)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 154)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 155)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 156)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 157)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 158)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 159)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 160)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 161)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 162)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 163)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 164)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 165)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 166)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 167)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 168)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 169)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 170)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 171)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 172)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 173)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 174)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 175)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 176)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 177)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 178)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 179)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 180)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 181)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 182)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 183)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 184)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 185)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 186)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 187)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 188)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 189)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 190)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 191)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 192)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 193)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 194)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 195)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 196)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 197)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 198)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 199)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 200)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 201)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 202)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 203)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 204)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 205)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 206)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 207)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 208)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 209)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 210)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 211)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 212)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 213)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 214)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 215)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 216)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 217)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 218)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 219)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 220)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 221)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 222)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 223)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 224)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 225)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 226)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 227)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 228)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 229)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 230)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 231)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 232)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 233)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 234)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 235)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 236)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 237)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 238)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 239)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 240)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 241)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 242)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 243)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 244)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 245)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 246)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 247)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 248)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 249)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 250)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 251)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 252)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 253)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 254)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 255)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 256)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 257)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 258)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 259)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 260)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 261)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 262)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 263)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 264)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 265)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 266)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 267)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 268)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 269)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 270)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 271)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 272)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 273)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 274)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 275)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 276)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 277)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 278)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 279)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 280)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 281)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 282)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 283)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 284)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 285)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 286)]);\n    adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 287)]);\n    ((float*)adaptive_pool_avg_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = (adaptive_pool_sum[0] * 3.472222e-03f);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "global_pool_avg[8__24__8__36]"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 32, 10, 2), \"float32\"), kernel: T.Buffer((74, 32, 3, 4), \"float32\"), bias: T.Buffer((1, 74, 1, 1), \"float32\"), conv2d_nchw: T.Buffer((6, 74, 10, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused in T.parallel(740):\n            conv2d_nchw_local = T.allocate([6], \"float32\", \"local\")\n            pad_temp = T.allocate([12], \"float32\", \"global\")\n            conv2d_nchw_local_1 = T.Buffer((6,), data=conv2d_nchw_local, scope=\"local\", align=16)\n            conv2d_nchw_local_1[0] = T.float32(0)\n            conv2d_nchw_local_1[1] = T.float32(0)\n            conv2d_nchw_local_1[2] = T.float32(0)\n            conv2d_nchw_local_1[3] = T.float32(0)\n            conv2d_nchw_local_1[4] = T.float32(0)\n            conv2d_nchw_local_1[5] = T.float32(0)\n            for rc_outer, ry_outer in T.grid(32, 3):\n                cse_var_5: T.int32 = nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 5\n                cse_var_4: T.int32 = nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 370 // 185\n                cse_var_3: T.int32 = cse_var_4 * 5 + ry_outer + cse_var_5\n                cse_var_2: T.int32 = nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused // 370 * 1920 + rc_outer * 20 + cse_var_4 * 10 + ry_outer * 2 + cse_var_5 * 2\n                cse_var_1: T.bool = 1 <= cse_var_3 and cse_var_3 < 11\n                pad_temp_1 = T.Buffer((12,), data=pad_temp, align=32)\n                pad_temp_1[0] = T.float32(0)\n                data_1 = T.Buffer((3840,), data=data.data)\n                pad_temp_1[1] = T.if_then_else(cse_var_1, data_1[cse_var_2 - 2], T.float32(0))\n                pad_temp_1[2] = T.if_then_else(cse_var_1, data_1[cse_var_2 - 1], T.float32(0))\n                pad_temp_1[3] = T.float32(0)\n                pad_temp_1[4] = T.float32(0)\n                pad_temp_1[5] = T.if_then_else(cse_var_1, data_1[cse_var_2 + 638], T.float32(0))\n                pad_temp_1[6] = T.if_then_else(cse_var_1, data_1[cse_var_2 + 639], T.float32(0))\n                pad_temp_1[7] = T.float32(0)\n                pad_temp_1[8] = T.float32(0)\n                pad_temp_1[9] = T.if_then_else(cse_var_1, data_1[cse_var_2 + 1278], T.float32(0))\n                pad_temp_1[10] = T.if_then_else(cse_var_1, data_1[cse_var_2 + 1279], T.float32(0))\n                pad_temp_1[11] = T.float32(0)\n                for nn_c_outer_inner in range(3):\n                    cse_var_12: T.int32 = nn_c_outer_inner * 4\n                    cse_var_11: T.int32 = nn_c_outer_inner * 2\n                    cse_var_10: T.int32 = cse_var_12 + 3\n                    cse_var_9: T.int32 = cse_var_12 + 2\n                    cse_var_8: T.int32 = cse_var_12 + 1\n                    cse_var_7: T.int32 = cse_var_11 + 1\n                    cse_var_6: T.int32 = nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 185 // 5 * 768 + rc_outer * 12 + ry_outer * 4\n                    kernel_1 = T.Buffer((28416,), data=kernel.data)\n                    conv2d_nchw_local_1[cse_var_11] = conv2d_nchw_local_1[cse_var_11] + pad_temp_1[cse_var_12] * kernel_1[cse_var_6]\n                    conv2d_nchw_local_1[cse_var_7] = conv2d_nchw_local_1[cse_var_7] + pad_temp_1[cse_var_12] * kernel_1[cse_var_6 + 384]\n                    conv2d_nchw_local_1[cse_var_11] = conv2d_nchw_local_1[cse_var_11] + pad_temp_1[cse_var_8] * kernel_1[cse_var_6 + 1]\n                    conv2d_nchw_local_1[cse_var_7] = conv2d_nchw_local_1[cse_var_7] + pad_temp_1[cse_var_8] * kernel_1[cse_var_6 + 385]\n                    conv2d_nchw_local_1[cse_var_11] = conv2d_nchw_local_1[cse_var_11] + pad_temp_1[cse_var_9] * kernel_1[cse_var_6 + 2]\n                    conv2d_nchw_local_1[cse_var_7] = conv2d_nchw_local_1[cse_var_7] + pad_temp_1[cse_var_9] * kernel_1[cse_var_6 + 386]\n                    conv2d_nchw_local_1[cse_var_11] = conv2d_nchw_local_1[cse_var_11] + pad_temp_1[cse_var_10] * kernel_1[cse_var_6 + 3]\n                    conv2d_nchw_local_1[cse_var_7] = conv2d_nchw_local_1[cse_var_7] + pad_temp_1[cse_var_10] * kernel_1[cse_var_6 + 387]\n            for nn_inner, ff_inner in T.grid(3, 2):\n                conv2d_nchw_1 = T.Buffer((4440,), data=conv2d_nchw.data)\n                conv2d_nchw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused // 370 * 2220 + nn_inner * 740 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 185 // 5 * 20 + ff_inner * 10 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 370 // 185 * 5 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 5] = conv2d_nchw_local_1[nn_inner * 2 + ff_inner]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(185) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(185) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[6];\n  __shared__ float pad_temp_shared[240];\n  __shared__ float kernel_shared[1184];\n  for (int nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 3; ++nn_c_outer_inner_init) {\n    conv2d_nchw_local[nn_c_outer_inner_init] = 0.000000e+00f;\n    conv2d_nchw_local[(nn_c_outer_inner_init + 3)] = 0.000000e+00f;\n  }\n  for (int rc_outer_outer = 0; rc_outer_outer < 2; ++rc_outer_outer) {\n    for (int ry_outer_outer = 0; ry_outer_outer < 3; ++ry_outer_outer) {\n      for (int rx_outer_outer = 0; rx_outer_outer < 4; ++rx_outer_outer) {\n        __syncthreads();\n        for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n          if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 37) + (((int)threadIdx.x) / 5)) < 48) {\n            pad_temp_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 185) + ((int)threadIdx.x))] = (((((1 <= ((((((int)blockIdx.x) & 1) * 5) + ry_outer_outer) + (((int)threadIdx.x) % 5))) && (((((((int)blockIdx.x) & 1) * 5) + ry_outer_outer) + (((int)threadIdx.x) % 5)) < 11)) && (1 <= rx_outer_outer)) && (rx_outer_outer < 3)) ? data[((((((((((((int)blockIdx.x) >> 1) * 1920) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 37) + (((int)threadIdx.x) / 5)) >> 4) * 640)) + (rc_outer_outer * 320)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 5) + (((int)threadIdx.x) / 5)) & 15) * 20)) + ((((int)blockIdx.x) & 1) * 10)) + (ry_outer_outer * 2)) + ((((int)threadIdx.x) % 5) * 2)) + rx_outer_outer) - 3)] : 0.000000e+00f);\n          }\n        }\n        for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 < 7; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) {\n          if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 5) + (((int)threadIdx.x) / 37)) < 32) {\n            kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 185) + ((int)threadIdx.x))] = kernel[((((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 185) + ((int)threadIdx.x)) >> 4) * 384) + (rc_outer_outer * 192)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 9) + ((int)threadIdx.x)) & 15) * 12)) + (ry_outer_outer * 4)) + rx_outer_outer)];\n          }\n        }\n        __syncthreads();\n        for (int rc_outer_inner = 0; rc_outer_inner < 16; ++rc_outer_inner) {\n          for (int nn_c_outer_inner = 0; nn_c_outer_inner < 3; ++nn_c_outer_inner) {\n            conv2d_nchw_local[nn_c_outer_inner] = (conv2d_nchw_local[nn_c_outer_inner] + (pad_temp_shared[(((nn_c_outer_inner * 80) + (rc_outer_inner * 5)) + (((int)threadIdx.x) % 5))] * kernel_shared[(((((int)threadIdx.x) / 5) * 16) + rc_outer_inner)]));\n            conv2d_nchw_local[(nn_c_outer_inner + 3)] = (conv2d_nchw_local[(nn_c_outer_inner + 3)] + (pad_temp_shared[(((nn_c_outer_inner * 80) + (rc_outer_inner * 5)) + (((int)threadIdx.x) % 5))] * kernel_shared[((((((int)threadIdx.x) / 5) * 16) + rc_outer_inner) + 592)]));\n          }\n        }\n      }\n    }\n  }\n  for (int nn_inner = 0; nn_inner < 3; ++nn_inner) {\n    conv2d_nchw[((((((((int)blockIdx.x) >> 1) * 2220) + (nn_inner * 740)) + ((((int)threadIdx.x) / 5) * 10)) + ((((int)blockIdx.x) & 1) * 5)) + (((int)threadIdx.x) % 5))] = conv2d_nchw_local[nn_inner];\n    conv2d_nchw[(((((((((int)blockIdx.x) >> 1) * 2220) + (nn_inner * 740)) + ((((int)threadIdx.x) / 5) * 10)) + ((((int)blockIdx.x) & 1) * 5)) + (((int)threadIdx.x) % 5)) + 370)] = conv2d_nchw_local[(nn_inner + 3)];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t bias_code = arg_type_ids[2];\n  int32_t conv2d_nchw_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* bias = (((TVMValue*)args)[2].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* bias_1 = (((DLTensor*)bias)[0].data);\n  void* default_function_bias_shape = (((DLTensor*)bias)[0].shape);\n  void* default_function_bias_strides = (((DLTensor*)bias)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_kernel_strides == NULL)) {\n  }\n  if (!(default_function_bias_strides == NULL)) {\n  }\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  }\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused < 740; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused) {\n    void* conv2d_nchw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)24, 2, 32);\n    if (conv2d_nchw_local == NULL) {\n      return -1;\n    }\n    float pad_temp[12];\n    ((float*)conv2d_nchw_local)[0] = 0.000000e+00f;\n    ((float*)conv2d_nchw_local)[1] = 0.000000e+00f;\n    ((float*)conv2d_nchw_local)[2] = 0.000000e+00f;\n    ((float*)conv2d_nchw_local)[3] = 0.000000e+00f;\n    ((float*)conv2d_nchw_local)[4] = 0.000000e+00f;\n    ((float*)conv2d_nchw_local)[5] = 0.000000e+00f;\n    for (int32_t rc_outer = 0; rc_outer < 32; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n        int32_t cse_var_5 = (nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 5);\n        int32_t cse_var_4 = ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 370) / 185);\n        int32_t cse_var_3 = (((cse_var_4 * 5) + ry_outer) + cse_var_5);\n        int32_t cse_var_2 = ((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused / 370) * 1920) + (rc_outer * 20)) + (cse_var_4 * 10)) + (ry_outer * 2)) + (cse_var_5 * 2));\n        bool cse_var_1 = ((1 <= cse_var_3) && (cse_var_3 < 11));\n        pad_temp[0] = 0.000000e+00f;\n        pad_temp[1] = (cse_var_1 ? ((float*)data_1)[(cse_var_2 - 2)] : 0.000000e+00f);\n        pad_temp[2] = (cse_var_1 ? ((float*)data_1)[(cse_var_2 - 1)] : 0.000000e+00f);\n        pad_temp[3] = 0.000000e+00f;\n        pad_temp[4] = 0.000000e+00f;\n        pad_temp[5] = (cse_var_1 ? ((float*)data_1)[(cse_var_2 + 638)] : 0.000000e+00f);\n        pad_temp[6] = (cse_var_1 ? ((float*)data_1)[(cse_var_2 + 639)] : 0.000000e+00f);\n        pad_temp[7] = 0.000000e+00f;\n        pad_temp[8] = 0.000000e+00f;\n        pad_temp[9] = (cse_var_1 ? ((float*)data_1)[(cse_var_2 + 1278)] : 0.000000e+00f);\n        pad_temp[10] = (cse_var_1 ? ((float*)data_1)[(cse_var_2 + 1279)] : 0.000000e+00f);\n        pad_temp[11] = 0.000000e+00f;\n        for (int32_t nn_c_outer_inner = 0; nn_c_outer_inner < 3; ++nn_c_outer_inner) {\n          int32_t cse_var_12 = (nn_c_outer_inner * 4);\n          int32_t cse_var_11 = (nn_c_outer_inner * 2);\n          int32_t cse_var_10 = (cse_var_12 + 3);\n          int32_t cse_var_9 = (cse_var_12 + 2);\n          int32_t cse_var_8 = (cse_var_12 + 1);\n          int32_t cse_var_7 = (cse_var_11 + 1);\n          int32_t cse_var_6 = (((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 185) / 5) * 768) + (rc_outer * 12)) + (ry_outer * 4));\n          ((float*)conv2d_nchw_local)[cse_var_11] = (((float*)conv2d_nchw_local)[cse_var_11] + (pad_temp[cse_var_12] * ((float*)kernel_1)[cse_var_6]));\n          ((float*)conv2d_nchw_local)[cse_var_7] = (((float*)conv2d_nchw_local)[cse_var_7] + (pad_temp[cse_var_12] * ((float*)kernel_1)[(cse_var_6 + 384)]));\n          ((float*)conv2d_nchw_local)[cse_var_11] = (((float*)conv2d_nchw_local)[cse_var_11] + (pad_temp[cse_var_8] * ((float*)kernel_1)[(cse_var_6 + 1)]));\n          ((float*)conv2d_nchw_local)[cse_var_7] = (((float*)conv2d_nchw_local)[cse_var_7] + (pad_temp[cse_var_8] * ((float*)kernel_1)[(cse_var_6 + 385)]));\n          ((float*)conv2d_nchw_local)[cse_var_11] = (((float*)conv2d_nchw_local)[cse_var_11] + (pad_temp[cse_var_9] * ((float*)kernel_1)[(cse_var_6 + 2)]));\n          ((float*)conv2d_nchw_local)[cse_var_7] = (((float*)conv2d_nchw_local)[cse_var_7] + (pad_temp[cse_var_9] * ((float*)kernel_1)[(cse_var_6 + 386)]));\n          ((float*)conv2d_nchw_local)[cse_var_11] = (((float*)conv2d_nchw_local)[cse_var_11] + (pad_temp[cse_var_10] * ((float*)kernel_1)[(cse_var_6 + 3)]));\n          ((float*)conv2d_nchw_local)[cse_var_7] = (((float*)conv2d_nchw_local)[cse_var_7] + (pad_temp[cse_var_10] * ((float*)kernel_1)[(cse_var_6 + 387)]));\n        }\n      }\n    }\n    for (int32_t nn_inner = 0; nn_inner < 3; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 2; ++ff_inner) {\n        ((float*)conv2d_nchw_1)[(((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused / 370) * 2220) + (nn_inner * 740)) + (((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 185) / 5) * 20)) + (ff_inner * 10)) + (((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 370) / 185) * 5)) + (nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 5))] = ((float*)conv2d_nchw_local)[((nn_inner * 2) + ff_inner)];\n      }\n    }\n    if (TVMBackendFreeWorkspace(1, dev_id, conv2d_nchw_local) != 0) {\n      return -1;\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "conv2d_nchw[6__10__2__74__32__3__4__(1__1)__(1__1)]"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 12, 20, 32), \"float32\"), T_divide: T.Buffer((4, 12, 20, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(48):\n            tensor = T.allocate([32], \"float32\", \"global\")\n            for ax2 in range(20):\n                tensor_1 = T.Buffer((32,), data=tensor)\n                data_1 = T.Buffer((30720,), data=data.data)\n                for ax3 in range(32):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 640 + ax2 * 32 + ax3\n                    tensor_1[ax3] = T.float32(0)\n                    tensor_1[ax3] = tensor_1[ax3] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax3 in range(32):\n                    cse_var_2: T.int32 = ax0_ax1_fused * 640 + ax2 * 32 + ax3\n                    T_divide_1 = T.Buffer((30720,), data=T_divide.data)\n                    T_divide_1[cse_var_2] = data_1[cse_var_2] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[ax3], T.float32(0.75))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])), 7.500000e-01f));\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float powf(float, float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_divide_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_divide = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_divide_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 48; ++ax0_ax1_fused) {\n    float tensor[32];\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 32; ++ax3) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 640) + (ax2 * 32)) + ax3);\n        tensor[ax3] = 0.000000e+00f;\n        tensor[ax3] = (tensor[ax3] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n      }\n      for (int32_t ax3_1 = 0; ax3_1 < 32; ++ax3_1) {\n        int32_t cse_var_2 = (((ax0_ax1_fused * 640) + (ax2 * 32)) + ax3_1);\n        ((float*)T_divide_1)[cse_var_2] = (((float*)data_1)[cse_var_2] / powf((2.000000e+00f + (1.000000e-04f * tensor[ax3_1])), 7.500000e-01f));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "lrn[4__12__20__32]"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 95, 7, 5), \"float32\"), kernel: T.Buffer((99, 95, 3, 4), \"float32\"), bias: T.Buffer((1, 99, 1, 1), \"float32\"), conv2d_nchw: T.Buffer((10, 99, 7, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused in T.parallel(55):\n            pad_temp = T.allocate([11970], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((11970,), data=pad_temp)\n            for i0, i1, i2 in T.grid(2, 95, 9):\n                cse_var_3: T.bool = 1 <= i2 and i2 < 8\n                cse_var_2: T.int32 = i0 * 5985 + i1 * 63 + i2 * 7\n                cse_var_1: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 11 * 6650 + i0 * 3325 + i1 * 35 + i2 * 5\n                pad_temp_1[cse_var_2] = T.float32(0)\n                data_1 = T.Buffer((33250,), data=data.data)\n                pad_temp_1[cse_var_2 + 1] = T.if_then_else(cse_var_3, data_1[cse_var_1 - 5], T.float32(0))\n                pad_temp_1[cse_var_2 + 2] = T.if_then_else(cse_var_3, data_1[cse_var_1 - 4], T.float32(0))\n                pad_temp_1[cse_var_2 + 3] = T.if_then_else(cse_var_3, data_1[cse_var_1 - 3], T.float32(0))\n                pad_temp_1[cse_var_2 + 4] = T.if_then_else(cse_var_3, data_1[cse_var_1 - 2], T.float32(0))\n                pad_temp_1[cse_var_2 + 5] = T.if_then_else(cse_var_3, data_1[cse_var_1 - 1], T.float32(0))\n                pad_temp_1[cse_var_2 + 6] = T.float32(0)\n            conv2d_nchw_1 = T.Buffer((13860,), data=conv2d_nchw.data)\n            for nn_outer_inner_init, yy_outer_inner_init in T.grid(2, 7):\n                cse_var_4: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 11 * 2772 + nn_outer_inner_init * 1386 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 11 * 126 + yy_outer_inner_init * 2\n                conv2d_nchw_1[cse_var_4:cse_var_4 + 2] = T.Broadcast(T.float32(0), 2)\n                conv2d_nchw_1[cse_var_4 + 14:cse_var_4 + 14 + 2] = T.Broadcast(T.float32(0), 2)\n                conv2d_nchw_1[cse_var_4 + 28:cse_var_4 + 28 + 2] = T.Broadcast(T.float32(0), 2)\n                conv2d_nchw_1[cse_var_4 + 42:cse_var_4 + 42 + 2] = T.Broadcast(T.float32(0), 2)\n                conv2d_nchw_1[cse_var_4 + 56:cse_var_4 + 56 + 2] = T.Broadcast(T.float32(0), 2)\n                conv2d_nchw_1[cse_var_4 + 70:cse_var_4 + 70 + 2] = T.Broadcast(T.float32(0), 2)\n                conv2d_nchw_1[cse_var_4 + 84:cse_var_4 + 84 + 2] = T.Broadcast(T.float32(0), 2)\n                conv2d_nchw_1[cse_var_4 + 98:cse_var_4 + 98 + 2] = T.Broadcast(T.float32(0), 2)\n                conv2d_nchw_1[cse_var_4 + 112:cse_var_4 + 112 + 2] = T.Broadcast(T.float32(0), 2)\n            for rc_outer, rx_outer, nn_outer_inner, yy_outer_inner, rc_inner, ry_inner in T.grid(19, 4, 2, 7, 5, 3):\n                cse_var_16: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 11\n                cse_var_15: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 11 * 2772 + nn_outer_inner * 1386 + cse_var_16 * 126 + yy_outer_inner * 2\n                cse_var_14: T.int32 = cse_var_16 * 10260 + rc_outer * 60 + rc_inner * 12 + ry_inner * 4 + rx_outer\n                cse_var_13: T.int32 = cse_var_15 + 98\n                cse_var_12: T.int32 = cse_var_15 + 84\n                cse_var_11: T.int32 = cse_var_15 + 70\n                cse_var_10: T.int32 = cse_var_15 + 56\n                cse_var_9: T.int32 = cse_var_15 + 42\n                cse_var_8: T.int32 = cse_var_15 + 28\n                cse_var_7: T.int32 = cse_var_15 + 14\n                cse_var_6: T.int32 = cse_var_15 + 112\n                cse_var_5: T.int32 = nn_outer_inner * 5985 + rc_outer * 315 + rc_inner * 63 + yy_outer_inner * 7 + ry_inner * 7 + rx_outer\n                kernel_1 = T.Buffer((112860,), data=kernel.data)\n                conv2d_nchw_1[cse_var_15:cse_var_15 + 2] = conv2d_nchw_1[cse_var_15:cse_var_15 + 2] + pad_temp_1[cse_var_5:cse_var_5 + 6:3] * T.Broadcast(kernel_1[cse_var_14], 2)\n                conv2d_nchw_1[cse_var_7:cse_var_7 + 2] = conv2d_nchw_1[cse_var_7:cse_var_7 + 2] + pad_temp_1[cse_var_5:cse_var_5 + 6:3] * T.Broadcast(kernel_1[cse_var_14 + 1140], 2)\n                conv2d_nchw_1[cse_var_8:cse_var_8 + 2] = conv2d_nchw_1[cse_var_8:cse_var_8 + 2] + pad_temp_1[cse_var_5:cse_var_5 + 6:3] * T.Broadcast(kernel_1[cse_var_14 + 2280], 2)\n                conv2d_nchw_1[cse_var_9:cse_var_9 + 2] = conv2d_nchw_1[cse_var_9:cse_var_9 + 2] + pad_temp_1[cse_var_5:cse_var_5 + 6:3] * T.Broadcast(kernel_1[cse_var_14 + 3420], 2)\n                conv2d_nchw_1[cse_var_10:cse_var_10 + 2] = conv2d_nchw_1[cse_var_10:cse_var_10 + 2] + pad_temp_1[cse_var_5:cse_var_5 + 6:3] * T.Broadcast(kernel_1[cse_var_14 + 4560], 2)\n                conv2d_nchw_1[cse_var_11:cse_var_11 + 2] = conv2d_nchw_1[cse_var_11:cse_var_11 + 2] + pad_temp_1[cse_var_5:cse_var_5 + 6:3] * T.Broadcast(kernel_1[cse_var_14 + 5700], 2)\n                conv2d_nchw_1[cse_var_12:cse_var_12 + 2] = conv2d_nchw_1[cse_var_12:cse_var_12 + 2] + pad_temp_1[cse_var_5:cse_var_5 + 6:3] * T.Broadcast(kernel_1[cse_var_14 + 6840], 2)\n                conv2d_nchw_1[cse_var_13:cse_var_13 + 2] = conv2d_nchw_1[cse_var_13:cse_var_13 + 2] + pad_temp_1[cse_var_5:cse_var_5 + 6:3] * T.Broadcast(kernel_1[cse_var_14 + 7980], 2)\n                conv2d_nchw_1[cse_var_6:cse_var_6 + 2] = conv2d_nchw_1[cse_var_6:cse_var_6 + 2] + pad_temp_1[cse_var_5:cse_var_5 + 6:3] * T.Broadcast(kernel_1[cse_var_14 + 9120], 2)\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(385) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(385) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[18];\n  __shared__ float pad_temp_shared[1125];\n  __shared__ float kernel_shared[2970];\n  conv2d_nchw_local[0] = 0.000000e+00f;\n  conv2d_nchw_local[9] = 0.000000e+00f;\n  conv2d_nchw_local[1] = 0.000000e+00f;\n  conv2d_nchw_local[10] = 0.000000e+00f;\n  conv2d_nchw_local[2] = 0.000000e+00f;\n  conv2d_nchw_local[11] = 0.000000e+00f;\n  conv2d_nchw_local[3] = 0.000000e+00f;\n  conv2d_nchw_local[12] = 0.000000e+00f;\n  conv2d_nchw_local[4] = 0.000000e+00f;\n  conv2d_nchw_local[13] = 0.000000e+00f;\n  conv2d_nchw_local[5] = 0.000000e+00f;\n  conv2d_nchw_local[14] = 0.000000e+00f;\n  conv2d_nchw_local[6] = 0.000000e+00f;\n  conv2d_nchw_local[15] = 0.000000e+00f;\n  conv2d_nchw_local[7] = 0.000000e+00f;\n  conv2d_nchw_local[16] = 0.000000e+00f;\n  conv2d_nchw_local[8] = 0.000000e+00f;\n  conv2d_nchw_local[17] = 0.000000e+00f;\n  for (int rc_outer_outer = 0; rc_outer_outer < 19; ++rc_outer_outer) {\n    for (int rx_outer_outer = 0; rx_outer_outer < 2; ++rx_outer_outer) {\n      __syncthreads();\n      pad_temp_shared[((int)threadIdx.x)] = (((((5 <= (((int)threadIdx.x) % 45)) && ((((int)threadIdx.x) % 45) < 40)) && (1 <= ((rx_outer_outer * 2) + (((int)threadIdx.x) % 5)))) && ((((((int)threadIdx.x) % 5) >> 1) + rx_outer_outer) < 3)) ? data[(((((((((int)blockIdx.x) * 16625) + ((((int)threadIdx.x) / 225) * 3325)) + (rc_outer_outer * 175)) + (((((int)threadIdx.x) % 225) / 45) * 35)) + (rx_outer_outer * 2)) + (((int)threadIdx.x) % 45)) - 6)] : 0.000000e+00f);\n      pad_temp_shared[((((((((int)threadIdx.x) + 385) / 225) * 225) + (((((((int)threadIdx.x) / 5) + 32) % 45) / 9) * 45)) + ((((((int)threadIdx.x) / 5) + 5) % 9) * 5)) + (((int)threadIdx.x) % 5))] = (((((1 <= (((((int)threadIdx.x) / 5) + 5) % 9)) && ((((((int)threadIdx.x) / 5) + 5) % 9) < 8)) && (1 <= ((rx_outer_outer * 2) + (((int)threadIdx.x) % 5)))) && ((((((int)threadIdx.x) % 5) >> 1) + rx_outer_outer) < 3)) ? data[((((((((((int)blockIdx.x) * 16625) + (((((int)threadIdx.x) + 385) / 225) * 3325)) + (rc_outer_outer * 175)) + (((((((int)threadIdx.x) / 5) + 32) % 45) / 9) * 35)) + ((((((int)threadIdx.x) / 5) + 5) % 9) * 5)) + (rx_outer_outer * 2)) + (((int)threadIdx.x) % 5)) - 6)] : 0.000000e+00f);\n      if (((int)threadIdx.x) < 355) {\n        pad_temp_shared[((((((((int)threadIdx.x) + 770) / 225) * 225) + (((((((int)threadIdx.x) / 5) + 19) % 45) / 9) * 45)) + ((((((int)threadIdx.x) / 5) + 1) % 9) * 5)) + (((int)threadIdx.x) % 5))] = (((((1 <= (((((int)threadIdx.x) / 5) + 1) % 9)) && ((((((int)threadIdx.x) / 5) + 1) % 9) < 8)) && (1 <= ((rx_outer_outer * 2) + (((int)threadIdx.x) % 5)))) && ((((((int)threadIdx.x) % 5) >> 1) + rx_outer_outer) < 3)) ? data[((((((((((int)blockIdx.x) * 16625) + (((((int)threadIdx.x) + 770) / 225) * 3325)) + (rc_outer_outer * 175)) + (((((((int)threadIdx.x) / 5) + 19) % 45) / 9) * 35)) + ((((((int)threadIdx.x) / 5) + 1) % 9) * 5)) + (rx_outer_outer * 2)) + (((int)threadIdx.x) % 5)) - 6)] : 0.000000e+00f);\n      }\n      kernel_shared[((int)threadIdx.x)] = kernel[((((((((int)threadIdx.x) / 30) * 1140) + (rc_outer_outer * 60)) + (((((int)threadIdx.x) % 30) >> 1) * 4)) + (rx_outer_outer * 2)) + (((int)threadIdx.x) & 1))];\n      kernel_shared[(((int)threadIdx.x) + 385)] = kernel[(((((((((int)threadIdx.x) + 385) / 30) * 1140) + (rc_outer_outer * 60)) + ((((((int)threadIdx.x) + 25) % 30) >> 1) * 4)) + (rx_outer_outer * 2)) + ((((int)threadIdx.x) + 1) & 1))];\n      kernel_shared[(((int)threadIdx.x) + 770)] = kernel[(((((((((int)threadIdx.x) + 770) / 30) * 1140) + (rc_outer_outer * 60)) + ((((((int)threadIdx.x) >> 1) + 10) % 15) * 4)) + (rx_outer_outer * 2)) + (((int)threadIdx.x) & 1))];\n      kernel_shared[(((int)threadIdx.x) + 1155)] = kernel[(((((((((int)threadIdx.x) + 1155) / 30) * 1140) + (rc_outer_outer * 60)) + ((((((int)threadIdx.x) + 15) % 30) >> 1) * 4)) + (rx_outer_outer * 2)) + ((((int)threadIdx.x) + 1) & 1))];\n      kernel_shared[(((int)threadIdx.x) + 1540)] = kernel[(((((((((int)threadIdx.x) + 1540) / 30) * 1140) + (rc_outer_outer * 60)) + ((((((int)threadIdx.x) >> 1) + 5) % 15) * 4)) + (rx_outer_outer * 2)) + (((int)threadIdx.x) & 1))];\n      kernel_shared[(((int)threadIdx.x) + 1925)] = kernel[(((((((((int)threadIdx.x) + 1925) / 30) * 1140) + (rc_outer_outer * 60)) + ((((((int)threadIdx.x) + 5) % 30) >> 1) * 4)) + (rx_outer_outer * 2)) + ((((int)threadIdx.x) + 1) & 1))];\n      kernel_shared[(((int)threadIdx.x) + 2310)] = kernel[(((((((((int)threadIdx.x) / 30) * 1140) + (rc_outer_outer * 60)) + (((((int)threadIdx.x) % 30) >> 1) * 4)) + (rx_outer_outer * 2)) + (((int)threadIdx.x) & 1)) + 87780)];\n      if (((int)threadIdx.x) < 275) {\n        kernel_shared[(((int)threadIdx.x) + 2695)] = kernel[(((((((((int)threadIdx.x) + 2695) / 30) * 1140) + (rc_outer_outer * 60)) + ((((((int)threadIdx.x) + 25) % 30) >> 1) * 4)) + (rx_outer_outer * 2)) + ((((int)threadIdx.x) + 1) & 1))];\n      }\n      __syncthreads();\n      for (int rc_outer_inner = 0; rc_outer_inner < 5; ++rc_outer_inner) {\n        conv2d_nchw_local[0] = (conv2d_nchw_local[0] + (pad_temp_shared[((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5))] * kernel_shared[((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6))]));\n        conv2d_nchw_local[9] = (conv2d_nchw_local[9] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 3)] * kernel_shared[((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6))]));\n        conv2d_nchw_local[0] = (conv2d_nchw_local[0] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 5)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 2)]));\n        conv2d_nchw_local[9] = (conv2d_nchw_local[9] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 8)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 2)]));\n        conv2d_nchw_local[0] = (conv2d_nchw_local[0] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 10)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 4)]));\n        conv2d_nchw_local[9] = (conv2d_nchw_local[9] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 13)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 4)]));\n        conv2d_nchw_local[1] = (conv2d_nchw_local[1] + (pad_temp_shared[((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5))] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 30)]));\n        conv2d_nchw_local[10] = (conv2d_nchw_local[10] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 3)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 30)]));\n        conv2d_nchw_local[1] = (conv2d_nchw_local[1] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 5)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 32)]));\n        conv2d_nchw_local[10] = (conv2d_nchw_local[10] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 8)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 32)]));\n        conv2d_nchw_local[1] = (conv2d_nchw_local[1] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 10)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 34)]));\n        conv2d_nchw_local[10] = (conv2d_nchw_local[10] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 13)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 34)]));\n        conv2d_nchw_local[2] = (conv2d_nchw_local[2] + (pad_temp_shared[((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5))] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 60)]));\n        conv2d_nchw_local[11] = (conv2d_nchw_local[11] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 3)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 60)]));\n        conv2d_nchw_local[2] = (conv2d_nchw_local[2] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 5)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 62)]));\n        conv2d_nchw_local[11] = (conv2d_nchw_local[11] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 8)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 62)]));\n        conv2d_nchw_local[2] = (conv2d_nchw_local[2] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 10)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 64)]));\n        conv2d_nchw_local[11] = (conv2d_nchw_local[11] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 13)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 64)]));\n        conv2d_nchw_local[3] = (conv2d_nchw_local[3] + (pad_temp_shared[((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5))] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 90)]));\n        conv2d_nchw_local[12] = (conv2d_nchw_local[12] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 3)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 90)]));\n        conv2d_nchw_local[3] = (conv2d_nchw_local[3] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 5)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 92)]));\n        conv2d_nchw_local[12] = (conv2d_nchw_local[12] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 8)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 92)]));\n        conv2d_nchw_local[3] = (conv2d_nchw_local[3] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 10)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 94)]));\n        conv2d_nchw_local[12] = (conv2d_nchw_local[12] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 13)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 94)]));\n        conv2d_nchw_local[4] = (conv2d_nchw_local[4] + (pad_temp_shared[((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5))] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 120)]));\n        conv2d_nchw_local[13] = (conv2d_nchw_local[13] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 3)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 120)]));\n        conv2d_nchw_local[4] = (conv2d_nchw_local[4] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 5)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 122)]));\n        conv2d_nchw_local[13] = (conv2d_nchw_local[13] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 8)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 122)]));\n        conv2d_nchw_local[4] = (conv2d_nchw_local[4] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 10)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 124)]));\n        conv2d_nchw_local[13] = (conv2d_nchw_local[13] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 13)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 124)]));\n        conv2d_nchw_local[5] = (conv2d_nchw_local[5] + (pad_temp_shared[((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5))] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 150)]));\n        conv2d_nchw_local[14] = (conv2d_nchw_local[14] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 3)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 150)]));\n        conv2d_nchw_local[5] = (conv2d_nchw_local[5] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 5)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 152)]));\n        conv2d_nchw_local[14] = (conv2d_nchw_local[14] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 8)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 152)]));\n        conv2d_nchw_local[5] = (conv2d_nchw_local[5] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 10)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 154)]));\n        conv2d_nchw_local[14] = (conv2d_nchw_local[14] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 13)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 154)]));\n        conv2d_nchw_local[6] = (conv2d_nchw_local[6] + (pad_temp_shared[((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5))] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 180)]));\n        conv2d_nchw_local[15] = (conv2d_nchw_local[15] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 3)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 180)]));\n        conv2d_nchw_local[6] = (conv2d_nchw_local[6] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 5)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 182)]));\n        conv2d_nchw_local[15] = (conv2d_nchw_local[15] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 8)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 182)]));\n        conv2d_nchw_local[6] = (conv2d_nchw_local[6] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 10)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 184)]));\n        conv2d_nchw_local[15] = (conv2d_nchw_local[15] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 13)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 184)]));\n        conv2d_nchw_local[7] = (conv2d_nchw_local[7] + (pad_temp_shared[((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5))] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 210)]));\n        conv2d_nchw_local[16] = (conv2d_nchw_local[16] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 3)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 210)]));\n        conv2d_nchw_local[7] = (conv2d_nchw_local[7] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 5)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 212)]));\n        conv2d_nchw_local[16] = (conv2d_nchw_local[16] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 8)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 212)]));\n        conv2d_nchw_local[7] = (conv2d_nchw_local[7] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 10)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 214)]));\n        conv2d_nchw_local[16] = (conv2d_nchw_local[16] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 13)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 214)]));\n        conv2d_nchw_local[8] = (conv2d_nchw_local[8] + (pad_temp_shared[((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5))] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 240)]));\n        conv2d_nchw_local[17] = (conv2d_nchw_local[17] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 3)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 240)]));\n        conv2d_nchw_local[8] = (conv2d_nchw_local[8] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 5)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 242)]));\n        conv2d_nchw_local[17] = (conv2d_nchw_local[17] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 8)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 242)]));\n        conv2d_nchw_local[8] = (conv2d_nchw_local[8] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 10)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 244)]));\n        conv2d_nchw_local[17] = (conv2d_nchw_local[17] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 13)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 244)]));\n        conv2d_nchw_local[0] = (conv2d_nchw_local[0] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 1)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 1)]));\n        conv2d_nchw_local[9] = (conv2d_nchw_local[9] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 4)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 1)]));\n        conv2d_nchw_local[0] = (conv2d_nchw_local[0] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 6)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 3)]));\n        conv2d_nchw_local[9] = (conv2d_nchw_local[9] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 9)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 3)]));\n        conv2d_nchw_local[0] = (conv2d_nchw_local[0] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 11)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 5)]));\n        conv2d_nchw_local[9] = (conv2d_nchw_local[9] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 14)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 5)]));\n        conv2d_nchw_local[1] = (conv2d_nchw_local[1] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 1)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 31)]));\n        conv2d_nchw_local[10] = (conv2d_nchw_local[10] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 4)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 31)]));\n        conv2d_nchw_local[1] = (conv2d_nchw_local[1] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 6)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 33)]));\n        conv2d_nchw_local[10] = (conv2d_nchw_local[10] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 9)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 33)]));\n        conv2d_nchw_local[1] = (conv2d_nchw_local[1] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 11)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 35)]));\n        conv2d_nchw_local[10] = (conv2d_nchw_local[10] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 14)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 35)]));\n        conv2d_nchw_local[2] = (conv2d_nchw_local[2] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 1)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 61)]));\n        conv2d_nchw_local[11] = (conv2d_nchw_local[11] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 4)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 61)]));\n        conv2d_nchw_local[2] = (conv2d_nchw_local[2] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 6)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 63)]));\n        conv2d_nchw_local[11] = (conv2d_nchw_local[11] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 9)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 63)]));\n        conv2d_nchw_local[2] = (conv2d_nchw_local[2] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 11)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 65)]));\n        conv2d_nchw_local[11] = (conv2d_nchw_local[11] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 14)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 65)]));\n        conv2d_nchw_local[3] = (conv2d_nchw_local[3] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 1)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 91)]));\n        conv2d_nchw_local[12] = (conv2d_nchw_local[12] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 4)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 91)]));\n        conv2d_nchw_local[3] = (conv2d_nchw_local[3] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 6)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 93)]));\n        conv2d_nchw_local[12] = (conv2d_nchw_local[12] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 9)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 93)]));\n        conv2d_nchw_local[3] = (conv2d_nchw_local[3] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 11)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 95)]));\n        conv2d_nchw_local[12] = (conv2d_nchw_local[12] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 14)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 95)]));\n        conv2d_nchw_local[4] = (conv2d_nchw_local[4] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 1)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 121)]));\n        conv2d_nchw_local[13] = (conv2d_nchw_local[13] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 4)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 121)]));\n        conv2d_nchw_local[4] = (conv2d_nchw_local[4] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 6)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 123)]));\n        conv2d_nchw_local[13] = (conv2d_nchw_local[13] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 9)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 123)]));\n        conv2d_nchw_local[4] = (conv2d_nchw_local[4] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 11)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 125)]));\n        conv2d_nchw_local[13] = (conv2d_nchw_local[13] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 14)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 125)]));\n        conv2d_nchw_local[5] = (conv2d_nchw_local[5] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 1)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 151)]));\n        conv2d_nchw_local[14] = (conv2d_nchw_local[14] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 4)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 151)]));\n        conv2d_nchw_local[5] = (conv2d_nchw_local[5] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 6)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 153)]));\n        conv2d_nchw_local[14] = (conv2d_nchw_local[14] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 9)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 153)]));\n        conv2d_nchw_local[5] = (conv2d_nchw_local[5] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 11)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 155)]));\n        conv2d_nchw_local[14] = (conv2d_nchw_local[14] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 14)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 155)]));\n        conv2d_nchw_local[6] = (conv2d_nchw_local[6] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 1)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 181)]));\n        conv2d_nchw_local[15] = (conv2d_nchw_local[15] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 4)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 181)]));\n        conv2d_nchw_local[6] = (conv2d_nchw_local[6] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 6)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 183)]));\n        conv2d_nchw_local[15] = (conv2d_nchw_local[15] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 9)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 183)]));\n        conv2d_nchw_local[6] = (conv2d_nchw_local[6] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 11)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 185)]));\n        conv2d_nchw_local[15] = (conv2d_nchw_local[15] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 14)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 185)]));\n        conv2d_nchw_local[7] = (conv2d_nchw_local[7] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 1)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 211)]));\n        conv2d_nchw_local[16] = (conv2d_nchw_local[16] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 4)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 211)]));\n        conv2d_nchw_local[7] = (conv2d_nchw_local[7] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 6)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 213)]));\n        conv2d_nchw_local[16] = (conv2d_nchw_local[16] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 9)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 213)]));\n        conv2d_nchw_local[7] = (conv2d_nchw_local[7] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 11)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 215)]));\n        conv2d_nchw_local[16] = (conv2d_nchw_local[16] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 14)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 215)]));\n        conv2d_nchw_local[8] = (conv2d_nchw_local[8] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 1)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 241)]));\n        conv2d_nchw_local[17] = (conv2d_nchw_local[17] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 4)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 241)]));\n        conv2d_nchw_local[8] = (conv2d_nchw_local[8] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 6)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 243)]));\n        conv2d_nchw_local[17] = (conv2d_nchw_local[17] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 9)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 243)]));\n        conv2d_nchw_local[8] = (conv2d_nchw_local[8] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 11)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 245)]));\n        conv2d_nchw_local[17] = (conv2d_nchw_local[17] + (pad_temp_shared[(((((((int)threadIdx.x) / 77) * 225) + (rc_outer_inner * 45)) + ((((int)threadIdx.x) % 7) * 5)) + 14)] * kernel_shared[(((((((int)threadIdx.x) % 77) / 7) * 270) + (rc_outer_inner * 6)) + 245)]));\n      }\n    }\n  }\n  for (int ff_inner = 0; ff_inner < 9; ++ff_inner) {\n    conv2d_nchw[((((((int)blockIdx.x) * 6930) + ((((int)threadIdx.x) / 7) * 126)) + (ff_inner * 14)) + ((((int)threadIdx.x) % 7) * 2))] = conv2d_nchw_local[ff_inner];\n    conv2d_nchw[(((((((int)blockIdx.x) * 6930) + ((((int)threadIdx.x) / 7) * 126)) + (ff_inner * 14)) + ((((int)threadIdx.x) % 7) * 2)) + 1)] = conv2d_nchw_local[(ff_inner + 9)];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t bias_code = arg_type_ids[2];\n  int32_t conv2d_nchw_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* bias = (((TVMValue*)args)[2].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* bias_1 = (((DLTensor*)bias)[0].data);\n  void* default_function_bias_shape = (((DLTensor*)bias)[0].shape);\n  void* default_function_bias_strides = (((DLTensor*)bias)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_kernel_strides == NULL)) {\n  }\n  if (!(default_function_bias_strides == NULL)) {\n  }\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  }\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused < 55; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)47880, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    }\n    for (int32_t i0 = 0; i0 < 2; ++i0) {\n      for (int32_t i1 = 0; i1 < 95; ++i1) {\n        for (int32_t i2 = 0; i2 < 9; ++i2) {\n          bool cse_var_3 = ((1 <= i2) && (i2 < 8));\n          int32_t cse_var_2 = (((i0 * 5985) + (i1 * 63)) + (i2 * 7));\n          int32_t cse_var_1 = (((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused / 11) * 6650) + (i0 * 3325)) + (i1 * 35)) + (i2 * 5));\n          ((float*)pad_temp)[cse_var_2] = 0.000000e+00f;\n          ((float*)pad_temp)[(cse_var_2 + 1)] = (cse_var_3 ? ((float*)data_1)[(cse_var_1 - 5)] : 0.000000e+00f);\n          ((float*)pad_temp)[(cse_var_2 + 2)] = (cse_var_3 ? ((float*)data_1)[(cse_var_1 - 4)] : 0.000000e+00f);\n          ((float*)pad_temp)[(cse_var_2 + 3)] = (cse_var_3 ? ((float*)data_1)[(cse_var_1 - 3)] : 0.000000e+00f);\n          ((float*)pad_temp)[(cse_var_2 + 4)] = (cse_var_3 ? ((float*)data_1)[(cse_var_1 - 2)] : 0.000000e+00f);\n          ((float*)pad_temp)[(cse_var_2 + 5)] = (cse_var_3 ? ((float*)data_1)[(cse_var_1 - 1)] : 0.000000e+00f);\n          ((float*)pad_temp)[(cse_var_2 + 6)] = 0.000000e+00f;\n        }\n      }\n    }\n    for (int32_t nn_outer_inner_init = 0; nn_outer_inner_init < 2; ++nn_outer_inner_init) {\n      for (int32_t yy_outer_inner_init = 0; yy_outer_inner_init < 7; ++yy_outer_inner_init) {\n        int32_t cse_var_4 = (((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused / 11) * 2772) + (nn_outer_inner_init * 1386)) + ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 11) * 126)) + (yy_outer_inner_init * 2));\n        *(float2*)(((float*)conv2d_nchw_1) + cse_var_4) = ((float2)(0.000000e+00f, 0.000000e+00f));\n        *(float2*)(((float*)conv2d_nchw_1) + (cse_var_4 + 14)) = ((float2)(0.000000e+00f, 0.000000e+00f));\n        *(float2*)(((float*)conv2d_nchw_1) + (cse_var_4 + 28)) = ((float2)(0.000000e+00f, 0.000000e+00f));\n        *(float2*)(((float*)conv2d_nchw_1) + (cse_var_4 + 42)) = ((float2)(0.000000e+00f, 0.000000e+00f));\n        *(float2*)(((float*)conv2d_nchw_1) + (cse_var_4 + 56)) = ((float2)(0.000000e+00f, 0.000000e+00f));\n        *(float2*)(((float*)conv2d_nchw_1) + (cse_var_4 + 70)) = ((float2)(0.000000e+00f, 0.000000e+00f));\n        *(float2*)(((float*)conv2d_nchw_1) + (cse_var_4 + 84)) = ((float2)(0.000000e+00f, 0.000000e+00f));\n        *(float2*)(((float*)conv2d_nchw_1) + (cse_var_4 + 98)) = ((float2)(0.000000e+00f, 0.000000e+00f));\n        *(float2*)(((float*)conv2d_nchw_1) + (cse_var_4 + 112)) = ((float2)(0.000000e+00f, 0.000000e+00f));\n      }\n    }\n    for (int32_t rc_outer = 0; rc_outer < 19; ++rc_outer) {\n      for (int32_t rx_outer = 0; rx_outer < 4; ++rx_outer) {\n        for (int32_t nn_outer_inner = 0; nn_outer_inner < 2; ++nn_outer_inner) {\n          for (int32_t yy_outer_inner = 0; yy_outer_inner < 7; ++yy_outer_inner) {\n            for (int32_t rc_inner = 0; rc_inner < 5; ++rc_inner) {\n              for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n                int32_t cse_var_16 = (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 11);\n                int32_t cse_var_15 = (((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused / 11) * 2772) + (nn_outer_inner * 1386)) + (cse_var_16 * 126)) + (yy_outer_inner * 2));\n                int32_t cse_var_14 = (((((cse_var_16 * 10260) + (rc_outer * 60)) + (rc_inner * 12)) + (ry_inner * 4)) + rx_outer);\n                int32_t cse_var_13 = (cse_var_15 + 98);\n                int32_t cse_var_12 = (cse_var_15 + 84);\n                int32_t cse_var_11 = (cse_var_15 + 70);\n                int32_t cse_var_10 = (cse_var_15 + 56);\n                int32_t cse_var_9 = (cse_var_15 + 42);\n                int32_t cse_var_8 = (cse_var_15 + 28);\n                int32_t cse_var_7 = (cse_var_15 + 14);\n                int32_t cse_var_6 = (cse_var_15 + 112);\n                int32_t cse_var_5 = ((((((nn_outer_inner * 5985) + (rc_outer * 315)) + (rc_inner * 63)) + (yy_outer_inner * 7)) + (ry_inner * 7)) + rx_outer);\n                int32_t2 v_ = int32_t2((cse_var_15)+(1*0), (cse_var_15)+(1*1));\n                int32_t2 v__1 = int32_t2((cse_var_5)+(3*0), (cse_var_5)+(3*1));\n                *(float2*)(((float*)conv2d_nchw_1) + cse_var_15) = ((float2(((float*)conv2d_nchw_1)[v_.s0],((float*)conv2d_nchw_1)[v_.s1])) + ((float2(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1])) * ((float2)(((float*)kernel_1)[cse_var_14], ((float*)kernel_1)[cse_var_14]))));\n                int32_t2 v__2 = int32_t2((cse_var_7)+(1*0), (cse_var_7)+(1*1));\n                *(float2*)(((float*)conv2d_nchw_1) + cse_var_7) = ((float2(((float*)conv2d_nchw_1)[v__2.s0],((float*)conv2d_nchw_1)[v__2.s1])) + ((float2(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1])) * ((float2)(((float*)kernel_1)[(cse_var_14 + 1140)], ((float*)kernel_1)[(cse_var_14 + 1140)]))));\n                int32_t2 v__3 = int32_t2((cse_var_8)+(1*0), (cse_var_8)+(1*1));\n                *(float2*)(((float*)conv2d_nchw_1) + cse_var_8) = ((float2(((float*)conv2d_nchw_1)[v__3.s0],((float*)conv2d_nchw_1)[v__3.s1])) + ((float2(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1])) * ((float2)(((float*)kernel_1)[(cse_var_14 + 2280)], ((float*)kernel_1)[(cse_var_14 + 2280)]))));\n                int32_t2 v__4 = int32_t2((cse_var_9)+(1*0), (cse_var_9)+(1*1));\n                *(float2*)(((float*)conv2d_nchw_1) + cse_var_9) = ((float2(((float*)conv2d_nchw_1)[v__4.s0],((float*)conv2d_nchw_1)[v__4.s1])) + ((float2(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1])) * ((float2)(((float*)kernel_1)[(cse_var_14 + 3420)], ((float*)kernel_1)[(cse_var_14 + 3420)]))));\n                int32_t2 v__5 = int32_t2((cse_var_10)+(1*0), (cse_var_10)+(1*1));\n                *(float2*)(((float*)conv2d_nchw_1) + cse_var_10) = ((float2(((float*)conv2d_nchw_1)[v__5.s0],((float*)conv2d_nchw_1)[v__5.s1])) + ((float2(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1])) * ((float2)(((float*)kernel_1)[(cse_var_14 + 4560)], ((float*)kernel_1)[(cse_var_14 + 4560)]))));\n                int32_t2 v__6 = int32_t2((cse_var_11)+(1*0), (cse_var_11)+(1*1));\n                *(float2*)(((float*)conv2d_nchw_1) + cse_var_11) = ((float2(((float*)conv2d_nchw_1)[v__6.s0],((float*)conv2d_nchw_1)[v__6.s1])) + ((float2(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1])) * ((float2)(((float*)kernel_1)[(cse_var_14 + 5700)], ((float*)kernel_1)[(cse_var_14 + 5700)]))));\n                int32_t2 v__7 = int32_t2((cse_var_12)+(1*0), (cse_var_12)+(1*1));\n                *(float2*)(((float*)conv2d_nchw_1) + cse_var_12) = ((float2(((float*)conv2d_nchw_1)[v__7.s0],((float*)conv2d_nchw_1)[v__7.s1])) + ((float2(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1])) * ((float2)(((float*)kernel_1)[(cse_var_14 + 6840)], ((float*)kernel_1)[(cse_var_14 + 6840)]))));\n                int32_t2 v__8 = int32_t2((cse_var_13)+(1*0), (cse_var_13)+(1*1));\n                *(float2*)(((float*)conv2d_nchw_1) + cse_var_13) = ((float2(((float*)conv2d_nchw_1)[v__8.s0],((float*)conv2d_nchw_1)[v__8.s1])) + ((float2(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1])) * ((float2)(((float*)kernel_1)[(cse_var_14 + 7980)], ((float*)kernel_1)[(cse_var_14 + 7980)]))));\n                int32_t2 v__9 = int32_t2((cse_var_6)+(1*0), (cse_var_6)+(1*1));\n                *(float2*)(((float*)conv2d_nchw_1) + cse_var_6) = ((float2(((float*)conv2d_nchw_1)[v__9.s0],((float*)conv2d_nchw_1)[v__9.s1])) + ((float2(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1])) * ((float2)(((float*)kernel_1)[(cse_var_14 + 9120)], ((float*)kernel_1)[(cse_var_14 + 9120)]))));\n              }\n            }\n          }\n        }\n      }\n    }\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "conv2d_nchw[10__7__5__99__95__3__4__(1__3)__(1__1)]"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 32, 24, 32), \"float32\"), compute: T.Buffer((10, 24576), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused in T.parallel(245760):\n            compute_1 = T.Buffer((245760,), data=compute.data)\n            data_1 = T.Buffer((245760,), data=data.data)\n            compute_1[i_j_fused] = data_1[i_j_fused]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i_j_fused = 0; i_j_fused < 245760; ++i_j_fused) {\n    ((float*)compute_1)[i_j_fused] = ((float*)data_1)[i_j_fused];\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flatten[10__32__24__32]"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 4, 48, 80), \"float32\"), resize: T.Buffer((6, 4, 96, 160), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2304):\n            for i3 in range(160):\n                resize_1 = T.Buffer((368640,), data=resize.data)\n                data_1 = T.Buffer((92160,), data=data.data)\n                resize_1[i0_i1_fused_i2_fused * 160 + i3] = data_1[i0_i1_fused_i2_fused // 96 * 3840 + T.Div(i0_i1_fused_i2_fused % 96, 2) * 80 + T.Div(i3, 2)]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ resize);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) / 320) * 3840) + ((((((((int)blockIdx.x) % 320) * 3) + (((int)threadIdx.x) >> 4)) / 10) / 2) * 80)) + ((((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) % 160) / 2))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t resize_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* resize = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* resize_1 = (((DLTensor*)resize)[0].data);\n  void* default_function_resize_shape = (((DLTensor*)resize)[0].shape);\n  void* default_function_resize_strides = (((DLTensor*)resize)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_resize_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2304; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 160; ++i3) {\n      ((float*)resize_1)[((i0_i1_fused_i2_fused * 160) + i3)] = ((float*)data_1)[((((i0_i1_fused_i2_fused / 96) * 3840) + (((i0_i1_fused_i2_fused % 96) / 2) * 80)) + (i3 / 2))];\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "upsampling[6__4__24__40]"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data_a: T.Buffer((1, 16, 24), \"float32\"), data_b: T.Buffer((1, 16, 24), \"float32\"), T_concat: T.Buffer((2, 16, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(2):\n            for ax1, ax2 in T.grid(16, 24):\n                cse_var_1: T.int32 = ax0 * 384 + ax1 * 24 + ax2\n                T_concat_1 = T.Buffer((768,), data=T_concat.data)\n                data_b_1 = T.Buffer((384,), data=data_b.data)\n                data_a_1 = T.Buffer((384,), data=data_a.data)\n                T_concat_1[cse_var_1] = T.if_then_else(ax0 == 1, data_b_1[cse_var_1 - 384], data_a_1[cse_var_1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ data_a, float* __restrict__ data_b);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ data_a, float* __restrict__ data_b) {\n  T_concat[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((6 <= ((int)blockIdx.x)) ? data_b[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) - 384)] : data_a[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_a_code = arg_type_ids[0];\n  int32_t data_b_code = arg_type_ids[1];\n  int32_t T_concat_code = arg_type_ids[2];\n  void* data_a = (((TVMValue*)args)[0].v_handle);\n  void* data_b = (((TVMValue*)args)[1].v_handle);\n  void* T_concat = (((TVMValue*)args)[2].v_handle);\n  void* data_a_1 = (((DLTensor*)data_a)[0].data);\n  void* default_function_data_a_shape = (((DLTensor*)data_a)[0].shape);\n  void* default_function_data_a_strides = (((DLTensor*)data_a)[0].strides);\n  int32_t dev_id = (((DLTensor*)data_a)[0].device.device_id);\n  void* data_b_1 = (((DLTensor*)data_b)[0].data);\n  void* default_function_data_b_shape = (((DLTensor*)data_b)[0].shape);\n  void* default_function_data_b_strides = (((DLTensor*)data_b)[0].strides);\n  void* T_concat_1 = (((DLTensor*)T_concat)[0].data);\n  void* default_function_T_concat_shape = (((DLTensor*)T_concat)[0].shape);\n  void* default_function_T_concat_strides = (((DLTensor*)T_concat)[0].strides);\n  if (!(default_function_data_a_strides == NULL)) {\n  }\n  if (!(default_function_data_b_strides == NULL)) {\n  }\n  if (!(default_function_T_concat_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 24; ++ax2) {\n        int32_t cse_var_1 = (((ax0 * 384) + (ax1 * 24)) + ax2);\n        ((float*)T_concat_1)[cse_var_1] = ((ax0 == 1) ? ((float*)data_b_1)[(cse_var_1 - 384)] : ((float*)data_a_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "concatenate[1__16__24__40]"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 32, 4, 16), \"float32\"), adaptive_pool_max: T.Buffer((6, 32, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(192):\n            for ax2, ax3 in T.grid(8, 8):\n                adaptive_pool_max_1 = T.Buffer((12288,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0_ax1_fused * 64 + ax2 * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0 in range(T.Let(T.Select((ax2 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax2 // 2, where={cse_var_1: (ax2 + 1) // 2})):\n                    cse_var_1 = T.int32()\n                    cse_var_4: T.int32 = ax0_ax1_fused * 64\n                    cse_var_3: T.int32 = cse_var_4 + ax2 * 8 + ax3\n                    cse_var_2: T.int32 = cse_var_4 + ax2 // 2 * 16 + rv0 * 16 + ax3 * 2\n                    data_1 = T.Buffer((12288,), data=data.data)\n                    adaptive_pool_max_1[cse_var_3] = T.max(adaptive_pool_max_1[cse_var_3], data_1[cse_var_2])\n                    adaptive_pool_max_1[cse_var_3] = T.max(adaptive_pool_max_1[cse_var_3], data_1[cse_var_2 + 1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < (((((((((int)threadIdx.x) >> 3) * 4) + 4) % 8) == 0) ? ((((int)threadIdx.x) + 8) >> 4) : (((((int)threadIdx.x) + 8) >> 4) + 1)) - (((int)threadIdx.x) >> 4)); ++rv0) {\n    for (int rv1 = 0; rv1 < 2; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[(((((((int)blockIdx.x) * 64) + ((((int)threadIdx.x) >> 4) * 16)) + (rv0 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv1)]);\n    }\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 192; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        ((float*)adaptive_pool_max_1)[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < ((((((ax2 * 4) + 4) % 8) == 0) ? ((ax2 + 1) >> 1) : (((ax2 + 1) >> 1) + 1)) - (ax2 >> 1)); ++rv0) {\n          int32_t cse_var_4 = (ax0_ax1_fused * 64);\n          int32_t cse_var_3 = ((cse_var_4 + (ax2 * 8)) + ax3);\n          int32_t cse_var_2 = (((cse_var_4 + ((ax2 >> 1) * 16)) + (rv0 * 16)) + (ax3 * 2));\n          float v_ = ((float*)adaptive_pool_max_1)[cse_var_3];\n          float v__1 = ((float*)data_1)[cse_var_2];\n          ((float*)adaptive_pool_max_1)[cse_var_3] = ((v_) > (v__1) ? (v_) : (v__1));\n          float v__2 = ((float*)data_1)[(cse_var_2 + 1)];\n          ((float*)adaptive_pool_max_1)[cse_var_3] = ((v_) > (v__2) ? (v_) : (v__2));\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "adaptive_pool_max[6__32__4__16]"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 24, 12, 24), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([64], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((64,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(64):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(216, 64):\n            data_1 = T.Buffer((13824,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] + data_1[k0_k1_fused_k2_fused_k3_fused_outer * 64 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(64):\n            data_red_1[0] = data_red_1[0] + data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]\n\n### CUDA Code:\n\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 432; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[64];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 216; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 64; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 64) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "sum[2__24__12__24]"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data_a: T.Buffer((6, 20, 12), \"float32\"), data_b: T.Buffer((6, 20, 12), \"float32\"), T_concat: T.Buffer((12, 20, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(2880):\n            T_concat_1 = T.Buffer((2880,), data=T_concat.data)\n            data_b_1 = T.Buffer((1440,), data=data_b.data)\n            data_a_1 = T.Buffer((1440,), data=data_a.data)\n            T_concat_1[ax0_ax1_fused_ax2_fused] = T.if_then_else(1440 <= ax0_ax1_fused_ax2_fused, data_b_1[ax0_ax1_fused_ax2_fused - 1440], data_a_1[ax0_ax1_fused_ax2_fused])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ data_a, float* __restrict__ data_b);\nextern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ data_a, float* __restrict__ data_b) {\n  T_concat[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = ((96 <= ((int)blockIdx.x)) ? data_b[(((((int)blockIdx.x) * 15) + ((int)threadIdx.x)) - 1440)] : data_a[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_a_code = arg_type_ids[0];\n  int32_t data_b_code = arg_type_ids[1];\n  int32_t T_concat_code = arg_type_ids[2];\n  void* data_a = (((TVMValue*)args)[0].v_handle);\n  void* data_b = (((TVMValue*)args)[1].v_handle);\n  void* T_concat = (((TVMValue*)args)[2].v_handle);\n  void* data_a_1 = (((DLTensor*)data_a)[0].data);\n  void* default_function_data_a_shape = (((DLTensor*)data_a)[0].shape);\n  void* default_function_data_a_strides = (((DLTensor*)data_a)[0].strides);\n  int32_t dev_id = (((DLTensor*)data_a)[0].device.device_id);\n  void* data_b_1 = (((DLTensor*)data_b)[0].data);\n  void* default_function_data_b_shape = (((DLTensor*)data_b)[0].shape);\n  void* default_function_data_b_strides = (((DLTensor*)data_b)[0].strides);\n  void* T_concat_1 = (((DLTensor*)T_concat)[0].data);\n  void* default_function_T_concat_shape = (((DLTensor*)T_concat)[0].shape);\n  void* default_function_T_concat_strides = (((DLTensor*)T_concat)[0].strides);\n  if (!(default_function_data_a_strides == NULL)) {\n  }\n  if (!(default_function_data_b_strides == NULL)) {\n  }\n  if (!(default_function_T_concat_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2880; ++ax0_ax1_fused_ax2_fused) {\n    ((float*)T_concat_1)[ax0_ax1_fused_ax2_fused] = ((1440 <= ax0_ax1_fused_ax2_fused) ? ((float*)data_b_1)[(ax0_ax1_fused_ax2_fused - 1440)] : ((float*)data_a_1)[ax0_ax1_fused_ax2_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "concatenate[6__20__12__24]"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 20, 16, 8), \"float32\"), compute: T.Buffer((3, 2560), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i in T.parallel(3):\n            for j in range(2560):\n                cse_var_1: T.int32 = i * 2560 + j\n                compute_1 = T.Buffer((7680,), data=compute.data)\n                data_1 = T.Buffer((7680,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i = 0; i < 3; ++i) {\n    for (int32_t j = 0; j < 2560; ++j) {\n      int32_t cse_var_1 = ((i * 2560) + j);\n      ((float*)compute_1)[cse_var_1] = ((float*)data_1)[cse_var_1];\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flatten[3__20__16__8]"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 4, 8, 36), \"float32\"), DilatedInput: T.Buffer((10, 4, 8, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(10):\n            for i1, i2, i3 in T.grid(4, 8, 36):\n                cse_var_1: T.int32 = i0 * 1152 + i1 * 288 + i2 * 36 + i3\n                DilatedInput_1 = T.Buffer((11520,), data=DilatedInput.data)\n                data_1 = T.Buffer((11520,), data=data.data)\n                DilatedInput_1[cse_var_1] = data_1[cse_var_1]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data) {\n  DilatedInput[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t DilatedInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* DilatedInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* DilatedInput_1 = (((DLTensor*)DilatedInput)[0].data);\n  void* default_function_DilatedInput_shape = (((DLTensor*)DilatedInput)[0].shape);\n  void* default_function_DilatedInput_strides = (((DLTensor*)DilatedInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_DilatedInput_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        for (int32_t i3 = 0; i3 < 36; ++i3) {\n          int32_t cse_var_1 = ((((i0 * 1152) + (i1 * 288)) + (i2 * 36)) + i3);\n          ((float*)DilatedInput_1)[cse_var_1] = ((float*)data_1)[cse_var_1];\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "dilate[10__4__8__36]"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 32, 8, 8), \"float32\"), adaptive_pool_max: T.Buffer((10, 32, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(320):\n            for ax2 in range(8):\n                cse_var_8: T.int32 = ax0_ax1_fused * 64 + ax2 * 8\n                cse_var_7: T.int32 = cse_var_8 + 7\n                cse_var_6: T.int32 = cse_var_8 + 6\n                cse_var_5: T.int32 = cse_var_8 + 5\n                cse_var_4: T.int32 = cse_var_8 + 4\n                cse_var_3: T.int32 = cse_var_8 + 3\n                cse_var_2: T.int32 = cse_var_8 + 2\n                cse_var_1: T.int32 = cse_var_8 + 1\n                adaptive_pool_max_1 = T.Buffer((20480,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[cse_var_8] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((20480,), data=data.data)\n                adaptive_pool_max_1[cse_var_8] = T.max(adaptive_pool_max_1[cse_var_8], data_1[cse_var_8])\n                adaptive_pool_max_1[cse_var_1] = T.float32(-3.4028234663852886e+38)\n                adaptive_pool_max_1[cse_var_1] = T.max(adaptive_pool_max_1[cse_var_1], data_1[cse_var_1])\n                adaptive_pool_max_1[cse_var_2] = T.float32(-3.4028234663852886e+38)\n                adaptive_pool_max_1[cse_var_2] = T.max(adaptive_pool_max_1[cse_var_2], data_1[cse_var_2])\n                adaptive_pool_max_1[cse_var_3] = T.float32(-3.4028234663852886e+38)\n                adaptive_pool_max_1[cse_var_3] = T.max(adaptive_pool_max_1[cse_var_3], data_1[cse_var_3])\n                adaptive_pool_max_1[cse_var_4] = T.float32(-3.4028234663852886e+38)\n                adaptive_pool_max_1[cse_var_4] = T.max(adaptive_pool_max_1[cse_var_4], data_1[cse_var_4])\n                adaptive_pool_max_1[cse_var_5] = T.float32(-3.4028234663852886e+38)\n                adaptive_pool_max_1[cse_var_5] = T.max(adaptive_pool_max_1[cse_var_5], data_1[cse_var_5])\n                adaptive_pool_max_1[cse_var_6] = T.float32(-3.4028234663852886e+38)\n                adaptive_pool_max_1[cse_var_6] = T.max(adaptive_pool_max_1[cse_var_6], data_1[cse_var_6])\n                adaptive_pool_max_1[cse_var_7] = T.float32(-3.4028234663852886e+38)\n                adaptive_pool_max_1[cse_var_7] = T.max(adaptive_pool_max_1[cse_var_7], data_1[cse_var_7])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 320; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      int32_t cse_var_8 = ((ax0_ax1_fused * 64) + (ax2 * 8));\n      int32_t cse_var_7 = (cse_var_8 + 7);\n      int32_t cse_var_6 = (cse_var_8 + 6);\n      int32_t cse_var_5 = (cse_var_8 + 5);\n      int32_t cse_var_4 = (cse_var_8 + 4);\n      int32_t cse_var_3 = (cse_var_8 + 3);\n      int32_t cse_var_2 = (cse_var_8 + 2);\n      int32_t cse_var_1 = (cse_var_8 + 1);\n      ((float*)adaptive_pool_max_1)[cse_var_8] = -3.402823e+38f;\n      float v_ = ((float*)adaptive_pool_max_1)[cse_var_8];\n      float v__1 = ((float*)data_1)[cse_var_8];\n      ((float*)adaptive_pool_max_1)[cse_var_8] = ((v_) > (v__1) ? (v_) : (v__1));\n      ((float*)adaptive_pool_max_1)[cse_var_1] = -3.402823e+38f;\n      float v__2 = ((float*)adaptive_pool_max_1)[cse_var_1];\n      float v__3 = ((float*)data_1)[cse_var_1];\n      ((float*)adaptive_pool_max_1)[cse_var_1] = ((v__2) > (v__3) ? (v__2) : (v__3));\n      ((float*)adaptive_pool_max_1)[cse_var_2] = -3.402823e+38f;\n      float v__4 = ((float*)adaptive_pool_max_1)[cse_var_2];\n      float v__5 = ((float*)data_1)[cse_var_2];\n      ((float*)adaptive_pool_max_1)[cse_var_2] = ((v__4) > (v__5) ? (v__4) : (v__5));\n      ((float*)adaptive_pool_max_1)[cse_var_3] = -3.402823e+38f;\n      float v__6 = ((float*)adaptive_pool_max_1)[cse_var_3];\n      float v__7 = ((float*)data_1)[cse_var_3];\n      ((float*)adaptive_pool_max_1)[cse_var_3] = ((v__6) > (v__7) ? (v__6) : (v__7));\n      ((float*)adaptive_pool_max_1)[cse_var_4] = -3.402823e+38f;\n      float v__8 = ((float*)adaptive_pool_max_1)[cse_var_4];\n      float v__9 = ((float*)data_1)[cse_var_4];\n      ((float*)adaptive_pool_max_1)[cse_var_4] = ((v__8) > (v__9) ? (v__8) : (v__9));\n      ((float*)adaptive_pool_max_1)[cse_var_5] = -3.402823e+38f;\n      float v__10 = ((float*)adaptive_pool_max_1)[cse_var_5];\n      float v__11 = ((float*)data_1)[cse_var_5];\n      ((float*)adaptive_pool_max_1)[cse_var_5] = ((v__10) > (v__11) ? (v__10) : (v__11));\n      ((float*)adaptive_pool_max_1)[cse_var_6] = -3.402823e+38f;\n      float v__12 = ((float*)adaptive_pool_max_1)[cse_var_6];\n      float v__13 = ((float*)data_1)[cse_var_6];\n      ((float*)adaptive_pool_max_1)[cse_var_6] = ((v__12) > (v__13) ? (v__12) : (v__13));\n      ((float*)adaptive_pool_max_1)[cse_var_7] = -3.402823e+38f;\n      float v__14 = ((float*)adaptive_pool_max_1)[cse_var_7];\n      float v__15 = ((float*)data_1)[cse_var_7];\n      ((float*)adaptive_pool_max_1)[cse_var_7] = ((v__14) > (v__15) ? (v__14) : (v__15));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "adaptive_pool_max[10__32__8__8]"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 4, 24, 28), \"float32\"), adaptive_pool_max: T.Buffer((2, 4, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(512):\n            adaptive_pool_max_1 = T.Buffer((512,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(3, T.Let(T.Let(T.Select((cse_var_2 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - cse_var_2 * 28 // 8, where={cse_var_1: (cse_var_2 * 7 + 7) // 2}), where={cse_var_2: ax0_ax1_fused_ax2_fused_ax3_fused % 8})):\n                cse_var_2 = T.int32()\n                cse_var_1 = T.int32()\n                data_1 = T.Buffer((5376,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused], data_1[ax0_ax1_fused_ax2_fused_ax3_fused // 8 * 84 + rv0 * 28 + ax0_ax1_fused_ax2_fused_ax3_fused % 8 * 28 // 8 + rv1])\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < (((((((((int)threadIdx.x) & 7) * 4) + 4) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 7) + 7) >> 1) : (((((((int)threadIdx.x) & 7) * 7) + 7) >> 1) + 1)) - (((((int)threadIdx.x) & 7) * 28) >> 3)); ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((((int)blockIdx.x) * 336) + ((((int)threadIdx.x) >> 3) * 84)) + (rv0 * 28)) + (((((int)threadIdx.x) & 7) * 28) >> 3)) + rv1)]);\n    }\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 512; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < (((((((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 4) + 4) % 8) == 0) ? ((((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 7) + 7) >> 1) : (((((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 7) + 7) >> 1) + 1)) - (((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 28) >> 3)); ++rv1) {\n        float v_ = ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n        float v__1 = ((float*)data_1)[(((((ax0_ax1_fused_ax2_fused_ax3_fused >> 3) * 84) + (rv0 * 28)) + (((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 28) >> 3)) + rv1)];\n        ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "adaptive_pool_max[2__4__24__28]"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 24, 4, 28), \"float32\"), weight: T.Buffer((28,), \"float32\"), T_cast: T.Buffer((5, 24, 4, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(480):\n            T_multiply_red = T.allocate([1], \"float32\", \"global\")\n            for ax3 in range(28):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 28 + ax3\n                T_multiply_red_1 = T.Buffer((1,), data=T_multiply_red, align=4)\n                T_multiply_red_1[0] = T.float32(0)\n                data_1 = T.Buffer((13440,), data=data.data)\n                for k1 in range(24):\n                    cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused // 96 * 2688 + k1 * 112 + ax0_ax1_fused_ax2_fused % 4 * 28 + ax3\n                    T_multiply_red_1[0] = T_multiply_red_1[0] + data_1[cse_var_2] * data_1[cse_var_2]\n                T_cast_1 = T.Buffer((13440,), data=T_cast.data)\n                T_cast_1[cse_var_1] = data_1[cse_var_1] * weight[ax0_ax1_fused_ax2_fused % 96 // 4] * T.rsqrt(T_multiply_red_1[0] * T.float32(0.041666666666666664) + T.float32(1.0000000000000001e-05))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * weight[((((((int)blockIdx.x) % 42) * 4) + (((int)threadIdx.x) >> 4)) / 7)]) * (1.000000e+00f / sqrtf(((T_multiply_red[(((((int)blockIdx.x) / 42) * 112) + (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 112))] * 4.166667e-02f) + 1.000000e-05f))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 24; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] + (data[((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3)) / 14) * 2688) + (k1 * 112)) + (((((int)blockIdx.x) * 40) + ((int)threadIdx.x)) % 112))] * data[((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3)) / 14) * 2688) + (k1 * 112)) + (((((int)blockIdx.x) * 40) + ((int)threadIdx.x)) % 112))]));\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t weight_code = arg_type_ids[1];\n  int32_t T_cast_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* weight = (((TVMValue*)args)[1].v_handle);\n  void* T_cast = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* weight_1 = (((DLTensor*)weight)[0].data);\n  void* default_function_weight_shape = (((DLTensor*)weight)[0].shape);\n  void* default_function_weight_strides = (((DLTensor*)weight)[0].strides);\n  void* T_cast_1 = (((DLTensor*)T_cast)[0].data);\n  void* default_function_T_cast_shape = (((DLTensor*)T_cast)[0].shape);\n  void* default_function_T_cast_strides = (((DLTensor*)T_cast)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_weight_strides == NULL)) {\n  }\n  if (!(default_function_T_cast_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 480; ++ax0_ax1_fused_ax2_fused) {\n    float T_multiply_red[1];\n    for (int32_t ax3 = 0; ax3 < 28; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 28) + ax3);\n      T_multiply_red[0] = 0.000000e+00f;\n      for (int32_t k1 = 0; k1 < 24; ++k1) {\n        int32_t cse_var_2 = (((((ax0_ax1_fused_ax2_fused / 96) * 2688) + (k1 * 112)) + ((ax0_ax1_fused_ax2_fused & 3) * 28)) + ax3);\n        T_multiply_red[0] = (T_multiply_red[0] + (((float*)data_1)[cse_var_2] * ((float*)data_1)[cse_var_2]));\n      }\n      ((float*)T_cast_1)[cse_var_1] = ((((float*)data_1)[cse_var_1] * ((float*)weight_1)[((ax0_ax1_fused_ax2_fused % 96) >> 2)]) * (1.000000e+00f / sqrtf(((T_multiply_red[0] * 4.166667e-02f) + 1.000000e-05f))));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "rms_norm[5__24__4__28]"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 40, 8, 24), \"float32\"), DilatedInput: T.Buffer((1, 40, 8, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(40, 8, 24):\n            cse_var_1: T.int32 = i1 * 192 + i2 * 24 + i3\n            DilatedInput_1 = T.Buffer((7680,), data=DilatedInput.data)\n            data_1 = T.Buffer((7680,), data=data.data)\n            DilatedInput_1[cse_var_1] = data_1[cse_var_1]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data) {\n  DilatedInput[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t DilatedInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* DilatedInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* DilatedInput_1 = (((DLTensor*)DilatedInput)[0].data);\n  void* default_function_DilatedInput_shape = (((DLTensor*)DilatedInput)[0].shape);\n  void* default_function_DilatedInput_strides = (((DLTensor*)DilatedInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_DilatedInput_strides == NULL)) {\n  }\n  for (int32_t i1 = 0; i1 < 40; ++i1) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_1 = (((i1 * 192) + (i2 * 24)) + i3);\n        ((float*)DilatedInput_1)[cse_var_1] = ((float*)data_1)[cse_var_1];\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "dilate[1__40__8__24]"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 28, 32, 28), \"float32\"), compute: T.Buffer((4, 25088), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused in T.parallel(100352):\n            compute_1 = T.Buffer((100352,), data=compute.data)\n            data_1 = T.Buffer((100352,), data=data.data)\n            compute_1[i_j_fused] = data_1[i_j_fused]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i_j_fused = 0; i_j_fused < 100352; ++i_j_fused) {\n    ((float*)compute_1)[i_j_fused] = ((float*)data_1)[i_j_fused];\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "flatten[4__28__32__28]"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 8, 20, 16), \"float32\"), T_strided_slice: T.Buffer((2, 16, 40, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(2):\n            T_reshape = T.allocate([640], \"float32x16\", \"global\")\n            T_reshape_1 = T.allocate([1], \"float32x16\", \"global\")\n            T_reshape_3 = T.Buffer((640,), \"float32x16\", data=T_reshape)\n            for ax1, ax2 in T.grid(16, 40):\n                T_reshape_2 = T.Buffer((1,), \"float32x16\", data=T_reshape_1, align=4)\n                data_1 = T.Buffer((25600,), data=data.data)\n                T_reshape_2[0] = data_1[ax1 % 2 * 10240 + ax2 % 2 * 5120 + ax0 * 2560 + ax1 // 2 * 320 + ax2 // 2 * 16:ax1 % 2 * 10240 + ax2 % 2 * 5120 + ax0 * 2560 + ax1 // 2 * 320 + ax2 // 2 * 16 + 16]\n                T_reshape_4 = T.Buffer((1,), \"float32x16\", data=T_reshape_1, align=4)\n                T_reshape_3[ax1 * 40 + ax2] = T_reshape_4[0]\n            for ax1, ax2 in T.grid(16, 40):\n                T_strided_slice_1 = T.Buffer((20480,), data=T_strided_slice.data)\n                T_strided_slice_1[ax0 * 10240 + ax1 * 640 + ax2 * 16:ax0 * 10240 + ax1 * 640 + ax2 * 16 + 16] = T_reshape_3[ax1 * 40 + ax2]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data) {\n  T_strided_slice[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((((((((int)blockIdx.x) % 20) / 10) * 10240) + (((((int)threadIdx.x) & 31) >> 4) * 5120)) + ((((int)blockIdx.x) / 20) * 320)) + ((((int)blockIdx.x) % 10) * 32)) + ((((int)threadIdx.x) >> 5) * 16)) + (((int)threadIdx.x) & 15))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    void* T_reshape = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)40960, 2, 32);\n    if (T_reshape == NULL) {\n      return -1;\n    }\n    float16 T_reshape_1[1];\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 40; ++ax2) {\n        T_reshape_1[0] = *(float16*)(((float*)data_1) + ((((((ax1 & 1) * 10240) + ((ax2 & 1) * 5120)) + (ax0 * 2560)) + ((ax1 >> 1) * 320)) + ((ax2 >> 1) * 16)));\n        ((float16*)T_reshape)[((ax1 * 40) + ax2)] = T_reshape_1[0];\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 16; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 40; ++ax2_1) {\n        *(float16*)(((float*)T_strided_slice_1) + (((ax0 * 10240) + (ax1_1 * 640)) + (ax2_1 * 16))) = ((float16*)T_reshape)[((ax1_1 * 40) + ax2_1)];\n      }\n    }\n    if (TVMBackendFreeWorkspace(1, dev_id, T_reshape) != 0) {\n      return -1;\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "batch_to_space_nd[10__8__20__16]"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 40, 20, 40), \"float32\"), buffer: T.Buffer((5, 40, 20, 40), \"float32\"), new_buffer: T.Buffer((5, 40, 20, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused in T.parallel(200):\n            for k, l in T.grid(20, 40):\n                cse_var_1: T.int32 = i_j_fused * 800 + k * 40 + l\n                new_buffer_1 = T.Buffer((160000,), data=new_buffer.data)\n                data_1 = T.Buffer((160000,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t buffer_code = arg_type_ids[1];\n  int32_t new_buffer_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* buffer = (((TVMValue*)args)[1].v_handle);\n  void* new_buffer = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* buffer_1 = (((DLTensor*)buffer)[0].data);\n  void* default_function_buffer_shape = (((DLTensor*)buffer)[0].shape);\n  void* default_function_buffer_strides = (((DLTensor*)buffer)[0].strides);\n  void* new_buffer_1 = (((DLTensor*)new_buffer)[0].data);\n  void* default_function_new_buffer_shape = (((DLTensor*)new_buffer)[0].shape);\n  void* default_function_new_buffer_strides = (((DLTensor*)new_buffer)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_buffer_strides == NULL)) {\n  }\n  if (!(default_function_new_buffer_strides == NULL)) {\n  }\n  for (int32_t i_j_fused = 0; i_j_fused < 200; ++i_j_fused) {\n    for (int32_t k = 0; k < 20; ++k) {\n      for (int32_t l = 0; l < 40; ++l) {\n        int32_t cse_var_1 = (((i_j_fused * 800) + (k * 40)) + l);\n        ((float*)new_buffer_1)[cse_var_1] = ((float*)data_1)[cse_var_1];\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fifo_buffer[5__40__20__40]"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 40, 28, 24), \"float32\"), depth_to_space: T.Buffer((7, 10, 56, 48), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(70):\n            for i2, i3 in T.grid(56, 48):\n                depth_to_space_1 = T.Buffer((188160,), data=depth_to_space.data)\n                data_1 = T.Buffer((188160,), data=data.data)\n                depth_to_space_1[i0_i1_fused * 2688 + i2 * 48 + i3] = data_1[i0_i1_fused // 10 * 26880 + T.truncmod(i2, 2) * 13440 + T.truncmod(i3, 2) * 6720 + i0_i1_fused % 10 * 672 + T.Div(i2, 2) * 24 + T.Div(i3, 2)]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) / 420) * 26880) + ((((((((int)blockIdx.x) % 42) * 4) + (((int)threadIdx.x) >> 4)) / 3) % 2) * 13440)) + (((((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) % 48) % 2) * 6720)) + (((((int)blockIdx.x) % 420) / 42) * 672)) + ((((((((int)blockIdx.x) % 42) * 4) + (((int)threadIdx.x) >> 4)) / 3) / 2) * 24)) + ((((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) % 48) / 2))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t depth_to_space_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* depth_to_space = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* depth_to_space_1 = (((DLTensor*)depth_to_space)[0].data);\n  void* default_function_depth_to_space_shape = (((DLTensor*)depth_to_space)[0].shape);\n  void* default_function_depth_to_space_strides = (((DLTensor*)depth_to_space)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_depth_to_space_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 70; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 56; ++i2) {\n      for (int32_t i3 = 0; i3 < 48; ++i3) {\n        ((float*)depth_to_space_1)[(((i0_i1_fused * 2688) + (i2 * 48)) + i3)] = ((float*)data_1)[(((((((i0_i1_fused / 10) * 26880) + ((i2 % 2) * 13440)) + ((i3 % 2) * 6720)) + ((i0_i1_fused % 10) * 672)) + ((i2 / 2) * 24)) + (i3 / 2))];\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "depth_to_space[7__40__28__24]"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 8, 28, 12), \"float32\"), T_divide: T.Buffer((7, 8, 28, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(56):\n            tensor = T.allocate([12], \"float32\", \"global\")\n            for ax2 in range(28):\n                cse_var_12: T.int32 = ax0_ax1_fused * 336 + ax2 * 12\n                cse_var_11: T.int32 = cse_var_12 + 9\n                cse_var_10: T.int32 = cse_var_12 + 8\n                cse_var_9: T.int32 = cse_var_12 + 7\n                cse_var_8: T.int32 = cse_var_12 + 6\n                cse_var_7: T.int32 = cse_var_12 + 5\n                cse_var_6: T.int32 = cse_var_12 + 4\n                cse_var_5: T.int32 = cse_var_12 + 3\n                cse_var_4: T.int32 = cse_var_12 + 2\n                cse_var_3: T.int32 = cse_var_12 + 11\n                cse_var_2: T.int32 = cse_var_12 + 10\n                cse_var_1: T.int32 = cse_var_12 + 1\n                tensor_1 = T.Buffer((12,), data=tensor, align=32)\n                tensor_1[0] = T.float32(0)\n                data_1 = T.Buffer((18816,), data=data.data)\n                tensor_1[0] = tensor_1[0] + data_1[cse_var_12] * data_1[cse_var_12]\n                tensor_1[1] = T.float32(0)\n                tensor_1[1] = tensor_1[1] + data_1[cse_var_1] * data_1[cse_var_1]\n                tensor_1[2] = T.float32(0)\n                tensor_1[2] = tensor_1[2] + data_1[cse_var_4] * data_1[cse_var_4]\n                tensor_1[3] = T.float32(0)\n                tensor_1[3] = tensor_1[3] + data_1[cse_var_5] * data_1[cse_var_5]\n                tensor_1[4] = T.float32(0)\n                tensor_1[4] = tensor_1[4] + data_1[cse_var_6] * data_1[cse_var_6]\n                tensor_1[5] = T.float32(0)\n                tensor_1[5] = tensor_1[5] + data_1[cse_var_7] * data_1[cse_var_7]\n                tensor_1[6] = T.float32(0)\n                tensor_1[6] = tensor_1[6] + data_1[cse_var_8] * data_1[cse_var_8]\n                tensor_1[7] = T.float32(0)\n                tensor_1[7] = tensor_1[7] + data_1[cse_var_9] * data_1[cse_var_9]\n                tensor_1[8] = T.float32(0)\n                tensor_1[8] = tensor_1[8] + data_1[cse_var_10] * data_1[cse_var_10]\n                tensor_1[9] = T.float32(0)\n                tensor_1[9] = tensor_1[9] + data_1[cse_var_11] * data_1[cse_var_11]\n                tensor_1[10] = T.float32(0)\n                tensor_1[10] = tensor_1[10] + data_1[cse_var_2] * data_1[cse_var_2]\n                tensor_1[11] = T.float32(0)\n                tensor_1[11] = tensor_1[11] + data_1[cse_var_3] * data_1[cse_var_3]\n                for ax3 in range(12):\n                    cse_var_13: T.int32 = cse_var_12 + ax3\n                    T_divide_1 = T.Buffer((18816,), data=T_divide.data)\n                    T_divide_1[cse_var_13] = data_1[cse_var_13] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[ax3], T.float32(0.75))\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])), 7.500000e-01f));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float powf(float, float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_divide_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_divide = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_divide_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 56; ++ax0_ax1_fused) {\n    float tensor[12];\n    for (int32_t ax2 = 0; ax2 < 28; ++ax2) {\n      int32_t cse_var_12 = ((ax0_ax1_fused * 336) + (ax2 * 12));\n      int32_t cse_var_11 = (cse_var_12 + 9);\n      int32_t cse_var_10 = (cse_var_12 + 8);\n      int32_t cse_var_9 = (cse_var_12 + 7);\n      int32_t cse_var_8 = (cse_var_12 + 6);\n      int32_t cse_var_7 = (cse_var_12 + 5);\n      int32_t cse_var_6 = (cse_var_12 + 4);\n      int32_t cse_var_5 = (cse_var_12 + 3);\n      int32_t cse_var_4 = (cse_var_12 + 2);\n      int32_t cse_var_3 = (cse_var_12 + 11);\n      int32_t cse_var_2 = (cse_var_12 + 10);\n      int32_t cse_var_1 = (cse_var_12 + 1);\n      tensor[0] = 0.000000e+00f;\n      tensor[0] = (tensor[0] + (((float*)data_1)[cse_var_12] * ((float*)data_1)[cse_var_12]));\n      tensor[1] = 0.000000e+00f;\n      tensor[1] = (tensor[1] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n      tensor[2] = 0.000000e+00f;\n      tensor[2] = (tensor[2] + (((float*)data_1)[cse_var_4] * ((float*)data_1)[cse_var_4]));\n      tensor[3] = 0.000000e+00f;\n      tensor[3] = (tensor[3] + (((float*)data_1)[cse_var_5] * ((float*)data_1)[cse_var_5]));\n      tensor[4] = 0.000000e+00f;\n      tensor[4] = (tensor[4] + (((float*)data_1)[cse_var_6] * ((float*)data_1)[cse_var_6]));\n      tensor[5] = 0.000000e+00f;\n      tensor[5] = (tensor[5] + (((float*)data_1)[cse_var_7] * ((float*)data_1)[cse_var_7]));\n      tensor[6] = 0.000000e+00f;\n      tensor[6] = (tensor[6] + (((float*)data_1)[cse_var_8] * ((float*)data_1)[cse_var_8]));\n      tensor[7] = 0.000000e+00f;\n      tensor[7] = (tensor[7] + (((float*)data_1)[cse_var_9] * ((float*)data_1)[cse_var_9]));\n      tensor[8] = 0.000000e+00f;\n      tensor[8] = (tensor[8] + (((float*)data_1)[cse_var_10] * ((float*)data_1)[cse_var_10]));\n      tensor[9] = 0.000000e+00f;\n      tensor[9] = (tensor[9] + (((float*)data_1)[cse_var_11] * ((float*)data_1)[cse_var_11]));\n      tensor[10] = 0.000000e+00f;\n      tensor[10] = (tensor[10] + (((float*)data_1)[cse_var_2] * ((float*)data_1)[cse_var_2]));\n      tensor[11] = 0.000000e+00f;\n      tensor[11] = (tensor[11] + (((float*)data_1)[cse_var_3] * ((float*)data_1)[cse_var_3]));\n      for (int32_t ax3 = 0; ax3 < 12; ++ax3) {\n        int32_t cse_var_13 = (cse_var_12 + ax3);\n        ((float*)T_divide_1)[cse_var_13] = (((float*)data_1)[cse_var_13] / powf((2.000000e+00f + (1.000000e-04f * tensor[ax3])), 7.500000e-01f));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "lrn[7__8__28__12]"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 20, 4, 40), \"float32\"), adaptive_pool_avg: T.Buffer((10, 20, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(200):\n            adaptive_pool_sum = T.allocate([8], \"float32\", \"global\")\n            for ax2 in range(8):\n                cse_var_1: T.int32 = (ax2 + 1) // 2\n                adaptive_pool_sum_1 = T.Buffer((8,), data=adaptive_pool_sum, align=32)\n                for ax3 in range(8):\n                    adaptive_pool_sum_1[ax3] = T.float32(0)\n                    for rv0 in range(T.Select((ax2 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax2 // 2):\n                        cse_var_2: T.int32 = ax0_ax1_fused * 160 + ax2 // 2 * 40 + rv0 * 40 + ax3 * 5\n                        data_1 = T.Buffer((32000,), data=data.data)\n                        adaptive_pool_sum_1[ax3] = adaptive_pool_sum_1[ax3] + data_1[cse_var_2]\n                        adaptive_pool_sum_1[ax3] = adaptive_pool_sum_1[ax3] + data_1[cse_var_2 + 1]\n                        adaptive_pool_sum_1[ax3] = adaptive_pool_sum_1[ax3] + data_1[cse_var_2 + 2]\n                        adaptive_pool_sum_1[ax3] = adaptive_pool_sum_1[ax3] + data_1[cse_var_2 + 3]\n                        adaptive_pool_sum_1[ax3] = adaptive_pool_sum_1[ax3] + data_1[cse_var_2 + 4]\n                adaptive_pool_avg_1 = T.Buffer((12800,), data=adaptive_pool_avg.data)\n                adaptive_pool_avg_1[ax0_ax1_fused * 64 + ax2 * 8:ax0_ax1_fused * 64 + ax2 * 8 + 8] = adaptive_pool_sum_1[0:8] / T.Broadcast(T.Cast(\"float32\", T.Select((ax2 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax2 // 2) * T.float32(5), 8)\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / (((float)(((((((((int)threadIdx.x) >> 3) * 4) + 4) % 8) == 0) ? ((((int)threadIdx.x) + 8) >> 4) : (((((int)threadIdx.x) + 8) >> 4) + 1)) - (((int)threadIdx.x) >> 4))) * 5.000000e+00f));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < (((((((((int)threadIdx.x) >> 3) * 4) + 4) % 8) == 0) ? ((((int)threadIdx.x) + 8) >> 4) : (((((int)threadIdx.x) + 8) >> 4) + 1)) - (((int)threadIdx.x) >> 4)); ++rv0) {\n    for (int rv1 = 0; rv1 < 5; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + data[(((((((int)blockIdx.x) * 160) + ((((int)threadIdx.x) >> 4) * 40)) + (rv0 * 40)) + ((((int)threadIdx.x) & 7) * 5)) + rv1)]);\n    }\n  }\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 200; ++ax0_ax1_fused) {\n    float adaptive_pool_sum[8];\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      int32_t cse_var_1 = ((ax2 + 1) >> 1);\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        adaptive_pool_sum[ax3] = 0.000000e+00f;\n        for (int32_t rv0 = 0; rv0 < ((((((ax2 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - (ax2 >> 1)); ++rv0) {\n          int32_t cse_var_2 = ((((ax0_ax1_fused * 160) + ((ax2 >> 1) * 40)) + (rv0 * 40)) + (ax3 * 5));\n          adaptive_pool_sum[ax3] = (adaptive_pool_sum[ax3] + ((float*)data_1)[cse_var_2]);\n          adaptive_pool_sum[ax3] = (adaptive_pool_sum[ax3] + ((float*)data_1)[(cse_var_2 + 1)]);\n          adaptive_pool_sum[ax3] = (adaptive_pool_sum[ax3] + ((float*)data_1)[(cse_var_2 + 2)]);\n          adaptive_pool_sum[ax3] = (adaptive_pool_sum[ax3] + ((float*)data_1)[(cse_var_2 + 3)]);\n          adaptive_pool_sum[ax3] = (adaptive_pool_sum[ax3] + ((float*)data_1)[(cse_var_2 + 4)]);\n        }\n      }\n      *(float8*)(((float*)adaptive_pool_avg_1) + ((ax0_ax1_fused * 64) + (ax2 * 8))) = (*(float8*)(adaptive_pool_sum + 0) / ((float8)((((float)((((((ax2 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - (ax2 >> 1))) * 5.000000e+00f), (((float)((((((ax2 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - (ax2 >> 1))) * 5.000000e+00f), (((float)((((((ax2 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - (ax2 >> 1))) * 5.000000e+00f), (((float)((((((ax2 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - (ax2 >> 1))) * 5.000000e+00f), (((float)((((((ax2 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - (ax2 >> 1))) * 5.000000e+00f), (((float)((((((ax2 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - (ax2 >> 1))) * 5.000000e+00f), (((float)((((((ax2 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - (ax2 >> 1))) * 5.000000e+00f), (((float)((((((ax2 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - (ax2 >> 1))) * 5.000000e+00f))));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "adaptive_pool_avg[10__20__4__40]"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.IR information corresponding to the CUDA code will also be provided.",
    "input": "### IR Code:\n\n# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 12, 4, 8), \"float32\"), buffer: T.Buffer((3, 12, 4, 8), \"float32\"), new_buffer: T.Buffer((3, 12, 4, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i in T.parallel(3):\n            for j, k, l in T.grid(12, 4, 8):\n                cse_var_1: T.int32 = i * 384 + j * 32 + k * 8 + l\n                new_buffer_1 = T.Buffer((1152,), data=new_buffer.data)\n                data_1 = T.Buffer((1152,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]\n\n### CUDA Code:\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))];\n}\n\n\n\n",
    "output": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t buffer_code = arg_type_ids[1];\n  int32_t new_buffer_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* buffer = (((TVMValue*)args)[1].v_handle);\n  void* new_buffer = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* buffer_1 = (((DLTensor*)buffer)[0].data);\n  void* default_function_buffer_shape = (((DLTensor*)buffer)[0].shape);\n  void* default_function_buffer_strides = (((DLTensor*)buffer)[0].strides);\n  void* new_buffer_1 = (((DLTensor*)new_buffer)[0].data);\n  void* default_function_new_buffer_shape = (((DLTensor*)new_buffer)[0].shape);\n  void* default_function_new_buffer_strides = (((DLTensor*)new_buffer)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_buffer_strides == NULL)) {\n  }\n  if (!(default_function_new_buffer_strides == NULL)) {\n  }\n  for (int32_t i = 0; i < 3; ++i) {\n    for (int32_t j = 0; j < 12; ++j) {\n      for (int32_t k = 0; k < 4; ++k) {\n        for (int32_t l = 0; l < 8; ++l) {\n          int32_t cse_var_1 = ((((i * 384) + (j * 32)) + (k * 8)) + l);\n          ((float*)new_buffer_1)[cse_var_1] = ((float*)data_1)[cse_var_1];\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n",
    "op_name": "fifo_buffer[3__12__4__8]"
  }
]