[
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(648) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(648) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ tarray) {\n  T_concat[((int)threadIdx.x)] = (((int64_t)324 <= ((int64_t)((int)threadIdx.x))) ? tarray[(((int)threadIdx.x) - 324)] : tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_concat[648];\n  for (int32_t ax0 = 0; ax0 < 12; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_concat[(((ax0 * 54) + (ax1 * 9)) + ax2)] = ((6 <= ax0) ? tarray[((((ax0 * 54) + (ax1 * 9)) + ax2) - 324)] : tarray[(((ax0 * 54) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "concatenate"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(float* __restrict__ T_repeat, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(float* __restrict__ T_repeat, float* __restrict__ tarray) {\n  T_repeat[((int)threadIdx.x)] = tarray[((int)threadIdx.x)];\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_repeat[128];\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        T_repeat[(((ax0 * 32) + (ax1 * 4)) + ax2)] = tarray[(((ax0 * 32) + (ax1 * 4)) + ax2)];\n      }\n    }\n  }\n}\n\n",
    "op_name": "repeat"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)512) + (((int64_t)((int)threadIdx.x)) >> (int64_t)1)) < (int64_t)1715) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = atanf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat atanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[3430];\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          compute[((((i0 * 490) + (i1 * 70)) + (i2 * 10)) + i3)] = atanf(tarray[((((i0 * 490) + (i1 * 70)) + (i2 * 10)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "atan"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ tarray) {\n  T_multiply[((int)threadIdx.x)] = (tarray[((int)threadIdx.x)] * tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_multiply[128];\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        T_multiply[(((ax0 * 32) + (ax1 * 4)) + ax2)] = (tarray[(((ax0 * 32) + (ax1 * 4)) + ax2)] * tarray[(((ax0 * 32) + (ax1 * 4)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "multiply"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(144) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(144) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = __expf(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[144];\n  for (int32_t i1 = 0; i1 < 8; ++i1) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      for (int32_t i3 = 0; i3 < 6; ++i3) {\n        compute[(((i1 * 18) + (i2 * 6)) + i3)] = expf(tarray[(((i1 * 18) + (i2 * 6)) + i3)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "exp"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ tarray, float* __restrict__ tarray_red);\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ tarray, float* __restrict__ tarray_red) {\n  tarray_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 4; ++k0) {\n    for (int k1 = 0; k1 < 5; ++k1) {\n      for (int k2 = 0; k2 < 4; ++k2) {\n        for (int k3 = 0; k3 < 9; ++k3) {\n          tarray_red[0] = max(tarray_red[0], tarray[((((k0 * 180) + (k1 * 36)) + (k2 * 9)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float tarray_red[1];\n  tarray_red[0] = -3.402823e+38f;\n  for (int32_t k0 = 0; k0 < 4; ++k0) {\n    for (int32_t k1 = 0; k1 < 5; ++k1) {\n      for (int32_t k2 = 0; k2 < 4; ++k2) {\n        for (int32_t k3 = 0; k3 < 9; ++k3) {\n          float v_ = tarray_red[0];\n          float v__1 = tarray[((((k0 * 180) + (k1 * 36)) + (k2 * 9)) + k3)];\n          tarray_red[0] = ((v_) > (v__1) ? (v_) : (v__1));\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "max"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ tarray) {\n  T_divide[((int)threadIdx.x)] = (tarray[((int)threadIdx.x)] / tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_divide[4];\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_divide[ax0] = (tarray[ax0] / tarray[ax0]);\n  }\n}\n\n",
    "op_name": "divide"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ tarray, float* __restrict__ tarray_red);\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ tarray, float* __restrict__ tarray_red) {\n  tarray_red[((int)threadIdx.x)] = 1.000000e+00f;\n  for (int k0 = 0; k0 < 2; ++k0) {\n    tarray_red[((int)threadIdx.x)] = (tarray_red[((int)threadIdx.x)] * tarray[((k0 * 9) + ((int)threadIdx.x))]);\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float tarray_red[9];\n  for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n    tarray_red[ax1] = 1.000000e+00f;\n    for (int32_t k0 = 0; k0 < 2; ++k0) {\n      tarray_red[ax1] = (tarray_red[ax1] * tarray[((k0 * 9) + ax1)]);\n    }\n  }\n}\n\n",
    "op_name": "prod"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)32) + (((int64_t)((int)threadIdx.x)) >> (int64_t)5)) < (int64_t)45) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[1440];\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        for (int32_t i3 = 0; i3 < 9; ++i3) {\n          compute[((((i0 * 180) + (i1 * 90)) + (i2 * 9)) + i3)] = tarray[((((i0 * 180) + (i1 * 90)) + (i2 * 9)) + i3)];\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "identity"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = tanhf(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat tanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[10];\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    compute[i0] = tanhf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "tanh"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = ceilf(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat ceilf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[12];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 6) + (i1 * 2)) + i2)] = ceilf(tarray[(((i0 * 6) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "ceil"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ tarray, float* __restrict__ tarray_red);\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ tarray, float* __restrict__ tarray_red) {\n  tarray_red[0] = 1.000000e+00f;\n  for (int k0 = 0; k0 < 8; ++k0) {\n    tarray_red[0] = (tarray_red[0] * tarray[k0]);\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float tarray_red[1];\n  tarray_red[0] = 1.000000e+00f;\n  for (int32_t k0 = 0; k0 < 8; ++k0) {\n    tarray_red[0] = (tarray_red[0] * tarray[k0]);\n  }\n}\n\n",
    "op_name": "prod"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(signed char* __restrict__ T_greater, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(signed char* __restrict__ T_greater, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)16) + (((int64_t)((int)threadIdx.x)) >> (int64_t)6)) < (int64_t)225) {\n    T_greater[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((signed char)(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] < tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]));\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  int8_t T_greater[14400];\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 10; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 3; ++ax4) {\n            T_greater[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)] = ((int8_t)(tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)] < tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)]));\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "greater"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(int64_t* __restrict__ const_vector_np_arange);\nextern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(int64_t* __restrict__ const_vector_np_arange) {\n  const_vector_np_arange[((int)threadIdx.x)] = ((((int)threadIdx.x) == 4) ? (int64_t)8 : ((((int)threadIdx.x) == 3) ? (int64_t)6 : ((((int)threadIdx.x) == 2) ? (int64_t)4 : ((((int)threadIdx.x) == 1) ? (int64_t)2 : (int64_t)0))));\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel();\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel() {\n  int64_t const_vector_np_arange[5];\n  for (int32_t i = 0; i < 5; ++i) {\n    const_vector_np_arange[i] = ((i == 4) ? (int64_t)8 : ((i == 3) ? (int64_t)6 : ((i == 2) ? (int64_t)4 : ((i == 1) ? (int64_t)2 : (int64_t)0))));\n  }\n}\n\n",
    "op_name": "const_vector_np_arange"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ T_full_like);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ T_full_like) {\n  T_full_like[((int)threadIdx.x)] = 1.000000e+00f;\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel();\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel() {\n  float T_full_like[18];\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      T_full_like[((ax0 * 2) + ax1)] = 1.000000e+00f;\n    }\n  }\n}\n\n",
    "op_name": "full_like"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(signed char* __restrict__ T_greater, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(signed char* __restrict__ T_greater, float* __restrict__ tarray) {\n  T_greater[((int)threadIdx.x)] = ((signed char)(tarray[((int)threadIdx.x)] < tarray[((int)threadIdx.x)]));\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  int8_t T_greater[8];\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    T_greater[ax0] = ((int8_t)(tarray[ax0] < tarray[ax0]));\n  }\n}\n\n",
    "op_name": "greater"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = coshf(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[24];\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      compute[((i0 * 8) + i1)] = coshf(tarray[((i0 * 8) + i1)]);\n    }\n  }\n}\n\n",
    "op_name": "cosh"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(int64_t* __restrict__ const_vector_np_arange);\nextern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(int64_t* __restrict__ const_vector_np_arange) {\n  const_vector_np_arange[((int)threadIdx.x)] = ((((int)threadIdx.x) == 4) ? (int64_t)8 : ((((int)threadIdx.x) == 3) ? (int64_t)6 : ((((int)threadIdx.x) == 2) ? (int64_t)4 : ((((int)threadIdx.x) == 1) ? (int64_t)2 : (int64_t)0))));\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel();\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel() {\n  int64_t const_vector_np_arange[5];\n  for (int32_t i = 0; i < 5; ++i) {\n    const_vector_np_arange[i] = ((i == 4) ? (int64_t)8 : ((i == 3) ? (int64_t)6 : ((i == 2) ? (int64_t)4 : ((i == 1) ? (int64_t)2 : (int64_t)0))));\n  }\n}\n\n",
    "op_name": "const_vector_np_arange"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(double* __restrict__ const_vector_np_zeros);\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(double* __restrict__ const_vector_np_zeros) {\n  const_vector_np_zeros[((int)threadIdx.x)] = 0.000000e+00;\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel();\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel() {\n  double const_vector_np_zeros[128];\n  for (int32_t i = 0; i < 128; ++i) {\n    const_vector_np_zeros[i] = 0.000000e+00;\n  }\n}\n\n",
    "op_name": "const_vector_np_zeros"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_repeat, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_repeat, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)32) + (((int64_t)((int)threadIdx.x)) >> (int64_t)5)) < (int64_t)45) {\n    T_repeat[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_repeat[1440];\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 9; ++ax3) {\n          T_repeat[((((ax0 * 180) + (ax1 * 90)) + (ax2 * 9)) + ax3)] = tarray[((((ax0 * 180) + (ax1 * 90)) + (ax2 * 9)) + ax3)];\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "repeat"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(signed char* __restrict__ T_equal, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(signed char* __restrict__ T_equal, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)128) + (((int64_t)((int)threadIdx.x)) >> (int64_t)3)) < (int64_t)315) {\n    T_equal[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((signed char)(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] == tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]));\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  int8_t T_equal[2520];\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 3; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 5; ++ax4) {\n            T_equal[(((((ax0 * 360) + (ax1 * 45)) + (ax2 * 15)) + (ax3 * 5)) + ax4)] = ((int8_t)(tarray[(((((ax0 * 360) + (ax1 * 45)) + (ax2 * 15)) + (ax3 * 5)) + ax4)] == tarray[(((((ax0 * 360) + (ax1 * 45)) + (ax2 * 15)) + (ax3 * 5)) + ax4)]));\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "equal"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)32) + (((int64_t)((int)threadIdx.x)) >> (int64_t)5)) < (int64_t)45) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = atanhf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat atanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[1440];\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        for (int32_t i3 = 0; i3 < 2; ++i3) {\n          for (int32_t i4 = 0; i4 < 9; ++i4) {\n            compute[(((((i0 * 144) + (i1 * 36)) + (i2 * 18)) + (i3 * 9)) + i4)] = atanhf(tarray[(((((i0 * 144) + (i1 * 36)) + (i2 * 18)) + (i3 * 9)) + i4)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "atanh"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(720) default_function_kernel(signed char* __restrict__ T_not_equal, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(720) default_function_kernel(signed char* __restrict__ T_not_equal, float* __restrict__ tarray) {\n  T_not_equal[((int)threadIdx.x)] = ((signed char)(tarray[((int)threadIdx.x)] != tarray[((int)threadIdx.x)]));\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  int8_t T_not_equal[720];\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 9; ++ax3) {\n          T_not_equal[((((ax0 * 180) + (ax1 * 36)) + (ax2 * 9)) + ax3)] = ((int8_t)(tarray[((((ax0 * 180) + (ax1 * 36)) + (ax2 * 9)) + ax3)] != tarray[((((ax0 * 180) + (ax1 * 36)) + (ax2 * 9)) + ax3)]));\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "not_equal"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(350) default_function_kernel(float* __restrict__ T_subtract);\nextern \"C\" __global__ void __launch_bounds__(350) default_function_kernel(float* __restrict__ T_subtract) {\n  T_subtract[((int)threadIdx.x)] = 0.000000e+00f;\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel();\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel() {\n  float T_subtract[350];\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        T_subtract[(((ax0 * 50) + (ax1 * 10)) + ax2)] = 0.000000e+00f;\n      }\n    }\n  }\n}\n\n",
    "op_name": "stack"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)16) + (((int64_t)((int)threadIdx.x)) >> (int64_t)6)) < (int64_t)225) {\n      int v_ = ((int)(floorf(((max(min(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_fast_exp[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_fast_exp[14400];\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 10; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 3; ++ax4) {\n              float v_ = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n              float v__1 = (v_) < (8.837627e+01f) ? (v_) : (8.837627e+01f);\n              int32_t v__2 = ((int32_t)(floorf(((((v__1) > (-8.837626e+01f) ? (v__1) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n            float v__3 = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n            float v__4 = (v__3) < (8.837627e+01f) ? (v__3) : (8.837627e+01f);\n            float v__5 = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n            float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n            float v__7 = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n            float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n            float v__9 = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n            float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n            float v__11 = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n            float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n            float v__13 = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n            float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n            float v__15 = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n            float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n            float v__17 = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n            float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n            float v__19 = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n            float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n            float v__21 = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n            float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n            float v__23 = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n            float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n            float v__25 = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n            float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n            float v__27 = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n            float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n            float v__29 = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n            float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n            float v__31 = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n            float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n            float v__33 = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n            float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n            float v__35 = (*(float *)(&(v__2))) * ((((((((((((((1.987569e-04f * (((v__4) > (-8.837626e+01f) ? (v__4) : (-8.837626e+01f)) - (floorf(((((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) - (floorf(((((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) - (floorf(((((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) - (floorf(((((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) - (floorf(((((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) - (floorf(((((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) - (floorf(((((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) - (floorf(((((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n            float v__36 = tarray[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)];\n            T_fast_exp[(((((ax0 * 2400) + (ax1 * 300)) + (ax2 * 30)) + (ax3 * 3)) + ax4)] = ((v__35) > (v__36) ? (v__35) : (v__36));\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "fast_exp"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sqrtf(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[18];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      compute[((i0 * 9) + i1)] = sqrtf(tarray[((i0 * 9) + i1)]);\n    }\n  }\n}\n\n",
    "op_name": "sqrt"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(378) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(378) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = coshf(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[378];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 63) + (i1 * 7)) + i2)] = coshf(tarray[(((i0 * 63) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "cosh"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_tile, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_tile, float* __restrict__ tarray) {\n  T_tile[((int)threadIdx.x)] = tarray[((((int)threadIdx.x) % 18) / 3)];\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tile[36];\n  for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      T_tile[((ax1 * 3) + ax2)] = tarray[(ax1 % 6)];\n    }\n  }\n}\n\n",
    "op_name": "tile"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(float* __restrict__ T_reshape, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(float* __restrict__ T_reshape, float* __restrict__ tarray) {\n  T_reshape[((int)threadIdx.x)] = tarray[(((int)threadIdx.x) % 6)];\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_reshape[128];\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 64; ++ax1) {\n      T_reshape[((ax0 * 64) + ax1)] = tarray[(((ax0 * 4) + ax1) % 6)];\n    }\n  }\n}\n\n",
    "op_name": "reshape_1"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_full_like);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_full_like) {\n  T_full_like[((int)threadIdx.x)] = 1.000000e+00f;\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel();\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel() {\n  float T_full_like[8];\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    T_full_like[ax0] = 1.000000e+00f;\n  }\n}\n\n",
    "op_name": "full_like"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = max(min(tarray[((int)threadIdx.x)], 1.000000e+02f), 0.000000e+00f);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[24];\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      float v_ = tarray[((i0 * 8) + i1)];\n      float v__1 = (v_) < (1.000000e+02f) ? (v_) : (1.000000e+02f);\n      compute[((i0 * 8) + i1)] = ((v__1) > (0.000000e+00f) ? (v__1) : (0.000000e+00f));\n    }\n  }\n}\n\n",
    "op_name": "clip"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(980) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(980) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - tarray[((int)threadIdx.x)]))));\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[980];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        for (int32_t i4 = 0; i4 < 7; ++i4) {\n          compute[((((i0 * 490) + (i1 * 49)) + (i2 * 7)) + i4)] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - tarray[((((i0 * 490) + (i1 * 49)) + (i2 * 7)) + i4)]))));\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "sigmoid"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(double* __restrict__ const_vector_np_zeros);\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(double* __restrict__ const_vector_np_zeros) {\n  const_vector_np_zeros[((int)threadIdx.x)] = 0.000000e+00;\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel();\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel() {\n  double const_vector_np_zeros[128];\n  for (int32_t i = 0; i < 128; ++i) {\n    const_vector_np_zeros[i] = 0.000000e+00;\n  }\n}\n\n",
    "op_name": "const_vector_np_zeros"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((int)((((int)threadIdx.x) == 1) ? (int64_t)1 : (int64_t)8));\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel();\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel() {\n  int32_t T_shape[2];\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    T_shape[ax0] = ((int32_t)((ax0 == 1) ? (int64_t)1 : (int64_t)8));\n  }\n}\n\n",
    "op_name": "shape"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(int64_t* __restrict__ const_vector_np_full);\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(int64_t* __restrict__ const_vector_np_full) {\n  const_vector_np_full[((int)threadIdx.x)] = (int64_t)7;\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel();\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel() {\n  int64_t const_vector_np_full[128];\n  for (int32_t i = 0; i < 128; ++i) {\n    const_vector_np_full[i] = (int64_t)7;\n  }\n}\n\n",
    "op_name": "const_vector_np_full"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_floor_divide, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_floor_divide, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)512) + (((int64_t)((int)threadIdx.x)) >> (int64_t)1)) < (int64_t)1715) {\n    T_floor_divide[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = floorf((tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] / tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]));\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_floor_divide[3430];\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 10; ++ax3) {\n          T_floor_divide[((((ax0 * 490) + (ax1 * 70)) + (ax2 * 10)) + ax3)] = floorf((tarray[((((ax0 * 490) + (ax1 * 70)) + (ax2 * 10)) + ax3)] / tarray[((((ax0 * 490) + (ax1 * 70)) + (ax2 * 10)) + ax3)]));\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "floor_divide"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = coshf(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[18];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      compute[((i0 * 9) + i1)] = coshf(tarray[((i0 * 9) + i1)]);\n    }\n  }\n}\n\n",
    "op_name": "cosh"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((int)((((int)threadIdx.x) == 4) ? (int64_t)5 : ((((int)threadIdx.x) == 3) ? (int64_t)3 : ((((int)threadIdx.x) == 2) ? (int64_t)3 : ((((int)threadIdx.x) == 1) ? (int64_t)8 : (int64_t)7)))));\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel();\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel() {\n  int32_t T_shape[5];\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    T_shape[ax0] = ((int32_t)((ax0 == 4) ? (int64_t)5 : ((ax0 == 3) ? (int64_t)3 : ((ax0 == 2) ? (int64_t)3 : ((ax0 == 1) ? (int64_t)8 : (int64_t)7)))));\n  }\n}\n\n",
    "op_name": "shape"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)128) + (((int64_t)((int)threadIdx.x)) >> (int64_t)3)) < (int64_t)245) {\n    T_concat[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = (((int64_t)245 <= ((((int64_t)((int)blockIdx.x)) * (int64_t)256) + (((int64_t)((int)threadIdx.x)) >> (int64_t)2))) ? tarray[(((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) - 980)] : tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_concat[1960];\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        for (int32_t ax4 = 0; ax4 < 7; ++ax4) {\n          T_concat[((((ax0 * 490) + (ax1 * 49)) + (ax2 * 7)) + ax4)] = ((2 <= ax0) ? tarray[(((((ax0 * 490) + (ax1 * 49)) + (ax2 * 7)) + ax4) - 980)] : tarray[((((ax0 * 490) + (ax1 * 49)) + (ax2 * 7)) + ax4)]);\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "concatenate"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = roundf(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat roundf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[324];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 54) + (i1 * 9)) + i2)] = roundf(tarray[(((i0 * 54) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "round"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ tarray, float* __restrict__ tarray_red);\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ tarray, float* __restrict__ tarray_red) {\n  tarray_red[0] = 3.402823e+38f;\n  for (int k0 = 0; k0 < 10; ++k0) {\n    for (int k1 = 0; k1 < 4; ++k1) {\n      for (int k2 = 0; k2 < 2; ++k2) {\n        for (int k3 = 0; k3 < 2; ++k3) {\n          for (int k4 = 0; k4 < 9; ++k4) {\n            tarray_red[0] = min(tarray_red[0], tarray[(((((k0 * 144) + (k1 * 36)) + (k2 * 18)) + (k3 * 9)) + k4)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float tarray_red[1];\n  tarray_red[0] = 3.402823e+38f;\n  for (int32_t k0 = 0; k0 < 10; ++k0) {\n    for (int32_t k1 = 0; k1 < 4; ++k1) {\n      for (int32_t k2 = 0; k2 < 2; ++k2) {\n        for (int32_t k3 = 0; k3 < 2; ++k3) {\n          for (int32_t k4 = 0; k4 < 9; ++k4) {\n            float v_ = tarray_red[0];\n            float v__1 = tarray[(((((k0 * 144) + (k1 * 36)) + (k2 * 18)) + (k3 * 9)) + k4)];\n            tarray_red[0] = ((v_) < (v__1) ? (v_) : (v__1));\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "min"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = __logf(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat logf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[12];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 6) + (i1 * 2)) + i2)] = logf(tarray[(((i0 * 6) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "log"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer < 25; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer) {\n    if ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 4096) + (((int)blockIdx.x) * 16)) + (((int)threadIdx.x) >> 6)) < 99225) {\n      T_tensordot[(((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 32768) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) >> 3)) / 315)] * tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_ax8_fused_ax9_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 2520)]);\n    }\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[6350400];\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 10; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 7; ++ax4) {\n            for (int32_t ax5 = 0; ax5 < 6; ++ax5) {\n              for (int32_t ax6 = 0; ax6 < 2; ++ax6) {\n                for (int32_t ax7 = 0; ax7 < 3; ++ax7) {\n                  for (int32_t ax8 = 0; ax8 < 10; ++ax8) {\n                    for (int32_t ax9 = 0; ax9 < 7; ++ax9) {\n                      T_tensordot[((((((((((ax0 * 1058400) + (ax1 * 529200)) + (ax2 * 176400)) + (ax3 * 17640)) + (ax4 * 2520)) + (ax5 * 420)) + (ax6 * 210)) + (ax7 * 70)) + (ax8 * 7)) + ax9)] = (tarray[(((((ax0 * 420) + (ax1 * 210)) + (ax2 * 70)) + (ax3 * 7)) + ax4)] * tarray[(((((ax5 * 420) + (ax6 * 210)) + (ax7 * 70)) + (ax8 * 7)) + ax9)]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = erff(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat erff(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[324];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 54) + (i1 * 9)) + i2)] = erff(tarray[(((i0 * 54) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "erf"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(350) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(350) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = atanf(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat atanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[350];\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        compute[(((i0 * 50) + (i1 * 10)) + i2)] = atanf(tarray[(((i0 * 50) + (i1 * 10)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "atan"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(signed char* __restrict__ T_greater, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(signed char* __restrict__ T_greater, float* __restrict__ tarray) {\n  T_greater[((int)threadIdx.x)] = ((signed char)(tarray[((int)threadIdx.x)] < tarray[((int)threadIdx.x)]));\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  int8_t T_greater[18];\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      T_greater[((ax0 * 2) + ax1)] = ((int8_t)(tarray[((ax0 * 2) + ax1)] < tarray[((ax0 * 2) + ax1)]));\n    }\n  }\n}\n\n",
    "op_name": "greater"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)128) + (((int64_t)((int)threadIdx.x)) >> (int64_t)3)) < (int64_t)315) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = __sinf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[2520];\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3 = 0; i3 < 3; ++i3) {\n          for (int32_t i4 = 0; i4 < 5; ++i4) {\n            compute[(((((i0 * 360) + (i1 * 45)) + (i2 * 15)) + (i3 * 5)) + i4)] = sinf(tarray[(((((i0 * 360) + (i1 * 45)) + (i2 * 15)) + (i3 * 5)) + i4)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "sin"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(int64_t* __restrict__ const_vector_np_full);\nextern \"C\" __global__ void __launch_bounds__(128) default_function_kernel(int64_t* __restrict__ const_vector_np_full) {\n  const_vector_np_full[((int)threadIdx.x)] = (int64_t)7;\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel();\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel() {\n  int64_t const_vector_np_full[128];\n  for (int32_t i = 0; i < 128; ++i) {\n    const_vector_np_full[i] = (int64_t)7;\n  }\n}\n\n",
    "op_name": "const_vector_np_full"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_reshape, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_reshape, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)128) + (((int64_t)((int)threadIdx.x)) >> (int64_t)3)) < (int64_t)315) {\n    T_reshape[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = tarray[((((((((((int)blockIdx.x) * 128) + (((int)threadIdx.x) >> 3)) / 45) * 360) + (((((((int)blockIdx.x) * 38) + (((int)threadIdx.x) >> 3)) % 45) / 15) * 120)) + (((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) % 15) / 5) * 40)) + (((((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 40) / 5) * 5)) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 5))];\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_reshape[2520];\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 5; ++ax4) {\n            T_reshape[(((((ax0 * 360) + (ax1 * 120)) + (ax2 * 40)) + (ax3 * 5)) + ax4)] = tarray[(((((ax0 * 360) + (ax1 * 120)) + ((((ax2 * 8) + ax3) / 3) * 15)) + ((((ax2 * 2) + ax3) % 3) * 5)) + ax4)];\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "reshape"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)128) + (((int64_t)((int)threadIdx.x)) >> (int64_t)3)) < (int64_t)315) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ceilf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat ceilf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[2520];\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3 = 0; i3 < 3; ++i3) {\n          for (int32_t i4 = 0; i4 < 5; ++i4) {\n            compute[(((((i0 * 360) + (i1 * 45)) + (i2 * 15)) + (i3 * 5)) + i4)] = ceilf(tarray[(((((i0 * 360) + (i1 * 45)) + (i2 * 15)) + (i3 * 5)) + i4)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "ceil"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(980) default_function_kernel(float* __restrict__ T_floor_mod, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(980) default_function_kernel(float* __restrict__ T_floor_mod, float* __restrict__ tarray) {\n  T_floor_mod[((int)threadIdx.x)] = (tarray[((int)threadIdx.x)] - (floorf((tarray[((int)threadIdx.x)] / tarray[((int)threadIdx.x)])) * tarray[((int)threadIdx.x)]));\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_floor_mod[980];\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        for (int32_t ax4 = 0; ax4 < 7; ++ax4) {\n          T_floor_mod[((((ax0 * 490) + (ax1 * 49)) + (ax2 * 7)) + ax4)] = (tarray[((((ax0 * 490) + (ax1 * 49)) + (ax2 * 7)) + ax4)] - (floorf((tarray[((((ax0 * 490) + (ax1 * 49)) + (ax2 * 7)) + ax4)] / tarray[((((ax0 * 490) + (ax1 * 49)) + (ax2 * 7)) + ax4)])) * tarray[((((ax0 * 490) + (ax1 * 49)) + (ax2 * 7)) + ax4)]));\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "floor_mod"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)8) + (((int64_t)((int)threadIdx.x)) >> (int64_t)7)) < (int64_t)9) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = __cosf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat cosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[1152];\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        for (int32_t i3 = 0; i3 < 4; ++i3) {\n          compute[((((i0 * 288) + (i1 * 32)) + (i2 * 4)) + i3)] = cosf(tarray[((((i0 * 288) + (i1 * 32)) + (i2 * 4)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "cos"
  },
  {
    "instruction": "Your deep learning application is trained in an environment with GPU support, but now you need to perform inference in an environment without GPUs. Please convert CUDA operators to CPU operators and optimize for performance to achieve the best execution efficiency on the CPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_subtract);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_subtract) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)32) + (((int64_t)((int)threadIdx.x)) >> (int64_t)5)) < (int64_t)45) {\n    T_subtract[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel();\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel() {\n  float T_subtract[1440];\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 9; ++ax3) {\n          T_subtract[((((ax0 * 180) + (ax1 * 90)) + (ax2 * 9)) + ax3)] = 0.000000e+00f;\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "stack"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(signed char* __restrict__ T_equal, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(signed char* __restrict__ T_equal, float* __restrict__ tarray) {\n  T_equal[((int)threadIdx.x)] = ((signed char)(tarray[((int)threadIdx.x)] == tarray[((int)threadIdx.x)]));\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  int8_t T_equal[12];\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        T_equal[(((ax0 * 6) + (ax1 * 2)) + ax2)] = ((int8_t)(tarray[(((ax0 * 6) + (ax1 * 2)) + ax2)] == tarray[(((ax0 * 6) + (ax1 * 2)) + ax2)]));\n      }\n    }\n  }\n}\n\n",
    "op_name": "equal"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(int* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(int* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)32) + (((int64_t)((int)threadIdx.x)) >> (int64_t)5)) < (int64_t)45) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((int)(((((2 != 0) ? (((int64_t)tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]) << ((int64_t)2)) : ((int64_t)tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))])) * (int64_t)2) + ((int64_t)1 << ((int64_t)((0 + 31) - 1)))) >> ((int64_t)(0 + 31))));\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  int32_t compute[1440];\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        for (int32_t i3 = 0; i3 < 9; ++i3) {\n          compute[((((i0 * 180) + (i1 * 90)) + (i2 * 9)) + i3)] = ((int32_t)(((((2 != 0) ? (((int64_t)tarray[((((i0 * 180) + (i1 * 90)) + (i2 * 9)) + i3)]) << ((int64_t)2)) : ((int64_t)tarray[((((i0 * 180) + (i1 * 90)) + (i2 * 9)) + i3)])) * (int64_t)2) + ((int64_t)1 << ((int64_t)((0 + 31) - 1)))) >> ((int64_t)(0 + 31))));\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "fixed_point_multiply"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(signed char* __restrict__ T_not_equal, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(signed char* __restrict__ T_not_equal, float* __restrict__ tarray) {\n  T_not_equal[((int)threadIdx.x)] = ((signed char)(tarray[((int)threadIdx.x)] != tarray[((int)threadIdx.x)]));\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  int8_t T_not_equal[4];\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_not_equal[ax0] = ((int8_t)(tarray[ax0] != tarray[ax0]));\n  }\n}\n\n",
    "op_name": "not_equal"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(144) default_function_kernel(signed char* __restrict__ T_not_equal, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(144) default_function_kernel(signed char* __restrict__ T_not_equal, float* __restrict__ tarray) {\n  T_not_equal[((int)threadIdx.x)] = ((signed char)(tarray[((int)threadIdx.x)] != tarray[((int)threadIdx.x)]));\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  int8_t T_not_equal[144];\n  for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 6; ++ax3) {\n        T_not_equal[(((ax1 * 18) + (ax2 * 6)) + ax3)] = ((int8_t)(tarray[(((ax1 * 18) + (ax2 * 6)) + ax3)] != tarray[(((ax1 * 18) + (ax2 * 6)) + ax3)]));\n      }\n    }\n  }\n}\n\n",
    "op_name": "not_equal"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ T_tile, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ T_tile, float* __restrict__ tarray) {\n  T_tile[((int)threadIdx.x)] = tarray[(((int)threadIdx.x) % 10)];\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tile[60];\n  for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 30; ++ax2) {\n      T_tile[((ax1 * 30) + ax2)] = tarray[(ax2 % 10)];\n    }\n  }\n}\n\n",
    "op_name": "tile"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(378) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(378) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = max(min(tarray[((int)threadIdx.x)], 1.000000e+02f), 0.000000e+00f);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[378];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        float v_ = tarray[(((i0 * 63) + (i1 * 7)) + i2)];\n        float v__1 = (v_) < (1.000000e+02f) ? (v_) : (1.000000e+02f);\n        compute[(((i0 * 63) + (i1 * 7)) + i2)] = ((v__1) > (0.000000e+00f) ? (v__1) : (0.000000e+00f));\n      }\n    }\n  }\n}\n\n",
    "op_name": "clip"
  },
  {
    "instruction": "Your deep learning model is trained in an environment that supports CUDA, and now you need to port it to an environment without GPUs. Please convert the CUDA operators to equivalent operators running on the CPU to ensure the portability of the model across different environments.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(350) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(350) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = acosf(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat acosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[350];\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        compute[(((i0 * 50) + (i1 * 10)) + i2)] = acosf(tarray[(((i0 * 50) + (i1 * 10)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "acos"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_maximum, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_maximum, float* __restrict__ tarray) {\n  T_maximum[((int)threadIdx.x)] = max(tarray[((int)threadIdx.x)], tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_maximum[2];\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    float v_ = tarray[ax0];\n    T_maximum[ax0] = ((v_) > (v_) ? (v_) : (v_));\n  }\n}\n\n",
    "op_name": "maximum"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(144) default_function_kernel(float* __restrict__ T_floor_mod, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(144) default_function_kernel(float* __restrict__ T_floor_mod, float* __restrict__ tarray) {\n  T_floor_mod[((int)threadIdx.x)] = (tarray[((int)threadIdx.x)] - (floorf((tarray[((int)threadIdx.x)] / tarray[((int)threadIdx.x)])) * tarray[((int)threadIdx.x)]));\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_floor_mod[144];\n  for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 6; ++ax3) {\n        T_floor_mod[(((ax1 * 18) + (ax2 * 6)) + ax3)] = (tarray[(((ax1 * 18) + (ax2 * 6)) + ax3)] - (floorf((tarray[(((ax1 * 18) + (ax2 * 6)) + ax3)] / tarray[(((ax1 * 18) + (ax2 * 6)) + ax3)])) * tarray[(((ax1 * 18) + (ax2 * 6)) + ax3)]));\n      }\n    }\n  }\n}\n\n",
    "op_name": "floor_mod"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_subtract);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_subtract) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)8) + (((int64_t)((int)threadIdx.x)) >> (int64_t)7)) < (int64_t)9) {\n    T_subtract[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel();\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel() {\n  float T_subtract[1152];\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 4; ++ax3) {\n          T_subtract[((((ax0 * 288) + (ax1 * 32)) + (ax2 * 4)) + ax3)] = 0.000000e+00f;\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "stack"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)32) + (((int64_t)((int)threadIdx.x)) >> (int64_t)5)) < (int64_t)45) {\n    T_fast_tanh[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_minimum[1440];\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 9; ++ax3) {\n          float v_ = tarray[((((ax0 * 180) + (ax1 * 90)) + (ax2 * 9)) + ax3)];\n          T_minimum[((((ax0 * 180) + (ax1 * 90)) + (ax2 * 9)) + ax3)] = ((9.000000e+00f) < (v_) ? (9.000000e+00f) : (v_));\n        }\n      }\n    }\n  }\n  for (int32_t ax0_1 = 0; ax0_1 < 8; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 2; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 10; ++ax2_1) {\n        for (int32_t ax3_1 = 0; ax3_1 < 9; ++ax3_1) {\n          float v__1 = T_minimum[((((ax0_1 * 180) + (ax1_1 * 90)) + (ax2_1 * 9)) + ax3_1)];\n          T_minimum[((((ax0_1 * 180) + (ax1_1 * 90)) + (ax2_1 * 9)) + ax3_1)] = ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1));\n        }\n      }\n    }\n  }\n  for (int32_t ax0_2 = 0; ax0_2 < 8; ++ax0_2) {\n    for (int32_t ax1_2 = 0; ax1_2 < 2; ++ax1_2) {\n      for (int32_t ax2_2 = 0; ax2_2 < 10; ++ax2_2) {\n        for (int32_t ax3_2 = 0; ax3_2 < 9; ++ax3_2) {\n          T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)] = ((T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)] * (((T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)] * T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)]) * (((T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)] * T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)]) * (((T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)] * T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)]) * (((T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)] * T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)]) * (((T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)] * T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)]) * (((T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)] * T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)]) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)] * T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)]) * (((T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)] * T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)]) * (((T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)] * T_minimum[((((ax0_2 * 180) + (ax1_2 * 90)) + (ax2_2 * 9)) + ax3_2)]) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "fast_tanh"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ tarray, float* __restrict__ tarray_red);\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ tarray, float* __restrict__ tarray_red) {\n  tarray_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 9; ++k0) {\n    tarray_red[0] = max(tarray_red[0], tarray[k0]);\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float tarray_red[1];\n  tarray_red[0] = -3.402823e+38f;\n  for (int32_t k0 = 0; k0 < 9; ++k0) {\n    float v_ = tarray_red[0];\n    float v__1 = tarray[k0];\n    tarray_red[0] = ((v_) > (v__1) ? (v_) : (v__1));\n  }\n}\n\n",
    "op_name": "max"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tensordot, float* __restrict__ tarray) {\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer) {\n    if ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 1024) + (((int)blockIdx.x) * 4)) + (((int)threadIdx.x) >> 8)) < 2025) {\n      T_tensordot[(((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x))] = (tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 16384) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.x) >> 4)) / 45)] * tarray[((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_ax5_fused_ax6_fused_ax7_fused_outer * 262144) + (((int)blockIdx.x) * 1024)) + ((int)threadIdx.x)) % 720)]);\n    }\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tensordot[518400];\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 9; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 4; ++ax4) {\n            for (int32_t ax5 = 0; ax5 < 5; ++ax5) {\n              for (int32_t ax6 = 0; ax6 < 4; ++ax6) {\n                for (int32_t ax7 = 0; ax7 < 9; ++ax7) {\n                  T_tensordot[((((((((ax0 * 129600) + (ax1 * 25920)) + (ax2 * 6480)) + (ax3 * 720)) + (ax4 * 180)) + (ax5 * 36)) + (ax6 * 9)) + ax7)] = (tarray[((((ax0 * 180) + (ax1 * 36)) + (ax2 * 9)) + ax3)] * tarray[((((ax4 * 180) + (ax5 * 36)) + (ax6 * 9)) + ax7)]);\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tensordot"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)128) + (((int64_t)((int)threadIdx.x)) >> (int64_t)3)) < (int64_t)315) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = sinhf(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[2520];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          for (int32_t i4 = 0; i4 < 7; ++i4) {\n            compute[(((((i0 * 420) + (i1 * 210)) + (i2 * 70)) + (i3 * 7)) + i4)] = sinhf(tarray[(((((i0 * 420) + (i1 * 210)) + (i2 * 70)) + (i3 * 7)) + i4)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "sinh"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)32) + (((int64_t)((int)threadIdx.x)) >> (int64_t)5)) < (int64_t)45) {\n    compute[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = erff(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]);\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat erff(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[1440];\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        for (int32_t i3 = 0; i3 < 2; ++i3) {\n          for (int32_t i4 = 0; i4 < 9; ++i4) {\n            compute[(((((i0 * 144) + (i1 * 36)) + (i2 * 18)) + (i3 * 9)) + i4)] = erff(tarray[(((((i0 * 144) + (i1 * 36)) + (i2 * 18)) + (i3 * 9)) + i4)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "erf"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = coshf(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[324];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 54) + (i1 * 9)) + i2)] = coshf(tarray[(((i0 * 54) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "cosh"
  },
  {
    "instruction": "Your deep learning application performs well on a small GPU cluster, but now you need to deploy it to a large CPU cluster. Convert existing CUDA operators to functionally identical CPU operators and ensure the scalability of your application in large-scale environments.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = sinhf(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[2];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    compute[i0] = sinhf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "sinh"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - tarray[((int)threadIdx.x)]))));\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[2];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    compute[i0] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - tarray[i0]))));\n  }\n}\n\n",
    "op_name": "sigmoid"
  },
  {
    "instruction": "Given a performance-sensitive application that includes CUDA operators, you want to run in an environment without a GPU. Please convert the CUDA operators provided to an efficient CPU implementation with as little performance penalty as possible.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(signed char* __restrict__ T_equal, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(signed char* __restrict__ T_equal, float* __restrict__ tarray) {\n  T_equal[((int)threadIdx.x)] = ((signed char)(tarray[((int)threadIdx.x)] == tarray[((int)threadIdx.x)]));\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  int8_t T_equal[4];\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_equal[ax0] = ((int8_t)(tarray[ax0] == tarray[ax0]));\n  }\n}\n\n",
    "op_name": "equal"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_expand_dims, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_expand_dims, float* __restrict__ tarray) {\n  if (((((int)((int64_t)((int)blockIdx.x))) * 8) + (((int)((int64_t)((int)threadIdx.x))) >> 7)) < 9) {\n    if (((((int64_t)((int)blockIdx.x)) * (int64_t)8) + (((int64_t)((int)threadIdx.x)) >> (int64_t)7)) < (int64_t)9) {\n      T_expand_dims[((((((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 5)) / 9) * 288) + ((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 5)) % 9) * 32)) + (((int)threadIdx.x) & 31))] = tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n    }\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_expand_dims[1152];\n  for (int32_t ax1 = 0; ax1 < 4; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        for (int32_t ax4 = 0; ax4 < 4; ++ax4) {\n          T_expand_dims[((((ax1 * 288) + (ax2 * 32)) + (ax3 * 4)) + ax4)] = tarray[((((ax1 * 288) + (ax2 * 32)) + (ax3 * 4)) + ax4)];\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "expand_dims"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((int)((((int)threadIdx.x) == 1) ? (int64_t)1 : (int64_t)6));\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel();\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel() {\n  int32_t T_shape[2];\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    T_shape[ax0] = ((int32_t)((ax0 == 1) ? (int64_t)1 : (int64_t)6));\n  }\n}\n\n",
    "op_name": "shape"
  },
  {
    "instruction": "You are integrating a deep learning model using CUDA operators into a distributed system where some nodes do not have GPU support. Please convert the provided CUDA operators to CPU operators to ensure consistency and performance throughout the system.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = floorf(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[18];\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      compute[((i0 * 2) + i1)] = floorf(tarray[((i0 * 2) + i1)]);\n    }\n  }\n}\n\n",
    "op_name": "floor"
  },
  {
    "instruction": "Given a CUDA operator, convert it to an equivalent CPU operator. Please ensure both performance and correctness, and maximize the execution efficiency on the CPU as much as possible.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ T_elemwise_sum, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(324) default_function_kernel(float* __restrict__ T_elemwise_sum, float* __restrict__ tarray) {\n  T_elemwise_sum[((int)threadIdx.x)] = (tarray[((int)threadIdx.x)] + tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_elemwise_sum[324];\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_elemwise_sum[(((ax0 * 54) + (ax1 * 9)) + ax2)] = (tarray[(((ax0 * 54) + (ax1 * 9)) + ax2)] + tarray[(((ax0 * 54) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
    "op_name": "elemwise_sum"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ T_elemwise_sum, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ T_elemwise_sum, float* __restrict__ tarray) {\n  T_elemwise_sum[((int)threadIdx.x)] = (tarray[((int)threadIdx.x)] + tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_elemwise_sum[6];\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    T_elemwise_sum[ax0] = (tarray[ax0] + tarray[ax0]);\n  }\n}\n\n",
    "op_name": "elemwise_sum"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(signed char* __restrict__ T_not_equal, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(signed char* __restrict__ T_not_equal, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)8) + (((int64_t)((int)threadIdx.x)) >> (int64_t)7)) < (int64_t)9) {\n    T_not_equal[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = ((signed char)(tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] != tarray[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))]));\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  int8_t T_not_equal[1152];\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 4; ++ax3) {\n          T_not_equal[((((ax0 * 288) + (ax1 * 32)) + (ax2 * 4)) + ax3)] = ((int8_t)(tarray[((((ax0 * 288) + (ax1 * 32)) + (ax2 * 4)) + ax3)] != tarray[((((ax0 * 288) + (ax1 * 32)) + (ax2 * 4)) + ax3)]));\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "not_equal"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tile, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(1024) default_function_kernel(float* __restrict__ T_tile, float* __restrict__ tarray) {\n  if (((((int64_t)((int)blockIdx.x)) * (int64_t)64) + (((int64_t)((int)threadIdx.x)) >> (int64_t)4)) < (int64_t)945) {\n    T_tile[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = tarray[((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 105) * 70) + (((((((int)blockIdx.x) * 184) + ((int)threadIdx.x)) % 210) / 21) * 7)) + (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 7))];\n  }\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_tile[15120];\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 21; ++ax4) {\n            T_tile[(((((ax0 * 2520) + (ax1 * 1260)) + (ax2 * 420)) + (ax3 * 21)) + ax4)] = tarray[(((((ax0 * 420) + (ax1 * 210)) + (ax2 * 70)) + ((ax3 % 10) * 7)) + (ax4 % 7))];\n          }\n        }\n      }\n    }\n  }\n}\n\n",
    "op_name": "tile"
  },
  {
    "instruction": "Consider a deep learning model using any deep learning framework (e.g. TensorFlow, PyTorch, etc.) that includes CUDA operators. Replace the CUDA operator in the model with the corresponding CPU operator to run in an environment without a GPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = asinf(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat asinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[2];\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    compute[i0] = asinf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "asin"
  },
  {
    "instruction": "Suppose your deep learning application is trained in a CUDA-enabled environment. Now, you need to port the application to an environment that doesn't support GPUs. Convert the CUDA operators provided to CPU operators to maintain application functionality in an environment without GPUs.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_floor_mod, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_floor_mod, float* __restrict__ tarray) {\n  T_floor_mod[((int)threadIdx.x)] = (tarray[((int)threadIdx.x)] - (floorf((tarray[((int)threadIdx.x)] / tarray[((int)threadIdx.x)])) * tarray[((int)threadIdx.x)]));\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float T_floor_mod[8];\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    T_floor_mod[ax0] = (tarray[ax0] - (floorf((tarray[ax0] / tarray[ax0])) * tarray[ax0]));\n  }\n}\n\n",
    "op_name": "floor_mod"
  },
  {
    "instruction": "You have a deep learning model trained in PyTorch, which includes CUDA operators. Please convert these CUDA operators to equivalent CPU operators that can run in an environment without a GPU, whether using TensorFlow, MXNet, or any other deep learning framework.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = acosf(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat acosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[6];\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    compute[i0] = acosf(tarray[i0]);\n  }\n}\n\n",
    "op_name": "acos"
  },
  {
    "instruction": "In an environment without GPU support, you want to reason about deep learning models with a large number of inputs. Please convert existing CUDA operators to corresponding CPU operators and take advantage of batch processing and parallelism to improve inference performance on the CPU.",
    "input": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ tarray) {\n  compute[((int)threadIdx.x)] = __sinf(tarray[((int)threadIdx.x)]);\n}\n\n",
    "output": "#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nfloat sinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nvoid default_function_kernel(float* tarray) {\n  float compute[24];\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      compute[((i0 * 8) + i1)] = sinf(tarray[((i0 * 8) + i1)]);\n    }\n  }\n}\n\n",
    "op_name": "sin"
  }
]