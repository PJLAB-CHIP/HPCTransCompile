[
    {
        "op_name": "add",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_code_1 = arg_type_ids[1];\n  int32_t T_add_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_1 = (((TVMValue*)args)[1].v_handle);\n  void* T_add = (((TVMValue*)args)[2].v_handle);\n  void* data_2 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_3 = (((DLTensor*)data_1)[0].data);\n  void* default_function_data_shape_1 = (((DLTensor*)data_1)[0].shape);\n  void* default_function_data_strides_1 = (((DLTensor*)data_1)[0].strides);\n  void* T_add_1 = (((DLTensor*)T_add)[0].data);\n  void* default_function_T_add_shape = (((DLTensor*)T_add)[0].shape);\n  void* default_function_T_add_strides = (((DLTensor*)T_add)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_data_strides_1 == NULL)) {\n  },\n  if (!(default_function_T_add_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 12; ++ax3) {\n          int32_t cse_var_1 = ((((ax0 * 3072) + (ax1 * 192)) + (ax2 * 12)) + ax3);\n          ((float*)T_add_1)[cse_var_1] = (((float*)data_2)[cse_var_1] + ((float*)data_3)[cse_var_1]);\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 16, 16, 12), \"float32\"), data_1: T.Buffer((3, 16, 16, 12), \"float32\"), T_add: T.Buffer((3, 16, 16, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(3):\n            for ax1, ax2, ax3 in T.grid(16, 16, 12):\n                cse_var_1: T.int32 = ax0 * 3072 + ax1 * 192 + ax2 * 12 + ax3\n                T_add_1 = T.Buffer((9216,), data=T_add.data)\n                data_2 = T.Buffer((9216,), data=data.data)\n                data_3 = T.Buffer((9216,), data=data_1.data)\n                T_add_1[cse_var_1] = data_2[cse_var_1] + data_3[cse_var_1]",
        "data": "3_16_16_12"
    },
    {
        "op_name": "add",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_code_1 = arg_type_ids[1];\n  int32_t T_add_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_1 = (((TVMValue*)args)[1].v_handle);\n  void* T_add = (((TVMValue*)args)[2].v_handle);\n  void* data_2 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_3 = (((DLTensor*)data_1)[0].data);\n  void* default_function_data_shape_1 = (((DLTensor*)data_1)[0].shape);\n  void* default_function_data_strides_1 = (((DLTensor*)data_1)[0].strides);\n  void* T_add_1 = (((DLTensor*)T_add)[0].data);\n  void* default_function_T_add_shape = (((DLTensor*)T_add)[0].shape);\n  void* default_function_T_add_strides = (((DLTensor*)T_add)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_data_strides_1 == NULL)) {\n  },\n  if (!(default_function_T_add_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 384; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 36; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 36) + ax3);\n      ((float*)T_add_1)[cse_var_1] = (((float*)data_2)[cse_var_1] + ((float*)data_3)[cse_var_1]);\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 12, 16, 36), \"float32\"), data_1: T.Buffer((2, 12, 16, 36), \"float32\"), T_add: T.Buffer((2, 12, 16, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(384):\n            for ax3 in range(36):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 36 + ax3\n                T_add_1 = T.Buffer((13824,), data=T_add.data)\n                data_2 = T.Buffer((13824,), data=data.data)\n                data_3 = T.Buffer((13824,), data=data_1.data)\n                T_add_1[cse_var_1] = data_2[cse_var_1] + data_3[cse_var_1]",
        "data": "2_12_16_36"
    },
    {
        "op_name": "add",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_code_1 = arg_type_ids[1];\n  int32_t T_add_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_1 = (((TVMValue*)args)[1].v_handle);\n  void* T_add = (((TVMValue*)args)[2].v_handle);\n  void* data_2 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_3 = (((DLTensor*)data_1)[0].data);\n  void* default_function_data_shape_1 = (((DLTensor*)data_1)[0].shape);\n  void* default_function_data_strides_1 = (((DLTensor*)data_1)[0].strides);\n  void* T_add_1 = (((DLTensor*)T_add)[0].data);\n  void* default_function_T_add_shape = (((DLTensor*)T_add)[0].shape);\n  void* default_function_T_add_strides = (((DLTensor*)T_add)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_data_strides_1 == NULL)) {\n  },\n  if (!(default_function_T_add_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 5120; ++ax0_ax1_fused_ax2_fused) {\n    int32_t cse_var_1 = (ax0_ax1_fused_ax2_fused * 8);\n    int32_t8 v_ = int32_t8((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7));\n    *(float8*)(((float*)T_add_1) + cse_var_1) = ((float8(((float*)data_2)[v_.s0],((float*)data_2)[v_.s1],((float*)data_2)[v_.s2],((float*)data_2)[v_.s3],((float*)data_2)[v_.s4],((float*)data_2)[v_.s5],((float*)data_2)[v_.s6],((float*)data_2)[v_.s7])) + (float8(((float*)data_3)[v_.s0],((float*)data_3)[v_.s1],((float*)data_3)[v_.s2],((float*)data_3)[v_.s3],((float*)data_3)[v_.s4],((float*)data_3)[v_.s5],((float*)data_3)[v_.s6],((float*)data_3)[v_.s7])));\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 32, 32, 8), \"float32\"), data_1: T.Buffer((5, 32, 32, 8), \"float32\"), T_add: T.Buffer((5, 32, 32, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(5120):\n            cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 8\n            T_add_1 = T.Buffer((40960,), data=T_add.data)\n            data_2 = T.Buffer((40960,), data=data.data)\n            data_3 = T.Buffer((40960,), data=data_1.data)\n            T_add_1[cse_var_1:cse_var_1 + 8] = data_2[cse_var_1:cse_var_1 + 8] + data_3[cse_var_1:cse_var_1 + 8]",
        "data": "5_32_32_8"
    },
    {
        "op_name": "add",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_code_1 = arg_type_ids[1];\n  int32_t T_add_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_1 = (((TVMValue*)args)[1].v_handle);\n  void* T_add = (((TVMValue*)args)[2].v_handle);\n  void* data_2 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_3 = (((DLTensor*)data_1)[0].data);\n  void* default_function_data_shape_1 = (((DLTensor*)data_1)[0].shape);\n  void* default_function_data_strides_1 = (((DLTensor*)data_1)[0].strides);\n  void* T_add_1 = (((DLTensor*)T_add)[0].data);\n  void* default_function_T_add_shape = (((DLTensor*)T_add)[0].shape);\n  void* default_function_T_add_strides = (((DLTensor*)T_add)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_data_strides_1 == NULL)) {\n  },\n  if (!(default_function_T_add_strides == NULL)) {\n  },\n  for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 28; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 40; ++ax3) {\n        int32_t cse_var_1 = (((ax1 * 1120) + (ax2 * 40)) + ax3);\n        ((float*)T_add_1)[cse_var_1] = (((float*)data_2)[cse_var_1] + ((float*)data_3)[cse_var_1]);\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 12, 28, 40), \"float32\"), data_1: T.Buffer((1, 12, 28, 40), \"float32\"), T_add: T.Buffer((1, 12, 28, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax1, ax2, ax3 in T.grid(12, 28, 40):\n            cse_var_1: T.int32 = ax1 * 1120 + ax2 * 40 + ax3\n            T_add_1 = T.Buffer((13440,), data=T_add.data)\n            data_2 = T.Buffer((13440,), data=data.data)\n            data_3 = T.Buffer((13440,), data=data_1.data)\n            T_add_1[cse_var_1] = data_2[cse_var_1] + data_3[cse_var_1]",
        "data": "1_12_28_40"
    },
    {
        "op_name": "add",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_code_1 = arg_type_ids[1];\n  int32_t T_add_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_1 = (((TVMValue*)args)[1].v_handle);\n  void* T_add = (((TVMValue*)args)[2].v_handle);\n  void* data_2 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_3 = (((DLTensor*)data_1)[0].data);\n  void* default_function_data_shape_1 = (((DLTensor*)data_1)[0].shape);\n  void* default_function_data_strides_1 = (((DLTensor*)data_1)[0].strides);\n  void* T_add_1 = (((DLTensor*)T_add)[0].data);\n  void* default_function_T_add_shape = (((DLTensor*)T_add)[0].shape);\n  void* default_function_T_add_strides = (((DLTensor*)T_add)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_data_strides_1 == NULL)) {\n  },\n  if (!(default_function_T_add_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 480; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 32; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 32) + ax3);\n      ((float*)T_add_1)[cse_var_1] = (((float*)data_2)[cse_var_1] + ((float*)data_3)[cse_var_1]);\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 8, 20, 32), \"float32\"), data_1: T.Buffer((3, 8, 20, 32), \"float32\"), T_add: T.Buffer((3, 8, 20, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(480):\n            for ax3 in range(32):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 32 + ax3\n                T_add_1 = T.Buffer((15360,), data=T_add.data)\n                data_2 = T.Buffer((15360,), data=data.data)\n                data_3 = T.Buffer((15360,), data=data_1.data)\n                T_add_1[cse_var_1] = data_2[cse_var_1] + data_3[cse_var_1]",
        "data": "3_8_20_32"
    },
    {
        "op_name": "add",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_code_1 = arg_type_ids[1];\n  int32_t T_add_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_1 = (((TVMValue*)args)[1].v_handle);\n  void* T_add = (((TVMValue*)args)[2].v_handle);\n  void* data_2 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_3 = (((DLTensor*)data_1)[0].data);\n  void* default_function_data_shape_1 = (((DLTensor*)data_1)[0].shape);\n  void* default_function_data_strides_1 = (((DLTensor*)data_1)[0].strides);\n  void* T_add_1 = (((DLTensor*)T_add)[0].data);\n  void* default_function_T_add_shape = (((DLTensor*)T_add)[0].shape);\n  void* default_function_T_add_strides = (((DLTensor*)T_add)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_data_strides_1 == NULL)) {\n  },\n  if (!(default_function_T_add_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 24; ++ax3) {\n          int32_t cse_var_1 = ((((ax0 * 5760) + (ax1 * 288)) + (ax2 * 24)) + ax3);\n          ((float*)T_add_1)[cse_var_1] = (((float*)data_2)[cse_var_1] + ((float*)data_3)[cse_var_1]);\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 20, 12, 24), \"float32\"), data_1: T.Buffer((3, 20, 12, 24), \"float32\"), T_add: T.Buffer((3, 20, 12, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(3):\n            for ax1, ax2, ax3 in T.grid(20, 12, 24):\n                cse_var_1: T.int32 = ax0 * 5760 + ax1 * 288 + ax2 * 24 + ax3\n                T_add_1 = T.Buffer((17280,), data=T_add.data)\n                data_2 = T.Buffer((17280,), data=data.data)\n                data_3 = T.Buffer((17280,), data=data_1.data)\n                T_add_1[cse_var_1] = data_2[cse_var_1] + data_3[cse_var_1]",
        "data": "3_20_12_24"
    },
    {
        "op_name": "add",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_code_1 = arg_type_ids[1];\n  int32_t T_add_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_1 = (((TVMValue*)args)[1].v_handle);\n  void* T_add = (((TVMValue*)args)[2].v_handle);\n  void* data_2 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_3 = (((DLTensor*)data_1)[0].data);\n  void* default_function_data_shape_1 = (((DLTensor*)data_1)[0].shape);\n  void* default_function_data_strides_1 = (((DLTensor*)data_1)[0].strides);\n  void* T_add_1 = (((DLTensor*)T_add)[0].data);\n  void* default_function_T_add_shape = (((DLTensor*)T_add)[0].shape);\n  void* default_function_T_add_strides = (((DLTensor*)T_add)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_data_strides_1 == NULL)) {\n  },\n  if (!(default_function_T_add_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 216; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 36; ++ax3) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 432) + (ax2 * 36)) + ax3);\n        ((float*)T_add_1)[cse_var_1] = (((float*)data_2)[cse_var_1] + ((float*)data_3)[cse_var_1]);\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 24, 12, 36), \"float32\"), data_1: T.Buffer((9, 24, 12, 36), \"float32\"), T_add: T.Buffer((9, 24, 12, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(216):\n            for ax2, ax3 in T.grid(12, 36):\n                cse_var_1: T.int32 = ax0_ax1_fused * 432 + ax2 * 36 + ax3\n                T_add_1 = T.Buffer((93312,), data=T_add.data)\n                data_2 = T.Buffer((93312,), data=data.data)\n                data_3 = T.Buffer((93312,), data=data_1.data)\n                T_add_1[cse_var_1] = data_2[cse_var_1] + data_3[cse_var_1]",
        "data": "9_24_12_36"
    },
    {
        "op_name": "add",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_code_1 = arg_type_ids[1];\n  int32_t T_add_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_1 = (((TVMValue*)args)[1].v_handle);\n  void* T_add = (((TVMValue*)args)[2].v_handle);\n  void* data_2 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_3 = (((DLTensor*)data_1)[0].data);\n  void* default_function_data_shape_1 = (((DLTensor*)data_1)[0].shape);\n  void* default_function_data_strides_1 = (((DLTensor*)data_1)[0].strides);\n  void* T_add_1 = (((DLTensor*)T_add)[0].data);\n  void* default_function_T_add_shape = (((DLTensor*)T_add)[0].shape);\n  void* default_function_T_add_strides = (((DLTensor*)T_add)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_data_strides_1 == NULL)) {\n  },\n  if (!(default_function_T_add_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 80; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n      int32_t cse_var_1 = ((ax0_ax1_fused * 96) + (ax2 * 8));\n      int32_t8 v_ = int32_t8((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7));\n      *(float8*)(((float*)T_add_1) + cse_var_1) = ((float8(((float*)data_2)[v_.s0],((float*)data_2)[v_.s1],((float*)data_2)[v_.s2],((float*)data_2)[v_.s3],((float*)data_2)[v_.s4],((float*)data_2)[v_.s5],((float*)data_2)[v_.s6],((float*)data_2)[v_.s7])) + (float8(((float*)data_3)[v_.s0],((float*)data_3)[v_.s1],((float*)data_3)[v_.s2],((float*)data_3)[v_.s3],((float*)data_3)[v_.s4],((float*)data_3)[v_.s5],((float*)data_3)[v_.s6],((float*)data_3)[v_.s7])));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 20, 12, 8), \"float32\"), data_1: T.Buffer((4, 20, 12, 8), \"float32\"), T_add: T.Buffer((4, 20, 12, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(80):\n            for ax2 in range(12):\n                cse_var_1: T.int32 = ax0_ax1_fused * 96 + ax2 * 8\n                T_add_1 = T.Buffer((7680,), data=T_add.data)\n                data_2 = T.Buffer((7680,), data=data.data)\n                data_3 = T.Buffer((7680,), data=data_1.data)\n                T_add_1[cse_var_1:cse_var_1 + 8] = data_2[cse_var_1:cse_var_1 + 8] + data_3[cse_var_1:cse_var_1 + 8]",
        "data": "4_20_12_8"
    },
    {
        "op_name": "add",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_code_1 = arg_type_ids[1];\n  int32_t T_add_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_1 = (((TVMValue*)args)[1].v_handle);\n  void* T_add = (((TVMValue*)args)[2].v_handle);\n  void* data_2 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_3 = (((DLTensor*)data_1)[0].data);\n  void* default_function_data_shape_1 = (((DLTensor*)data_1)[0].shape);\n  void* default_function_data_strides_1 = (((DLTensor*)data_1)[0].strides);\n  void* T_add_1 = (((DLTensor*)T_add)[0].data);\n  void* default_function_T_add_shape = (((DLTensor*)T_add)[0].shape);\n  void* default_function_T_add_strides = (((DLTensor*)T_add)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_data_strides_1 == NULL)) {\n  },\n  if (!(default_function_T_add_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 5760; ++ax0_ax1_fused_ax2_fused) {\n    int32_t cse_var_1 = (ax0_ax1_fused_ax2_fused * 16);\n    int32_t16 v_ = int32_t16((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7), (cse_var_1)+(1*8), (cse_var_1)+(1*9), (cse_var_1)+(1*10), (cse_var_1)+(1*11), (cse_var_1)+(1*12), (cse_var_1)+(1*13), (cse_var_1)+(1*14), (cse_var_1)+(1*15));\n    *(float16*)(((float*)T_add_1) + cse_var_1) = ((float16(((float*)data_2)[v_.s0],((float*)data_2)[v_.s1],((float*)data_2)[v_.s2],((float*)data_2)[v_.s3],((float*)data_2)[v_.s4],((float*)data_2)[v_.s5],((float*)data_2)[v_.s6],((float*)data_2)[v_.s7],((float*)data_2)[v_.s8],((float*)data_2)[v_.s9],((float*)data_2)[v_.sa],((float*)data_2)[v_.sb],((float*)data_2)[v_.sc],((float*)data_2)[v_.sd],((float*)data_2)[v_.se],((float*)data_2)[v_.sf])) + (float16(((float*)data_3)[v_.s0],((float*)data_3)[v_.s1],((float*)data_3)[v_.s2],((float*)data_3)[v_.s3],((float*)data_3)[v_.s4],((float*)data_3)[v_.s5],((float*)data_3)[v_.s6],((float*)data_3)[v_.s7],((float*)data_3)[v_.s8],((float*)data_3)[v_.s9],((float*)data_3)[v_.sa],((float*)data_3)[v_.sb],((float*)data_3)[v_.sc],((float*)data_3)[v_.sd],((float*)data_3)[v_.se],((float*)data_3)[v_.sf])));\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1);\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 32, 36, 16), \"float32\"), data_1: T.Buffer((5, 32, 36, 16), \"float32\"), T_add: T.Buffer((5, 32, 36, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(5760):\n            cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 16\n            T_add_1 = T.Buffer((92160,), data=T_add.data)\n            data_2 = T.Buffer((92160,), data=data.data)\n            data_3 = T.Buffer((92160,), data=data_1.data)\n            T_add_1[cse_var_1:cse_var_1 + 16] = data_2[cse_var_1:cse_var_1 + 16] + data_3[cse_var_1:cse_var_1 + 16]",
        "data": "5_32_36_16"
    },
    {
        "op_name": "add",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_code_1 = arg_type_ids[1];\n  int32_t T_add_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_1 = (((TVMValue*)args)[1].v_handle);\n  void* T_add = (((TVMValue*)args)[2].v_handle);\n  void* data_2 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_3 = (((DLTensor*)data_1)[0].data);\n  void* default_function_data_shape_1 = (((DLTensor*)data_1)[0].shape);\n  void* default_function_data_strides_1 = (((DLTensor*)data_1)[0].strides);\n  void* T_add_1 = (((DLTensor*)T_add)[0].data);\n  void* default_function_T_add_shape = (((DLTensor*)T_add)[0].shape);\n  void* default_function_T_add_strides = (((DLTensor*)T_add)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_data_strides_1 == NULL)) {\n  },\n  if (!(default_function_T_add_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 10080; ++ax0_ax1_fused_ax2_fused) {\n    int32_t cse_var_1 = (ax0_ax1_fused_ax2_fused * 12);\n    int32_t12 v_ = int32_t12((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7), (cse_var_1)+(1*8), (cse_var_1)+(1*9), (cse_var_1)+(1*10), (cse_var_1)+(1*11));\n    *(float12*)(((float*)T_add_1) + cse_var_1) = ((float12(((float*)data_2)[v_.s0],((float*)data_2)[v_.s1],((float*)data_2)[v_.s2],((float*)data_2)[v_.s3],((float*)data_2)[v_.s4],((float*)data_2)[v_.s5],((float*)data_2)[v_.s6],((float*)data_2)[v_.s7],((float*)data_2)[v_.s8],((float*)data_2)[v_.s9],((float*)data_2)[v_.sa],((float*)data_2)[v_.sb])) + (float12(((float*)data_3)[v_.s0],((float*)data_3)[v_.s1],((float*)data_3)[v_.s2],((float*)data_3)[v_.s3],((float*)data_3)[v_.s4],((float*)data_3)[v_.s5],((float*)data_3)[v_.s6],((float*)data_3)[v_.s7],((float*)data_3)[v_.s8],((float*)data_3)[v_.s9],((float*)data_3)[v_.sa],((float*)data_3)[v_.sb])));\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 36, 28, 12), \"float32\"), data_1: T.Buffer((10, 36, 28, 12), \"float32\"), T_add: T.Buffer((10, 36, 28, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(10080):\n            cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 12\n            T_add_1 = T.Buffer((120960,), data=T_add.data)\n            data_2 = T.Buffer((120960,), data=data.data)\n            data_3 = T.Buffer((120960,), data=data_1.data)\n            T_add_1[cse_var_1:cse_var_1 + 12] = data_2[cse_var_1:cse_var_1 + 12] + data_3[cse_var_1:cse_var_1 + 12]",
        "data": "10_36_28_12"
    },
    {
        "op_name": "add",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_code_1 = arg_type_ids[1];\n  int32_t T_add_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_1 = (((TVMValue*)args)[1].v_handle);\n  void* T_add = (((TVMValue*)args)[2].v_handle);\n  void* data_2 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_3 = (((DLTensor*)data_1)[0].data);\n  void* default_function_data_shape_1 = (((DLTensor*)data_1)[0].shape);\n  void* default_function_data_strides_1 = (((DLTensor*)data_1)[0].strides);\n  void* T_add_1 = (((DLTensor*)T_add)[0].data);\n  void* default_function_T_add_shape = (((DLTensor*)T_add)[0].shape);\n  void* default_function_T_add_strides = (((DLTensor*)T_add)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_data_strides_1 == NULL)) {\n  },\n  if (!(default_function_T_add_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 32; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 24; ++ax2) {\n        int32_t cse_var_1 = (((ax0 * 6144) + (ax1 * 192)) + (ax2 * 8));\n        int32_t8 v_ = int32_t8((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7));\n        *(float8*)(((float*)T_add_1) + cse_var_1) = ((float8(((float*)data_2)[v_.s0],((float*)data_2)[v_.s1],((float*)data_2)[v_.s2],((float*)data_2)[v_.s3],((float*)data_2)[v_.s4],((float*)data_2)[v_.s5],((float*)data_2)[v_.s6],((float*)data_2)[v_.s7])) + (float8(((float*)data_3)[v_.s0],((float*)data_3)[v_.s1],((float*)data_3)[v_.s2],((float*)data_3)[v_.s3],((float*)data_3)[v_.s4],((float*)data_3)[v_.s5],((float*)data_3)[v_.s6],((float*)data_3)[v_.s7])));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 32, 24, 8), \"float32\"), data_1: T.Buffer((9, 32, 24, 8), \"float32\"), T_add: T.Buffer((9, 32, 24, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(9):\n            for ax1, ax2 in T.grid(32, 24):\n                cse_var_1: T.int32 = ax0 * 6144 + ax1 * 192 + ax2 * 8\n                T_add_1 = T.Buffer((55296,), data=T_add.data)\n                data_2 = T.Buffer((55296,), data=data.data)\n                data_3 = T.Buffer((55296,), data=data_1.data)\n                T_add_1[cse_var_1:cse_var_1 + 8] = data_2[cse_var_1:cse_var_1 + 8] + data_3[cse_var_1:cse_var_1 + 8]",
        "data": "9_32_24_8"
    },
    {
        "op_name": "add",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_code_1 = arg_type_ids[1];\n  int32_t T_add_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_1 = (((TVMValue*)args)[1].v_handle);\n  void* T_add = (((TVMValue*)args)[2].v_handle);\n  void* data_2 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_3 = (((DLTensor*)data_1)[0].data);\n  void* default_function_data_shape_1 = (((DLTensor*)data_1)[0].shape);\n  void* default_function_data_strides_1 = (((DLTensor*)data_1)[0].strides);\n  void* T_add_1 = (((DLTensor*)T_add)[0].data);\n  void* default_function_T_add_shape = (((DLTensor*)T_add)[0].shape);\n  void* default_function_T_add_strides = (((DLTensor*)T_add)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_data_strides_1 == NULL)) {\n  },\n  if (!(default_function_T_add_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 168; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 40; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 36; ++ax3) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 1440) + (ax2 * 36)) + ax3);\n        ((float*)T_add_1)[cse_var_1] = (((float*)data_2)[cse_var_1] + ((float*)data_3)[cse_var_1]);\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 24, 40, 36), \"float32\"), data_1: T.Buffer((7, 24, 40, 36), \"float32\"), T_add: T.Buffer((7, 24, 40, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(168):\n            for ax2, ax3 in T.grid(40, 36):\n                cse_var_1: T.int32 = ax0_ax1_fused * 1440 + ax2 * 36 + ax3\n                T_add_1 = T.Buffer((241920,), data=T_add.data)\n                data_2 = T.Buffer((241920,), data=data.data)\n                data_3 = T.Buffer((241920,), data=data_1.data)\n                T_add_1[cse_var_1] = data_2[cse_var_1] + data_3[cse_var_1]",
        "data": "7_24_40_36"
    },
    {
        "op_name": "adaptive_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 128; ++ax0_ax1_fused) {\n    float adaptive_pool_sum[8];\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        adaptive_pool_sum[ax3] = 0.000000e+00f;\n        for (int32_t rv0 = 0; rv0 < ((((((ax2 * 4) + 4) % 8) == 0) ? (((ax2 * 5) + 5) >> 1) : ((((ax2 * 5) + 5) >> 1) + 1)) - ((ax2 * 20) >> 3)); ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            adaptive_pool_sum[ax3] = (adaptive_pool_sum[ax3] + ((float*)data_1)[(((((ax0_ax1_fused * 480) + (((ax2 * 20) >> 3) * 24)) + (rv0 * 24)) + (ax3 * 3)) + rv1)]);\n          },\n        },\n      },\n      for (int32_t ax3_1 = 0; ax3_1 < 8; ++ax3_1) {\n        int32_t cse_var_2 = (((ax2 * 5) + 5) >> 1);\n        ((float*)adaptive_pool_avg_1)[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3_1)] = (adaptive_pool_sum[ax3_1] / (((float)((((((ax2 * 4) + 4) % 8) == 0) ? cse_var_2 : (cse_var_2 + 1)) - ((ax2 * 20) >> 3))) * 3.000000e+00f));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / (((float)(((((((((int)threadIdx.x) >> 3) * 4) + 4) % 8) == 0) ? ((((((int)threadIdx.x) >> 3) * 5) + 5) >> 1) : (((((((int)threadIdx.x) >> 3) * 5) + 5) >> 1) + 1)) - (((((int)threadIdx.x) >> 3) * 20) >> 3))) * 3.000000e+00f));\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < ((((((((((int)threadIdx.x) >> 3) * 4) + 4) % 8) == 0) ? (((((int)blockIdx.x) & 1) * 10) + ((((((int)threadIdx.x) >> 3) * 5) + 5) >> 1)) : ((((((int)blockIdx.x) & 1) * 10) + ((((((int)threadIdx.x) >> 3) * 5) + 5) >> 1)) + 1)) - (((((int)threadIdx.x) >> 3) * 20) >> 3)) - ((((int)blockIdx.x) & 1) * 10)); ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + data[(((((((int)blockIdx.x) * 240) + ((((((int)threadIdx.x) >> 3) * 20) >> 3) * 24)) + (rv0 * 24)) + ((((int)threadIdx.x) & 7) * 3)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 16, 20, 24), \"float32\"), adaptive_pool_avg: T.Buffer((8, 16, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(128):\n            adaptive_pool_sum = T.allocate([8], \"float32\", \"global\")\n            for ax2 in range(8):\n                adaptive_pool_sum_1 = T.Buffer((8,), data=adaptive_pool_sum, align=32)\n                for ax3 in range(8):\n                    adaptive_pool_sum_1[ax3] = T.float32(0)\n                    for rv0, rv1 in T.grid(T.Let(T.Select((ax2 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax2 * 20 // 8, where={cse_var_1: (ax2 * 5 + 5) // 2},), 3):\n                        cse_var_1 = T.int32()\n                        data_1 = T.Buffer((61440,), data=data.data)\n                        adaptive_pool_sum_1[ax3] = adaptive_pool_sum_1[ax3] + data_1[ax0_ax1_fused * 480 + ax2 * 20 // 8 * 24 + rv0 * 24 + ax3 * 3 + rv1]\n                for ax3 in range(8):\n                    cse_var_2: T.int32 = (ax2 * 5 + 5) // 2\n                    adaptive_pool_avg_1 = T.Buffer((8192,), data=adaptive_pool_avg.data)\n                    adaptive_pool_avg_1[ax0_ax1_fused * 64 + ax2 * 8 + ax3] = adaptive_pool_sum_1[ax3] / (T.Cast(\"float32\", T.Select((ax2 * 4 + 4) % 8 == 0, cse_var_2, cse_var_2 + 1) - ax2 * 20 // 8) * T.float32(3))",
        "data": "8_16_20_24"
    },
    {
        "op_name": "adaptive_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  float adaptive_pool_sum[64];\n  for (int32_t ax1 = 0; ax1 < 40; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        adaptive_pool_sum[((ax2 * 8) + ax3)] = 0.000000e+00f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 5; ++rv1) {\n            int32_t cse_var_1 = ((ax2 * 8) + ax3);\n            adaptive_pool_sum[cse_var_1] = (adaptive_pool_sum[cse_var_1] + ((float*)data_1)[(((((ax1 * 960) + (ax2 * 120)) + (rv0 * 40)) + (ax3 * 5)) + rv1)]);\n          },\n        },\n      },\n    },\n    for (int32_t ax2_1 = 0; ax2_1 < 8; ++ax2_1) {\n      for (int32_t ax3_1 = 0; ax3_1 < 8; ++ax3_1) {\n        int32_t cse_var_2 = (ax2_1 * 8);\n        ((float*)adaptive_pool_avg_1)[(((ax1 * 64) + cse_var_2) + ax3_1)] = (adaptive_pool_sum[(cse_var_2 + ax3_1)] * 6.666667e-02f);\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * 6.666667e-02f);\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 5; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + data[(((((((int)blockIdx.x) * 480) + ((((int)threadIdx.x) >> 3) * 120)) + (rv0 * 40)) + ((((int)threadIdx.x) & 7) * 5)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 40, 24, 40), \"float32\"), adaptive_pool_avg: T.Buffer((1, 40, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        adaptive_pool_sum = T.allocate([64], \"float32\", \"global\")\n        for ax1 in range(40):\n            adaptive_pool_sum_1 = T.Buffer((64,), data=adaptive_pool_sum)\n            for ax2, ax3 in T.grid(8, 8):\n                adaptive_pool_sum_1[ax2 * 8 + ax3] = T.float32(0)\n                for rv0, rv1 in T.grid(3, 5):\n                    cse_var_1: T.int32 = ax2 * 8 + ax3\n                    data_1 = T.Buffer((38400,), data=data.data)\n                    adaptive_pool_sum_1[cse_var_1] = adaptive_pool_sum_1[cse_var_1] + data_1[ax1 * 960 + ax2 * 120 + rv0 * 40 + ax3 * 5 + rv1]\n            for ax2, ax3 in T.grid(8, 8):\n                cse_var_2: T.int32 = ax2 * 8\n                adaptive_pool_avg_1 = T.Buffer((2560,), data=adaptive_pool_avg.data)\n                adaptive_pool_avg_1[ax1 * 64 + cse_var_2 + ax3] = adaptive_pool_sum_1[cse_var_2 + ax3] * T.float32(0.066666666666666666)",
        "data": "1_40_24_40"
    },
    {
        "op_name": "adaptive_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 800; ++ax0_ax1_fused_ax2_fused) {\n    float adaptive_pool_sum[1];\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      int32_t cse_var_2 = (ax0_ax1_fused_ax2_fused & 7);\n      int32_t cse_var_1 = ((cse_var_2 + 1) >> 1);\n      adaptive_pool_sum[0] = 0.000000e+00f;\n      for (int32_t rv0 = 0; rv0 < ((((((cse_var_2 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - (cse_var_2 >> 1)); ++rv0) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[((((ax0_ax1_fused_ax2_fused >> 1) * 8) + (rv0 * 8)) + ax3)]);\n      },\n      ((float*)adaptive_pool_avg_1)[((ax0_ax1_fused_ax2_fused * 8) + ax3)] = (adaptive_pool_sum[0] / ((float)((((((cse_var_2 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - (cse_var_2 >> 1))));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] / ((float)((((((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 2)) & 15) >> 1) * 4) + 4) % 8) == 0) ? (((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 2)) & 15) + 2) >> 2) : ((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 2)) & 15) + 2) >> 2) + 1)) - ((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 2)) & 15) >> 2))));\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < ((((((((((int)threadIdx.x) >> 3) * 4) + 4) % 8) == 0) ? (((((int)blockIdx.x) & 1) * 2) + ((((int)threadIdx.x) + 8) >> 4)) : ((((((int)blockIdx.x) & 1) * 2) + ((((int)threadIdx.x) + 8) >> 4)) + 1)) - (((int)threadIdx.x) >> 4)) - ((((int)blockIdx.x) & 1) * 2)); ++rv0) {\n    adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + data[((((((int)blockIdx.x) * 16) + ((((int)threadIdx.x) >> 4) * 8)) + (rv0 * 8)) + (((int)threadIdx.x) & 7))]);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 20, 4, 8), \"float32\"), adaptive_pool_avg: T.Buffer((5, 20, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(800):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            for ax3 in range(8):\n                cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused % 8\n                cse_var_1: T.int32 = (cse_var_2 + 1) // 2\n                adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n                adaptive_pool_sum_1[0] = T.float32(0)\n                for rv0 in range(T.Select((cse_var_2 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - cse_var_2 // 2):\n                    data_1 = T.Buffer((3200,), data=data.data)\n                    adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0_ax1_fused_ax2_fused // 2 * 8 + rv0 * 8 + ax3]\n                adaptive_pool_avg_1 = T.Buffer((6400,), data=adaptive_pool_avg.data)\n                adaptive_pool_avg_1[ax0_ax1_fused_ax2_fused * 8 + ax3] = adaptive_pool_sum_1[0] / T.Cast(\"float32\", T.Select((cse_var_2 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - cse_var_2 // 2)",
        "data": "5_20_4_8"
    },
    {
        "op_name": "adaptive_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 128; ++ax0_ax1_fused_ax2_fused) {\n    float adaptive_pool_sum[1];\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      int32_t cse_var_1 = (((ax3 * 3) + 3) >> 1);\n      adaptive_pool_sum[0] = 0.000000e+00f;\n      for (int32_t rv0 = 0; rv0 < 4; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < ((((((ax3 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - ((ax3 * 12) >> 3)); ++rv1) {\n          adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[((((ax0_ax1_fused_ax2_fused * 48) + (rv0 * 12)) + ((ax3 * 12) >> 3)) + rv1)]);\n        },\n      },\n      ((float*)adaptive_pool_avg_1)[((ax0_ax1_fused_ax2_fused * 8) + ax3)] = (adaptive_pool_sum[0] / (4.000000e+00f * ((float)((((((ax3 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - ((ax3 * 12) >> 3)))));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / (4.000000e+00f * ((float)(((((((((int)threadIdx.x) & 7) * 4) + 4) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 3) + 3) >> 1) : (((((((int)threadIdx.x) & 7) * 3) + 3) >> 1) + 1)) - (((((int)threadIdx.x) & 7) * 12) >> 3)))));\n},\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 4; ++rv0) {\n    for (int rv1 = 0; rv1 < (((((((((int)threadIdx.x) & 7) * 4) + 4) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 3) + 3) >> 1) : (((((((int)threadIdx.x) & 7) * 3) + 3) >> 1) + 1)) - (((((int)threadIdx.x) & 7) * 12) >> 3)); ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + data[(((((((int)blockIdx.x) * 96) + ((((int)threadIdx.x) >> 3) * 48)) + (rv0 * 12)) + (((((int)threadIdx.x) & 7) * 12) >> 3)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 8, 32, 12), \"float32\"), adaptive_pool_avg: T.Buffer((2, 8, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(128):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            for ax3 in range(8):\n                cse_var_1: T.int32 = (ax3 * 3 + 3) // 2\n                adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n                adaptive_pool_sum_1[0] = T.float32(0)\n                for rv0, rv1 in T.grid(4, T.Select((ax3 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax3 * 12 // 8):\n                    data_1 = T.Buffer((6144,), data=data.data)\n                    adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0_ax1_fused_ax2_fused * 48 + rv0 * 12 + ax3 * 12 // 8 + rv1]\n                adaptive_pool_avg_1 = T.Buffer((1024,), data=adaptive_pool_avg.data)\n                adaptive_pool_avg_1[ax0_ax1_fused_ax2_fused * 8 + ax3] = adaptive_pool_sum_1[0] / (T.float32(4) * T.Cast(\"float32\", T.Select((ax3 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax3 * 12 // 8))",
        "data": "2_8_32_12"
    },
    {
        "op_name": "adaptive_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 32; ++ax0_ax1_fused) {\n    float adaptive_pool_sum[1];\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        int32_t cse_var_2 = (((ax3 * 3) + 3) >> 1);\n        int32_t cse_var_1 = (((ax2 * 7) + 7) >> 1);\n        adaptive_pool_sum[0] = 0.000000e+00f;\n        for (int32_t rv0 = 0; rv0 < ((((((ax2 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - ((ax2 * 28) >> 3)); ++rv0) {\n          for (int32_t rv1 = 0; rv1 < ((((((ax3 * 4) + 4) % 8) == 0) ? cse_var_2 : (cse_var_2 + 1)) - ((ax3 * 12) >> 3)); ++rv1) {\n            adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(((((ax0_ax1_fused * 336) + (((ax2 * 28) >> 3) * 12)) + (rv0 * 12)) + ((ax3 * 12) >> 3)) + rv1)]);\n          },\n        },\n        ((float*)adaptive_pool_avg_1)[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)] = (adaptive_pool_sum[0] / (((float)((((((ax2 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - ((ax2 * 28) >> 3))) * ((float)((((((ax3 * 4) + 4) % 8) == 0) ? cse_var_2 : (cse_var_2 + 1)) - ((ax3 * 12) >> 3)))));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / (((float)((((((((((int)threadIdx.x) >> 3) * 4) + 4) % 8) == 0) ? (((((int)blockIdx.x) & 1) * 14) + ((((((int)threadIdx.x) >> 3) * 7) + 7) >> 1)) : ((((((int)blockIdx.x) & 1) * 14) + ((((((int)threadIdx.x) >> 3) * 7) + 7) >> 1)) + 1)) - (((((int)threadIdx.x) >> 3) * 28) >> 3)) - ((((int)blockIdx.x) & 1) * 14))) * ((float)(((((((((int)threadIdx.x) & 7) * 4) + 4) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 3) + 3) >> 1) : (((((((int)threadIdx.x) & 7) * 3) + 3) >> 1) + 1)) - (((((int)threadIdx.x) & 7) * 12) >> 3)))));\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < ((((((((((int)threadIdx.x) >> 3) * 4) + 4) % 8) == 0) ? (((((int)blockIdx.x) & 1) * 14) + ((((((int)threadIdx.x) >> 3) * 7) + 7) >> 1)) : ((((((int)blockIdx.x) & 1) * 14) + ((((((int)threadIdx.x) >> 3) * 7) + 7) >> 1)) + 1)) - (((((int)threadIdx.x) >> 3) * 28) >> 3)) - ((((int)blockIdx.x) & 1) * 14)); ++rv0) {\n    for (int rv1 = 0; rv1 < (((((((((int)threadIdx.x) & 7) * 4) + 4) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 3) + 3) >> 1) : (((((((int)threadIdx.x) & 7) * 3) + 3) >> 1) + 1)) - (((((int)threadIdx.x) & 7) * 12) >> 3)); ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + data[(((((((int)blockIdx.x) * 168) + ((((((int)threadIdx.x) >> 3) * 28) >> 3) * 12)) + (rv0 * 12)) + (((((int)threadIdx.x) & 7) * 12) >> 3)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 16, 28, 12), \"float32\"), adaptive_pool_avg: T.Buffer((2, 16, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(32):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            for ax2, ax3 in T.grid(8, 8):\n                cse_var_2: T.int32 = (ax3 * 3 + 3) // 2\n                cse_var_1: T.int32 = (ax2 * 7 + 7) // 2\n                adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n                adaptive_pool_sum_1[0] = T.float32(0)\n                for rv0, rv1 in T.grid(T.Select((ax2 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax2 * 28 // 8, T.Select((ax3 * 4 + 4) % 8 == 0, cse_var_2, cse_var_2 + 1) - ax3 * 12 // 8):\n                    data_1 = T.Buffer((10752,), data=data.data)\n                    adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0_ax1_fused * 336 + ax2 * 28 // 8 * 12 + rv0 * 12 + ax3 * 12 // 8 + rv1]\n                adaptive_pool_avg_1 = T.Buffer((2048,), data=adaptive_pool_avg.data)\n                adaptive_pool_avg_1[ax0_ax1_fused * 64 + ax2 * 8 + ax3] = adaptive_pool_sum_1[0] / (T.Cast(\"float32\", T.Select((ax2 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax2 * 28 // 8) * T.Cast(\"float32\", T.Select((ax3 * 4 + 4) % 8 == 0, cse_var_2, cse_var_2 + 1) - ax3 * 12 // 8))",
        "data": "2_16_28_12"
    },
    {
        "op_name": "adaptive_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  float adaptive_pool_sum[1];\n  for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        adaptive_pool_sum[0] = 0.000000e+00f;\n        for (int32_t rv0 = 0; rv0 < 4; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 4; ++rv1) {\n            adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(((((ax1 * 1024) + (ax2 * 128)) + (rv0 * 32)) + (ax3 * 4)) + rv1)]);\n          },\n        },\n        ((float*)adaptive_pool_avg_1)[(((ax1 * 64) + (ax2 * 8)) + ax3)] = (adaptive_pool_sum[0] * 6.250000e-02f);\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * 6.250000e-02f);\n},\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 4; ++rv0) {\n    for (int rv1 = 0; rv1 < 4; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + data[(((((((int)blockIdx.x) * 256) + ((((int)threadIdx.x) >> 3) * 128)) + (rv0 * 32)) + ((((int)threadIdx.x) & 7) * 4)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 12, 32, 32), \"float32\"), adaptive_pool_avg: T.Buffer((1, 12, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n        for ax1, ax2, ax3 in T.grid(12, 8, 8):\n            adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n            adaptive_pool_sum_1[0] = T.float32(0)\n            for rv0, rv1 in T.grid(4, 4):\n                data_1 = T.Buffer((12288,), data=data.data)\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax1 * 1024 + ax2 * 128 + rv0 * 32 + ax3 * 4 + rv1]\n            adaptive_pool_avg_1 = T.Buffer((768,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax1 * 64 + ax2 * 8 + ax3] = adaptive_pool_sum_1[0] * T.float32(0.0625)",
        "data": "1_12_32_32"
    },
    {
        "op_name": "adaptive_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1728; ++ax0_ax1_fused_ax2_fused) {\n    int32_t8 cse_var_1 = (int32_t8((36)+(36*0), (36)+(36*1), (36)+(36*2), (36)+(36*3), (36)+(36*4), (36)+(36*5), (36)+(36*6), (36)+(36*7)) / ((int32_t8)(8, 8, 8, 8, 8, 8, 8, 8)));\n    float adaptive_pool_sum[8];\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      adaptive_pool_sum[ax3] = 0.000000e+00f;\n      for (int32_t rv0 = 0; rv0 < 2; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < ((((((ax3 * 4) + 4) % 8) == 0) ? (((ax3 * 9) + 9) >> 1) : ((((ax3 * 9) + 9) >> 1) + 1)) - ((ax3 * 36) >> 3)); ++rv1) {\n          adaptive_pool_sum[ax3] = (adaptive_pool_sum[ax3] + ((float*)data_1)[((((ax0_ax1_fused_ax2_fused * 72) + (rv0 * 36)) + ((ax3 * 36) >> 3)) + rv1)]);\n        },\n      },\n    },\n    *(float8*)(((float*)adaptive_pool_avg_1) + (ax0_ax1_fused_ax2_fused * 8)) = (*(float8*)(adaptive_pool_sum + 0) / (((float8)(2.000000e+00f, 2.000000e+00f, 2.000000e+00f, 2.000000e+00f, 2.000000e+00f, 2.000000e+00f, 2.000000e+00f, 2.000000e+00f)) * ((float8)((((int32_t8((4)+(4*0), (4)+(4*1), (4)+(4*2), (4)+(4*3), (4)+(4*4), (4)+(4*5), (4)+(4*6), (4)+(4*7)) % ((int32_t8)(8, 8, 8, 8, 8, 8, 8, 8))) == ((int32_t8)(0, 0, 0, 0, 0, 0, 0, 0))) ? cse_var_1 : (cse_var_1 + ((int32_t8)(1, 1, 1, 1, 1, 1, 1, 1)))) - (int32_t8((0)+(36*0), (0)+(36*1), (0)+(36*2), (0)+(36*3), (0)+(36*4), (0)+(36*5), (0)+(36*6), (0)+(36*7)) / ((int32_t8)(8, 8, 8, 8, 8, 8, 8, 8)))))));\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / (2.000000e+00f * ((float)(((((((((int)threadIdx.x) & 7) * 4) + 4) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 9) + 9) >> 1) : (((((((int)threadIdx.x) & 7) * 9) + 9) >> 1) + 1)) - (((((int)threadIdx.x) & 7) * 36) >> 3)))));\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 2; ++rv0) {\n    for (int rv1 = 0; rv1 < (((((((((int)threadIdx.x) & 7) * 4) + 4) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 9) + 9) >> 1) : (((((((int)threadIdx.x) & 7) * 9) + 9) >> 1) + 1)) - (((((int)threadIdx.x) & 7) * 36) >> 3)); ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + data[(((((((int)blockIdx.x) * 288) + ((((int)threadIdx.x) >> 3) * 72)) + (rv0 * 36)) + (((((int)threadIdx.x) & 7) * 36) >> 3)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 36, 16, 36), \"float32\"), adaptive_pool_avg: T.Buffer((6, 36, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1728):\n            cse_var_1: T.int32x8 = T.Ramp(36, 36, 8) // T.Broadcast(8, 8)\n            adaptive_pool_sum = T.allocate([8], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((8,), data=adaptive_pool_sum, align=32)\n            for ax3 in range(8):\n                adaptive_pool_sum_1[ax3] = T.float32(0)\n                for rv0, rv1 in T.grid(2, T.Let(T.Select((ax3 * 4 + 4) % 8 == 0, cse_var_2, cse_var_2 + 1) - ax3 * 36 // 8, where={cse_var_2: (ax3 * 9 + 9) // 2},)):\n                    cse_var_2 = T.int32()\n                    data_1 = T.Buffer((124416,), data=data.data)\n                    adaptive_pool_sum_1[ax3] = adaptive_pool_sum_1[ax3] + data_1[ax0_ax1_fused_ax2_fused * 72 + rv0 * 36 + ax3 * 36 // 8 + rv1]\n            adaptive_pool_avg_1 = T.Buffer((13824,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax0_ax1_fused_ax2_fused * 8:ax0_ax1_fused_ax2_fused * 8 + 8] = adaptive_pool_sum_1[0:8] / (T.Broadcast(T.float32(2), 8) * T.Cast(\"float32x8\", T.Select(T.Ramp(4, 4, 8) % T.Broadcast(8, 8) == T.Broadcast(0, 8), cse_var_1, cse_var_1 + T.Broadcast(1, 8)) - T.Ramp(0, 36, 8) // T.Broadcast(8, 8)))",
        "data": "6_36_16_36"
    },
    {
        "op_name": "adaptive_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 200; ++ax0_ax1_fused) {\n    float adaptive_pool_sum[64];\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        adaptive_pool_sum[((ax2 * 8) + ax3)] = 0.000000e+00f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 4; ++rv1) {\n            int32_t cse_var_1 = ((ax2 * 8) + ax3);\n            adaptive_pool_sum[cse_var_1] = (adaptive_pool_sum[cse_var_1] + ((float*)data_1)[(((((ax0_ax1_fused * 768) + (ax2 * 96)) + (rv0 * 32)) + (ax3 * 4)) + rv1)]);\n          },\n        },\n      },\n    },\n    for (int32_t ax2_1 = 0; ax2_1 < 8; ++ax2_1) {\n      int32_t cse_var_2 = (ax2_1 * 8);\n      int32_t8 v_ = int32_t8((cse_var_2)+(1*0), (cse_var_2)+(1*1), (cse_var_2)+(1*2), (cse_var_2)+(1*3), (cse_var_2)+(1*4), (cse_var_2)+(1*5), (cse_var_2)+(1*6), (cse_var_2)+(1*7));\n      *(float8*)(((float*)adaptive_pool_avg_1) + ((ax0_ax1_fused * 64) + cse_var_2)) = ((float8(adaptive_pool_sum[v_.s0],adaptive_pool_sum[v_.s1],adaptive_pool_sum[v_.s2],adaptive_pool_sum[v_.s3],adaptive_pool_sum[v_.s4],adaptive_pool_sum[v_.s5],adaptive_pool_sum[v_.s6],adaptive_pool_sum[v_.s7])) * ((float8)(8.333333e-02f, 8.333333e-02f, 8.333333e-02f, 8.333333e-02f, 8.333333e-02f, 8.333333e-02f, 8.333333e-02f, 8.333333e-02f)));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] * 8.333333e-02f);\n},\n\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 4; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] + data[(((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 2)) >> 1) * 96) + (rv0 * 32)) + ((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) & 7) * 4)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 20, 24, 32), \"float32\"), adaptive_pool_avg: T.Buffer((10, 20, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(200):\n            adaptive_pool_sum = T.allocate([64], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((64,), data=adaptive_pool_sum)\n            for ax2, ax3 in T.grid(8, 8):\n                adaptive_pool_sum_1[ax2 * 8 + ax3] = T.float32(0)\n                for rv0, rv1 in T.grid(3, 4):\n                    cse_var_1: T.int32 = ax2 * 8 + ax3\n                    data_1 = T.Buffer((153600,), data=data.data)\n                    adaptive_pool_sum_1[cse_var_1] = adaptive_pool_sum_1[cse_var_1] + data_1[ax0_ax1_fused * 768 + ax2 * 96 + rv0 * 32 + ax3 * 4 + rv1]\n            for ax2 in range(8):\n                cse_var_2: T.int32 = ax2 * 8\n                adaptive_pool_avg_1 = T.Buffer((12800,), data=adaptive_pool_avg.data)\n                adaptive_pool_avg_1[ax0_ax1_fused * 64 + cse_var_2:ax0_ax1_fused * 64 + cse_var_2 + 8] = adaptive_pool_sum_1[cse_var_2:cse_var_2 + 8] * T.Broadcast(T.float32(0.083333333333333329), 8)",
        "data": "10_20_24_32"
    },
    {
        "op_name": "adaptive_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 640; ++ax0_ax1_fused_ax2_fused) {\n    float adaptive_pool_sum[1];\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      int32_t cse_var_1 = (((ax3 * 5) + 5) >> 1);\n      adaptive_pool_sum[0] = 0.000000e+00f;\n      for (int32_t rv0 = 0; rv0 < 4; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < ((((((ax3 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - ((ax3 * 20) >> 3)); ++rv1) {\n          adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[((((ax0_ax1_fused_ax2_fused * 80) + (rv0 * 20)) + ((ax3 * 20) >> 3)) + rv1)]);\n        },\n      },\n      ((float*)adaptive_pool_avg_1)[((ax0_ax1_fused_ax2_fused * 8) + ax3)] = (adaptive_pool_sum[0] / (4.000000e+00f * ((float)((((((ax3 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - ((ax3 * 20) >> 3)))));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / (4.000000e+00f * ((float)(((((((((int)threadIdx.x) & 7) * 4) + 4) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 5) + 5) >> 1) : (((((((int)threadIdx.x) & 7) * 5) + 5) >> 1) + 1)) - (((((int)threadIdx.x) & 7) * 20) >> 3)))));\n},\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 4; ++rv0) {\n    for (int rv1 = 0; rv1 < ((((((((int)threadIdx.x) * 4) + 4) % 8) == 0) ? (((((int)threadIdx.x) * 5) + 5) >> 1) : ((((((int)threadIdx.x) * 5) + 5) >> 1) + 1)) - ((((int)threadIdx.x) * 20) >> 3)); ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + data[((((((int)blockIdx.x) * 80) + (rv0 * 20)) + ((((int)threadIdx.x) * 20) >> 3)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 20, 32, 20), \"float32\"), adaptive_pool_avg: T.Buffer((4, 20, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(640):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            for ax3 in range(8):\n                cse_var_1: T.int32 = (ax3 * 5 + 5) // 2\n                adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n                adaptive_pool_sum_1[0] = T.float32(0)\n                for rv0, rv1 in T.grid(4, T.Select((ax3 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax3 * 20 // 8):\n                    data_1 = T.Buffer((51200,), data=data.data)\n                    adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0_ax1_fused_ax2_fused * 80 + rv0 * 20 + ax3 * 20 // 8 + rv1]\n                adaptive_pool_avg_1 = T.Buffer((5120,), data=adaptive_pool_avg.data)\n                adaptive_pool_avg_1[ax0_ax1_fused_ax2_fused * 8 + ax3] = adaptive_pool_sum_1[0] / (T.float32(4) * T.Cast(\"float32\", T.Select((ax3 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax3 * 20 // 8))",
        "data": "4_20_32_20"
    },
    {
        "op_name": "adaptive_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 84; ++ax0_ax1_fused) {\n    float adaptive_pool_sum[1];\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        int32_t cse_var_1 = ((ax2 + 1) >> 1);\n        adaptive_pool_sum[0] = 0.000000e+00f;\n        for (int32_t rv0 = 0; rv0 < ((((((ax2 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - (ax2 >> 1)); ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 2; ++rv1) {\n            adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(((((ax0_ax1_fused * 64) + ((ax2 >> 1) * 16)) + (rv0 * 16)) + (ax3 * 2)) + rv1)]);\n          },\n        },\n        ((float*)adaptive_pool_avg_1)[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)] = (adaptive_pool_sum[0] / (((float)((((((ax2 * 4) + 4) % 8) == 0) ? cse_var_1 : (cse_var_1 + 1)) - (ax2 >> 1))) * 2.000000e+00f));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / (((float)(((((((((int)threadIdx.x) >> 3) * 4) + 4) % 8) == 0) ? ((((int)threadIdx.x) + 8) >> 4) : (((((int)threadIdx.x) + 8) >> 4) + 1)) - (((int)threadIdx.x) >> 4))) * 2.000000e+00f));\n},\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < (((((((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 3)) & 7) * 4) + 4) % 8) == 0) ? (((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 3)) & 7) + 1) >> 1) : ((((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 3)) & 7) + 1) >> 1) + 1)) - (((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 4)) & 3)); ++rv0) {\n    for (int rv1 = 0; rv1 < 2; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] + data[(((((((int)blockIdx.x) * 48) + ((((int)threadIdx.x) >> 4) * 16)) + (rv0 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 28, 4, 16), \"float32\"), adaptive_pool_avg: T.Buffer((3, 28, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(84):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            for ax2, ax3 in T.grid(8, 8):\n                cse_var_1: T.int32 = (ax2 + 1) // 2\n                adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n                adaptive_pool_sum_1[0] = T.float32(0)\n                for rv0, rv1 in T.grid(T.Select((ax2 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax2 // 2, 2):\n                    data_1 = T.Buffer((5376,), data=data.data)\n                    adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0_ax1_fused * 64 + ax2 // 2 * 16 + rv0 * 16 + ax3 * 2 + rv1]\n                adaptive_pool_avg_1 = T.Buffer((5376,), data=adaptive_pool_avg.data)\n                adaptive_pool_avg_1[ax0_ax1_fused * 64 + ax2 * 8 + ax3] = adaptive_pool_sum_1[0] / (T.Cast(\"float32\", T.Select((ax2 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax2 // 2) * T.float32(2))",
        "data": "3_28_4_16"
    },
    {
        "op_name": "adaptive_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n          int32_t cse_var_1 = ((((ax0 * 1024) + (ax1 * 64)) + (ax2 * 8)) + ax3);\n          ((float*)adaptive_pool_max_1)[cse_var_1] = -3.402823e+38f;\n          float v_ = ((float*)adaptive_pool_max_1)[cse_var_1];\n          float v__1 = ((float*)data_1)[cse_var_1];\n          ((float*)adaptive_pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 16, 8, 8), \"float32\"), adaptive_pool_max: T.Buffer((8, 16, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(8):\n            for ax1, ax2, ax3 in T.grid(16, 8, 8):\n                cse_var_1: T.int32 = ax0 * 1024 + ax1 * 64 + ax2 * 8 + ax3\n                adaptive_pool_max_1 = T.Buffer((8192,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[cse_var_1] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((8192,), data=data.data)\n                adaptive_pool_max_1[cse_var_1] = T.max(adaptive_pool_max_1[cse_var_1], data_1[cse_var_1])",
        "data": "8_16_8_8"
    },
    {
        "op_name": "adaptive_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 80; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        ((float*)adaptive_pool_max_1)[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < ((((((ax2 * 4) + 4) % 8) == 0) ? (((ax2 * 3) + 3) >> 1) : ((((ax2 * 3) + 3) >> 1) + 1)) - ((ax2 * 12) >> 3)); ++rv0) {\n          for (int32_t rv1 = 0; rv1 < ((((((ax3 * 4) + 4) % 8) == 0) ? ((ax3 + 1) >> 1) : (((ax3 + 1) >> 1) + 1)) - (ax3 >> 1)); ++rv1) {\n            int32_t cse_var_3 = (((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3);\n            float v_ = ((float*)adaptive_pool_max_1)[cse_var_3];\n            float v__1 = ((float*)data_1)[(((((ax0_ax1_fused * 48) + (((ax2 * 12) >> 3) * 4)) + (rv0 * 4)) + (ax3 >> 1)) + rv1)];\n            ((float*)adaptive_pool_max_1)[cse_var_3] = ((v_) > (v__1) ? (v_) : (v__1));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < ((((((((((int)threadIdx.x) >> 3) * 4) + 4) % 8) == 0) ? (((((int)blockIdx.x) & 1) * 6) + ((((((int)threadIdx.x) >> 3) * 3) + 3) >> 1)) : ((((((int)blockIdx.x) & 1) * 6) + ((((((int)threadIdx.x) >> 3) * 3) + 3) >> 1)) + 1)) - (((((int)threadIdx.x) >> 3) * 12) >> 3)) - ((((int)blockIdx.x) & 1) * 6)); ++rv0) {\n    for (int rv1 = 0; rv1 < (((((((((int)threadIdx.x) & 7) * 4) + 4) % 8) == 0) ? (((((int)threadIdx.x) & 7) + 1) >> 1) : ((((((int)threadIdx.x) & 7) + 1) >> 1) + 1)) - ((((int)threadIdx.x) & 7) >> 1)); ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((((int)blockIdx.x) * 24) + ((((((int)threadIdx.x) >> 3) * 12) >> 3) * 4)) + (rv0 * 4)) + ((((int)threadIdx.x) & 7) >> 1)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 20, 12, 4), \"float32\"), adaptive_pool_max: T.Buffer((4, 20, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(80):\n            for ax2, ax3 in T.grid(8, 8):\n                adaptive_pool_max_1 = T.Buffer((5120,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0_ax1_fused * 64 + ax2 * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(T.Let(T.Select((ax2 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax2 * 12 // 8, where={cse_var_1: (ax2 * 3 + 3) // 2},), T.Let(T.Select((ax3 * 4 + 4) % 8 == 0, cse_var_2, cse_var_2 + 1) - ax3 // 2, where={cse_var_2: (ax3 + 1) // 2},)):\n                    cse_var_1 = T.int32()\n                    cse_var_2 = T.int32()\n                    cse_var_3: T.int32 = ax0_ax1_fused * 64 + ax2 * 8 + ax3\n                    data_1 = T.Buffer((3840,), data=data.data)\n                    adaptive_pool_max_1[cse_var_3] = T.max(adaptive_pool_max_1[cse_var_3], data_1[ax0_ax1_fused * 48 + ax2 * 12 // 8 * 4 + rv0 * 4 + ax3 // 2 + rv1])",
        "data": "4_20_12_4"
    },
    {
        "op_name": "adaptive_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 80; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        ((float*)adaptive_pool_max_1)[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 2; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < ((((((ax3 * 4) + 4) % 8) == 0) ? ((ax3 + 1) >> 1) : (((ax3 + 1) >> 1) + 1)) - (ax3 >> 1)); ++rv1) {\n            int32_t cse_var_3 = ((ax0_ax1_fused * 64) + (ax2 * 8));\n            int32_t cse_var_2 = (cse_var_3 + ax3);\n            float v_ = ((float*)adaptive_pool_max_1)[cse_var_2];\n            float v__1 = ((float*)data_1)[(((cse_var_3 + (rv0 * 4)) + (ax3 >> 1)) + rv1)];\n            ((float*)adaptive_pool_max_1)[cse_var_2] = ((v_) > (v__1) ? (v_) : (v__1));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 2; ++rv0) {\n    for (int rv1 = 0; rv1 < (((((((((int)threadIdx.x) & 7) * 4) + 4) % 8) == 0) ? (((((int)threadIdx.x) & 7) + 1) >> 1) : ((((((int)threadIdx.x) & 7) + 1) >> 1) + 1)) - ((((int)threadIdx.x) & 7) >> 1)); ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[(((((((int)blockIdx.x) * 64) + ((((int)threadIdx.x) >> 3) * 8)) + (rv0 * 4)) + ((((int)threadIdx.x) & 7) >> 1)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 8, 16, 4), \"float32\"), adaptive_pool_max: T.Buffer((10, 8, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(80):\n            for ax2, ax3 in T.grid(8, 8):\n                adaptive_pool_max_1 = T.Buffer((5120,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0_ax1_fused * 64 + ax2 * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(2, T.Let(T.Select((ax3 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax3 // 2, where={cse_var_1: (ax3 + 1) // 2},)):\n                    cse_var_1 = T.int32()\n                    cse_var_3: T.int32 = ax0_ax1_fused * 64 + ax2 * 8\n                    cse_var_2: T.int32 = cse_var_3 + ax3\n                    data_1 = T.Buffer((5120,), data=data.data)\n                    adaptive_pool_max_1[cse_var_2] = T.max(adaptive_pool_max_1[cse_var_2], data_1[cse_var_3 + rv0 * 4 + ax3 // 2 + rv1])",
        "data": "10_8_16_4"
    },
    {
        "op_name": "adaptive_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 256; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      ((float*)adaptive_pool_max_1)[((ax0_ax1_fused_ax2_fused * 8) + ax3)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < (((((((ax0_ax1_fused_ax2_fused & 7) * 4) + 4) % 8) == 0) ? ((((ax0_ax1_fused_ax2_fused & 7) * 3) + 3) >> 1) : (((((ax0_ax1_fused_ax2_fused & 7) * 3) + 3) >> 1) + 1)) - (((ax0_ax1_fused_ax2_fused & 7) * 12) >> 3)); ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 4; ++rv1) {\n          int32_t cse_var_3 = ((ax0_ax1_fused_ax2_fused * 8) + ax3);\n          float v_ = ((float*)adaptive_pool_max_1)[cse_var_3];\n          float v__1 = ((float*)data_1)[((((((ax0_ax1_fused_ax2_fused >> 3) * 384) + ((((ax0_ax1_fused_ax2_fused & 7) * 12) >> 3) * 32)) + (rv0 * 32)) + (ax3 * 4)) + rv1)];\n          ((float*)adaptive_pool_max_1)[cse_var_3] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < ((((((((((int)threadIdx.x) >> 3) * 4) + 4) % 8) == 0) ? (((((int)blockIdx.x) & 1) * 6) + ((((((int)threadIdx.x) >> 3) * 3) + 3) >> 1)) : ((((((int)blockIdx.x) & 1) * 6) + ((((((int)threadIdx.x) >> 3) * 3) + 3) >> 1)) + 1)) - (((((int)threadIdx.x) >> 3) * 12) >> 3)) - ((((int)blockIdx.x) & 1) * 6)); ++rv0) {\n    for (int rv1 = 0; rv1 < 4; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((((int)blockIdx.x) * 192) + ((((((int)threadIdx.x) >> 3) * 12) >> 3) * 32)) + (rv0 * 32)) + ((((int)threadIdx.x) & 7) * 4)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 16, 12, 32), \"float32\"), adaptive_pool_max: T.Buffer((2, 16, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(256):\n            for ax3 in range(8):\n                adaptive_pool_max_1 = T.Buffer((2048,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(T.Let(T.Let(T.Select((cse_var_2 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - cse_var_2 * 12 // 8, where={cse_var_1: (cse_var_2 * 3 + 3) // 2},), where={cse_var_2: ax0_ax1_fused_ax2_fused % 8},), 4):\n                    cse_var_2 = T.int32()\n                    cse_var_1 = T.int32()\n                    cse_var_3: T.int32 = ax0_ax1_fused_ax2_fused * 8 + ax3\n                    data_1 = T.Buffer((12288,), data=data.data)\n                    adaptive_pool_max_1[cse_var_3] = T.max(adaptive_pool_max_1[cse_var_3], data_1[ax0_ax1_fused_ax2_fused // 8 * 384 + ax0_ax1_fused_ax2_fused % 8 * 12 // 8 * 32 + rv0 * 32 + ax3 * 4 + rv1])",
        "data": "2_16_12_32"
    },
    {
        "op_name": "adaptive_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 896; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      ((float*)adaptive_pool_max_1)[((ax0_ax1_fused_ax2_fused * 8) + ax3)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < (((((((ax0_ax1_fused_ax2_fused & 7) * 4) + 4) % 8) == 0) ? ((((ax0_ax1_fused_ax2_fused & 7) * 7) + 7) >> 1) : (((((ax0_ax1_fused_ax2_fused & 7) * 7) + 7) >> 1) + 1)) - (((ax0_ax1_fused_ax2_fused & 7) * 28) >> 3)); ++rv0) {\n        for (int32_t rv1 = 0; rv1 < ((((((ax3 * 4) + 4) % 8) == 0) ? (((ax3 * 7) + 7) >> 1) : ((((ax3 * 7) + 7) >> 1) + 1)) - ((ax3 * 28) >> 3)); ++rv1) {\n          int32_t cse_var_4 = ((ax0_ax1_fused_ax2_fused * 8) + ax3);\n          float v_ = ((float*)adaptive_pool_max_1)[cse_var_4];\n          float v__1 = ((float*)data_1)[((((((ax0_ax1_fused_ax2_fused >> 3) * 784) + ((((ax0_ax1_fused_ax2_fused & 7) * 28) >> 3) * 28)) + (rv0 * 28)) + ((ax3 * 28) >> 3)) + rv1)];\n          ((float*)adaptive_pool_max_1)[cse_var_4] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < (((((((((int)threadIdx.x) >> 3) * 4) + 4) % 8) == 0) ? ((((((int)threadIdx.x) >> 3) * 7) + 7) >> 1) : (((((((int)threadIdx.x) >> 3) * 7) + 7) >> 1) + 1)) - (((((int)threadIdx.x) >> 3) * 28) >> 3)); ++rv0) {\n    for (int rv1 = 0; rv1 < (((((((((int)threadIdx.x) & 7) * 4) + 4) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 7) + 7) >> 1) : (((((((int)threadIdx.x) & 7) * 7) + 7) >> 1) + 1)) - (((((int)threadIdx.x) & 7) * 28) >> 3)); ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[(((((((int)blockIdx.x) * 784) + ((((((int)threadIdx.x) >> 3) * 28) >> 3) * 28)) + (rv0 * 28)) + (((((int)threadIdx.x) & 7) * 28) >> 3)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 28, 28, 28), \"float32\"), adaptive_pool_max: T.Buffer((4, 28, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(896):\n            for ax3 in range(8):\n                adaptive_pool_max_1 = T.Buffer((7168,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(T.Let(T.Let(T.Select((cse_var_2 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - cse_var_2 * 28 // 8, where={cse_var_1: (cse_var_2 * 7 + 7) // 2},), where={cse_var_2: ax0_ax1_fused_ax2_fused % 8},), T.Let(T.Select((ax3 * 4 + 4) % 8 == 0, cse_var_3, cse_var_3 + 1) - ax3 * 28 // 8, where={cse_var_3: (ax3 * 7 + 7) // 2},)):\n                    cse_var_2 = T.int32()\n                    cse_var_1 = T.int32()\n                    cse_var_3 = T.int32()\n                    cse_var_4: T.int32 = ax0_ax1_fused_ax2_fused * 8 + ax3\n                    data_1 = T.Buffer((87808,), data=data.data)\n                    adaptive_pool_max_1[cse_var_4] = T.max(adaptive_pool_max_1[cse_var_4], data_1[ax0_ax1_fused_ax2_fused // 8 * 784 + ax0_ax1_fused_ax2_fused % 8 * 28 // 8 * 28 + rv0 * 28 + ax3 * 28 // 8 + rv1])",
        "data": "4_28_28_28"
    },
    {
        "op_name": "adaptive_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 512; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      ((float*)adaptive_pool_max_1)[((ax0_ax1_fused_ax2_fused * 8) + ax3)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < ((((((ax3 * 4) + 4) % 8) == 0) ? ((ax3 + 1) >> 1) : (((ax3 + 1) >> 1) + 1)) - (ax3 >> 1)); ++rv1) {\n          int32_t cse_var_2 = ((ax0_ax1_fused_ax2_fused * 8) + ax3);\n          float v_ = ((float*)adaptive_pool_max_1)[cse_var_2];\n          float v__1 = ((float*)data_1)[((((ax0_ax1_fused_ax2_fused * 12) + (rv0 * 4)) + (ax3 >> 1)) + rv1)];\n          ((float*)adaptive_pool_max_1)[cse_var_2] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < (((((((((int)threadIdx.x) & 7) * 4) + 4) % 8) == 0) ? (((((int)threadIdx.x) & 7) + 1) >> 1) : ((((((int)threadIdx.x) & 7) + 1) >> 1) + 1)) - ((((int)threadIdx.x) & 7) >> 1)); ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], data[(((((((int)blockIdx.x) * 24) + ((((int)threadIdx.x) >> 3) * 12)) + (rv0 * 4)) + ((((int)threadIdx.x) & 7) >> 1)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 16, 24, 4), \"float32\"), adaptive_pool_max: T.Buffer((4, 16, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(512):\n            for ax3 in range(8):\n                adaptive_pool_max_1 = T.Buffer((4096,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, T.Let(T.Select((ax3 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax3 // 2, where={cse_var_1: (ax3 + 1) // 2},)):\n                    cse_var_1 = T.int32()\n                    cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused * 8 + ax3\n                    data_1 = T.Buffer((6144,), data=data.data)\n                    adaptive_pool_max_1[cse_var_2] = T.max(adaptive_pool_max_1[cse_var_2], data_1[ax0_ax1_fused_ax2_fused * 12 + rv0 * 4 + ax3 // 2 + rv1])",
        "data": "4_16_24_4"
    },
    {
        "op_name": "adaptive_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 28; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        ((float*)adaptive_pool_max_1)[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < ((((((ax2 * 4) + 4) % 8) == 0) ? (((ax2 * 9) + 9) >> 1) : ((((ax2 * 9) + 9) >> 1) + 1)) - ((ax2 * 36) >> 3)); ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 4; ++rv1) {\n            int32_t cse_var_2 = (((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3);\n            float v_ = ((float*)adaptive_pool_max_1)[cse_var_2];\n            float v__1 = ((float*)data_1)[(((((ax0_ax1_fused * 1152) + (((ax2 * 36) >> 3) * 32)) + (rv0 * 32)) + (ax3 * 4)) + rv1)];\n            ((float*)adaptive_pool_max_1)[cse_var_2] = ((v_) > (v__1) ? (v_) : (v__1));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < ((((((((((int)threadIdx.x) >> 3) * 4) + 4) % 8) == 0) ? (((((int)blockIdx.x) & 1) * 18) + ((((((int)threadIdx.x) >> 3) * 9) + 9) >> 1)) : ((((((int)blockIdx.x) & 1) * 18) + ((((((int)threadIdx.x) >> 3) * 9) + 9) >> 1)) + 1)) - (((((int)threadIdx.x) >> 3) * 36) >> 3)) - ((((int)blockIdx.x) & 1) * 18)); ++rv0) {\n    for (int rv1 = 0; rv1 < 4; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((((int)blockIdx.x) * 576) + ((((((int)threadIdx.x) >> 3) * 36) >> 3) * 32)) + (rv0 * 32)) + ((((int)threadIdx.x) & 7) * 4)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 28, 36, 32), \"float32\"), adaptive_pool_max: T.Buffer((1, 28, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(28):\n            for ax2, ax3 in T.grid(8, 8):\n                adaptive_pool_max_1 = T.Buffer((1792,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0_ax1_fused * 64 + ax2 * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(T.Let(T.Select((ax2 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax2 * 36 // 8, where={cse_var_1: (ax2 * 9 + 9) // 2},), 4):\n                    cse_var_1 = T.int32()\n                    cse_var_2: T.int32 = ax0_ax1_fused * 64 + ax2 * 8 + ax3\n                    data_1 = T.Buffer((32256,), data=data.data)\n                    adaptive_pool_max_1[cse_var_2] = T.max(adaptive_pool_max_1[cse_var_2], data_1[ax0_ax1_fused * 1152 + ax2 * 36 // 8 * 32 + rv0 * 32 + ax3 * 4 + rv1])",
        "data": "1_28_36_32"
    },
    {
        "op_name": "adaptive_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2304; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      ((float*)adaptive_pool_max_1)[((ax0_ax1_fused_ax2_fused * 8) + ax3)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 5; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < ((((((ax3 * 4) + 4) % 8) == 0) ? (((ax3 * 3) + 3) >> 1) : ((((ax3 * 3) + 3) >> 1) + 1)) - ((ax3 * 12) >> 3)); ++rv1) {\n          int32_t cse_var_2 = ((ax0_ax1_fused_ax2_fused * 8) + ax3);\n          float v_ = ((float*)adaptive_pool_max_1)[cse_var_2];\n          float v__1 = ((float*)data_1)[((((ax0_ax1_fused_ax2_fused * 60) + (rv0 * 12)) + ((ax3 * 12) >> 3)) + rv1)];\n          ((float*)adaptive_pool_max_1)[cse_var_2] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 5; ++rv0) {\n    for (int rv1 = 0; rv1 < (((((((((int)threadIdx.x) & 7) * 4) + 4) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 3) + 3) >> 1) : (((((((int)threadIdx.x) & 7) * 3) + 3) >> 1) + 1)) - (((((int)threadIdx.x) & 7) * 12) >> 3)); ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[(((((((int)blockIdx.x) * 480) + ((((int)threadIdx.x) >> 3) * 60)) + (rv0 * 12)) + (((((int)threadIdx.x) & 7) * 12) >> 3)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 36, 40, 12), \"float32\"), adaptive_pool_max: T.Buffer((8, 36, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2304):\n            for ax3 in range(8):\n                adaptive_pool_max_1 = T.Buffer((18432,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(5, T.Let(T.Select((ax3 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax3 * 12 // 8, where={cse_var_1: (ax3 * 3 + 3) // 2},)):\n                    cse_var_1 = T.int32()\n                    cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused * 8 + ax3\n                    data_1 = T.Buffer((138240,), data=data.data)\n                    adaptive_pool_max_1[cse_var_2] = T.max(adaptive_pool_max_1[cse_var_2], data_1[ax0_ax1_fused_ax2_fused * 60 + rv0 * 12 + ax3 * 12 // 8 + rv1])",
        "data": "8_36_40_12"
    },
    {
        "op_name": "adaptive_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3200; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      ((float*)adaptive_pool_max_1)[((ax0_ax1_fused_ax2_fused * 8) + ax3)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < (((((((ax0_ax1_fused_ax2_fused & 7) * 4) + 4) % 8) == 0) ? ((((ax0_ax1_fused_ax2_fused & 7) * 3) + 3) >> 1) : (((((ax0_ax1_fused_ax2_fused & 7) * 3) + 3) >> 1) + 1)) - (((ax0_ax1_fused_ax2_fused & 7) * 12) >> 3)); ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n          int32_t cse_var_3 = ((ax0_ax1_fused_ax2_fused * 8) + ax3);\n          float v_ = ((float*)adaptive_pool_max_1)[cse_var_3];\n          float v__1 = ((float*)data_1)[((((((ax0_ax1_fused_ax2_fused >> 3) * 288) + ((((ax0_ax1_fused_ax2_fused & 7) * 12) >> 3) * 24)) + (rv0 * 24)) + (ax3 * 3)) + rv1)];\n          ((float*)adaptive_pool_max_1)[cse_var_3] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < (((((((((int)threadIdx.x) >> 3) * 4) + 4) % 8) == 0) ? ((((((int)threadIdx.x) >> 3) * 3) + 3) >> 1) : (((((((int)threadIdx.x) >> 3) * 3) + 3) >> 1) + 1)) - (((((int)threadIdx.x) >> 3) * 12) >> 3)); ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[(((((((int)blockIdx.x) * 288) + ((((((int)threadIdx.x) >> 3) * 12) >> 3) * 24)) + (rv0 * 24)) + ((((int)threadIdx.x) & 7) * 3)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 40, 12, 24), \"float32\"), adaptive_pool_max: T.Buffer((10, 40, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3200):\n            for ax3 in range(8):\n                adaptive_pool_max_1 = T.Buffer((25600,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(T.Let(T.Let(T.Select((cse_var_2 * 4 + 4) % 8 == 0, cse_var_1, cse_var_1 + 1) - cse_var_2 * 12 // 8, where={cse_var_1: (cse_var_2 * 3 + 3) // 2},), where={cse_var_2: ax0_ax1_fused_ax2_fused % 8},), 3):\n                    cse_var_2 = T.int32()\n                    cse_var_1 = T.int32()\n                    cse_var_3: T.int32 = ax0_ax1_fused_ax2_fused * 8 + ax3\n                    data_1 = T.Buffer((115200,), data=data.data)\n                    adaptive_pool_max_1[cse_var_3] = T.max(adaptive_pool_max_1[cse_var_3], data_1[ax0_ax1_fused_ax2_fused // 8 * 288 + ax0_ax1_fused_ax2_fused % 8 * 12 // 8 * 24 + rv0 * 24 + ax3 * 3 + rv1])",
        "data": "10_40_12_24"
    },
    {
        "op_name": "fast_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1152; ++i0_i1_fused_i2_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    T_softmax_maxelem[0] = -3.402823e+38f;\n    for (int32_t k = 0; k < 32; ++k) {\n      float v_ = T_softmax_maxelem[0];\n      float v__1 = ((float*)data_1)[((i0_i1_fused_i2_fused * 32) + k)];\n      T_softmax_maxelem[0] = ((v_) > (v__1) ? (v_) : (v__1));\n    },\n    T_softmax_expsum[0] = 0.000000e+00f;\n    for (int32_t k_1 = 0; k_1 < 32; ++k_1) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 32) + k_1);\n        float v__2 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n        int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__5 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n      float v__7 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n      float v__9 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n      float v__11 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n      float v__13 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n      float v__15 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n      float v__17 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n      float v__19 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n      float v__21 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n      float v__23 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n      float v__25 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n      float v__27 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n      float v__29 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n      float v__31 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n      float v__33 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n      float v__35 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n      float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__38 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      T_softmax_expsum[0] = (T_softmax_expsum[0] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n    },\n    for (int32_t i3 = 0; i3 < 32; ++i3) {\n      int32_t cse_var_2 = ((i0_i1_fused_i2_fused * 32) + i3);\n        float v__39 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n        int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__42 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n      float v__44 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n      float v__46 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n      float v__48 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n      float v__50 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n      float v__52 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n      float v__54 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n      float v__56 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n      float v__58 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n      float v__60 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n      float v__62 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n      float v__64 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n      float v__66 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n      float v__68 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n      float v__70 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n      float v__72 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n      float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__75 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      ((float*)T_softmax_norm_1)[cse_var_2] = (((v__74) > (v__75) ? (v__74) : (v__75)) / T_softmax_expsum[0]);\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 32; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))])) / T_softmax_expsum[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]);\n},\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 32; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)]);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 8, 36, 32), \"float32\"), T_softmax_norm: T.Buffer((4, 8, 36, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(1152):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n            T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n            data_1 = T.Buffer((36864,), data=data.data)\n            for k in range(32):\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused_i2_fused * 32 + k])\n            T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n            T_softmax_expsum_1[0] = T.float32(0)\n            for k in range(32):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 32 + k\n                T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0])\n            for i3 in range(32):\n                cse_var_2: T.int32 = i0_i1_fused_i2_fused * 32 + i3\n                T_softmax_norm_1 = T.Buffer((36864,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]",
        "data": "4_8_36_32"
    },
    {
        "op_name": "fast_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 384; ++i0_i1_fused_i2_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    T_softmax_maxelem[0] = -3.402823e+38f;\n    for (int32_t k = 0; k < 16; ++k) {\n      float v_ = T_softmax_maxelem[0];\n      float v__1 = ((float*)data_1)[((i0_i1_fused_i2_fused * 16) + k)];\n      T_softmax_maxelem[0] = ((v_) > (v__1) ? (v_) : (v__1));\n    },\n    T_softmax_expsum[0] = 0.000000e+00f;\n    for (int32_t k_1 = 0; k_1 < 16; ++k_1) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 16) + k_1);\n        float v__2 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n        int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__5 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n      float v__7 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n      float v__9 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n      float v__11 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n      float v__13 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n      float v__15 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n      float v__17 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n      float v__19 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n      float v__21 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n      float v__23 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n      float v__25 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n      float v__27 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n      float v__29 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n      float v__31 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n      float v__33 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n      float v__35 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n      float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__38 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      T_softmax_expsum[0] = (T_softmax_expsum[0] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n    },\n    for (int32_t i3_s = 0; i3_s < 16; ++i3_s) {\n      int32_t cse_var_2 = ((i0_i1_fused_i2_fused * 16) + i3_s);\n        float v__39 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n        int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__42 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n      float v__44 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n      float v__46 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n      float v__48 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n      float v__50 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n      float v__52 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n      float v__54 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n      float v__56 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n      float v__58 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n      float v__60 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n      float v__62 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n      float v__64 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n      float v__66 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n      float v__68 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n      float v__70 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n      float v__72 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n      float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__75 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      ((float*)T_softmax_norm_1)[cse_var_2] = (((v__74) > (v__75) ? (v__74) : (v__75)) / T_softmax_expsum[0]);\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 16; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))])) / T_softmax_expsum[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]);\n},\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 16; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) * 16)) + k)]);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 24, 4, 16), \"float32\"), T_softmax_norm: T.Buffer((4, 24, 4, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(384):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n            T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n            data_1 = T.Buffer((6144,), data=data.data)\n            for k in range(16):\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused_i2_fused * 16 + k])\n            T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n            T_softmax_expsum_1[0] = T.float32(0)\n            for k in range(16):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 16 + k\n                T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0])\n            for i3_s in range(16):\n                cse_var_2: T.int32 = i0_i1_fused_i2_fused * 16 + i3_s\n                T_softmax_norm_1 = T.Buffer((6144,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]",
        "data": "4_24_4_16"
    },
    {
        "op_name": "fast_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  void* T_softmax_maxelem = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)15360, 2, 32);\n  if (T_softmax_maxelem == NULL) {\n    return -1;\n  },\n  void* T_softmax_expsum = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)15360, 2, 32);\n  if (T_softmax_expsum == NULL) {\n    return -1;\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3840; ++i0_i1_fused_i2_fused) {\n    ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused] = -3.402823e+38f;\n    for (int32_t k = 0; k < 20; ++k) {\n      float v_ = ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused];\n      float v__1 = ((float*)data_1)[((i0_i1_fused_i2_fused * 20) + k)];\n      ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n    },\n  },\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3840; ++i0_i1_fused_i2_fused_1) {\n    ((float*)T_softmax_expsum)[i0_i1_fused_i2_fused_1] = 0.000000e+00f;\n    for (int32_t k_1 = 0; k_1 < 20; ++k_1) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused_1 * 20) + k_1);\n        float v__2 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n        float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n        int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__5 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n      float v__7 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n      float v__9 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n      float v__11 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n      float v__13 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n      float v__15 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n      float v__17 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n      float v__19 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n      float v__21 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n      float v__23 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n      float v__25 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n      float v__27 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n      float v__29 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n      float v__31 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n      float v__33 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n      float v__35 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n      float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__38 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      ((float*)T_softmax_expsum)[i0_i1_fused_i2_fused_1] = (((float*)T_softmax_expsum)[i0_i1_fused_i2_fused_1] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n    },\n  },\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 3840; ++i0_i1_fused_i2_fused_2) {\n    for (int32_t i3 = 0; i3 < 20; ++i3) {\n      int32_t cse_var_2 = ((i0_i1_fused_i2_fused_2 * 20) + i3);\n        float v__39 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n        float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n        int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__42 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n      float v__44 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n      float v__46 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n      float v__48 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n      float v__50 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n      float v__52 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n      float v__54 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n      float v__56 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n      float v__58 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n      float v__60 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n      float v__62 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n      float v__64 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n      float v__66 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n      float v__68 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n      float v__70 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n      float v__72 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n      float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__75 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      ((float*)T_softmax_norm_1)[cse_var_2] = (((v__74) > (v__75) ? (v__74) : (v__75)) / ((float*)T_softmax_expsum)[i0_i1_fused_i2_fused_2]);\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_expsum) != 0) {\n    return -1;\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_maxelem) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 20; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 20)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)])) / T_softmax_expsum[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]);\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 20; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 40, 32, 20), \"float32\"), T_softmax_norm: T.Buffer((3, 40, 32, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        T_softmax_maxelem = T.allocate([3840], \"float32\", \"global\")\n        T_softmax_expsum = T.allocate([3840], \"float32\", \"global\")\n        T_softmax_maxelem_1 = T.Buffer((3840,), data=T_softmax_maxelem)\n        data_1 = T.Buffer((76800,), data=data.data)\n        for i0_i1_fused_i2_fused in T.parallel(3840):\n            T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.float32(-3.4028234663852886e+38)\n            for k in range(20):\n                T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.max(T_softmax_maxelem_1[i0_i1_fused_i2_fused], data_1[i0_i1_fused_i2_fused * 20 + k])\n        T_softmax_expsum_1 = T.Buffer((3840,), data=T_softmax_expsum)\n        for i0_i1_fused_i2_fused in T.parallel(3840):\n            T_softmax_expsum_1[i0_i1_fused_i2_fused] = T.float32(0)\n            for k in range(20):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 20 + k\n                T_softmax_expsum_1[i0_i1_fused_i2_fused] = T_softmax_expsum_1[i0_i1_fused_i2_fused] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(3840):\n            for i3 in range(20):\n                cse_var_2: T.int32 = i0_i1_fused_i2_fused * 20 + i3\n                T_softmax_norm_1 = T.Buffer((76800,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused]) / T_softmax_expsum_1[i0_i1_fused_i2_fused]",
        "data": "3_40_32_20"
    },
    {
        "op_name": "fast_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  void* T_softmax_maxelem = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)2304, 2, 32);\n  if (T_softmax_maxelem == NULL) {\n    return -1;\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 576; ++i0_i1_fused_i2_fused) {\n    ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused] = -3.402823e+38f;\n    for (int32_t k = 0; k < 24; ++k) {\n      float v_ = ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused];\n      float v__1 = ((float*)data_1)[((i0_i1_fused_i2_fused * 24) + k)];\n      ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n    },\n  },\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    float T_softmax_expsum[1];\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        T_softmax_expsum[0] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 24; ++k_1) {\n          int32_t cse_var_2 = (((i0 * 64) + (i1 * 8)) + i2);\n          int32_t cse_var_1 = ((((i0 * 1536) + (i1 * 192)) + (i2 * 24)) + k_1);\n            float v__2 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n            float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n            int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          float v__5 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n          float v__7 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n          float v__9 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n          float v__11 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n          float v__13 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n          float v__15 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n          float v__17 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n          float v__19 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n          float v__21 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n          float v__23 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n          float v__25 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n          float v__27 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n          float v__29 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n          float v__31 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n          float v__33 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n          float v__35 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n          float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n          float v__38 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          T_softmax_expsum[0] = (T_softmax_expsum[0] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n        },\n        for (int32_t i3 = 0; i3 < 24; ++i3) {\n          int32_t cse_var_4 = (((i0 * 64) + (i1 * 8)) + i2);\n          int32_t cse_var_3 = ((((i0 * 1536) + (i1 * 192)) + (i2 * 24)) + i3);\n            float v__39 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n            float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n            int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          float v__42 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n          float v__44 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n          float v__46 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n          float v__48 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n          float v__50 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n          float v__52 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n          float v__54 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n          float v__56 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n          float v__58 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n          float v__60 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n          float v__62 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n          float v__64 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n          float v__66 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n          float v__68 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n          float v__70 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n          float v__72 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n          float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n          float v__75 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          ((float*)T_softmax_norm_1)[cse_var_3] = (((v__74) > (v__75) ? (v__74) : (v__75)) / T_softmax_expsum[0]);\n        },\n      },\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_maxelem) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)])) / T_softmax_expsum[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 3)]);\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 24; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 24; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)]);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 8, 8, 24), \"float32\"), T_softmax_norm: T.Buffer((9, 8, 8, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        T_softmax_maxelem = T.allocate([576], \"float32\", \"global\")\n        T_softmax_maxelem_1 = T.Buffer((576,), data=T_softmax_maxelem)\n        data_1 = T.Buffer((13824,), data=data.data)\n        for i0_i1_fused_i2_fused in T.parallel(576):\n            T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.float32(-3.4028234663852886e+38)\n            for k in range(24):\n                T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.max(T_softmax_maxelem_1[i0_i1_fused_i2_fused], data_1[i0_i1_fused_i2_fused * 24 + k])\n        for i0 in T.parallel(9):\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            for i1, i2 in T.grid(8, 8):\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(24):\n                    cse_var_2: T.int32 = i0 * 64 + i1 * 8 + i2\n                    cse_var_1: T.int32 = i0 * 1536 + i1 * 192 + i2 * 24 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2])\n                for i3 in range(24):\n                    cse_var_4: T.int32 = i0 * 64 + i1 * 8 + i2\n                    cse_var_3: T.int32 = i0 * 1536 + i1 * 192 + i2 * 24 + i3\n                    T_softmax_norm_1 = T.Buffer((13824,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_3] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4]) / T_softmax_expsum_1[0]",
        "data": "9_8_8_24"
    },
    {
        "op_name": "fast_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  void* T_softmax_maxelem = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)18432, 2, 32);\n  if (T_softmax_maxelem == NULL) {\n    return -1;\n  },\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 36; ++i1) {\n      for (int32_t i2 = 0; i2 < 32; ++i2) {\n        ((float*)T_softmax_maxelem)[(((i0 * 1152) + (i1 * 32)) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 20; ++k) {\n          int32_t cse_var_1 = (((i0 * 1152) + (i1 * 32)) + i2);\n          float v_ = ((float*)T_softmax_maxelem)[cse_var_1];\n          float v__1 = ((float*)data_1)[((((i0 * 23040) + (i1 * 640)) + (i2 * 20)) + k)];\n          ((float*)T_softmax_maxelem)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 144; ++i0_i1_fused) {\n    float T_softmax_expsum[32];\n    for (int32_t i2_1 = 0; i2_1 < 32; ++i2_1) {\n      T_softmax_expsum[i2_1] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 20; ++k_1) {\n        int32_t cse_var_3 = ((i0_i1_fused * 32) + i2_1);\n        int32_t cse_var_2 = (((i0_i1_fused * 640) + (i2_1 * 20)) + k_1);\n          float v__2 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n          int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__5 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n        float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n        float v__7 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n        float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n        float v__9 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n        float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n        float v__11 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n        float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n        float v__13 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n        float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n        float v__15 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n        float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n        float v__17 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n        float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n        float v__19 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n        float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n        float v__21 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n        float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n        float v__23 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n        float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n        float v__25 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n        float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n        float v__27 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n        float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n        float v__29 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n        float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n        float v__31 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n        float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n        float v__33 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n        float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n        float v__35 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n        float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n        float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__38 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n      },\n    },\n    for (int32_t i2_2 = 0; i2_2 < 32; ++i2_2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        int32_t cse_var_5 = ((i0_i1_fused * 32) + i2_2);\n        int32_t cse_var_4 = (((i0_i1_fused * 640) + (i2_2 * 20)) + i3);\n          float v__39 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n          int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__42 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n        float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n        float v__44 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n        float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n        float v__46 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n        float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n        float v__48 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n        float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n        float v__50 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n        float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n        float v__52 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n        float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n        float v__54 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n        float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n        float v__56 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n        float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n        float v__58 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n        float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n        float v__60 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n        float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n        float v__62 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n        float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n        float v__64 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n        float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n        float v__66 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n        float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n        float v__68 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n        float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n        float v__70 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n        float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n        float v__72 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n        float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n        float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__75 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n        ((float*)T_softmax_norm_1)[cse_var_4] = (((v__74) > (v__75) ? (v__74) : (v__75)) / T_softmax_expsum[i2_2]);\n      },\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_maxelem) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)])) / T_softmax_expsum[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]);\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 20; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 20; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)]);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 36, 32, 20), \"float32\"), T_softmax_norm: T.Buffer((4, 36, 32, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        T_softmax_maxelem = T.allocate([4608], \"float32\", \"global\")\n        T_softmax_maxelem_1 = T.Buffer((4608,), data=T_softmax_maxelem)\n        data_1 = T.Buffer((92160,), data=data.data)\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(36, 32):\n                T_softmax_maxelem_1[i0 * 1152 + i1 * 32 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(20):\n                    cse_var_1: T.int32 = i0 * 1152 + i1 * 32 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 23040 + i1 * 640 + i2 * 20 + k])\n        for i0_i1_fused in T.parallel(144):\n            T_softmax_expsum = T.allocate([32], \"float32\", \"global\")\n            T_softmax_expsum_1 = T.Buffer((32,), data=T_softmax_expsum)\n            for i2 in range(32):\n                T_softmax_expsum_1[i2] = T.float32(0)\n                for k in range(20):\n                    cse_var_3: T.int32 = i0_i1_fused * 32 + i2\n                    cse_var_2: T.int32 = i0_i1_fused * 640 + i2 * 20 + k\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3])\n            for i2, i3 in T.grid(32, 20):\n                cse_var_5: T.int32 = i0_i1_fused * 32 + i2\n                cse_var_4: T.int32 = i0_i1_fused * 640 + i2 * 20 + i3\n                T_softmax_norm_1 = T.Buffer((92160,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_4] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5]) / T_softmax_expsum_1[i2]",
        "data": "4_36_32_20"
    },
    {
        "op_name": "fast_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 128; ++i0_i1_fused) {\n    float T_softmax_maxelem[24];\n    float T_softmax_expsum[1];\n    for (int32_t i2 = 0; i2 < 24; ++i2) {\n      T_softmax_maxelem[i2] = -3.402823e+38f;\n      for (int32_t k = 0; k < 12; ++k) {\n        float v_ = T_softmax_maxelem[i2];\n        float v__1 = ((float*)data_1)[(((i0_i1_fused * 288) + (i2 * 12)) + k)];\n        T_softmax_maxelem[i2] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n    for (int32_t i2_1 = 0; i2_1 < 24; ++i2_1) {\n      T_softmax_expsum[0] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 12; ++k_1) {\n        int32_t cse_var_1 = (((i0_i1_fused * 288) + (i2_1 * 12)) + k_1);\n          float v__2 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n          float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n          int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__5 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n        float v__7 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n        float v__9 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n        float v__11 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n        float v__13 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n        float v__15 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n        float v__17 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n        float v__19 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n        float v__21 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n        float v__23 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n        float v__25 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n        float v__27 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n        float v__29 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n        float v__31 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n        float v__33 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n        float v__35 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n        float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__38 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        T_softmax_expsum[0] = (T_softmax_expsum[0] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n      },\n      for (int32_t i3_s = 0; i3_s < 12; ++i3_s) {\n        int32_t cse_var_2 = (((i0_i1_fused * 288) + (i2_1 * 12)) + i3_s);\n          float v__39 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n          int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__42 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n        float v__44 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n        float v__46 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n        float v__48 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n        float v__50 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n        float v__52 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n        float v__54 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n        float v__56 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n        float v__58 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n        float v__60 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n        float v__62 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n        float v__64 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n        float v__66 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n        float v__68 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n        float v__70 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n        float v__72 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n        float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__75 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        ((float*)T_softmax_norm_1)[cse_var_2] = (((v__74) > (v__75) ? (v__74) : (v__75)) / T_softmax_expsum[0]);\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 12; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))])) / T_softmax_expsum[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 12))]);\n},\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 12; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 12)) + k)]);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 32, 24, 12), \"float32\"), T_softmax_norm: T.Buffer((4, 32, 24, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(128):\n            T_softmax_maxelem = T.allocate([24], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((24,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((36864,), data=data.data)\n            for i2 in range(24):\n                T_softmax_maxelem_1[i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(12):\n                    T_softmax_maxelem_1[i2] = T.max(T_softmax_maxelem_1[i2], data_1[i0_i1_fused * 288 + i2 * 12 + k])\n            for i2 in range(24):\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(12):\n                    cse_var_1: T.int32 = i0_i1_fused * 288 + i2 * 12 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[i2])\n                for i3_s in range(12):\n                    cse_var_2: T.int32 = i0_i1_fused * 288 + i2 * 12 + i3_s\n                    T_softmax_norm_1 = T.Buffer((36864,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[i2]) / T_softmax_expsum_1[0]",
        "data": "4_32_24_12"
    },
    {
        "op_name": "fast_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  void* T_softmax_maxelem = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)24192, 2, 32);\n  if (T_softmax_maxelem == NULL) {\n    return -1;\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 6048; ++i0_i1_fused_i2_fused) {\n    ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused] = -3.402823e+38f;\n    for (int32_t k = 0; k < 40; ++k) {\n      float v_ = ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused];\n      float v__1 = ((float*)data_1)[((i0_i1_fused_i2_fused * 40) + k)];\n      ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n    },\n  },\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 6048; ++i0_i1_fused_i2_fused_1) {\n    float T_softmax_expsum[1];\n    T_softmax_expsum[0] = 0.000000e+00f;\n    for (int32_t k_1 = 0; k_1 < 40; ++k_1) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused_1 * 40) + k_1);\n        float v__2 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n        float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n        int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__5 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n      float v__7 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n      float v__9 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n      float v__11 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n      float v__13 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n      float v__15 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n      float v__17 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n      float v__19 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n      float v__21 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n      float v__23 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n      float v__25 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n      float v__27 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n      float v__29 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n      float v__31 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n      float v__33 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n      float v__35 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n      float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__38 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      T_softmax_expsum[0] = (T_softmax_expsum[0] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n    },\n    for (int32_t i3 = 0; i3 < 40; ++i3) {\n      int32_t cse_var_2 = ((i0_i1_fused_i2_fused_1 * 40) + i3);\n        float v__39 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n        float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n        int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__42 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n      float v__44 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n      float v__46 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n      float v__48 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n      float v__50 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n      float v__52 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n      float v__54 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n      float v__56 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n      float v__58 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n      float v__60 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n      float v__62 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n      float v__64 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n      float v__66 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n      float v__68 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n      float v__70 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n      float v__72 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n      float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__75 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      ((float*)T_softmax_norm_1)[cse_var_2] = (((v__74) > (v__75) ? (v__74) : (v__75)) / T_softmax_expsum[0]);\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_maxelem) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  float normal_reduce_temp0[1];\n  __shared__ float red_buf0[40];\n  __shared__ float T_softmax_maxelem[1];\n  float normal_reduce_temp0_1[1];\n  __shared__ float red_buf0_1[40];\n  __shared__ float T_softmax_expsum[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n  __syncthreads();\n  ((volatile float*)red_buf0)[((int)threadIdx.x)] = normal_reduce_temp0[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 8) {\n    ((volatile float*)red_buf0)[((int)threadIdx.x)] = max(((volatile float*)red_buf0)[((int)threadIdx.x)], ((volatile float*)red_buf0)[(((int)threadIdx.x) + 32)]);\n  },\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0 = max(((volatile float*)red_buf0)[((int)threadIdx.x)], ((volatile float*)red_buf0)[(((int)threadIdx.x) + 16)]);\n    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_16_0;\n    float w_8_0 = max(((volatile float*)red_buf0)[((int)threadIdx.x)], ((volatile float*)red_buf0)[(((int)threadIdx.x) + 8)]);\n    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_8_0;\n    float w_4_0 = max(((volatile float*)red_buf0)[((int)threadIdx.x)], ((volatile float*)red_buf0)[(((int)threadIdx.x) + 4)]);\n    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_4_0;\n    float w_2_0 = max(((volatile float*)red_buf0)[((int)threadIdx.x)], ((volatile float*)red_buf0)[(((int)threadIdx.x) + 2)]);\n    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_2_0;\n    float w_1_0 = max(((volatile float*)red_buf0)[((int)threadIdx.x)], ((volatile float*)red_buf0)[(((int)threadIdx.x) + 1)]);\n    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_1_0;\n  },\n  __syncthreads();\n  if (((int)threadIdx.x) == 0) {\n    T_softmax_maxelem[0] = ((volatile float*)red_buf0)[0];\n  },\n  normal_reduce_temp0_1[0] = 0.000000e+00f;\n  __syncthreads();\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  normal_reduce_temp0_1[0] = (normal_reduce_temp0_1[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0])));\n  __syncthreads();\n  ((volatile float*)red_buf0_1)[((int)threadIdx.x)] = normal_reduce_temp0_1[0];\n  __syncthreads();\n  if (((int)threadIdx.x) < 8) {\n    ((volatile float*)red_buf0_1)[((int)threadIdx.x)] = (((volatile float*)red_buf0_1)[((int)threadIdx.x)] + ((volatile float*)red_buf0_1)[(((int)threadIdx.x) + 32)]);\n  },\n  __syncthreads();\n  if (((int)threadIdx.x) < 16) {\n    float w_16_0_1 = (((volatile float*)red_buf0_1)[((int)threadIdx.x)] + ((volatile float*)red_buf0_1)[(((int)threadIdx.x) + 16)]);\n    ((volatile float*)red_buf0_1)[((int)threadIdx.x)] = w_16_0_1;\n    float w_8_0_1 = (((volatile float*)red_buf0_1)[((int)threadIdx.x)] + ((volatile float*)red_buf0_1)[(((int)threadIdx.x) + 8)]);\n    ((volatile float*)red_buf0_1)[((int)threadIdx.x)] = w_8_0_1;\n    float w_4_0_1 = (((volatile float*)red_buf0_1)[((int)threadIdx.x)] + ((volatile float*)red_buf0_1)[(((int)threadIdx.x) + 4)]);\n    ((volatile float*)red_buf0_1)[((int)threadIdx.x)] = w_4_0_1;\n    float w_2_0_1 = (((volatile float*)red_buf0_1)[((int)threadIdx.x)] + ((volatile float*)red_buf0_1)[(((int)threadIdx.x) + 2)]);\n    ((volatile float*)red_buf0_1)[((int)threadIdx.x)] = w_2_0_1;\n    float w_1_0_1 = (((volatile float*)red_buf0_1)[((int)threadIdx.x)] + ((volatile float*)red_buf0_1)[(((int)threadIdx.x) + 1)]);\n    ((volatile float*)red_buf0_1)[((int)threadIdx.x)] = w_1_0_1;\n  },\n  __syncthreads();\n  if (((int)threadIdx.x) == 0) {\n    T_softmax_expsum[0] = ((volatile float*)red_buf0_1)[0];\n  },\n  __syncthreads();\n    int v__1 = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 24, 28, 40), \"float32\"), T_softmax_norm: T.Buffer((9, 24, 28, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        T_softmax_maxelem = T.allocate([6048], \"float32\", \"global\")\n        T_softmax_maxelem_1 = T.Buffer((6048,), data=T_softmax_maxelem)\n        data_1 = T.Buffer((241920,), data=data.data)\n        for i0_i1_fused_i2_fused in T.parallel(6048):\n            T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.float32(-3.4028234663852886e+38)\n            for k in range(40):\n                T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.max(T_softmax_maxelem_1[i0_i1_fused_i2_fused], data_1[i0_i1_fused_i2_fused * 40 + k])\n        for i0_i1_fused_i2_fused in T.parallel(6048):\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n            T_softmax_expsum_1[0] = T.float32(0)\n            for k in range(40):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 40 + k\n                T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused])\n            for i3 in range(40):\n                cse_var_2: T.int32 = i0_i1_fused_i2_fused * 40 + i3\n                T_softmax_norm_1 = T.Buffer((241920,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused]) / T_softmax_expsum_1[0]",
        "data": "9_24_28_40"
    },
    {
        "op_name": "fast_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 216; ++i0_i1_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      T_softmax_maxelem[0] = -3.402823e+38f;\n      for (int32_t k = 0; k < 20; ++k) {\n        float v_ = T_softmax_maxelem[0];\n        float v__1 = ((float*)data_1)[(((i0_i1_fused * 80) + (i2 * 20)) + k)];\n        T_softmax_maxelem[0] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n      T_softmax_expsum[0] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 20; ++k_1) {\n        int32_t cse_var_1 = (((i0_i1_fused * 80) + (i2 * 20)) + k_1);\n          float v__2 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n          float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n          int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__5 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n        float v__7 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n        float v__9 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n        float v__11 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n        float v__13 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n        float v__15 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n        float v__17 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n        float v__19 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n        float v__21 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n        float v__23 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n        float v__25 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n        float v__27 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n        float v__29 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n        float v__31 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n        float v__33 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n        float v__35 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n        float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__38 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        T_softmax_expsum[0] = (T_softmax_expsum[0] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n      },\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        int32_t cse_var_2 = (((i0_i1_fused * 80) + (i2 * 20)) + i3);\n          float v__39 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n          float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n          int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__42 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n        float v__44 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n        float v__46 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n        float v__48 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n        float v__50 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n        float v__52 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n        float v__54 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n        float v__56 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n        float v__58 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n        float v__60 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n        float v__62 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n        float v__64 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n        float v__66 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n        float v__68 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n        float v__70 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n        float v__72 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n        float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__75 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        ((float*)T_softmax_norm_1)[cse_var_2] = (((v__74) > (v__75) ? (v__74) : (v__75)) / T_softmax_expsum[0]);\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 20; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)])) / T_softmax_expsum[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]);\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 20; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)]);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 24, 4, 20), \"float32\"), T_softmax_norm: T.Buffer((9, 24, 4, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(216):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            for i2 in range(4):\n                T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n                T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((17280,), data=data.data)\n                for k in range(20):\n                    T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused * 80 + i2 * 20 + k])\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(20):\n                    cse_var_1: T.int32 = i0_i1_fused * 80 + i2 * 20 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0])\n                for i3 in range(20):\n                    cse_var_2: T.int32 = i0_i1_fused * 80 + i2 * 20 + i3\n                    T_softmax_norm_1 = T.Buffer((17280,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]",
        "data": "9_24_4_20"
    },
    {
        "op_name": "fast_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  void* T_softmax_maxelem = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)10752, 2, 32);\n  if (T_softmax_maxelem == NULL) {\n    return -1;\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2688; ++i0_i1_fused_i2_fused) {\n    ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused] = -3.402823e+38f;\n    for (int32_t k = 0; k < 28; ++k) {\n      float v_ = ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused];\n      float v__1 = ((float*)data_1)[((i0_i1_fused_i2_fused * 28) + k)];\n      ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n    },\n  },\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2688; ++i0_i1_fused_i2_fused_1) {\n    float T_softmax_expsum[1];\n    T_softmax_expsum[0] = 0.000000e+00f;\n    for (int32_t k_1 = 0; k_1 < 28; ++k_1) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused_1 * 28) + k_1);\n        float v__2 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n        float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n        int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__5 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n      float v__7 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n      float v__9 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n      float v__11 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n      float v__13 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n      float v__15 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n      float v__17 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n      float v__19 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n      float v__21 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n      float v__23 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n      float v__25 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n      float v__27 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n      float v__29 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n      float v__31 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n      float v__33 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n      float v__35 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n      float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__38 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      T_softmax_expsum[0] = (T_softmax_expsum[0] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n    },\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_2 = ((i0_i1_fused_i2_fused_1 * 28) + i3);\n        float v__39 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n        float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n        int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__42 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n      float v__44 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n      float v__46 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n      float v__48 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n      float v__50 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n      float v__52 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n      float v__54 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n      float v__56 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n      float v__58 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n      float v__60 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n      float v__62 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n      float v__64 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n      float v__66 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n      float v__68 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n      float v__70 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n      float v__72 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n      float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__75 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      ((float*)T_softmax_norm_1)[cse_var_2] = (((v__74) > (v__75) ? (v__74) : (v__75)) / T_softmax_expsum[0]);\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_maxelem) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)])) / T_softmax_expsum[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]);\n},\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 28; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 28)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 28; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 28, 16, 28), \"float32\"), T_softmax_norm: T.Buffer((6, 28, 16, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        T_softmax_maxelem = T.allocate([2688], \"float32\", \"global\")\n        T_softmax_maxelem_1 = T.Buffer((2688,), data=T_softmax_maxelem)\n        data_1 = T.Buffer((75264,), data=data.data)\n        for i0_i1_fused_i2_fused in T.parallel(2688):\n            T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.float32(-3.4028234663852886e+38)\n            for k in range(28):\n                T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.max(T_softmax_maxelem_1[i0_i1_fused_i2_fused], data_1[i0_i1_fused_i2_fused * 28 + k])\n        for i0_i1_fused_i2_fused in T.parallel(2688):\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n            T_softmax_expsum_1[0] = T.float32(0)\n            for k in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + k\n                T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused])\n            for i3 in range(28):\n                cse_var_2: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                T_softmax_norm_1 = T.Buffer((75264,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused]) / T_softmax_expsum_1[0]",
        "data": "6_28_16_28"
    },
    {
        "op_name": "fast_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1440; ++i0_i1_fused_i2_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    T_softmax_maxelem[0] = -3.402823e+38f;\n    for (int32_t k = 0; k < 40; ++k) {\n      float v_ = T_softmax_maxelem[0];\n      float v__1 = ((float*)data_1)[((i0_i1_fused_i2_fused * 40) + k)];\n      T_softmax_maxelem[0] = ((v_) > (v__1) ? (v_) : (v__1));\n    },\n    T_softmax_expsum[0] = 0.000000e+00f;\n    for (int32_t k_1 = 0; k_1 < 40; ++k_1) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 40) + k_1);\n        float v__2 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n        int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__5 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n      float v__7 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n      float v__9 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n      float v__11 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n      float v__13 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n      float v__15 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n      float v__17 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n      float v__19 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n      float v__21 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n      float v__23 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n      float v__25 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n      float v__27 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n      float v__29 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n      float v__31 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n      float v__33 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n      float v__35 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n      float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__38 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      T_softmax_expsum[0] = (T_softmax_expsum[0] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n    },\n    for (int32_t i3 = 0; i3 < 40; ++i3) {\n      int32_t cse_var_2 = ((i0_i1_fused_i2_fused * 40) + i3);\n        float v__39 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n        int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__42 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n      float v__44 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n      float v__46 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n      float v__48 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n      float v__50 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n      float v__52 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n      float v__54 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n      float v__56 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n      float v__58 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n      float v__60 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n      float v__62 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n      float v__64 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n      float v__66 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n      float v__68 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n      float v__70 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n      float v__72 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n      float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__75 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      ((float*)T_softmax_norm_1)[cse_var_2] = (((v__74) > (v__75) ? (v__74) : (v__75)) / T_softmax_expsum[0]);\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  __shared__ float T_softmax_expsum[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k_outer = 0; k_outer < 10; ++k_outer) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 40) + (k_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)])));\n  },\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    T_softmax_expsum[0] = red_buf0[0];\n  },\n  __syncthreads();\n  for (int i3_outer = 0; i3_outer < 10; ++i3_outer) {\n      int v__1 = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 40) + (i3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)])) / T_softmax_expsum[0]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 40; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 40)) + k)]);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 20, 12, 40), \"float32\"), T_softmax_norm: T.Buffer((6, 20, 12, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(1440):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n            T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n            data_1 = T.Buffer((57600,), data=data.data)\n            for k in range(40):\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused_i2_fused * 40 + k])\n            T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n            T_softmax_expsum_1[0] = T.float32(0)\n            for k in range(40):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 40 + k\n                T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0])\n            for i3 in range(40):\n                cse_var_2: T.int32 = i0_i1_fused_i2_fused * 40 + i3\n                T_softmax_norm_1 = T.Buffer((57600,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]",
        "data": "6_20_12_40"
    },
    {
        "op_name": "batch_to_space_nd",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 32; ++ax0_ax1_fused) {\n    void* T_transpose = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)2560, 2, 32);\n    if (T_transpose == NULL) {\n      return -1;\n    },\n    float T_reshape[80];\n    float T_reshape_1[1];\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n        for (int32_t ax5 = 0; ax5 < 40; ++ax5) {\n          T_reshape[((ax1 * 40) + ax5)] = ((float*)data_1)[((((((ax0_ax1_fused & 1) * 10240) + (ax1 * 5120)) + ((ax0_ax1_fused >> 1) * 320)) + (ax3 * 40)) + ax5)];\n        },\n      },\n      for (int32_t ax4 = 0; ax4 < 2; ++ax4) {\n        for (int32_t ax5_1 = 0; ax5_1 < 40; ++ax5_1) {\n          int32_t cse_var_1 = (ax4 * 40);\n          ((float*)T_transpose)[(((ax3 * 80) + cse_var_1) + ax5_1)] = T_reshape[(cse_var_1 + ax5_1)];\n        },\n      },\n    },\n    for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n      for (int32_t ax3_1 = 0; ax3_1 < 40; ++ax3_1) {\n        int32_t cse_var_2 = (ax2 * 40);\n        T_reshape_1[0] = ((float*)T_transpose)[(cse_var_2 + ax3_1)];\n        ((float*)T_strided_slice_1)[(((ax0_ax1_fused * 640) + cse_var_2) + ax3_1)] = T_reshape_1[0];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, T_transpose) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data) {\n  T_strided_slice[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) & 31) >> 4) * 10240) + ((((int)blockIdx.x) & 1) * 5120)) + ((((int)blockIdx.x) >> 5) * 320)) + (((((int)blockIdx.x) & 15) >> 1) * 40)) + ((int)threadIdx.x))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 8, 8, 40), \"float32\"), T_strided_slice: T.Buffer((2, 16, 16, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(32):\n            T_transpose = T.allocate([640], \"float32\", \"global\")\n            T_reshape = T.allocate([80], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n            T_transpose_1 = T.Buffer((640,), data=T_transpose)\n            for ax3 in range(8):\n                T_reshape_2 = T.Buffer((80,), data=T_reshape)\n                for ax1, ax5 in T.grid(2, 40):\n                    data_1 = T.Buffer((23040,), data=data.data)\n                    T_reshape_2[ax1 * 40 + ax5] = data_1[ax0_ax1_fused % 2 * 10240 + ax1 * 5120 + ax0_ax1_fused // 2 * 320 + ax3 * 40 + ax5]\n                for ax4, ax5 in T.grid(2, 40):\n                    cse_var_1: T.int32 = ax4 * 40\n                    T_transpose_1[ax3 * 80 + cse_var_1 + ax5] = T_reshape_2[cse_var_1 + ax5]\n            for ax2, ax3 in T.grid(16, 40):\n                cse_var_2: T.int32 = ax2 * 40\n                T_reshape_2 = T.Buffer((1,), data=T_reshape_1, align=4)\n                T_reshape_2[0] = T_transpose_1[cse_var_2 + ax3]\n                T_strided_slice_1 = T.Buffer((20480,), data=T_strided_slice.data)\n                T_strided_slice_1[ax0_ax1_fused * 640 + cse_var_2 + ax3] = T_reshape_2[0]",
        "data": "9_8_8_40"
    },
    {
        "op_name": "batch_to_space_nd",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  },\n  void* T_reshape = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)2560, 2, 32);\n  if (T_reshape == NULL) {\n    return -1;\n  },\n  float T_reshape_1[8];\n  float T_transpose[1];\n  for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 80; ++ax2) {\n      *(float8*)(T_reshape_1 + 0) = *(float8*)(((float*)data_1) + (((((ax1 & 1) * 5120) + ((ax2 & 1) * 2560)) + ((ax1 >> 1) * 320)) + ((ax2 >> 1) * 8)));\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        T_transpose[0] = T_reshape_1[ax3];\n        ((float*)T_reshape)[((ax2 * 8) + ax3)] = T_transpose[0];\n      },\n    },\n    for (int32_t ax2_1 = 0; ax2_1 < 80; ++ax2_1) {\n      int32_t cse_var_1 = (ax2_1 * 8);\n      int32_t8 v_ = int32_t8((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7));\n      *(float8*)(((float*)T_strided_slice_1) + ((ax1 * 640) + cse_var_1)) = (float8(((float*)T_reshape)[v_.s0],((float*)T_reshape)[v_.s1],((float*)T_reshape)[v_.s2],((float*)T_reshape)[v_.s3],((float*)T_reshape)[v_.s4],((float*)T_reshape)[v_.s5],((float*)T_reshape)[v_.s6],((float*)T_reshape)[v_.s7]));\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_reshape) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data) {\n  T_strided_slice[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((((((((int)blockIdx.x) % 20) / 10) * 5120) + (((((int)threadIdx.x) & 15) >> 3) * 2560)) + ((((int)blockIdx.x) / 20) * 320)) + ((((int)blockIdx.x) % 10) * 32)) + ((((int)threadIdx.x) >> 4) * 8)) + (((int)threadIdx.x) & 7))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 8, 40, 8), \"float32\"), T_strided_slice: T.Buffer((1, 16, 80, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        T_reshape = T.allocate([640], \"float32\", \"global\")\n        T_reshape_1 = T.allocate([8], \"float32\", \"global\")\n        T_transpose = T.allocate([1], \"float32\", \"global\")\n        for ax1 in range(16):\n            T_reshape_3 = T.Buffer((640,), data=T_reshape)\n            for ax2 in range(80):\n                T_reshape_2 = T.Buffer((8,), data=T_reshape_1, align=32)\n                data_1 = T.Buffer((12800,), data=data.data)\n                T_reshape_2[0:8] = data_1[ax1 % 2 * 5120 + ax2 % 2 * 2560 + ax1 // 2 * 320 + ax2 // 2 * 8:ax1 % 2 * 5120 + ax2 % 2 * 2560 + ax1 // 2 * 320 + ax2 // 2 * 8 + 8]\n                for ax3 in range(8):\n                    T_transpose_1 = T.Buffer((1,), data=T_transpose, align=4)\n                    T_transpose_1[0] = T_reshape_2[ax3]\n                    T_reshape_3[ax2 * 8 + ax3] = T_transpose_1[0]\n            for ax2 in range(80):\n                cse_var_1: T.int32 = ax2 * 8\n                T_strided_slice_1 = T.Buffer((10240,), data=T_strided_slice.data)\n                T_strided_slice_1[ax1 * 640 + cse_var_1:ax1 * 640 + cse_var_1 + 8] = T_reshape_3[cse_var_1:cse_var_1 + 8]",
        "data": "5_8_40_8"
    },
    {
        "op_name": "batch_to_space_nd",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 7680; ++ax0_ax1_fused_ax2_fused) {\n    float T_reshape[32];\n    if (0 <= (((ax0_ax1_fused_ax2_fused % 3840) / 48) - (((ax0_ax1_fused_ax2_fused % 3840) / 96) * 2))) {\n      for (int32_t ax5 = 0; ax5 < 32; ++ax5) {\n        int32_t cse_var_3 = (ax0_ax1_fused_ax2_fused % 3840);\n        int32_t cse_var_2 = (cse_var_3 / 96);\n        T_reshape[ax5] = ((float*)data_1)[((((((((((cse_var_3 / 48) * 4) + ((ax0_ax1_fused_ax2_fused & 1) * 2)) + (ax0_ax1_fused_ax2_fused / 3840)) + cse_var_2) % 9) * 30720) + (cse_var_2 * 768)) + (((ax0_ax1_fused_ax2_fused % 48) >> 1) * 32)) + ax5)];\n      },\n    },\n    for (int32_t ax3 = 0; ax3 < 32; ++ax3) {\n      ((float*)T_strided_slice_1)[((ax0_ax1_fused_ax2_fused * 32) + ax3)] = T_reshape[ax3];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data) {\n  T_strided_slice[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) % 96) / 48) * 122880) + ((((int)blockIdx.x) & 1) * 61440)) + ((((int)blockIdx.x) / 96) * 768)) + (((((int)blockIdx.x) % 48) >> 1) * 32)) + ((int)threadIdx.x))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 40, 24, 32), \"float32\"), T_strided_slice: T.Buffer((2, 80, 48, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(7680):\n            T_reshape = T.allocate([32], \"float32\", \"global\")\n            cse_var_1 = T.int32()\n            if T.Let(T.likely(0 <= cse_var_1 // 48 - cse_var_1 // 96 * 2), where={cse_var_1: ax0_ax1_fused_ax2_fused % 3840},):\n                for ax5 in range(32):\n                    cse_var_3: T.int32 = ax0_ax1_fused_ax2_fused % 3840\n                    cse_var_2: T.int32 = cse_var_3 // 96\n                    T_reshape_1 = T.Buffer((32,), data=T_reshape)\n                    data_1 = T.Buffer((276480,), data=data.data)\n                    T_reshape_1[ax5] = data_1[(cse_var_3 // 48 * 4 + ax0_ax1_fused_ax2_fused % 2 * 2 + ax0_ax1_fused_ax2_fused // 3840 + cse_var_2) % 9 * 30720 + cse_var_2 * 768 + ax0_ax1_fused_ax2_fused % 48 // 2 * 32 + ax5]\n            for ax3 in range(32):\n                T_strided_slice_1 = T.Buffer((245760,), data=T_strided_slice.data)\n                T_reshape_1 = T.Buffer((32,), data=T_reshape)\n                T_strided_slice_1[ax0_ax1_fused_ax2_fused * 32 + ax3] = T_reshape_1[ax3]",
        "data": "9_40_24_32"
    },
    {
        "op_name": "batch_to_space_nd",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 8064; ++ax0_ax1_fused_ax2_fused) {\n    float T_reshape[28];\n    float T_reshape_1[1];\n    if (0 <= (((ax0_ax1_fused_ax2_fused % 4032) / 72) - (((ax0_ax1_fused_ax2_fused % 4032) / 144) * 2))) {\n      for (int32_t ax5 = 0; ax5 < 28; ++ax5) {\n        int32_t cse_var_3 = (ax0_ax1_fused_ax2_fused % 4032);\n        int32_t cse_var_2 = (cse_var_3 / 144);\n        T_reshape[ax5] = ((float*)data_1)[((((((((((cse_var_3 / 72) * 4) + ((ax0_ax1_fused_ax2_fused & 1) * 2)) + (ax0_ax1_fused_ax2_fused / 4032)) + cse_var_2) % 9) * 28224) + (cse_var_2 * 1008)) + (((ax0_ax1_fused_ax2_fused % 72) >> 1) * 28)) + ax5)];\n      },\n    },\n    for (int32_t ax3 = 0; ax3 < 28; ++ax3) {\n      T_reshape_1[0] = T_reshape[ax3];\n      ((float*)T_strided_slice_1)[((ax0_ax1_fused_ax2_fused * 28) + ax3)] = T_reshape_1[0];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data) {\n  T_strided_slice[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) & 63) >> 5) * 112896) + (((((((int)threadIdx.x) / 7) + ((int)blockIdx.x)) & 7) >> 2) * 56448)) + ((((int)blockIdx.x) >> 6) * 1008)) + (((((((int)blockIdx.x) & 31) * 9) + (((int)threadIdx.x) / 7)) >> 3) * 28)) + (((((int)blockIdx.x) * 7) + ((int)threadIdx.x)) % 28))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 28, 36, 28), \"float32\"), T_strided_slice: T.Buffer((2, 56, 72, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(8064):\n            T_reshape = T.allocate([28], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n            cse_var_1 = T.int32()\n            if T.Let(T.likely(0 <= cse_var_1 // 72 - cse_var_1 // 144 * 2), where={cse_var_1: ax0_ax1_fused_ax2_fused % 4032},):\n                for ax5 in range(28):\n                    cse_var_3: T.int32 = ax0_ax1_fused_ax2_fused % 4032\n                    cse_var_2: T.int32 = cse_var_3 // 144\n                    T_reshape_2 = T.Buffer((28,), data=T_reshape)\n                    data_1 = T.Buffer((254016,), data=data.data)\n                    T_reshape_2[ax5] = data_1[(cse_var_3 // 72 * 4 + ax0_ax1_fused_ax2_fused % 2 * 2 + ax0_ax1_fused_ax2_fused // 4032 + cse_var_2) % 9 * 28224 + cse_var_2 * 1008 + ax0_ax1_fused_ax2_fused % 72 // 2 * 28 + ax5]\n            for ax3 in range(28):\n                T_reshape_2 = T.Buffer((1,), data=T_reshape_1, align=4)\n                T_reshape_3 = T.Buffer((28,), data=T_reshape)\n                T_reshape_2[0] = T_reshape_3[ax3]\n                T_strided_slice_1 = T.Buffer((225792,), data=T_strided_slice.data)\n                T_strided_slice_1[ax0_ax1_fused_ax2_fused * 28 + ax3] = T_reshape_2[0]",
        "data": "9_28_36_28"
    },
    {
        "op_name": "batch_to_space_nd",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  },\n  void* T_transpose = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)8064, 2, 32);\n  if (T_transpose == NULL) {\n    return -1;\n  },\n  float T_reshape[28];\n  for (int32_t ax1 = 0; ax1 < 32; ++ax1) {\n    for (int32_t ax3 = 0; ax3 < 36; ++ax3) {\n      for (int32_t ax4 = 0; ax4 < 2; ++ax4) {\n        for (int32_t ax5 = 0; ax5 < 28; ++ax5) {\n          T_reshape[ax5] = ((float*)data_1)[((((((ax1 & 1) * 32256) + (ax4 * 16128)) + ((ax1 >> 1) * 1008)) + (ax3 * 28)) + ax5)];\n        },\n        for (int32_t ax5_1 = 0; ax5_1 < 28; ++ax5_1) {\n          ((float*)T_transpose)[(((ax3 * 56) + (ax4 * 28)) + ax5_1)] = T_reshape[ax5_1];\n        },\n      },\n    },\n    for (int32_t ax2 = 0; ax2 < 72; ++ax2) {\n      for (int32_t ax3_1 = 0; ax3_1 < 28; ++ax3_1) {\n        int32_t cse_var_1 = (ax2 * 28);\n        ((float*)T_strided_slice_1)[(((ax1 * 2016) + cse_var_1) + ax3_1)] = ((float*)T_transpose)[(cse_var_1 + ax3_1)];\n      },\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_transpose) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data) {\n  T_strided_slice[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) % 84) / 42) * 32256) + (((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 2)) % 14) / 7) * 16128)) + ((((int)blockIdx.x) / 84) * 1008)) + (((((((int)blockIdx.x) % 42) * 6) + (((int)threadIdx.x) >> 3)) / 7) * 28)) + (((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) % 28))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 16, 36, 28), \"float32\"), T_strided_slice: T.Buffer((1, 32, 72, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        T_transpose = T.allocate([2016], \"float32\", \"global\")\n        T_reshape = T.allocate([28], \"float32\", \"global\")\n        for ax1 in range(32):\n            for ax3, ax4 in T.grid(36, 2):\n                T_reshape_1 = T.Buffer((28,), data=T_reshape)\n                for ax5 in range(28):\n                    data_1 = T.Buffer((96768,), data=data.data)\n                    T_reshape_1[ax5] = data_1[ax1 % 2 * 32256 + ax4 * 16128 + ax1 // 2 * 1008 + ax3 * 28 + ax5]\n                for ax5 in range(28):\n                    T_transpose_1 = T.Buffer((2016,), data=T_transpose)\n                    T_transpose_1[ax3 * 56 + ax4 * 28 + ax5] = T_reshape_1[ax5]\n            for ax2, ax3 in T.grid(72, 28):\n                cse_var_1: T.int32 = ax2 * 28\n                T_strided_slice_1 = T.Buffer((64512,), data=T_strided_slice.data)\n                T_transpose_1 = T.Buffer((2016,), data=T_transpose)\n                T_strided_slice_1[ax1 * 2016 + cse_var_1 + ax3] = T_transpose_1[cse_var_1 + ax3]",
        "data": "6_16_36_28"
    },
    {
        "op_name": "batch_to_space_nd",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  },\n  float T_reshape[36];\n  float T_reshape_1[1];\n  for (int32_t ax1 = 0; ax1 < 48; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n      for (int32_t ax5 = 0; ax5 < 36; ++ax5) {\n        T_reshape[ax5] = ((float*)data_1)[((((((ax1 & 1) * 13824) + ((ax2 & 1) * 6912)) + ((ax1 >> 1) * 288)) + ((ax2 >> 1) * 36)) + ax5)];\n      },\n      for (int32_t ax3 = 0; ax3 < 36; ++ax3) {\n        T_reshape_1[0] = T_reshape[ax3];\n        ((float*)T_strided_slice_1)[(((ax1 * 576) + (ax2 * 36)) + ax3)] = T_reshape_1[0];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data) {\n  T_strided_slice[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) % 36) / 18) * 13824) + (((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) % 18) / 9) * 6912)) + ((((int)blockIdx.x) / 36) * 288)) + (((((((int)blockIdx.x) % 18) * 4) + (((int)threadIdx.x) >> 3)) / 9) * 36)) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 36))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 24, 8, 36), \"float32\"), T_strided_slice: T.Buffer((1, 48, 16, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        T_reshape = T.allocate([36], \"float32\", \"global\")\n        T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n        for ax1, ax2 in T.grid(48, 16):\n            for ax5 in range(36):\n                T_reshape_2 = T.Buffer((36,), data=T_reshape)\n                data_1 = T.Buffer((34560,), data=data.data)\n                T_reshape_2[ax5] = data_1[ax1 % 2 * 13824 + ax2 % 2 * 6912 + ax1 // 2 * 288 + ax2 // 2 * 36 + ax5]\n            for ax3 in range(36):\n                T_reshape_2 = T.Buffer((1,), data=T_reshape_1, align=4)\n                T_reshape_3 = T.Buffer((36,), data=T_reshape)\n                T_reshape_2[0] = T_reshape_3[ax3]\n                T_strided_slice_1 = T.Buffer((27648,), data=T_strided_slice.data)\n                T_strided_slice_1[ax1 * 576 + ax2 * 36 + ax3] = T_reshape_2[0]",
        "data": "5_24_8_36"
    },
    {
        "op_name": "batch_to_space_nd",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  },\n  void* T_reshape = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)2560, 2, 32);\n  if (T_reshape == NULL) {\n    return -1;\n  },\n  void* T_transpose = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)2560, 2, 32);\n  if (T_transpose == NULL) {\n    return -1;\n  },\n  float T_reshape_1[1];\n  for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 2; ++ax1_1) {\n      for (int32_t ax4 = 0; ax4 < 20; ++ax4) {\n        ((float16*)T_reshape)[((ax1_1 * 20) + ax4)] = *(float16*)(((float*)data_1) + (((((ax1 & 1) * 5120) + (ax1_1 * 2560)) + ((ax1 >> 1) * 320)) + (ax4 * 16)));\n      },\n    },\n    for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n      for (int32_t ax4_1 = 0; ax4_1 < 2; ++ax4_1) {\n        *(float16*)(((float*)T_transpose) + ((ax3 * 32) + (ax4_1 * 16))) = ((float16*)T_reshape)[((ax4_1 * 20) + ax3)];\n      },\n    },\n    for (int32_t ax2 = 0; ax2 < 40; ++ax2) {\n      for (int32_t ax3_1 = 0; ax3_1 < 16; ++ax3_1) {\n        int32_t cse_var_1 = (ax2 * 16);\n        T_reshape_1[0] = ((float*)T_transpose)[(cse_var_1 + ax3_1)];\n        ((float*)T_strided_slice_1)[(((ax1 * 640) + cse_var_1) + ax3_1)] = T_reshape_1[0];\n      },\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_transpose) != 0) {\n    return -1;\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_reshape) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data) {\n  T_strided_slice[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) % 40) / 20) * 5120) + ((((int)threadIdx.x) >> 4) * 2560)) + ((((int)blockIdx.x) / 40) * 320)) + ((((int)blockIdx.x) % 20) * 16)) + (((int)threadIdx.x) & 15))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 8, 20, 16), \"float32\"), T_strided_slice: T.Buffer((1, 16, 40, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        T_reshape = T.allocate([40], \"float32x16\", \"global\")\n        T_transpose = T.allocate([640], \"float32\", \"global\")\n        T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n        for ax1 in range(16):\n            T_reshape_2 = T.Buffer((40,), \"float32x16\", data=T_reshape)\n            for ax1_1, ax4 in T.grid(2, 20):\n                data_1 = T.Buffer((12800,), data=data.data)\n                T_reshape_2[ax1_1 * 20 + ax4] = data_1[ax1 % 2 * 5120 + ax1_1 * 2560 + ax1 // 2 * 320 + ax4 * 16:ax1 % 2 * 5120 + ax1_1 * 2560 + ax1 // 2 * 320 + ax4 * 16 + 16]\n            T_transpose_1 = T.Buffer((640,), data=T_transpose)\n            for ax3, ax4 in T.grid(20, 2):\n                T_transpose_1[ax3 * 32 + ax4 * 16:ax3 * 32 + ax4 * 16 + 16] = T_reshape_2[ax4 * 20 + ax3]\n            for ax2, ax3 in T.grid(40, 16):\n                cse_var_1: T.int32 = ax2 * 16\n                T_reshape_3 = T.Buffer((1,), data=T_reshape_1, align=4)\n                T_reshape_3[0] = T_transpose_1[cse_var_1 + ax3]\n                T_strided_slice_1 = T.Buffer((10240,), data=T_strided_slice.data)\n                T_strided_slice_1[ax1 * 640 + cse_var_1 + ax3] = T_reshape_3[0]",
        "data": "5_8_20_16"
    },
    {
        "op_name": "global_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 240; ++ax0_ax1_fused_ax2_fused) {\n    ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 16; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 20; ++rv1) {\n        float v_ = ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused];\n        float v__1 = ((float*)data_1)[(((ax0_ax1_fused_ax2_fused * 320) + (rv0 * 20)) + rv1)];\n        ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 16; ++rv0) {\n    for (int rv1 = 0; rv1 < 20; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 320)) + (rv0 * 20)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 24, 16, 20), \"float32\"), adaptive_pool_max: T.Buffer((10, 24, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(240):\n            adaptive_pool_max_1 = T.Buffer((240,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(16, 20):\n                data_1 = T.Buffer((76800,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused], data_1[ax0_ax1_fused_ax2_fused * 320 + rv0 * 20 + rv1])",
        "data": "10_24_16_20"
    },
    {
        "op_name": "global_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 72; ++ax0_ax1_fused_ax2_fused) {\n    ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 40; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 4; ++rv1) {\n        float v_ = ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused];\n        float v__1 = ((float*)data_1)[(((ax0_ax1_fused_ax2_fused * 160) + (rv0 * 4)) + rv1)];\n        ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 40; ++rv0) {\n    for (int rv1 = 0; rv1 < 4; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 160)) + (rv0 * 4)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 12, 40, 4), \"float32\"), adaptive_pool_max: T.Buffer((6, 12, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(72):\n            adaptive_pool_max_1 = T.Buffer((72,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(40, 4):\n                data_1 = T.Buffer((11520,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused], data_1[ax0_ax1_fused_ax2_fused * 160 + rv0 * 4 + rv1])",
        "data": "6_12_40_4"
    },
    {
        "op_name": "global_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 72; ++ax0_ax1_fused) {\n    ((float*)adaptive_pool_max_1)[ax0_ax1_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 32; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 12; ++rv1) {\n        float v_ = ((float*)adaptive_pool_max_1)[ax0_ax1_fused];\n        float v__1 = ((float*)data_1)[(((ax0_ax1_fused * 384) + (rv0 * 12)) + rv1)];\n        ((float*)adaptive_pool_max_1)[ax0_ax1_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 32; ++rv0) {\n    for (int rv1 = 0; rv1 < 12; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 384)) + (rv0 * 12)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 24, 32, 12), \"float32\"), adaptive_pool_max: T.Buffer((3, 24, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(72):\n            adaptive_pool_max_1 = T.Buffer((72,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(32, 12):\n                data_1 = T.Buffer((27648,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused], data_1[ax0_ax1_fused * 384 + rv0 * 12 + rv1])",
        "data": "3_24_32_12"
    },
    {
        "op_name": "global_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 200; ++ax0_ax1_fused_ax2_fused) {\n    ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 12; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 20; ++rv1) {\n        float v_ = ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused];\n        float v__1 = ((float*)data_1)[(((ax0_ax1_fused_ax2_fused * 240) + (rv0 * 20)) + rv1)];\n        ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 12; ++rv0) {\n    for (int rv1 = 0; rv1 < 20; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 240)) + (rv0 * 20)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 40, 12, 20), \"float32\"), adaptive_pool_max: T.Buffer((5, 40, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(200):\n            adaptive_pool_max_1 = T.Buffer((200,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(12, 20):\n                data_1 = T.Buffer((48000,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused], data_1[ax0_ax1_fused_ax2_fused * 240 + rv0 * 20 + rv1])",
        "data": "5_40_12_20"
    },
    {
        "op_name": "global_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax1 = 0; ax1 < 24; ++ax1) {\n    ((float*)adaptive_pool_max_1)[ax1] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 8; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 4; ++rv1) {\n        float v_ = ((float*)adaptive_pool_max_1)[ax1];\n        float v__1 = ((float*)data_1)[(((ax1 * 32) + (rv0 * 4)) + rv1)];\n        ((float*)adaptive_pool_max_1)[ax1] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((int)threadIdx.x)] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 8; ++rv0) {\n    for (int rv1 = 0; rv1 < 4; ++rv1) {\n      adaptive_pool_max[((int)threadIdx.x)] = max(adaptive_pool_max[((int)threadIdx.x)], data[(((((int)threadIdx.x) * 32) + (rv0 * 4)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 24, 8, 4), \"float32\"), adaptive_pool_max: T.Buffer((1, 24, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax1 in range(24):\n            adaptive_pool_max_1 = T.Buffer((24,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax1] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(8, 4):\n                data_1 = T.Buffer((768,), data=data.data)\n                adaptive_pool_max_1[ax1] = T.max(adaptive_pool_max_1[ax1], data_1[ax1 * 32 + rv0 * 4 + rv1])",
        "data": "1_24_8_4"
    },
    {
        "op_name": "global_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 4; ++ax1) {\n      ((float*)adaptive_pool_max_1)[((ax0 * 4) + ax1)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 24; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 8; ++rv1) {\n          int32_t cse_var_1 = ((ax0 * 4) + ax1);\n          float v_ = ((float*)adaptive_pool_max_1)[cse_var_1];\n          float v__1 = ((float*)data_1)[((((ax0 * 768) + (ax1 * 192)) + (rv0 * 8)) + rv1)];\n          ((float*)adaptive_pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((int)threadIdx.x)] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 24; ++rv0) {\n    for (int rv1 = 0; rv1 < 8; ++rv1) {\n      adaptive_pool_max[((int)threadIdx.x)] = max(adaptive_pool_max[((int)threadIdx.x)], data[(((((int)threadIdx.x) * 192) + (rv0 * 8)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 4, 24, 8), \"float32\"), adaptive_pool_max: T.Buffer((6, 4, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(6):\n            for ax1 in range(4):\n                adaptive_pool_max_1 = T.Buffer((24,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0 * 4 + ax1] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(24, 8):\n                    cse_var_1: T.int32 = ax0 * 4 + ax1\n                    data_1 = T.Buffer((4608,), data=data.data)\n                    adaptive_pool_max_1[cse_var_1] = T.max(adaptive_pool_max_1[cse_var_1], data_1[ax0 * 768 + ax1 * 192 + rv0 * 8 + rv1])",
        "data": "6_4_24_8"
    },
    {
        "op_name": "global_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 252; ++ax0_ax1_fused_ax2_fused) {\n    ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 12; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 12; ++rv1) {\n        float v_ = ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused];\n        float v__1 = ((float*)data_1)[(((ax0_ax1_fused_ax2_fused * 144) + (rv0 * 12)) + rv1)];\n        ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 12; ++rv0) {\n    for (int rv1 = 0; rv1 < 12; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 144)) + (rv0 * 12)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 28, 12, 12), \"float32\"), adaptive_pool_max: T.Buffer((9, 28, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(252):\n            adaptive_pool_max_1 = T.Buffer((252,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(12, 12):\n                data_1 = T.Buffer((36288,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused], data_1[ax0_ax1_fused_ax2_fused * 144 + rv0 * 12 + rv1])",
        "data": "9_28_12_12"
    },
    {
        "op_name": "global_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 120; ++ax0_ax1_fused_ax2_fused) {\n    ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 24; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 36; ++rv1) {\n        float v_ = ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused];\n        float v__1 = ((float*)data_1)[(((ax0_ax1_fused_ax2_fused * 864) + (rv0 * 36)) + rv1)];\n        ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 24; ++rv0) {\n    for (int rv1 = 0; rv1 < 36; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 2592) + (((int)threadIdx.x) * 864)) + (rv0 * 36)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 12, 24, 36), \"float32\"), adaptive_pool_max: T.Buffer((10, 12, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(120):\n            adaptive_pool_max_1 = T.Buffer((120,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(24, 36):\n                data_1 = T.Buffer((103680,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused], data_1[ax0_ax1_fused_ax2_fused * 864 + rv0 * 36 + rv1])",
        "data": "10_12_24_36"
    },
    {
        "op_name": "global_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 168; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 16; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 20; ++rv1) {\n        float v_ = ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n        float v__1 = ((float*)data_1)[(((ax0_ax1_fused_ax2_fused_ax3_fused * 320) + (rv0 * 20)) + rv1)];\n        ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 16; ++rv0) {\n    for (int rv1 = 0; rv1 < 20; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 320)) + (rv0 * 20)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 28, 16, 20), \"float32\"), adaptive_pool_max: T.Buffer((6, 28, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(168):\n            adaptive_pool_max_1 = T.Buffer((168,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(16, 20):\n                data_1 = T.Buffer((53760,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused], data_1[ax0_ax1_fused_ax2_fused_ax3_fused * 320 + rv0 * 20 + rv1])",
        "data": "6_28_16_20"
    },
    {
        "op_name": "global_pool_max",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 160; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 32; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 28; ++rv1) {\n        float v_ = ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n        float v__1 = ((float*)data_1)[(((ax0_ax1_fused_ax2_fused_ax3_fused * 896) + (rv0 * 28)) + rv1)];\n        ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 32; ++rv0) {\n    for (int rv1 = 0; rv1 < 28; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 1792) + (((int)threadIdx.x) * 896)) + (rv0 * 28)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 20, 32, 28), \"float32\"), adaptive_pool_max: T.Buffer((8, 20, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(160):\n            adaptive_pool_max_1 = T.Buffer((160,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(32, 28):\n                data_1 = T.Buffer((143360,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused], data_1[ax0_ax1_fused_ax2_fused_ax3_fused * 896 + rv0 * 28 + rv1])",
        "data": "8_20_32_28"
    },
    {
        "op_name": "global_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 40; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < 8; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 16; ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(((ax0_ax1_fused_ax2_fused_ax3_fused * 128) + (rv0 * 16)) + rv1)]);\n      },\n    },\n    ((float*)adaptive_pool_avg_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = (adaptive_pool_sum[0] * 7.812500e-03f);\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((int)threadIdx.x)] = (adaptive_pool_sum[((int)threadIdx.x)] * 7.812500e-03f);\n},\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 8; ++rv0) {\n    for (int rv1 = 0; rv1 < 16; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + data[((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) * 128)) + (rv0 * 16)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 20, 8, 16), \"float32\"), adaptive_pool_avg: T.Buffer((2, 20, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(40):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n            adaptive_pool_sum_1[0] = T.float32(0)\n            for rv0, rv1 in T.grid(8, 16):\n                data_1 = T.Buffer((5120,), data=data.data)\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0_ax1_fused_ax2_fused_ax3_fused * 128 + rv0 * 16 + rv1]\n            adaptive_pool_avg_1 = T.Buffer((40,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax0_ax1_fused_ax2_fused_ax3_fused] = adaptive_pool_sum_1[0] * T.float32(0.0078125)",
        "data": "2_20_8_16"
    },
    {
        "op_name": "global_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 108; ++ax0_ax1_fused_ax2_fused) {\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < 28; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 32; ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(((ax0_ax1_fused_ax2_fused * 896) + (rv0 * 32)) + rv1)]);\n      },\n    },\n    ((float*)adaptive_pool_avg_1)[ax0_ax1_fused_ax2_fused] = (adaptive_pool_sum[0] * 1.116071e-03f);\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] * 1.116071e-03f);\n},\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 28; ++rv0) {\n    for (int rv1 = 0; rv1 < 32; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + data[((((((int)blockIdx.x) * 1792) + (((int)threadIdx.x) * 896)) + (rv0 * 32)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 36, 28, 32), \"float32\"), adaptive_pool_avg: T.Buffer((3, 36, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(108):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n            adaptive_pool_sum_1[0] = T.float32(0)\n            for rv0, rv1 in T.grid(28, 32):\n                data_1 = T.Buffer((96768,), data=data.data)\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0_ax1_fused_ax2_fused * 896 + rv0 * 32 + rv1]\n            adaptive_pool_avg_1 = T.Buffer((108,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax0_ax1_fused_ax2_fused] = adaptive_pool_sum_1[0] * T.float32(0.0011160714285714285)",
        "data": "3_36_28_32"
    },
    {
        "op_name": "global_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 320; ++ax0_ax1_fused) {\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < 8; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 28; ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(((ax0_ax1_fused * 224) + (rv0 * 28)) + rv1)]);\n      },\n    },\n    ((float*)adaptive_pool_avg_1)[ax0_ax1_fused] = (adaptive_pool_sum[0] * 4.464286e-03f);\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((int)blockIdx.x)] = (adaptive_pool_sum[((int)blockIdx.x)] * 4.464286e-03f);\n},\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 8; ++rv0) {\n    for (int rv1 = 0; rv1 < 28; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + data[((((((int)blockIdx.x) * 448) + (((int)threadIdx.x) * 224)) + (rv0 * 28)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 32, 8, 28), \"float32\"), adaptive_pool_avg: T.Buffer((10, 32, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(320):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n            adaptive_pool_sum_1[0] = T.float32(0)\n            for rv0, rv1 in T.grid(8, 28):\n                data_1 = T.Buffer((71680,), data=data.data)\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0_ax1_fused * 224 + rv0 * 28 + rv1]\n            adaptive_pool_avg_1 = T.Buffer((320,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax0_ax1_fused] = adaptive_pool_sum_1[0] * T.float32(0.004464285714285714)",
        "data": "10_32_8_28"
    },
    {
        "op_name": "global_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 96; ++ax0_ax1_fused_ax2_fused) {\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < 24; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 32; ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(((ax0_ax1_fused_ax2_fused * 768) + (rv0 * 32)) + rv1)]);\n      },\n    },\n    ((float*)adaptive_pool_avg_1)[ax0_ax1_fused_ax2_fused] = (adaptive_pool_sum[0] * 1.302083e-03f);\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] * 1.302083e-03f);\n},\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 24; ++rv0) {\n    for (int rv1 = 0; rv1 < 32; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + data[((((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + (rv0 * 32)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 16, 24, 32), \"float32\"), adaptive_pool_avg: T.Buffer((6, 16, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(96):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n            adaptive_pool_sum_1[0] = T.float32(0)\n            for rv0, rv1 in T.grid(24, 32):\n                data_1 = T.Buffer((73728,), data=data.data)\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0_ax1_fused_ax2_fused * 768 + rv0 * 32 + rv1]\n            adaptive_pool_avg_1 = T.Buffer((96,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax0_ax1_fused_ax2_fused] = adaptive_pool_sum_1[0] * T.float32(0.0013020833333333333)",
        "data": "6_16_24_32"
    },
    {
        "op_name": "global_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 80; ++ax0_ax1_fused_ax2_fused) {\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < 20; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 28; ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(((ax0_ax1_fused_ax2_fused * 560) + (rv0 * 28)) + rv1)]);\n      },\n    },\n    ((float*)adaptive_pool_avg_1)[ax0_ax1_fused_ax2_fused] = (adaptive_pool_sum[0] * 1.785714e-03f);\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] * 1.785714e-03f);\n},\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 20; ++rv0) {\n    for (int rv1 = 0; rv1 < 28; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + data[((((((int)blockIdx.x) * 1120) + (((int)threadIdx.x) * 560)) + (rv0 * 28)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 8, 20, 28), \"float32\"), adaptive_pool_avg: T.Buffer((10, 8, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(80):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n            adaptive_pool_sum_1[0] = T.float32(0)\n            for rv0, rv1 in T.grid(20, 28):\n                data_1 = T.Buffer((44800,), data=data.data)\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0_ax1_fused_ax2_fused * 560 + rv0 * 28 + rv1]\n            adaptive_pool_avg_1 = T.Buffer((80,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax0_ax1_fused_ax2_fused] = adaptive_pool_sum_1[0] * T.float32(0.0017857142857142857)",
        "data": "10_8_20_28"
    },
    {
        "op_name": "global_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 144; ++ax0_ax1_fused_ax2_fused) {\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < 28; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 8; ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(((ax0_ax1_fused_ax2_fused * 224) + (rv0 * 8)) + rv1)]);\n      },\n    },\n    ((float*)adaptive_pool_avg_1)[ax0_ax1_fused_ax2_fused] = (adaptive_pool_sum[0] * 4.464286e-03f);\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] * 4.464286e-03f);\n},\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 28; ++rv0) {\n    for (int rv1 = 0; rv1 < 8; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + data[((((((int)blockIdx.x) * 1792) + (((int)threadIdx.x) * 224)) + (rv0 * 8)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 24, 28, 8), \"float32\"), adaptive_pool_avg: T.Buffer((6, 24, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(144):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n            adaptive_pool_sum_1[0] = T.float32(0)\n            for rv0, rv1 in T.grid(28, 8):\n                data_1 = T.Buffer((32256,), data=data.data)\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0_ax1_fused_ax2_fused * 224 + rv0 * 8 + rv1]\n            adaptive_pool_avg_1 = T.Buffer((144,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax0_ax1_fused_ax2_fused] = adaptive_pool_sum_1[0] * T.float32(0.004464285714285714)",
        "data": "6_24_28_8"
    },
    {
        "op_name": "global_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 120; ++ax0_ax1_fused) {\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < 36; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 28; ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(((ax0_ax1_fused * 1008) + (rv0 * 28)) + rv1)]);\n      },\n    },\n    ((float*)adaptive_pool_avg_1)[ax0_ax1_fused] = (adaptive_pool_sum[0] * 9.920635e-04f);\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] * 9.920635e-04f);\n},\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 36; ++rv0) {\n    for (int rv1 = 0; rv1 < 28; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + data[((((((int)blockIdx.x) * 2016) + (((int)threadIdx.x) * 1008)) + (rv0 * 28)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 12, 36, 28), \"float32\"), adaptive_pool_avg: T.Buffer((10, 12, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(120):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n            adaptive_pool_sum_1[0] = T.float32(0)\n            for rv0, rv1 in T.grid(36, 28):\n                data_1 = T.Buffer((120960,), data=data.data)\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0_ax1_fused * 1008 + rv0 * 28 + rv1]\n            adaptive_pool_avg_1 = T.Buffer((120,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax0_ax1_fused] = adaptive_pool_sum_1[0] * T.float32(0.00099206349206349201)",
        "data": "10_12_36_28"
    },
    {
        "op_name": "global_pool_avg",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 288; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < 16; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 12; ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(((ax0_ax1_fused_ax2_fused_ax3_fused * 192) + (rv0 * 12)) + rv1)]);\n      },\n    },\n    ((float*)adaptive_pool_avg_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = (adaptive_pool_sum[0] * 5.208333e-03f);\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] * 5.208333e-03f);\n},\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 16; ++rv0) {\n    for (int rv1 = 0; rv1 < 12; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + data[((((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 192)) + (rv0 * 12)) + rv1)]);\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 36, 16, 12), \"float32\"), adaptive_pool_avg: T.Buffer((8, 36, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(288):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n            adaptive_pool_sum_1[0] = T.float32(0)\n            for rv0, rv1 in T.grid(16, 12):\n                data_1 = T.Buffer((55296,), data=data.data)\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0_ax1_fused_ax2_fused_ax3_fused * 192 + rv0 * 12 + rv1]\n            adaptive_pool_avg_1 = T.Buffer((288,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax0_ax1_fused_ax2_fused_ax3_fused] = adaptive_pool_sum_1[0] * T.float32(0.005208333333333333)",
        "data": "8_36_16_12"
    },
    {
        "op_name": "dilate",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t DilatedInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* DilatedInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* DilatedInput_1 = (((DLTensor*)DilatedInput)[0].data);\n  void* default_function_DilatedInput_shape = (((DLTensor*)DilatedInput)[0].shape);\n  void* default_function_DilatedInput_strides = (((DLTensor*)DilatedInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_DilatedInput_strides == NULL)) {\n  },\n  for (int32_t i1 = 0; i1 < 36; ++i1) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_1 = (((i1 * 192) + (i2 * 24)) + i3);\n        ((float*)DilatedInput_1)[cse_var_1] = ((float*)data_1)[cse_var_1];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data) {\n  DilatedInput[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 36, 8, 24), \"float32\"), DilatedInput: T.Buffer((1, 36, 8, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i1, i2, i3 in T.grid(36, 8, 24):\n            cse_var_1: T.int32 = i1 * 192 + i2 * 24 + i3\n            DilatedInput_1 = T.Buffer((6912,), data=DilatedInput.data)\n            data_1 = T.Buffer((6912,), data=data.data)\n            DilatedInput_1[cse_var_1] = data_1[cse_var_1]",
        "data": "1_36_8_24"
    },
    {
        "op_name": "flatten",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i_j_fused = 0; i_j_fused < 201600; ++i_j_fused) {\n    ((float*)compute_1)[i_j_fused] = ((float*)data_1)[i_j_fused];\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 40, 20, 28), \"float32\"), compute: T.Buffer((9, 22400), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i_j_fused in T.parallel(201600):\n            compute_1 = T.Buffer((201600,), data=compute.data)\n            data_1 = T.Buffer((201600,), data=data.data)\n            compute_1[i_j_fused] = data_1[i_j_fused]",
        "data": "9_40_20_28"
    },
    {
        "op_name": "flatten",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i_j_fused = 0; i_j_fused < 9600; ++i_j_fused) {\n    ((float*)compute_1)[i_j_fused] = ((float*)data_1)[i_j_fused];\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 12, 40, 4), \"float32\"), compute: T.Buffer((5, 1920), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i_j_fused in T.parallel(9600):\n            compute_1 = T.Buffer((9600,), data=compute.data)\n            data_1 = T.Buffer((9600,), data=data.data)\n            compute_1[i_j_fused] = data_1[i_j_fused]",
        "data": "5_12_40_4"
    },
    {
        "op_name": "flatten",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i = 0; i < 2; ++i) {\n    for (int32_t j = 0; j < 1280; ++j) {\n      int32_t cse_var_1 = ((i * 1280) + j);\n      ((float*)compute_1)[cse_var_1] = ((float*)data_1)[cse_var_1];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 4, 20, 16), \"float32\"), compute: T.Buffer((2, 1280), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i in T.parallel(2):\n            for j in range(1280):\n                cse_var_1: T.int32 = i * 1280 + j\n                compute_1 = T.Buffer((2560,), data=compute.data)\n                data_1 = T.Buffer((2560,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1]",
        "data": "2_4_20_16"
    },
    {
        "op_name": "flatten",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t j = 0; j < 384; ++j) {\n    ((float*)compute_1)[j] = ((float*)data_1)[j];\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 4, 12, 8), \"float32\"), compute: T.Buffer((1, 384), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for j in range(384):\n            compute_1 = T.Buffer((384,), data=compute.data)\n            data_1 = T.Buffer((384,), data=data.data)\n            compute_1[j] = data_1[j]",
        "data": "1_4_12_8"
    },
    {
        "op_name": "flatten",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i = 0; i < 6; ++i) {\n    for (int32_t j = 0; j < 5760; ++j) {\n      int32_t cse_var_1 = ((i * 5760) + j);\n      ((float*)compute_1)[cse_var_1] = ((float*)data_1)[cse_var_1];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 36, 4, 40), \"float32\"), compute: T.Buffer((6, 5760), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i in T.parallel(6):\n            for j in range(5760):\n                cse_var_1: T.int32 = i * 5760 + j\n                compute_1 = T.Buffer((34560,), data=compute.data)\n                data_1 = T.Buffer((34560,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1]",
        "data": "6_36_4_40"
    },
    {
        "op_name": "fifo_buffer",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t buffer_code = arg_type_ids[1];\n  int32_t new_buffer_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* buffer = (((TVMValue*)args)[1].v_handle);\n  void* new_buffer = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* buffer_1 = (((DLTensor*)buffer)[0].data);\n  void* default_function_buffer_shape = (((DLTensor*)buffer)[0].shape);\n  void* default_function_buffer_strides = (((DLTensor*)buffer)[0].strides);\n  void* new_buffer_1 = (((DLTensor*)new_buffer)[0].data);\n  void* default_function_new_buffer_shape = (((DLTensor*)new_buffer)[0].shape);\n  void* default_function_new_buffer_strides = (((DLTensor*)new_buffer)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_buffer_strides == NULL)) {\n  },\n  if (!(default_function_new_buffer_strides == NULL)) {\n  },\n  for (int32_t i_j_fused_k_fused = 0; i_j_fused_k_fused < 1920; ++i_j_fused_k_fused) {\n    for (int32_t l = 0; l < 36; ++l) {\n      int32_t cse_var_1 = ((i_j_fused_k_fused * 36) + l);\n      ((float*)new_buffer_1)[cse_var_1] = ((float*)data_1)[cse_var_1];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 40, 8, 36), \"float32\"), buffer: T.Buffer((6, 40, 8, 36), \"float32\"), new_buffer: T.Buffer((6, 40, 8, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i_j_fused_k_fused in T.parallel(1920):\n            for l in range(36):\n                cse_var_1: T.int32 = i_j_fused_k_fused * 36 + l\n                new_buffer_1 = T.Buffer((69120,), data=new_buffer.data)\n                data_1 = T.Buffer((69120,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]",
        "data": "6_40_8_36",
        "buffer": "6_40_8_36"
    },
    {
        "op_name": "fifo_buffer",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t buffer_code = arg_type_ids[1];\n  int32_t new_buffer_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* buffer = (((TVMValue*)args)[1].v_handle);\n  void* new_buffer = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* buffer_1 = (((DLTensor*)buffer)[0].data);\n  void* default_function_buffer_shape = (((DLTensor*)buffer)[0].shape);\n  void* default_function_buffer_strides = (((DLTensor*)buffer)[0].strides);\n  void* new_buffer_1 = (((DLTensor*)new_buffer)[0].data);\n  void* default_function_new_buffer_shape = (((DLTensor*)new_buffer)[0].shape);\n  void* default_function_new_buffer_strides = (((DLTensor*)new_buffer)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_buffer_strides == NULL)) {\n  },\n  if (!(default_function_new_buffer_strides == NULL)) {\n  },\n  for (int32_t i_j_fused_k_fused = 0; i_j_fused_k_fused < 8192; ++i_j_fused_k_fused) {\n    for (int32_t l = 0; l < 24; ++l) {\n      int32_t cse_var_1 = ((i_j_fused_k_fused * 24) + l);\n      ((float*)new_buffer_1)[cse_var_1] = ((float*)data_1)[cse_var_1];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 32, 32, 24), \"float32\"), buffer: T.Buffer((8, 32, 32, 24), \"float32\"), new_buffer: T.Buffer((8, 32, 32, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i_j_fused_k_fused in T.parallel(8192):\n            for l in range(24):\n                cse_var_1: T.int32 = i_j_fused_k_fused * 24 + l\n                new_buffer_1 = T.Buffer((196608,), data=new_buffer.data)\n                data_1 = T.Buffer((196608,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]",
        "data": "8_32_32_24",
        "buffer": "8_32_32_24"
    },
    {
        "op_name": "fifo_buffer",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t buffer_code = arg_type_ids[1];\n  int32_t new_buffer_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* buffer = (((TVMValue*)args)[1].v_handle);\n  void* new_buffer = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* buffer_1 = (((DLTensor*)buffer)[0].data);\n  void* default_function_buffer_shape = (((DLTensor*)buffer)[0].shape);\n  void* default_function_buffer_strides = (((DLTensor*)buffer)[0].strides);\n  void* new_buffer_1 = (((DLTensor*)new_buffer)[0].data);\n  void* default_function_new_buffer_shape = (((DLTensor*)new_buffer)[0].shape);\n  void* default_function_new_buffer_strides = (((DLTensor*)new_buffer)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_buffer_strides == NULL)) {\n  },\n  if (!(default_function_new_buffer_strides == NULL)) {\n  },\n  for (int32_t j = 0; j < 8; ++j) {\n    for (int32_t k = 0; k < 28; ++k) {\n      for (int32_t l = 0; l < 24; ++l) {\n        int32_t cse_var_1 = (((j * 672) + (k * 24)) + l);\n        ((float*)new_buffer_1)[cse_var_1] = ((float*)data_1)[cse_var_1];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 8, 28, 24), \"float32\"), buffer: T.Buffer((1, 8, 28, 24), \"float32\"), new_buffer: T.Buffer((1, 8, 28, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for j, k, l in T.grid(8, 28, 24):\n            cse_var_1: T.int32 = j * 672 + k * 24 + l\n            new_buffer_1 = T.Buffer((5376,), data=new_buffer.data)\n            data_1 = T.Buffer((5376,), data=data.data)\n            new_buffer_1[cse_var_1] = data_1[cse_var_1]",
        "data": "1_8_28_24",
        "buffer": "1_8_28_24"
    },
    {
        "op_name": "fifo_buffer",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t buffer_code = arg_type_ids[1];\n  int32_t new_buffer_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* buffer = (((TVMValue*)args)[1].v_handle);\n  void* new_buffer = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* buffer_1 = (((DLTensor*)buffer)[0].data);\n  void* default_function_buffer_shape = (((DLTensor*)buffer)[0].shape);\n  void* default_function_buffer_strides = (((DLTensor*)buffer)[0].strides);\n  void* new_buffer_1 = (((DLTensor*)new_buffer)[0].data);\n  void* default_function_new_buffer_shape = (((DLTensor*)new_buffer)[0].shape);\n  void* default_function_new_buffer_strides = (((DLTensor*)new_buffer)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_buffer_strides == NULL)) {\n  },\n  if (!(default_function_new_buffer_strides == NULL)) {\n  },\n  for (int32_t i_j_fused_k_fused = 0; i_j_fused_k_fused < 4320; ++i_j_fused_k_fused) {\n    int32_t cse_var_1 = (i_j_fused_k_fused * 12);\n    int32_t12 v_ = int32_t12((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7), (cse_var_1)+(1*8), (cse_var_1)+(1*9), (cse_var_1)+(1*10), (cse_var_1)+(1*11));\n    *(float12*)(((float*)new_buffer_1) + cse_var_1) = (float12(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb]));\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 36, 20, 12), \"float32\"), buffer: T.Buffer((6, 36, 20, 12), \"float32\"), new_buffer: T.Buffer((6, 36, 20, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i_j_fused_k_fused in T.parallel(4320):\n            cse_var_1: T.int32 = i_j_fused_k_fused * 12\n            new_buffer_1 = T.Buffer((51840,), data=new_buffer.data)\n            data_1 = T.Buffer((51840,), data=data.data)\n            new_buffer_1[cse_var_1:cse_var_1 + 12] = data_1[cse_var_1:cse_var_1 + 12]",
        "data": "6_36_20_12",
        "buffer": "6_36_20_12"
    },
    {
        "op_name": "concatenate",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_a_code = arg_type_ids[0];\n  int32_t data_b_code = arg_type_ids[1];\n  int32_t T_concat_code = arg_type_ids[2];\n  void* data_a = (((TVMValue*)args)[0].v_handle);\n  void* data_b = (((TVMValue*)args)[1].v_handle);\n  void* T_concat = (((TVMValue*)args)[2].v_handle);\n  void* data_a_1 = (((DLTensor*)data_a)[0].data);\n  void* default_function_data_a_shape = (((DLTensor*)data_a)[0].shape);\n  void* default_function_data_a_strides = (((DLTensor*)data_a)[0].strides);\n  int32_t dev_id = (((DLTensor*)data_a)[0].device.device_id);\n  void* data_b_1 = (((DLTensor*)data_b)[0].data);\n  void* default_function_data_b_shape = (((DLTensor*)data_b)[0].shape);\n  void* default_function_data_b_strides = (((DLTensor*)data_b)[0].strides);\n  void* T_concat_1 = (((DLTensor*)T_concat)[0].data);\n  void* default_function_T_concat_shape = (((DLTensor*)T_concat)[0].shape);\n  void* default_function_T_concat_strides = (((DLTensor*)T_concat)[0].strides);\n  if (!(default_function_data_a_strides == NULL)) {\n  },\n  if (!(default_function_data_b_strides == NULL)) {\n  },\n  if (!(default_function_T_concat_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 4; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 28; ++ax2) {\n        int32_t cse_var_1 = (((ax0 * 112) + (ax1 * 28)) + ax2);\n        ((float*)T_concat_1)[cse_var_1] = ((7 <= ax0) ? ((float*)data_b_1)[(cse_var_1 - 784)] : ((float*)data_a_1)[cse_var_1]);\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ data_a, float* __restrict__ data_b);\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ data_a, float* __restrict__ data_b) {\n  T_concat[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] = ((16 <= ((int)blockIdx.x)) ? data_b[(((((int)blockIdx.x) * 49) + ((int)threadIdx.x)) - 784)] : data_a[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data_a: T.Buffer((7, 4, 28), \"float32\"), data_b: T.Buffer((7, 4, 28), \"float32\"), T_concat: T.Buffer((14, 4, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(14):\n            for ax1, ax2 in T.grid(4, 28):\n                cse_var_1: T.int32 = ax0 * 112 + ax1 * 28 + ax2\n                T_concat_1 = T.Buffer((1568,), data=T_concat.data)\n                data_b_1 = T.Buffer((784,), data=data_b.data)\n                data_a_1 = T.Buffer((784,), data=data_a.data)\n                T_concat_1[cse_var_1] = T.if_then_else(7 <= ax0, data_b_1[cse_var_1 - 784], data_a_1[cse_var_1])",
        "data_a": "7_4_28",
        "data_b": "7_4_28"
    },
    {
        "op_name": "concatenate",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_a_code = arg_type_ids[0];\n  int32_t data_b_code = arg_type_ids[1];\n  int32_t T_concat_code = arg_type_ids[2];\n  void* data_a = (((TVMValue*)args)[0].v_handle);\n  void* data_b = (((TVMValue*)args)[1].v_handle);\n  void* T_concat = (((TVMValue*)args)[2].v_handle);\n  void* data_a_1 = (((DLTensor*)data_a)[0].data);\n  void* default_function_data_a_shape = (((DLTensor*)data_a)[0].shape);\n  void* default_function_data_a_strides = (((DLTensor*)data_a)[0].strides);\n  int32_t dev_id = (((DLTensor*)data_a)[0].device.device_id);\n  void* data_b_1 = (((DLTensor*)data_b)[0].data);\n  void* default_function_data_b_shape = (((DLTensor*)data_b)[0].shape);\n  void* default_function_data_b_strides = (((DLTensor*)data_b)[0].strides);\n  void* T_concat_1 = (((DLTensor*)T_concat)[0].data);\n  void* default_function_T_concat_shape = (((DLTensor*)T_concat)[0].shape);\n  void* default_function_T_concat_strides = (((DLTensor*)T_concat)[0].strides);\n  if (!(default_function_data_a_strides == NULL)) {\n  },\n  if (!(default_function_data_b_strides == NULL)) {\n  },\n  if (!(default_function_T_concat_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 28; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n        int32_t cse_var_1 = (((ax0 * 560) + (ax1 * 20)) + ax2);\n        ((float*)T_concat_1)[cse_var_1] = ((4 <= ax0) ? ((float*)data_b_1)[(cse_var_1 - 2240)] : ((float*)data_a_1)[cse_var_1]);\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ data_a, float* __restrict__ data_b);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ data_a, float* __restrict__ data_b) {\n  T_concat[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((35 <= ((int)blockIdx.x)) ? data_b[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) - 2240)] : data_a[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data_a: T.Buffer((4, 28, 20), \"float32\"), data_b: T.Buffer((4, 28, 20), \"float32\"), T_concat: T.Buffer((8, 28, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(8):\n            for ax1, ax2 in T.grid(28, 20):\n                cse_var_1: T.int32 = ax0 * 560 + ax1 * 20 + ax2\n                T_concat_1 = T.Buffer((4480,), data=T_concat.data)\n                data_b_1 = T.Buffer((2240,), data=data_b.data)\n                data_a_1 = T.Buffer((2240,), data=data_a.data)\n                T_concat_1[cse_var_1] = T.if_then_else(4 <= ax0, data_b_1[cse_var_1 - 2240], data_a_1[cse_var_1])",
        "data_a": "4_28_20",
        "data_b": "4_28_20"
    },
    {
        "op_name": "concatenate",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_a_code = arg_type_ids[0];\n  int32_t data_b_code = arg_type_ids[1];\n  int32_t T_concat_code = arg_type_ids[2];\n  void* data_a = (((TVMValue*)args)[0].v_handle);\n  void* data_b = (((TVMValue*)args)[1].v_handle);\n  void* T_concat = (((TVMValue*)args)[2].v_handle);\n  void* data_a_1 = (((DLTensor*)data_a)[0].data);\n  void* default_function_data_a_shape = (((DLTensor*)data_a)[0].shape);\n  void* default_function_data_a_strides = (((DLTensor*)data_a)[0].strides);\n  int32_t dev_id = (((DLTensor*)data_a)[0].device.device_id);\n  void* data_b_1 = (((DLTensor*)data_b)[0].data);\n  void* default_function_data_b_shape = (((DLTensor*)data_b)[0].shape);\n  void* default_function_data_b_strides = (((DLTensor*)data_b)[0].strides);\n  void* T_concat_1 = (((DLTensor*)T_concat)[0].data);\n  void* default_function_T_concat_shape = (((DLTensor*)T_concat)[0].shape);\n  void* default_function_T_concat_strides = (((DLTensor*)T_concat)[0].strides);\n  if (!(default_function_data_a_strides == NULL)) {\n  },\n  if (!(default_function_data_b_strides == NULL)) {\n  },\n  if (!(default_function_T_concat_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 32; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n        int32_t cse_var_1 = (((ax0 * 384) + (ax1 * 12)) + ax2);\n        ((float*)T_concat_1)[cse_var_1] = ((7 <= ax0) ? ((float*)data_b_1)[(cse_var_1 - 2688)] : ((float*)data_a_1)[cse_var_1]);\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ data_a, float* __restrict__ data_b);\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ data_a, float* __restrict__ data_b) {\n  T_concat[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = ((64 <= ((int)blockIdx.x)) ? data_b[(((((int)blockIdx.x) * 42) + ((int)threadIdx.x)) - 2688)] : data_a[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data_a: T.Buffer((7, 32, 12), \"float32\"), data_b: T.Buffer((7, 32, 12), \"float32\"), T_concat: T.Buffer((14, 32, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(14):\n            for ax1, ax2 in T.grid(32, 12):\n                cse_var_1: T.int32 = ax0 * 384 + ax1 * 12 + ax2\n                T_concat_1 = T.Buffer((5376,), data=T_concat.data)\n                data_b_1 = T.Buffer((2688,), data=data_b.data)\n                data_a_1 = T.Buffer((2688,), data=data_a.data)\n                T_concat_1[cse_var_1] = T.if_then_else(7 <= ax0, data_b_1[cse_var_1 - 2688], data_a_1[cse_var_1])",
        "data_a": "7_32_12",
        "data_b": "7_32_12"
    },
    {
        "op_name": "concatenate",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_a_code = arg_type_ids[0];\n  int32_t data_b_code = arg_type_ids[1];\n  int32_t T_concat_code = arg_type_ids[2];\n  void* data_a = (((TVMValue*)args)[0].v_handle);\n  void* data_b = (((TVMValue*)args)[1].v_handle);\n  void* T_concat = (((TVMValue*)args)[2].v_handle);\n  void* data_a_1 = (((DLTensor*)data_a)[0].data);\n  void* default_function_data_a_shape = (((DLTensor*)data_a)[0].shape);\n  void* default_function_data_a_strides = (((DLTensor*)data_a)[0].strides);\n  int32_t dev_id = (((DLTensor*)data_a)[0].device.device_id);\n  void* data_b_1 = (((DLTensor*)data_b)[0].data);\n  void* default_function_data_b_shape = (((DLTensor*)data_b)[0].shape);\n  void* default_function_data_b_strides = (((DLTensor*)data_b)[0].strides);\n  void* T_concat_1 = (((DLTensor*)T_concat)[0].data);\n  void* default_function_T_concat_shape = (((DLTensor*)T_concat)[0].shape);\n  void* default_function_T_concat_strides = (((DLTensor*)T_concat)[0].strides);\n  if (!(default_function_data_a_strides == NULL)) {\n  },\n  if (!(default_function_data_b_strides == NULL)) {\n  },\n  if (!(default_function_T_concat_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 28; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 24; ++ax2) {\n        int32_t cse_var_1 = (((ax0 * 672) + (ax1 * 24)) + ax2);\n        ((float*)T_concat_1)[cse_var_1] = ((4 <= ax0) ? ((float*)data_b_1)[(cse_var_1 - 2688)] : ((float*)data_a_1)[cse_var_1]);\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ data_a, float* __restrict__ data_b);\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ data_a, float* __restrict__ data_b) {\n  T_concat[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = ((64 <= ((int)blockIdx.x)) ? data_b[(((((int)blockIdx.x) * 42) + ((int)threadIdx.x)) - 2688)] : data_a[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data_a: T.Buffer((4, 28, 24), \"float32\"), data_b: T.Buffer((4, 28, 24), \"float32\"), T_concat: T.Buffer((8, 28, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(8):\n            for ax1, ax2 in T.grid(28, 24):\n                cse_var_1: T.int32 = ax0 * 672 + ax1 * 24 + ax2\n                T_concat_1 = T.Buffer((5376,), data=T_concat.data)\n                data_b_1 = T.Buffer((2688,), data=data_b.data)\n                data_a_1 = T.Buffer((2688,), data=data_a.data)\n                T_concat_1[cse_var_1] = T.if_then_else(4 <= ax0, data_b_1[cse_var_1 - 2688], data_a_1[cse_var_1])",
        "data_a": "4_28_24",
        "data_b": "4_28_24"
    },
    {
        "op_name": "depth_to_space",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t depth_to_space_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* depth_to_space = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* depth_to_space_1 = (((DLTensor*)depth_to_space)[0].data);\n  void* default_function_depth_to_space_shape = (((DLTensor*)depth_to_space)[0].shape);\n  void* default_function_depth_to_space_strides = (((DLTensor*)depth_to_space)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_depth_to_space_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 800; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 48; ++i3) {\n      int32_t cse_var_1 = (i0_i1_fused_i2_fused % 80);\n      ((float*)depth_to_space_1)[((i0_i1_fused_i2_fused * 48) + i3)] = ((float*)data_1)[(((((((i0_i1_fused_i2_fused / 400) * 19200) + ((cse_var_1 % 2) * 9600)) + ((i3 % 2) * 4800)) + (((i0_i1_fused_i2_fused % 400) / 80) * 960)) + ((cse_var_1 / 2) * 24)) + (i3 / 2))];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) / 320) * 19200) + ((((((((int)blockIdx.x) & 63) * 5) + (((int)threadIdx.x) / 12)) >> 2) % 2) * 9600)) + (((((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) % 48) % 2) * 4800)) + (((((int)blockIdx.x) % 320) >> 6) * 960)) + ((((((((int)blockIdx.x) & 63) * 5) + (((int)threadIdx.x) / 12)) >> 2) / 2) * 24)) + ((((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) % 48) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 20, 40, 24), \"float32\"), depth_to_space: T.Buffer((2, 5, 80, 48), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(800):\n            for i3 in range(48):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused % 80\n                depth_to_space_1 = T.Buffer((38400,), data=depth_to_space.data)\n                data_1 = T.Buffer((38400,), data=data.data)\n                depth_to_space_1[i0_i1_fused_i2_fused * 48 + i3] = data_1[i0_i1_fused_i2_fused // 400 * 19200 + T.truncmod(cse_var_1, 2) * 9600 + T.truncmod(i3, 2) * 4800 + i0_i1_fused_i2_fused % 400 // 80 * 960 + T.Div(cse_var_1, 2) * 24 + T.Div(i3, 2)]",
        "data": "2_20_40_24"
    },
    {
        "op_name": "depth_to_space",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t depth_to_space_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* depth_to_space = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* depth_to_space_1 = (((DLTensor*)depth_to_space)[0].data);\n  void* default_function_depth_to_space_shape = (((DLTensor*)depth_to_space)[0].shape);\n  void* default_function_depth_to_space_strides = (((DLTensor*)depth_to_space)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_depth_to_space_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1600; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 80; ++i3) {\n      int32_t cse_var_1 = (i0_i1_fused_i2_fused & 31);\n      ((float*)depth_to_space_1)[((i0_i1_fused_i2_fused * 80) + i3)] = ((float*)data_1)[(((((((i0_i1_fused_i2_fused / 160) * 12800) + ((cse_var_1 % 2) * 6400)) + ((i3 % 2) * 3200)) + (((i0_i1_fused_i2_fused % 160) >> 5) * 640)) + ((cse_var_1 / 2) * 40)) + (i3 / 2))];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) / 200) * 12800) + ((((((((int)blockIdx.x) % 40) * 4) + (((int)threadIdx.x) >> 4)) / 5) % 2) * 6400)) + (((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 80) % 2) * 3200)) + (((((int)blockIdx.x) % 200) / 40) * 640)) + ((((((((int)blockIdx.x) % 40) * 4) + (((int)threadIdx.x) >> 4)) / 5) / 2) * 40)) + ((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 80) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 20, 16, 40), \"float32\"), depth_to_space: T.Buffer((10, 5, 32, 80), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(1600):\n            for i3 in range(80):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused % 32\n                depth_to_space_1 = T.Buffer((128000,), data=depth_to_space.data)\n                data_1 = T.Buffer((128000,), data=data.data)\n                depth_to_space_1[i0_i1_fused_i2_fused * 80 + i3] = data_1[i0_i1_fused_i2_fused // 160 * 12800 + T.truncmod(cse_var_1, 2) * 6400 + T.truncmod(i3, 2) * 3200 + i0_i1_fused_i2_fused % 160 // 32 * 640 + T.Div(cse_var_1, 2) * 40 + T.Div(i3, 2)]",
        "data": "10_20_16_40"
    },
    {
        "op_name": "depth_to_space",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t depth_to_space_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* depth_to_space = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* depth_to_space_1 = (((DLTensor*)depth_to_space)[0].data);\n  void* default_function_depth_to_space_shape = (((DLTensor*)depth_to_space)[0].shape);\n  void* default_function_depth_to_space_strides = (((DLTensor*)depth_to_space)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_depth_to_space_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 18; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      for (int32_t i3 = 0; i3 < 32; ++i3) {\n        ((float*)depth_to_space_1)[(((i0_i1_fused * 512) + (i2 * 32)) + i3)] = ((float*)data_1)[(((((((i0_i1_fused / 9) * 4608) + ((i2 % 2) * 2304)) + ((i3 % 2) * 1152)) + ((i0_i1_fused % 9) * 128)) + ((i2 / 2) * 16)) + (i3 / 2))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) / 192) * 4608) + ((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 3)) & 63) >> 2) % 2) * 2304)) + (((((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) & 31) % 2) * 1152)) + (((((((int)blockIdx.x) % 192) * 3) + (((int)threadIdx.x) >> 3)) >> 6) * 128)) + ((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 3)) & 63) >> 2) / 2) * 16)) + ((((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) & 31) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 36, 8, 16), \"float32\"), depth_to_space: T.Buffer((2, 9, 16, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(18):\n            for i2, i3 in T.grid(16, 32):\n                depth_to_space_1 = T.Buffer((9216,), data=depth_to_space.data)\n                data_1 = T.Buffer((9216,), data=data.data)\n                depth_to_space_1[i0_i1_fused * 512 + i2 * 32 + i3] = data_1[i0_i1_fused // 9 * 4608 + T.truncmod(i2, 2) * 2304 + T.truncmod(i3, 2) * 1152 + i0_i1_fused % 9 * 128 + T.Div(i2, 2) * 16 + T.Div(i3, 2)]",
        "data": "2_36_8_16"
    },
    {
        "op_name": "depth_to_space",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t depth_to_space_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* depth_to_space = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* depth_to_space_1 = (((DLTensor*)depth_to_space)[0].data);\n  void* default_function_depth_to_space_shape = (((DLTensor*)depth_to_space)[0].shape);\n  void* default_function_depth_to_space_strides = (((DLTensor*)depth_to_space)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_depth_to_space_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 40; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 80; ++i2) {\n      for (int32_t i3 = 0; i3 < 40; ++i3) {\n        ((float*)depth_to_space_1)[(((i0_i1_fused * 3200) + (i2 * 40)) + i3)] = ((float*)data_1)[(((((((i0_i1_fused / 10) * 32000) + ((i2 % 2) * 16000)) + ((i3 % 2) * 8000)) + ((i0_i1_fused % 10) * 800)) + ((i2 / 2) * 20)) + (i3 / 2))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) / 500) * 32000) + ((((((((int)blockIdx.x) % 50) * 8) + (((int)threadIdx.x) >> 3)) / 5) % 2) * 16000)) + (((((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 40) % 2) * 8000)) + (((((int)blockIdx.x) % 500) / 50) * 800)) + ((((((((int)blockIdx.x) % 50) * 8) + (((int)threadIdx.x) >> 3)) / 5) / 2) * 20)) + ((((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 40) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 40, 40, 20), \"float32\"), depth_to_space: T.Buffer((4, 10, 80, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(40):\n            for i2, i3 in T.grid(80, 40):\n                depth_to_space_1 = T.Buffer((128000,), data=depth_to_space.data)\n                data_1 = T.Buffer((128000,), data=data.data)\n                depth_to_space_1[i0_i1_fused * 3200 + i2 * 40 + i3] = data_1[i0_i1_fused // 10 * 32000 + T.truncmod(i2, 2) * 16000 + T.truncmod(i3, 2) * 8000 + i0_i1_fused % 10 * 800 + T.Div(i2, 2) * 20 + T.Div(i3, 2)]",
        "data": "4_40_40_20"
    },
    {
        "op_name": "depth_to_space",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t depth_to_space_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* depth_to_space = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* depth_to_space_1 = (((DLTensor*)depth_to_space)[0].data);\n  void* default_function_depth_to_space_shape = (((DLTensor*)depth_to_space)[0].shape);\n  void* default_function_depth_to_space_strides = (((DLTensor*)depth_to_space)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_depth_to_space_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 40; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 64; ++i2) {\n      for (int32_t i3 = 0; i3 < 56; ++i3) {\n        ((float*)depth_to_space_1)[(((i0_i1_fused * 3584) + (i2 * 56)) + i3)] = ((float*)data_1)[(((((((i0_i1_fused / 5) * 17920) + ((i2 % 2) * 8960)) + ((i3 % 2) * 4480)) + ((i0_i1_fused % 5) * 896)) + ((i2 / 2) * 28)) + (i3 / 2))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space);\nextern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) >> 9) * 17920) + ((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 7)) & 511) >> 3) % 2) * 8960)) + (((((((int)blockIdx.x) * 35) + ((int)threadIdx.x)) % 56) % 2) * 4480)) + (((((((int)blockIdx.x) & 511) * 5) + (((int)threadIdx.x) / 7)) >> 9) * 896)) + ((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 7)) & 511) >> 3) / 2) * 28)) + ((((((int)blockIdx.x) * 35) + ((int)threadIdx.x)) % 56) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 20, 32, 28), \"float32\"), depth_to_space: T.Buffer((8, 5, 64, 56), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(40):\n            for i2, i3 in T.grid(64, 56):\n                depth_to_space_1 = T.Buffer((143360,), data=depth_to_space.data)\n                data_1 = T.Buffer((143360,), data=data.data)\n                depth_to_space_1[i0_i1_fused * 3584 + i2 * 56 + i3] = data_1[i0_i1_fused // 5 * 17920 + T.truncmod(i2, 2) * 8960 + T.truncmod(i3, 2) * 4480 + i0_i1_fused % 5 * 896 + T.Div(i2, 2) * 28 + T.Div(i3, 2)]",
        "data": "8_20_32_28"
    },
    {
        "op_name": "depth_to_space",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t depth_to_space_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* depth_to_space = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* depth_to_space_1 = (((DLTensor*)depth_to_space)[0].data);\n  void* default_function_depth_to_space_shape = (((DLTensor*)depth_to_space)[0].shape);\n  void* default_function_depth_to_space_strides = (((DLTensor*)depth_to_space)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_depth_to_space_strides == NULL)) {\n  },\n  for (int32_t i1 = 0; i1 < 8; ++i1) {\n    for (int32_t i2 = 0; i2 < 72; ++i2) {\n      for (int32_t i3 = 0; i3 < 48; ++i3) {\n        ((float*)depth_to_space_1)[(((i1 * 3456) + (i2 * 48)) + i3)] = ((float*)data_1)[((((((i2 % 2) * 13824) + ((i3 % 2) * 6912)) + (i1 * 864)) + ((i2 / 2) * 24)) + (i3 / 2))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) % 72) % 2) * 13824) + ((((int)threadIdx.x) % 2) * 6912)) + ((((int)blockIdx.x) / 72) * 864)) + (((((int)blockIdx.x) % 72) / 2) * 24)) + (((int)threadIdx.x) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 32, 36, 24), \"float32\"), depth_to_space: T.Buffer((1, 8, 72, 48), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i1, i2, i3 in T.grid(8, 72, 48):\n            depth_to_space_1 = T.Buffer((27648,), data=depth_to_space.data)\n            data_1 = T.Buffer((27648,), data=data.data)\n            depth_to_space_1[i1 * 3456 + i2 * 48 + i3] = data_1[T.truncmod(i2, 2) * 13824 + T.truncmod(i3, 2) * 6912 + i1 * 864 + T.Div(i2, 2) * 24 + T.Div(i3, 2)]",
        "data": "1_32_36_24"
    },
    {
        "op_name": "depth_to_space",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t depth_to_space_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* depth_to_space = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* depth_to_space_1 = (((DLTensor*)depth_to_space)[0].data);\n  void* default_function_depth_to_space_shape = (((DLTensor*)depth_to_space)[0].shape);\n  void* default_function_depth_to_space_strides = (((DLTensor*)depth_to_space)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_depth_to_space_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 768; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 72; ++i3) {\n      int32_t cse_var_1 = (i0_i1_fused_i2_fused % 24);\n      ((float*)depth_to_space_1)[((i0_i1_fused_i2_fused * 72) + i3)] = ((float*)data_1)[(((((((i0_i1_fused_i2_fused / 96) * 6912) + ((cse_var_1 % 2) * 3456)) + ((i3 % 2) * 1728)) + (((i0_i1_fused_i2_fused % 96) / 24) * 432)) + ((cse_var_1 / 2) * 36)) + (i3 / 2))];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) >> 7) * 6912) + ((((((((int)blockIdx.x) & 31) * 3) + (((int)threadIdx.x) / 18)) >> 2) % 2) * 3456)) + (((((((int)blockIdx.x) * 54) + ((int)threadIdx.x)) % 72) % 2) * 1728)) + (((((int)blockIdx.x) & 127) >> 5) * 432)) + ((((((((int)blockIdx.x) & 31) * 3) + (((int)threadIdx.x) / 18)) >> 2) / 2) * 36)) + ((((((int)blockIdx.x) * 54) + ((int)threadIdx.x)) % 72) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 16, 12, 36), \"float32\"), depth_to_space: T.Buffer((8, 4, 24, 72), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(768):\n            for i3 in range(72):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused % 24\n                depth_to_space_1 = T.Buffer((55296,), data=depth_to_space.data)\n                data_1 = T.Buffer((55296,), data=data.data)\n                depth_to_space_1[i0_i1_fused_i2_fused * 72 + i3] = data_1[i0_i1_fused_i2_fused // 96 * 6912 + T.truncmod(cse_var_1, 2) * 3456 + T.truncmod(i3, 2) * 1728 + i0_i1_fused_i2_fused % 96 // 24 * 432 + T.Div(cse_var_1, 2) * 36 + T.Div(i3, 2)]",
        "data": "8_16_12_36"
    },
    {
        "op_name": "depth_to_space",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t depth_to_space_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* depth_to_space = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* depth_to_space_1 = (((DLTensor*)depth_to_space)[0].data);\n  void* default_function_depth_to_space_shape = (((DLTensor*)depth_to_space)[0].shape);\n  void* default_function_depth_to_space_strides = (((DLTensor*)depth_to_space)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_depth_to_space_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        for (int32_t i3 = 0; i3 < 40; ++i3) {\n          int32_t cse_var_1 = (i0 * 1920);\n          ((float*)depth_to_space_1)[(((cse_var_1 + (i1 * 320)) + (i2 * 40)) + i3)] = ((float*)data_1)[(((((cse_var_1 + ((i2 % 2) * 960)) + ((i3 % 2) * 480)) + (i1 * 80)) + ((i2 / 2) * 20)) + (i3 / 2))];\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) >> 5) * 1920) + ((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 20)) & 15) >> 1) % 2) * 960)) + (((((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) % 40) % 2) * 480)) + (((((((int)blockIdx.x) & 31) * 3) + (((int)threadIdx.x) / 20)) >> 4) * 80)) + ((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 20)) & 15) >> 1) / 2) * 20)) + ((((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) % 40) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 24, 4, 20), \"float32\"), depth_to_space: T.Buffer((8, 6, 8, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(8):\n            for i1, i2, i3 in T.grid(6, 8, 40):\n                cse_var_1: T.int32 = i0 * 1920\n                depth_to_space_1 = T.Buffer((15360,), data=depth_to_space.data)\n                data_1 = T.Buffer((15360,), data=data.data)\n                depth_to_space_1[cse_var_1 + i1 * 320 + i2 * 40 + i3] = data_1[cse_var_1 + T.truncmod(i2, 2) * 960 + T.truncmod(i3, 2) * 480 + i1 * 80 + T.Div(i2, 2) * 20 + T.Div(i3, 2)]",
        "data": "8_24_4_20"
    },
    {
        "op_name": "leaky_relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        int32_t cse_var_1 = (((i0 * 2048) + (i1 * 128)) + (i2 * 8));\n        int32_t8 v_ = int32_t8((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7));\n        *(float8*)(((float*)compute_1) + cse_var_1) = ((((float8)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f)) < (float8(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7]))) ? (float8(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7])) : ((float8(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7])) * ((float8)(5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f))));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] * 5.000000e-01f));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 16, 16, 8), \"float32\"), compute: T.Buffer((5, 16, 16, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(16, 16):\n                cse_var_1: T.int32 = i0 * 2048 + i1 * 128 + i2 * 8\n                compute_1 = T.Buffer((10240,), data=compute.data)\n                data_1 = T.Buffer((10240,), data=data.data)\n                compute_1[cse_var_1:cse_var_1 + 8] = T.Select(T.Broadcast(T.float32(0), 8) < data_1[cse_var_1:cse_var_1 + 8], data_1[cse_var_1:cse_var_1 + 8], data_1[cse_var_1:cse_var_1 + 8] * T.Broadcast(T.float32(0.5), 8))",
        "data": "5_16_16_8"
    },
    {
        "op_name": "leaky_relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1152; ++i0_i1_fused_i2_fused) {\n    int32_t cse_var_1 = (i0_i1_fused_i2_fused * 16);\n    int32_t16 v_ = int32_t16((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7), (cse_var_1)+(1*8), (cse_var_1)+(1*9), (cse_var_1)+(1*10), (cse_var_1)+(1*11), (cse_var_1)+(1*12), (cse_var_1)+(1*13), (cse_var_1)+(1*14), (cse_var_1)+(1*15));\n    *(float16*)(((float*)compute_1) + cse_var_1) = ((((float16)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f)) < (float16(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb],((float*)data_1)[v_.sc],((float*)data_1)[v_.sd],((float*)data_1)[v_.se],((float*)data_1)[v_.sf]))) ? (float16(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb],((float*)data_1)[v_.sc],((float*)data_1)[v_.sd],((float*)data_1)[v_.se],((float*)data_1)[v_.sf])) : ((float16(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb],((float*)data_1)[v_.sc],((float*)data_1)[v_.sd],((float*)data_1)[v_.se],((float*)data_1)[v_.sf])) * ((float16)(5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f, 5.000000e-01f))));\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * 5.000000e-01f));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 16, 8, 16), \"float32\"), compute: T.Buffer((9, 16, 8, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(1152):\n            cse_var_1: T.int32 = i0_i1_fused_i2_fused * 16\n            compute_1 = T.Buffer((18432,), data=compute.data)\n            data_1 = T.Buffer((18432,), data=data.data)\n            compute_1[cse_var_1:cse_var_1 + 16] = T.Select(T.Broadcast(T.float32(0), 16) < data_1[cse_var_1:cse_var_1 + 16], data_1[cse_var_1:cse_var_1 + 16], data_1[cse_var_1:cse_var_1 + 16] * T.Broadcast(T.float32(0.5), 16))",
        "data": "9_16_8_16"
    },
    {
        "op_name": "leaky_relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 144; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 28; ++i2) {\n      for (int32_t i3 = 0; i3 < 36; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 1008) + (i2 * 36)) + i3);\n        ((float*)compute_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? ((float*)data_1)[cse_var_1] : (((float*)data_1)[cse_var_1] * 5.000000e-01f));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * 5.000000e-01f));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 36, 28, 36), \"float32\"), compute: T.Buffer((4, 36, 28, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(144):\n            for i2, i3 in T.grid(28, 36):\n                cse_var_1: T.int32 = i0_i1_fused * 1008 + i2 * 36 + i3\n                compute_1 = T.Buffer((145152,), data=compute.data)\n                data_1 = T.Buffer((145152,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * T.float32(0.5))",
        "data": "4_36_28_36"
    },
    {
        "op_name": "leaky_relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        for (int32_t i3 = 0; i3 < 8; ++i3) {\n          int32_t cse_var_1 = ((((i0 * 512) + (i1 * 128)) + (i2 * 8)) + i3);\n          ((float*)compute_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? ((float*)data_1)[cse_var_1] : (((float*)data_1)[cse_var_1] * 5.000000e-01f));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * 5.000000e-01f));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 4, 16, 8), \"float32\"), compute: T.Buffer((8, 4, 16, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(8):\n            for i1, i2, i3 in T.grid(4, 16, 8):\n                cse_var_1: T.int32 = i0 * 512 + i1 * 128 + i2 * 8 + i3\n                compute_1 = T.Buffer((4096,), data=compute.data)\n                data_1 = T.Buffer((4096,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * T.float32(0.5))",
        "data": "8_4_16_8"
    },
    {
        "op_name": "leaky_relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        for (int32_t i3 = 0; i3 < 16; ++i3) {\n          int32_t cse_var_1 = ((((i0 * 3840) + (i1 * 192)) + (i2 * 16)) + i3);\n          ((float*)compute_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? ((float*)data_1)[cse_var_1] : (((float*)data_1)[cse_var_1] * 5.000000e-01f));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] * 5.000000e-01f));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 20, 12, 16), \"float32\"), compute: T.Buffer((3, 20, 12, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(3):\n            for i1, i2, i3 in T.grid(20, 12, 16):\n                cse_var_1: T.int32 = i0 * 3840 + i1 * 192 + i2 * 16 + i3\n                compute_1 = T.Buffer((11520,), data=compute.data)\n                data_1 = T.Buffer((11520,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * T.float32(0.5))",
        "data": "3_20_12_16"
    },
    {
        "op_name": "leaky_relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i1 = 0; i1 < 28; ++i1) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      for (int32_t i3 = 0; i3 < 36; ++i3) {\n        int32_t cse_var_1 = (((i1 * 288) + (i2 * 36)) + i3);\n        ((float*)compute_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? ((float*)data_1)[cse_var_1] : (((float*)data_1)[cse_var_1] * 5.000000e-01f));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] * 5.000000e-01f));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 28, 8, 36), \"float32\"), compute: T.Buffer((1, 28, 8, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i1, i2, i3 in T.grid(28, 8, 36):\n            cse_var_1: T.int32 = i1 * 288 + i2 * 36 + i3\n            compute_1 = T.Buffer((8064,), data=compute.data)\n            data_1 = T.Buffer((8064,), data=data.data)\n            compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * T.float32(0.5))",
        "data": "1_28_8_36"
    },
    {
        "op_name": "leaky_relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 24; ++i2) {\n        for (int32_t i3 = 0; i3 < 32; ++i3) {\n          int32_t cse_var_1 = ((((i0 * 6144) + (i1 * 768)) + (i2 * 32)) + i3);\n          ((float*)compute_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? ((float*)data_1)[cse_var_1] : (((float*)data_1)[cse_var_1] * 5.000000e-01f));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] * 5.000000e-01f));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 8, 24, 32), \"float32\"), compute: T.Buffer((5, 8, 24, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(5):\n            for i1, i2, i3 in T.grid(8, 24, 32):\n                cse_var_1: T.int32 = i0 * 6144 + i1 * 768 + i2 * 32 + i3\n                compute_1 = T.Buffer((30720,), data=compute.data)\n                data_1 = T.Buffer((30720,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * T.float32(0.5))",
        "data": "5_8_24_32"
    },
    {
        "op_name": "leaky_relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 28; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 28; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 112) + (i2 * 28)) + i3);\n        ((float*)compute_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? ((float*)data_1)[cse_var_1] : (((float*)data_1)[cse_var_1] * 5.000000e-01f));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] * 5.000000e-01f));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 4, 4, 28), \"float32\"), compute: T.Buffer((7, 4, 4, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(28):\n            for i2, i3 in T.grid(4, 28):\n                cse_var_1: T.int32 = i0_i1_fused * 112 + i2 * 28 + i3\n                compute_1 = T.Buffer((3136,), data=compute.data)\n                data_1 = T.Buffer((3136,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * T.float32(0.5))",
        "data": "7_4_4_28"
    },
    {
        "op_name": "leaky_relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 24; ++i2) {\n        for (int32_t i3 = 0; i3 < 28; ++i3) {\n          int32_t cse_var_1 = ((((i0 * 5376) + (i1 * 672)) + (i2 * 28)) + i3);\n          ((float*)compute_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? ((float*)data_1)[cse_var_1] : (((float*)data_1)[cse_var_1] * 5.000000e-01f));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] * 5.000000e-01f));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 8, 24, 28), \"float32\"), compute: T.Buffer((3, 8, 24, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(3):\n            for i1, i2, i3 in T.grid(8, 24, 28):\n                cse_var_1: T.int32 = i0 * 5376 + i1 * 672 + i2 * 28 + i3\n                compute_1 = T.Buffer((16128,), data=compute.data)\n                data_1 = T.Buffer((16128,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * T.float32(0.5))",
        "data": "3_8_24_28"
    },
    {
        "op_name": "leaky_relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        for (int32_t i3 = 0; i3 < 20; ++i3) {\n          int32_t cse_var_1 = ((((i0 * 960) + (i1 * 80)) + (i2 * 20)) + i3);\n          ((float*)compute_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? ((float*)data_1)[cse_var_1] : (((float*)data_1)[cse_var_1] * 5.000000e-01f));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] * 5.000000e-01f));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 12, 4, 20), \"float32\"), compute: T.Buffer((3, 12, 4, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(3):\n            for i1, i2, i3 in T.grid(12, 4, 20):\n                cse_var_1: T.int32 = i0 * 960 + i1 * 80 + i2 * 20 + i3\n                compute_1 = T.Buffer((2880,), data=compute.data)\n                data_1 = T.Buffer((2880,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * T.float32(0.5))",
        "data": "3_12_4_20"
    },
    {
        "op_name": "log_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float logf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused = 0; i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused < 32; ++i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused) {\n    float T_softmax_maxelem[64];\n    float compute_2[64];\n    for (int32_t i0 = 0; i0 < 8; ++i0) {\n      for (int32_t i1 = 0; i1 < 4; ++i1) {\n        for (int32_t i2 = 0; i2 < 2; ++i2) {\n          T_softmax_maxelem[(((i0 * 8) + (i1 * 2)) + i2)] = -3.402823e+38f;\n          for (int32_t k = 0; k < 16; ++k) {\n            int32_t cse_var_1 = (((i0 * 8) + (i1 * 2)) + i2);\n            float v_ = T_softmax_maxelem[cse_var_1];\n            float v__1 = ((float*)data_1)[((((((i0 * 4096) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused >> 3) * 1024)) + (i1 * 256)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused & 7) * 32)) + (i2 * 16)) + k)];\n            T_softmax_maxelem[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n          },\n        },\n      },\n    },\n    for (int32_t i3_outer_outer_outer = 0; i3_outer_outer_outer < 2; ++i3_outer_outer_outer) {\n      for (int32_t i0_1 = 0; i0_1 < 8; ++i0_1) {\n        for (int32_t i1_1 = 0; i1_1 < 4; ++i1_1) {\n          for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n            compute_2[(((i0_1 * 8) + (i1_1 * 2)) + i2_1)] = 0.000000e+00f;\n            for (int32_t k_1 = 0; k_1 < 16; ++k_1) {\n              int32_t cse_var_2 = (((i0_1 * 8) + (i1_1 * 2)) + i2_1);\n              compute_2[cse_var_2] = (compute_2[cse_var_2] + expf((((float*)data_1)[((((((i0_1 * 4096) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused >> 3) * 1024)) + (i1_1 * 256)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused & 7) * 32)) + (i2_1 * 16)) + k_1)] - T_softmax_maxelem[cse_var_2])));\n            },\n          },\n        },\n      },\n      for (int32_t i3_outer_inner = 0; i3_outer_inner < 2; ++i3_outer_inner) {\n        for (int32_t i0_inner = 0; i0_inner < 8; ++i0_inner) {\n          for (int32_t i1_inner = 0; i1_inner < 4; ++i1_inner) {\n            for (int32_t i2_inner = 0; i2_inner < 2; ++i2_inner) {\n              int32_t cse_var_4 = (((i0_inner * 8) + (i1_inner * 2)) + i2_inner);\n              int32_t cse_var_3 = (((((((i0_inner * 4096) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused >> 3) * 1024)) + (i1_inner * 256)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused & 7) * 32)) + (i2_inner * 16)) + (i3_outer_outer_outer * 8)) + (i3_outer_inner * 4));\n              int32_t4 v__2 = int32_t4((cse_var_3)+(1*0), (cse_var_3)+(1*1), (cse_var_3)+(1*2), (cse_var_3)+(1*3));\n              *(float4*)(((float*)compute_1) + cse_var_3) = (((float4(((float*)data_1)[v__2.s0],((float*)data_1)[v__2.s1],((float*)data_1)[v__2.s2],((float*)data_1)[v__2.s3])) - ((float4)(T_softmax_maxelem[cse_var_4], T_softmax_maxelem[cse_var_4], T_softmax_maxelem[cse_var_4], T_softmax_maxelem[cse_var_4]))) - ((float4)(logf(compute_2[cse_var_4]), logf(compute_2[cse_var_4]), logf(compute_2[cse_var_4]), logf(compute_2[cse_var_4]))));\n            },\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 16; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) * 16)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 16; ++k) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 1024) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]) - __logf(compute_1[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 16, 16, 16), \"float32\"), compute: T.Buffer((8, 16, 16, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused in T.parallel(32):\n            T_softmax_maxelem = T.allocate([64], \"float32\", \"global\")\n            compute_1 = T.allocate([64], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((64,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((32768,), data=data.data)\n            for i0, i1, i2 in T.grid(8, 4, 2):\n                T_softmax_maxelem_1[i0 * 8 + i1 * 2 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(16):\n                    cse_var_1: T.int32 = i0 * 8 + i1 * 2 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 4096 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused // 8 * 1024 + i1 * 256 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused % 8 * 32 + i2 * 16 + k])\n            for i3_outer_outer_outer in range(2):\n                compute_2 = T.Buffer((64,), data=compute_1)\n                for i0, i1, i2 in T.grid(8, 4, 2):\n                    compute_2[i0 * 8 + i1 * 2 + i2] = T.float32(0)\n                    for k in range(16):\n                        cse_var_2: T.int32 = i0 * 8 + i1 * 2 + i2\n                        compute_2[cse_var_2] = compute_2[cse_var_2] + T.exp(data_1[i0 * 4096 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused // 8 * 1024 + i1 * 256 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused % 8 * 32 + i2 * 16 + k] - T_softmax_maxelem_1[cse_var_2])\n                for i3_outer_inner, i0_inner, i1_inner, i2_inner in T.grid(2, 8, 4, 2):\n                    cse_var_4: T.int32 = i0_inner * 8 + i1_inner * 2 + i2_inner\n                    cse_var_3: T.int32 = i0_inner * 4096 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused // 8 * 1024 + i1_inner * 256 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused % 8 * 32 + i2_inner * 16 + i3_outer_outer_outer * 8 + i3_outer_inner * 4\n                    compute_3 = T.Buffer((32768,), data=compute.data)\n                    compute_3[cse_var_3:cse_var_3 + 4] = data_1[cse_var_3:cse_var_3 + 4] - T.Broadcast(T_softmax_maxelem_1[cse_var_4], 4) - T.Broadcast(T.log(compute_2[cse_var_4]), 4)",
        "data": "8_16_16_16"
    },
    {
        "op_name": "log_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float logf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused = 0; i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused < 16; ++i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused) {\n    float T_softmax_maxelem[48];\n    float compute_2[8];\n    for (int32_t i0 = 0; i0 < 4; ++i0) {\n      for (int32_t i1 = 0; i1 < 12; ++i1) {\n        T_softmax_maxelem[((i0 * 12) + i1)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 20; ++k) {\n          int32_t cse_var_1 = ((i0 * 12) + i1);\n          float v_ = T_softmax_maxelem[cse_var_1];\n          float v__1 = ((float*)data_1)[((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused >> 3) * 7680) + (i0 * 1920)) + (i1 * 160)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused & 7) * 20)) + k)];\n          T_softmax_maxelem[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n    for (int32_t i1_outer_outer_inner = 0; i1_outer_outer_inner < 6; ++i1_outer_outer_inner) {\n      for (int32_t i0_1 = 0; i0_1 < 4; ++i0_1) {\n        for (int32_t i1_1 = 0; i1_1 < 2; ++i1_1) {\n          compute_2[((i0_1 * 2) + i1_1)] = 0.000000e+00f;\n          for (int32_t k_1 = 0; k_1 < 20; ++k_1) {\n            int32_t cse_var_2 = ((i0_1 * 2) + i1_1);\n            compute_2[cse_var_2] = (compute_2[cse_var_2] + expf((((float*)data_1)[(((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused >> 3) * 7680) + (i0_1 * 1920)) + (i1_outer_outer_inner * 320)) + (i1_1 * 160)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused & 7) * 20)) + k_1)] - T_softmax_maxelem[(((i0_1 * 12) + (i1_outer_outer_inner * 2)) + i1_1)])));\n          },\n        },\n      },\n      for (int32_t i3_outer_outer_inner = 0; i3_outer_outer_inner < 10; ++i3_outer_outer_inner) {\n        for (int32_t i0_outer_inner = 0; i0_outer_inner < 4; ++i0_outer_inner) {\n          for (int32_t i1_inner = 0; i1_inner < 2; ++i1_inner) {\n            int32_t cse_var_3 = (((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused >> 3) * 7680) + (i0_outer_inner * 1920)) + (i1_outer_outer_inner * 320)) + (i1_inner * 160)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused & 7) * 20)) + (i3_outer_outer_inner * 2));\n            int32_t2 v__2 = int32_t2((cse_var_3)+(1*0), (cse_var_3)+(1*1));\n            *(float2*)(((float*)compute_1) + cse_var_3) = (((float2(((float*)data_1)[v__2.s0],((float*)data_1)[v__2.s1])) - ((float2)(T_softmax_maxelem[(((i0_outer_inner * 12) + (i1_outer_outer_inner * 2)) + i1_inner)], T_softmax_maxelem[(((i0_outer_inner * 12) + (i1_outer_outer_inner * 2)) + i1_inner)]))) - ((float2)(logf(compute_2[((i0_outer_inner * 2) + i1_inner)]), logf(compute_2[((i0_outer_inner * 2) + i1_inner)]))));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 20))]) - __logf(compute_1[((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 20))]));\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 20; ++k) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 20; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)]);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 12, 8, 20), \"float32\"), compute: T.Buffer((8, 12, 8, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused in T.parallel(16):\n            T_softmax_maxelem = T.allocate([48], \"float32\", \"global\")\n            compute_1 = T.allocate([8], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((48,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((15360,), data=data.data)\n            for i0, i1 in T.grid(4, 12):\n                T_softmax_maxelem_1[i0 * 12 + i1] = T.float32(-3.4028234663852886e+38)\n                for k in range(20):\n                    cse_var_1: T.int32 = i0 * 12 + i1\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused // 8 * 7680 + i0 * 1920 + i1 * 160 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused % 8 * 20 + k])\n            for i1_outer_outer_inner in range(6):\n                compute_2 = T.Buffer((8,), data=compute_1, align=32)\n                for i0, i1 in T.grid(4, 2):\n                    compute_2[i0 * 2 + i1] = T.float32(0)\n                    for k in range(20):\n                        cse_var_2: T.int32 = i0 * 2 + i1\n                        compute_2[cse_var_2] = compute_2[cse_var_2] + T.exp(data_1[i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused // 8 * 7680 + i0 * 1920 + i1_outer_outer_inner * 320 + i1 * 160 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused % 8 * 20 + k] - T_softmax_maxelem_1[i0 * 12 + i1_outer_outer_inner * 2 + i1])\n                for i3_outer_outer_inner, i0_outer_inner, i1_inner in T.grid(10, 4, 2):\n                    cse_var_3: T.int32 = i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused // 8 * 7680 + i0_outer_inner * 1920 + i1_outer_outer_inner * 320 + i1_inner * 160 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused % 8 * 20 + i3_outer_outer_inner * 2\n                    compute_3 = T.Buffer((15360,), data=compute.data)\n                    compute_3[cse_var_3:cse_var_3 + 2] = data_1[cse_var_3:cse_var_3 + 2] - T.Broadcast(T_softmax_maxelem_1[i0_outer_inner * 12 + i1_outer_outer_inner * 2 + i1_inner], 2) - T.Broadcast(T.log(compute_2[i0_outer_inner * 2 + i1_inner]), 2)",
        "data": "8_12_8_20"
    },
    {
        "op_name": "log_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float logf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused = 0; i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused < 400; ++i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused) {\n    float T_softmax_maxelem[4];\n    float compute_2[4];\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        T_softmax_maxelem[((i1 * 2) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 40; ++k) {\n          int32_t cse_var_1 = ((i1 * 2) + i2);\n          float v_ = T_softmax_maxelem[cse_var_1];\n          float v__1 = ((float*)data_1)[((((((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 100) / 25) * 16000) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused / 200) * 8000)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 25) / 5) * 1600)) + (i1 * 800)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 200) / 100) * 400)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 5) * 80)) + (i2 * 40)) + k)];\n          T_softmax_maxelem[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n    for (int32_t i1_1 = 0; i1_1 < 2; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n        compute_2[((i1_1 * 2) + i2_1)] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 40; ++k_1) {\n          int32_t cse_var_2 = ((i1_1 * 2) + i2_1);\n          compute_2[cse_var_2] = (compute_2[cse_var_2] + expf((((float*)data_1)[((((((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 100) / 25) * 16000) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused / 200) * 8000)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 25) / 5) * 1600)) + (i1_1 * 800)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 200) / 100) * 400)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 5) * 80)) + (i2_1 * 40)) + k_1)] - T_softmax_maxelem[cse_var_2])));\n        },\n      },\n    },\n    for (int32_t i3_outer_outer_inner = 0; i3_outer_outer_inner < 2; ++i3_outer_outer_inner) {\n      for (int32_t i2_outer_inner = 0; i2_outer_inner < 2; ++i2_outer_inner) {\n        for (int32_t i3_outer_inner = 0; i3_outer_inner < 2; ++i3_outer_inner) {\n          for (int32_t i1_inner = 0; i1_inner < 2; ++i1_inner) {\n            int32_t cse_var_4 = ((i1_inner * 2) + i2_outer_inner);\n            int32_t cse_var_3 = (((((((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 100) / 25) * 16000) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused / 200) * 8000)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 25) / 5) * 1600)) + (i1_inner * 800)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 200) / 100) * 400)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 5) * 80)) + (i2_outer_inner * 40)) + (i3_outer_outer_inner * 20)) + (i3_outer_inner * 10));\n            int32_t10 v__2 = int32_t10((cse_var_3)+(1*0), (cse_var_3)+(1*1), (cse_var_3)+(1*2), (cse_var_3)+(1*3), (cse_var_3)+(1*4), (cse_var_3)+(1*5), (cse_var_3)+(1*6), (cse_var_3)+(1*7), (cse_var_3)+(1*8), (cse_var_3)+(1*9));\n            *(float10*)(((float*)compute_1) + cse_var_3) = (((float10(((float*)data_1)[v__2.s0],((float*)data_1)[v__2.s1],((float*)data_1)[v__2.s2],((float*)data_1)[v__2.s3],((float*)data_1)[v__2.s4],((float*)data_1)[v__2.s5],((float*)data_1)[v__2.s6],((float*)data_1)[v__2.s7],((float*)data_1)[v__2.s8],((float*)data_1)[v__2.s9])) - ((float10)(T_softmax_maxelem[cse_var_4], T_softmax_maxelem[cse_var_4], T_softmax_maxelem[cse_var_4], T_softmax_maxelem[cse_var_4], T_softmax_maxelem[cse_var_4], T_softmax_maxelem[cse_var_4], T_softmax_maxelem[cse_var_4], T_softmax_maxelem[cse_var_4], T_softmax_maxelem[cse_var_4], T_softmax_maxelem[cse_var_4]))) - ((float10)(logf(compute_2[cse_var_4]), logf(compute_2[cse_var_4]), logf(compute_2[cse_var_4]), logf(compute_2[cse_var_4]), logf(compute_2[cse_var_4]), logf(compute_2[cse_var_4]), logf(compute_2[cse_var_4]), logf(compute_2[cse_var_4]), logf(compute_2[cse_var_4]), logf(compute_2[cse_var_4]))));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(5) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(5) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  __shared__ float compute_1[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k_outer = 0; k_outer < 8; ++k_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + __expf((data[(((((int)blockIdx.x) * 40) + (k_outer * 5)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)])));\n  },\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  if (((int)threadIdx.x) < 1) {\n    red_buf0[0] = (red_buf0[0] + t0[0]);\n  },\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    compute_1[0] = red_buf0[0];\n  },\n  __syncthreads();\n  for (int i3_outer = 0; i3_outer < 8; ++i3_outer) {\n    compute[(((((int)blockIdx.x) * 40) + (i3_outer * 5)) + ((int)threadIdx.x))] = ((data[(((((int)blockIdx.x) * 40) + (i3_outer * 5)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]) - __logf(compute_1[0]));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 40; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 40)) + k)]);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 20, 20, 40), \"float32\"), compute: T.Buffer((4, 20, 20, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused in T.parallel(400):\n            T_softmax_maxelem = T.allocate([4], \"float32\", \"global\")\n            compute_1 = T.allocate([4], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((4,), data=T_softmax_maxelem, align=16)\n            data_1 = T.Buffer((64000,), data=data.data)\n            for i1, i2 in T.grid(2, 2):\n                T_softmax_maxelem_1[i1 * 2 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(40):\n                    cse_var_1: T.int32 = i1 * 2 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 100 // 25 * 16000 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused // 200 * 8000 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 25 // 5 * 1600 + i1 * 800 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 200 // 100 * 400 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 5 * 80 + i2 * 40 + k])\n            compute_2 = T.Buffer((4,), data=compute_1, align=16)\n            for i1, i2 in T.grid(2, 2):\n                compute_2[i1 * 2 + i2] = T.float32(0)\n                for k in range(40):\n                    cse_var_2: T.int32 = i1 * 2 + i2\n                    compute_2[cse_var_2] = compute_2[cse_var_2] + T.exp(data_1[i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 100 // 25 * 16000 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused // 200 * 8000 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 25 // 5 * 1600 + i1 * 800 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 200 // 100 * 400 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 5 * 80 + i2 * 40 + k] - T_softmax_maxelem_1[cse_var_2])\n            for i3_outer_outer_inner, i2_outer_inner, i3_outer_inner, i1_inner in T.grid(2, 2, 2, 2):\n                cse_var_4: T.int32 = i1_inner * 2 + i2_outer_inner\n                cse_var_3: T.int32 = i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 100 // 25 * 16000 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused // 200 * 8000 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 25 // 5 * 1600 + i1_inner * 800 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 200 // 100 * 400 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 5 * 80 + i2_outer_inner * 40 + i3_outer_outer_inner * 20 + i3_outer_inner * 10\n                compute_3 = T.Buffer((64000,), data=compute.data)\n                compute_3[cse_var_3:cse_var_3 + 10] = data_1[cse_var_3:cse_var_3 + 10] - T.Broadcast(T_softmax_maxelem_1[cse_var_4], 10) - T.Broadcast(T.log(compute_2[cse_var_4]), 10)",
        "data": "4_20_20_40"
    },
    {
        "op_name": "log_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float logf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused = 0; i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused < 14; ++i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused) {\n    float T_softmax_maxelem[16];\n    float compute_2[4];\n    for (int32_t i0 = 0; i0 < 2; ++i0) {\n      for (int32_t i1 = 0; i1 < 2; ++i1) {\n        for (int32_t i2 = 0; i2 < 4; ++i2) {\n          T_softmax_maxelem[(((i0 * 8) + (i1 * 4)) + i2)] = -3.402823e+38f;\n          for (int32_t k = 0; k < 28; ++k) {\n            int32_t cse_var_1 = (((i0 * 8) + (i1 * 4)) + i2);\n            float v_ = T_softmax_maxelem[cse_var_1];\n            float v__1 = ((float*)data_1)[(((((i0 * 3136) + (i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused * 224)) + (i1 * 112)) + (i2 * 28)) + k)];\n            T_softmax_maxelem[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n          },\n        },\n      },\n    },\n    for (int32_t i1_outer_outer_inner = 0; i1_outer_outer_inner < 2; ++i1_outer_outer_inner) {\n      for (int32_t i2_outer_inner = 0; i2_outer_inner < 2; ++i2_outer_inner) {\n        for (int32_t i0_1 = 0; i0_1 < 2; ++i0_1) {\n          for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n            compute_2[((i0_1 * 2) + i2_1)] = 0.000000e+00f;\n            for (int32_t k_1 = 0; k_1 < 28; ++k_1) {\n              int32_t cse_var_2 = ((i0_1 * 2) + i2_1);\n              compute_2[cse_var_2] = (compute_2[cse_var_2] + expf((((float*)data_1)[((((((i0_1 * 3136) + (i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused * 224)) + (i1_outer_outer_inner * 112)) + (i2_outer_inner * 56)) + (i2_1 * 28)) + k_1)] - T_softmax_maxelem[((((i0_1 * 8) + (i1_outer_outer_inner * 4)) + (i2_outer_inner * 2)) + i2_1)])));\n            },\n          },\n        },\n        for (int32_t i3_outer_inner = 0; i3_outer_inner < 14; ++i3_outer_inner) {\n          for (int32_t i0_inner = 0; i0_inner < 2; ++i0_inner) {\n            for (int32_t i2_inner = 0; i2_inner < 2; ++i2_inner) {\n              int32_t cse_var_3 = ((((((i0_inner * 3136) + (i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused * 224)) + (i1_outer_outer_inner * 112)) + (i2_outer_inner * 56)) + (i2_inner * 28)) + (i3_outer_inner * 2));\n              int32_t2 v__2 = int32_t2((cse_var_3)+(1*0), (cse_var_3)+(1*1));\n              *(float2*)(((float*)compute_1) + cse_var_3) = (((float2(((float*)data_1)[v__2.s0],((float*)data_1)[v__2.s1])) - ((float2)(T_softmax_maxelem[((((i0_inner * 8) + (i1_outer_outer_inner * 4)) + (i2_outer_inner * 2)) + i2_inner)], T_softmax_maxelem[((((i0_inner * 8) + (i1_outer_outer_inner * 4)) + (i2_outer_inner * 2)) + i2_inner)]))) - ((float2)(logf(compute_2[((i0_inner * 2) + i2_inner)]), logf(compute_2[((i0_inner * 2) + i2_inner)]))));\n            },\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 28; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 28)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 28; ++k) {\n    compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]) - __logf(compute_1[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 28, 4, 28), \"float32\"), compute: T.Buffer((2, 28, 4, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused in T.parallel(14):\n            T_softmax_maxelem = T.allocate([16], \"float32\", \"global\")\n            compute_1 = T.allocate([4], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((16,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((6272,), data=data.data)\n            for i0, i1, i2 in T.grid(2, 2, 4):\n                T_softmax_maxelem_1[i0 * 8 + i1 * 4 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(28):\n                    cse_var_1: T.int32 = i0 * 8 + i1 * 4 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 3136 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused * 224 + i1 * 112 + i2 * 28 + k])\n            for i1_outer_outer_inner, i2_outer_inner in T.grid(2, 2):\n                compute_2 = T.Buffer((4,), data=compute_1, align=16)\n                for i0, i2 in T.grid(2, 2):\n                    compute_2[i0 * 2 + i2] = T.float32(0)\n                    for k in range(28):\n                        cse_var_2: T.int32 = i0 * 2 + i2\n                        compute_2[cse_var_2] = compute_2[cse_var_2] + T.exp(data_1[i0 * 3136 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused * 224 + i1_outer_outer_inner * 112 + i2_outer_inner * 56 + i2 * 28 + k] - T_softmax_maxelem_1[i0 * 8 + i1_outer_outer_inner * 4 + i2_outer_inner * 2 + i2])\n                for i3_outer_inner, i0_inner, i2_inner in T.grid(14, 2, 2):\n                    cse_var_3: T.int32 = i0_inner * 3136 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused * 224 + i1_outer_outer_inner * 112 + i2_outer_inner * 56 + i2_inner * 28 + i3_outer_inner * 2\n                    compute_3 = T.Buffer((6272,), data=compute.data)\n                    compute_3[cse_var_3:cse_var_3 + 2] = data_1[cse_var_3:cse_var_3 + 2] - T.Broadcast(T_softmax_maxelem_1[i0_inner * 8 + i1_outer_outer_inner * 4 + i2_outer_inner * 2 + i2_inner], 2) - T.Broadcast(T.log(compute_2[i0_inner * 2 + i2_inner]), 2)",
        "data": "2_28_4_28"
    },
    {
        "op_name": "log_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float logf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused = 0; i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused < 40; ++i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused) {\n    float T_softmax_maxelem[48];\n    float compute_2[48];\n    for (int32_t i0 = 0; i0 < 2; ++i0) {\n      for (int32_t i1 = 0; i1 < 12; ++i1) {\n        for (int32_t i2 = 0; i2 < 2; ++i2) {\n          T_softmax_maxelem[(((i0 * 24) + (i1 * 2)) + i2)] = -3.402823e+38f;\n          for (int32_t k = 0; k < 28; ++k) {\n            int32_t cse_var_1 = (((i0 * 24) + (i1 * 2)) + i2);\n            float v_ = T_softmax_maxelem[cse_var_1];\n            float v__1 = ((float*)data_1)[((((((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused & 7) >> 2) * 26880) + (i0 * 13440)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused & 3) >> 1) * 6720)) + (i1 * 560)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused >> 3) * 112)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused & 1) * 56)) + (i2 * 28)) + k)];\n            T_softmax_maxelem[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n          },\n        },\n      },\n    },\n    for (int32_t i0_1 = 0; i0_1 < 2; ++i0_1) {\n      for (int32_t i1_1 = 0; i1_1 < 12; ++i1_1) {\n        for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n          compute_2[(((i0_1 * 24) + (i1_1 * 2)) + i2_1)] = 0.000000e+00f;\n          for (int32_t k_1 = 0; k_1 < 28; ++k_1) {\n            int32_t cse_var_2 = (((i0_1 * 24) + (i1_1 * 2)) + i2_1);\n            compute_2[cse_var_2] = (compute_2[cse_var_2] + expf((((float*)data_1)[((((((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused & 7) >> 2) * 26880) + (i0_1 * 13440)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused & 3) >> 1) * 6720)) + (i1_1 * 560)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused >> 3) * 112)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused & 1) * 56)) + (i2_1 * 28)) + k_1)] - T_softmax_maxelem[cse_var_2])));\n          },\n        },\n      },\n    },\n    for (int32_t i1_outer_inner = 0; i1_outer_inner < 4; ++i1_outer_inner) {\n      for (int32_t i2_outer_inner = 0; i2_outer_inner < 2; ++i2_outer_inner) {\n        for (int32_t i0_inner = 0; i0_inner < 2; ++i0_inner) {\n          for (int32_t i1_inner = 0; i1_inner < 3; ++i1_inner) {\n            for (int32_t i3_inner = 0; i3_inner < 28; ++i3_inner) {\n              int32_t cse_var_4 = ((((i0_inner * 24) + (i1_outer_inner * 6)) + (i1_inner * 2)) + i2_outer_inner);\n              int32_t cse_var_3 = (((((((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused & 7) >> 2) * 26880) + (i0_inner * 13440)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused & 3) >> 1) * 6720)) + (i1_outer_inner * 1680)) + (i1_inner * 560)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused >> 3) * 112)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused & 1) * 56)) + (i2_outer_inner * 28)) + i3_inner);\n              ((float*)compute_1)[cse_var_3] = ((((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_4]) - logf(compute_2[cse_var_4]));\n            },\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 28; ++k) {\n    compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 56) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 28; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 28)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 2)) / 7)]) - __logf(compute_1[(((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 2)) / 7)]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 24, 20, 28), \"float32\"), compute: T.Buffer((4, 24, 20, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused in T.parallel(40):\n            T_softmax_maxelem = T.allocate([48], \"float32\", \"global\")\n            compute_1 = T.allocate([48], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((48,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((53760,), data=data.data)\n            for i0, i1, i2 in T.grid(2, 12, 2):\n                T_softmax_maxelem_1[i0 * 24 + i1 * 2 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(28):\n                    cse_var_1: T.int32 = i0 * 24 + i1 * 2 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 8 // 4 * 26880 + i0 * 13440 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 4 // 2 * 6720 + i1 * 560 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused // 8 * 112 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 2 * 56 + i2 * 28 + k])\n            compute_2 = T.Buffer((48,), data=compute_1)\n            for i0, i1, i2 in T.grid(2, 12, 2):\n                compute_2[i0 * 24 + i1 * 2 + i2] = T.float32(0)\n                for k in range(28):\n                    cse_var_2: T.int32 = i0 * 24 + i1 * 2 + i2\n                    compute_2[cse_var_2] = compute_2[cse_var_2] + T.exp(data_1[i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 8 // 4 * 26880 + i0 * 13440 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 4 // 2 * 6720 + i1 * 560 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused // 8 * 112 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 2 * 56 + i2 * 28 + k] - T_softmax_maxelem_1[cse_var_2])\n            for i1_outer_inner, i2_outer_inner, i0_inner, i1_inner, i3_inner in T.grid(4, 2, 2, 3, 28):\n                cse_var_4: T.int32 = i0_inner * 24 + i1_outer_inner * 6 + i1_inner * 2 + i2_outer_inner\n                cse_var_3: T.int32 = i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 8 // 4 * 26880 + i0_inner * 13440 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 4 // 2 * 6720 + i1_outer_inner * 1680 + i1_inner * 560 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused // 8 * 112 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 2 * 56 + i2_outer_inner * 28 + i3_inner\n                compute_3 = T.Buffer((53760,), data=compute.data)\n                compute_3[cse_var_3] = data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4] - T.log(compute_2[cse_var_4])",
        "data": "4_24_20_28"
    },
    {
        "op_name": "log_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float logf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused = 0; i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused < 14; ++i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused) {\n    float T_softmax_maxelem[64];\n    float compute_2[16];\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        T_softmax_maxelem[((i1 * 4) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 28; ++k) {\n          int32_t cse_var_1 = ((i1 * 4) + i2);\n          float v_ = T_softmax_maxelem[cse_var_1];\n          float v__1 = ((float*)data_1)[((((i1 * 784) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused >> 1) * 112)) + (i2 * 28)) + k)];\n          T_softmax_maxelem[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n    for (int32_t i1_outer_inner = 0; i1_outer_inner < 4; ++i1_outer_inner) {\n      for (int32_t i1_1 = 0; i1_1 < 4; ++i1_1) {\n        for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n          compute_2[((i1_1 * 4) + i2_1)] = 0.000000e+00f;\n          for (int32_t k_1 = 0; k_1 < 28; ++k_1) {\n            int32_t cse_var_3 = (i1_1 * 4);\n            int32_t cse_var_2 = (cse_var_3 + i2_1);\n            compute_2[cse_var_2] = (compute_2[cse_var_2] + expf((((float*)data_1)[(((((i1_outer_inner * 3136) + (i1_1 * 784)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused >> 1) * 112)) + (i2_1 * 28)) + k_1)] - T_softmax_maxelem[(((i1_outer_inner * 16) + cse_var_3) + i2_1)])));\n          },\n        },\n      },\n      for (int32_t i1_inner = 0; i1_inner < 4; ++i1_inner) {\n        for (int32_t i2_inner = 0; i2_inner < 4; ++i2_inner) {\n          int32_t cse_var_5 = (i1_inner * 4);\n          int32_t cse_var_4 = (((((i1_outer_inner * 3136) + (i1_inner * 784)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused >> 1) * 112)) + (i2_inner * 28)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused & 1) * 14));\n          int32_t14 v__2 = int32_t14((cse_var_4)+(1*0), (cse_var_4)+(1*1), (cse_var_4)+(1*2), (cse_var_4)+(1*3), (cse_var_4)+(1*4), (cse_var_4)+(1*5), (cse_var_4)+(1*6), (cse_var_4)+(1*7), (cse_var_4)+(1*8), (cse_var_4)+(1*9), (cse_var_4)+(1*10), (cse_var_4)+(1*11), (cse_var_4)+(1*12), (cse_var_4)+(1*13));\n          *(float14*)(((float*)compute_1) + cse_var_4) = (((float14(((float*)data_1)[v__2.s0],((float*)data_1)[v__2.s1],((float*)data_1)[v__2.s2],((float*)data_1)[v__2.s3],((float*)data_1)[v__2.s4],((float*)data_1)[v__2.s5],((float*)data_1)[v__2.s6],((float*)data_1)[v__2.s7],((float*)data_1)[v__2.s8],((float*)data_1)[v__2.s9],((float*)data_1)[v__2.sa],((float*)data_1)[v__2.sb],((float*)data_1)[v__2.sc],((float*)data_1)[v__2.sd])) - ((float14)(T_softmax_maxelem[(((i1_outer_inner * 16) + cse_var_5) + i2_inner)], T_softmax_maxelem[(((i1_outer_inner * 16) + cse_var_5) + i2_inner)], T_softmax_maxelem[(((i1_outer_inner * 16) + cse_var_5) + i2_inner)], T_softmax_maxelem[(((i1_outer_inner * 16) + cse_var_5) + i2_inner)], T_softmax_maxelem[(((i1_outer_inner * 16) + cse_var_5) + i2_inner)], T_softmax_maxelem[(((i1_outer_inner * 16) + cse_var_5) + i2_inner)], T_softmax_maxelem[(((i1_outer_inner * 16) + cse_var_5) + i2_inner)], T_softmax_maxelem[(((i1_outer_inner * 16) + cse_var_5) + i2_inner)], T_softmax_maxelem[(((i1_outer_inner * 16) + cse_var_5) + i2_inner)], T_softmax_maxelem[(((i1_outer_inner * 16) + cse_var_5) + i2_inner)], T_softmax_maxelem[(((i1_outer_inner * 16) + cse_var_5) + i2_inner)], T_softmax_maxelem[(((i1_outer_inner * 16) + cse_var_5) + i2_inner)], T_softmax_maxelem[(((i1_outer_inner * 16) + cse_var_5) + i2_inner)], T_softmax_maxelem[(((i1_outer_inner * 16) + cse_var_5) + i2_inner)]))) - ((float14)(logf(compute_2[(cse_var_5 + i2_inner)]), logf(compute_2[(cse_var_5 + i2_inner)]), logf(compute_2[(cse_var_5 + i2_inner)]), logf(compute_2[(cse_var_5 + i2_inner)]), logf(compute_2[(cse_var_5 + i2_inner)]), logf(compute_2[(cse_var_5 + i2_inner)]), logf(compute_2[(cse_var_5 + i2_inner)]), logf(compute_2[(cse_var_5 + i2_inner)]), logf(compute_2[(cse_var_5 + i2_inner)]), logf(compute_2[(cse_var_5 + i2_inner)]), logf(compute_2[(cse_var_5 + i2_inner)]), logf(compute_2[(cse_var_5 + i2_inner)]), logf(compute_2[(cse_var_5 + i2_inner)]), logf(compute_2[(cse_var_5 + i2_inner)]))));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 28; ++k) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 28; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 7) + (((int)threadIdx.x) / 7)) >> 2)]) - __logf(compute_1[(((((int)blockIdx.x) * 7) + (((int)threadIdx.x) / 7)) >> 2)]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 16, 28, 28), \"float32\"), compute: T.Buffer((1, 16, 28, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused in T.parallel(14):\n            T_softmax_maxelem = T.allocate([64], \"float32\", \"global\")\n            compute_1 = T.allocate([16], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((64,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((12544,), data=data.data)\n            for i1, i2 in T.grid(16, 4):\n                T_softmax_maxelem_1[i1 * 4 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(28):\n                    cse_var_1: T.int32 = i1 * 4 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i1 * 784 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused // 2 * 112 + i2 * 28 + k])\n            for i1_outer_inner in range(4):\n                compute_2 = T.Buffer((16,), data=compute_1)\n                for i1, i2 in T.grid(4, 4):\n                    compute_2[i1 * 4 + i2] = T.float32(0)\n                    for k in range(28):\n                        cse_var_3: T.int32 = i1 * 4\n                        cse_var_2: T.int32 = cse_var_3 + i2\n                        compute_2[cse_var_2] = compute_2[cse_var_2] + T.exp(data_1[i1_outer_inner * 3136 + i1 * 784 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused // 2 * 112 + i2 * 28 + k] - T_softmax_maxelem_1[i1_outer_inner * 16 + cse_var_3 + i2])\n                for i1_inner, i2_inner in T.grid(4, 4):\n                    cse_var_5: T.int32 = i1_inner * 4\n                    cse_var_4: T.int32 = i1_outer_inner * 3136 + i1_inner * 784 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused // 2 * 112 + i2_inner * 28 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused % 2 * 14\n                    compute_3 = T.Buffer((12544,), data=compute.data)\n                    compute_3[cse_var_4:cse_var_4 + 14] = data_1[cse_var_4:cse_var_4 + 14] - T.Broadcast(T_softmax_maxelem_1[i1_outer_inner * 16 + cse_var_5 + i2_inner], 14) - T.Broadcast(T.log(compute_2[cse_var_5 + i2_inner]), 14)",
        "data": "1_16_28_28"
    },
    {
        "op_name": "log_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float logf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused = 0; i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused < 24; ++i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused) {\n    float T_softmax_maxelem[96];\n    float compute_2[96];\n    for (int32_t i0 = 0; i0 < 4; ++i0) {\n      for (int32_t i1 = 0; i1 < 6; ++i1) {\n        for (int32_t i2 = 0; i2 < 4; ++i2) {\n          T_softmax_maxelem[(((i0 * 24) + (i1 * 4)) + i2)] = -3.402823e+38f;\n          for (int32_t k = 0; k < 32; ++k) {\n            int32_t cse_var_1 = (((i0 * 24) + (i1 * 4)) + i2);\n            float v_ = T_softmax_maxelem[cse_var_1];\n            float v__1 = ((float*)data_1)[((((((i0 * 18432) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused >> 2) * 3072)) + (i1 * 512)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused & 3) * 128)) + (i2 * 32)) + k)];\n            T_softmax_maxelem[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n          },\n        },\n      },\n    },\n    for (int32_t i0_1 = 0; i0_1 < 4; ++i0_1) {\n      for (int32_t i1_1 = 0; i1_1 < 6; ++i1_1) {\n        for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n          compute_2[(((i0_1 * 24) + (i1_1 * 4)) + i2_1)] = 0.000000e+00f;\n          for (int32_t k_1 = 0; k_1 < 32; ++k_1) {\n            int32_t cse_var_2 = (((i0_1 * 24) + (i1_1 * 4)) + i2_1);\n            compute_2[cse_var_2] = (compute_2[cse_var_2] + expf((((float*)data_1)[((((((i0_1 * 18432) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused >> 2) * 3072)) + (i1_1 * 512)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused & 3) * 128)) + (i2_1 * 32)) + k_1)] - T_softmax_maxelem[cse_var_2])));\n          },\n        },\n      },\n    },\n    for (int32_t i3_outer_outer_inner = 0; i3_outer_outer_inner < 8; ++i3_outer_outer_inner) {\n      for (int32_t i1_outer_inner = 0; i1_outer_inner < 2; ++i1_outer_inner) {\n        for (int32_t i3_outer_inner = 0; i3_outer_inner < 2; ++i3_outer_inner) {\n          for (int32_t i0_inner = 0; i0_inner < 4; ++i0_inner) {\n            for (int32_t i1_inner = 0; i1_inner < 3; ++i1_inner) {\n              for (int32_t i2_inner = 0; i2_inner < 4; ++i2_inner) {\n                int32_t cse_var_4 = ((((i0_inner * 24) + (i1_outer_inner * 12)) + (i1_inner * 4)) + i2_inner);\n                int32_t cse_var_3 = ((((((((i0_inner * 18432) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused >> 2) * 3072)) + (i1_outer_inner * 1536)) + (i1_inner * 512)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused & 3) * 128)) + (i2_inner * 32)) + (i3_outer_outer_inner * 4)) + (i3_outer_inner * 2));\n                int32_t2 v__2 = int32_t2((cse_var_3)+(1*0), (cse_var_3)+(1*1));\n                *(float2*)(((float*)compute_1) + cse_var_3) = (((float2(((float*)data_1)[v__2.s0],((float*)data_1)[v__2.s1])) - ((float2)(T_softmax_maxelem[cse_var_4], T_softmax_maxelem[cse_var_4]))) - ((float2)(logf(compute_2[cse_var_4]), logf(compute_2[cse_var_4]))));\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 32; ++k) {\n    compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 32; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 1024) + (((int)threadIdx.x) * 32)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]) - __logf(compute_1[((int)blockIdx.x)]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 36, 16, 32), \"float32\"), compute: T.Buffer((4, 36, 16, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused in T.parallel(24):\n            T_softmax_maxelem = T.allocate([96], \"float32\", \"global\")\n            compute_1 = T.allocate([96], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((96,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((73728,), data=data.data)\n            for i0, i1, i2 in T.grid(4, 6, 4):\n                T_softmax_maxelem_1[i0 * 24 + i1 * 4 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(32):\n                    cse_var_1: T.int32 = i0 * 24 + i1 * 4 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 18432 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused // 4 * 3072 + i1 * 512 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused % 4 * 128 + i2 * 32 + k])\n            compute_2 = T.Buffer((96,), data=compute_1)\n            for i0, i1, i2 in T.grid(4, 6, 4):\n                compute_2[i0 * 24 + i1 * 4 + i2] = T.float32(0)\n                for k in range(32):\n                    cse_var_2: T.int32 = i0 * 24 + i1 * 4 + i2\n                    compute_2[cse_var_2] = compute_2[cse_var_2] + T.exp(data_1[i0 * 18432 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused // 4 * 3072 + i1 * 512 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused % 4 * 128 + i2 * 32 + k] - T_softmax_maxelem_1[cse_var_2])\n            for i3_outer_outer_inner, i1_outer_inner, i3_outer_inner, i0_inner, i1_inner, i2_inner in T.grid(8, 2, 2, 4, 3, 4):\n                cse_var_4: T.int32 = i0_inner * 24 + i1_outer_inner * 12 + i1_inner * 4 + i2_inner\n                cse_var_3: T.int32 = i0_inner * 18432 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused // 4 * 3072 + i1_outer_inner * 1536 + i1_inner * 512 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused % 4 * 128 + i2_inner * 32 + i3_outer_outer_inner * 4 + i3_outer_inner * 2\n                compute_3 = T.Buffer((73728,), data=compute.data)\n                compute_3[cse_var_3:cse_var_3 + 2] = data_1[cse_var_3:cse_var_3 + 2] - T.Broadcast(T_softmax_maxelem_1[cse_var_4], 2) - T.Broadcast(T.log(compute_2[cse_var_4]), 2)",
        "data": "4_36_16_32"
    },
    {
        "op_name": "log_softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float logf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused = 0; i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused < 64; ++i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused) {\n    float T_softmax_maxelem[20];\n    float compute_2[2];\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        T_softmax_maxelem[((i1 * 2) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 28; ++k) {\n          int32_t cse_var_1 = ((i1 * 2) + i2);\n          float v_ = T_softmax_maxelem[cse_var_1];\n          float v__1 = ((float*)data_1)[((((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused >> 4) * 8960) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused & 1) * 4480)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused & 15) >> 2) * 1120)) + (i1 * 112)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused & 3) >> 1) * 56)) + (i2 * 28)) + k)];\n          T_softmax_maxelem[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n    for (int32_t i1_outer_outer_inner = 0; i1_outer_outer_inner < 5; ++i1_outer_outer_inner) {\n      for (int32_t i2_outer_inner = 0; i2_outer_inner < 2; ++i2_outer_inner) {\n        for (int32_t i1_1 = 0; i1_1 < 2; ++i1_1) {\n          compute_2[i1_1] = 0.000000e+00f;\n          for (int32_t k_1 = 0; k_1 < 28; ++k_1) {\n            compute_2[i1_1] = (compute_2[i1_1] + expf((((float*)data_1)[(((((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused >> 4) * 8960) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused & 1) * 4480)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused & 15) >> 2) * 1120)) + (i1_outer_outer_inner * 224)) + (i1_1 * 112)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused & 3) >> 1) * 56)) + (i2_outer_inner * 28)) + k_1)] - T_softmax_maxelem[(((i1_outer_outer_inner * 4) + (i1_1 * 2)) + i2_outer_inner)])));\n          },\n        },\n        for (int32_t i3_outer_inner = 0; i3_outer_inner < 14; ++i3_outer_inner) {\n          for (int32_t i1_inner = 0; i1_inner < 2; ++i1_inner) {\n            int32_t cse_var_2 = (((((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused >> 4) * 8960) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused & 1) * 4480)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused & 15) >> 2) * 1120)) + (i1_outer_outer_inner * 224)) + (i1_inner * 112)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused & 3) >> 1) * 56)) + (i2_outer_inner * 28)) + (i3_outer_inner * 2));\n            int32_t2 v__2 = int32_t2((cse_var_2)+(1*0), (cse_var_2)+(1*1));\n            *(float2*)(((float*)compute_1) + cse_var_2) = (((float2(((float*)data_1)[v__2.s0],((float*)data_1)[v__2.s1])) - ((float2)(T_softmax_maxelem[(((i1_outer_outer_inner * 4) + (i1_inner * 2)) + i2_outer_inner)], T_softmax_maxelem[(((i1_outer_outer_inner * 4) + (i1_inner * 2)) + i2_outer_inner)]))) - ((float2)(logf(compute_2[i1_inner]), logf(compute_2[i1_inner]))));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 28; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]) - __logf(compute_1[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 7)]));\n},\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 28; ++k) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 448) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 40, 4, 28), \"float32\"), compute: T.Buffer((8, 40, 4, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused in T.parallel(64):\n            T_softmax_maxelem = T.allocate([20], \"float32\", \"global\")\n            compute_1 = T.allocate([2], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((20,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((35840,), data=data.data)\n            for i1, i2 in T.grid(10, 2):\n                T_softmax_maxelem_1[i1 * 2 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(28):\n                    cse_var_1: T.int32 = i1 * 2 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused // 16 * 8960 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused % 2 * 4480 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused % 16 // 4 * 1120 + i1 * 112 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused % 4 // 2 * 56 + i2 * 28 + k])\n            for i1_outer_outer_inner, i2_outer_inner in T.grid(5, 2):\n                compute_2 = T.Buffer((2,), data=compute_1, align=8)\n                for i1 in range(2):\n                    compute_2[i1] = T.float32(0)\n                    for k in range(28):\n                        compute_2[i1] = compute_2[i1] + T.exp(data_1[i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused // 16 * 8960 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused % 2 * 4480 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused % 16 // 4 * 1120 + i1_outer_outer_inner * 224 + i1 * 112 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused % 4 // 2 * 56 + i2_outer_inner * 28 + k] - T_softmax_maxelem_1[i1_outer_outer_inner * 4 + i1 * 2 + i2_outer_inner])\n                for i3_outer_inner, i1_inner in T.grid(14, 2):\n                    cse_var_2: T.int32 = i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused // 16 * 8960 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused % 2 * 4480 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused % 16 // 4 * 1120 + i1_outer_outer_inner * 224 + i1_inner * 112 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused % 4 // 2 * 56 + i2_outer_inner * 28 + i3_outer_inner * 2\n                    compute_3 = T.Buffer((35840,), data=compute.data)\n                    compute_3[cse_var_2:cse_var_2 + 2] = data_1[cse_var_2:cse_var_2 + 2] - T.Broadcast(T_softmax_maxelem_1[i1_outer_outer_inner * 4 + i1_inner * 2 + i2_outer_inner], 2) - T.Broadcast(T.log(compute_2[i1_inner]), 2)",
        "data": "8_40_4_28"
    },
    {
        "op_name": "lrn",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float powf(float, float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_divide_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_divide = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_divide_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1680; ++ax0_ax1_fused_ax2_fused) {\n    float tensor[1];\n    for (int32_t ax3 = 0; ax3 < 32; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 32) + ax3);\n      tensor[0] = 0.000000e+00f;\n      tensor[0] = (tensor[0] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n      ((float*)T_divide_1)[cse_var_1] = (((float*)data_1)[cse_var_1] / powf((2.000000e+00f + (1.000000e-04f * tensor[0])), 7.500000e-01f));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])), 7.500000e-01f));\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 28, 12, 32), \"float32\"), T_divide: T.Buffer((5, 28, 12, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1680):\n            tensor = T.allocate([1], \"float32\", \"global\")\n            for ax3 in range(32):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 32 + ax3\n                tensor_1 = T.Buffer((1,), data=tensor, align=4)\n                tensor_1[0] = T.float32(0)\n                data_1 = T.Buffer((53760,), data=data.data)\n                tensor_1[0] = tensor_1[0] + data_1[cse_var_1] * data_1[cse_var_1]\n                T_divide_1 = T.Buffer((53760,), data=T_divide.data)\n                T_divide_1[cse_var_1] = data_1[cse_var_1] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[0], T.float32(0.75))",
        "data": "5_28_12_32"
    },
    {
        "op_name": "lrn",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float powf(float, float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_divide_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_divide = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_divide_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 8; ++ax0_ax1_fused) {\n    float tensor[12];\n    for (int32_t ax2 = 0; ax2 < 40; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 12; ++ax3) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 480) + (ax2 * 12)) + ax3);\n        tensor[ax3] = 0.000000e+00f;\n        tensor[ax3] = (tensor[ax3] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n      },\n      for (int32_t ax3_1 = 0; ax3_1 < 12; ++ax3_1) {\n        int32_t cse_var_2 = (((ax0_ax1_fused * 480) + (ax2 * 12)) + ax3_1);\n        ((float*)T_divide_1)[cse_var_2] = (((float*)data_1)[cse_var_2] / powf((2.000000e+00f + (1.000000e-04f * tensor[ax3_1])), 7.500000e-01f));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  T_divide[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))])), 7.500000e-01f));\n},\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 4, 40, 12), \"float32\"), T_divide: T.Buffer((2, 4, 40, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(8):\n            tensor = T.allocate([12], \"float32\", \"global\")\n            for ax2 in range(40):\n                tensor_1 = T.Buffer((12,), data=tensor, align=32)\n                data_1 = T.Buffer((3840,), data=data.data)\n                for ax3 in range(12):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 480 + ax2 * 12 + ax3\n                    tensor_1[ax3] = T.float32(0)\n                    tensor_1[ax3] = tensor_1[ax3] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax3 in range(12):\n                    cse_var_2: T.int32 = ax0_ax1_fused * 480 + ax2 * 12 + ax3\n                    T_divide_1 = T.Buffer((3840,), data=T_divide.data)\n                    T_divide_1[cse_var_2] = data_1[cse_var_2] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[ax3], T.float32(0.75))",
        "data": "2_4_40_12"
    },
    {
        "op_name": "lrn",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float powf(float, float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_divide_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_divide = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_divide_strides == NULL)) {\n  },\n  void* tensor = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)702464, 2, 32);\n  if (tensor == NULL) {\n    return -1;\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 175616; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    ((float*)tensor)[ax0_ax1_fused_ax2_fused_ax3_fused] = 0.000000e+00f;\n    ((float*)tensor)[ax0_ax1_fused_ax2_fused_ax3_fused] = (((float*)tensor)[ax0_ax1_fused_ax2_fused_ax3_fused] + (((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused] * ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused]));\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 6272; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 28; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 28) + ax3);\n      ((float*)T_divide_1)[cse_var_1] = (((float*)data_1)[cse_var_1] / powf((2.000000e+00f + (1.000000e-04f * ((float*)tensor)[cse_var_1])), 7.500000e-01f));\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, tensor) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  T_divide[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])), 7.500000e-01f));\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 28, 32, 28), \"float32\"), T_divide: T.Buffer((7, 28, 32, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        tensor = T.allocate([175616], \"float32\", \"global\")\n        tensor_1 = T.Buffer((175616,), data=tensor)\n        data_1 = T.Buffer((175616,), data=data.data)\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(175616):\n            tensor_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.float32(0)\n            tensor_1[ax0_ax1_fused_ax2_fused_ax3_fused] = tensor_1[ax0_ax1_fused_ax2_fused_ax3_fused] + data_1[ax0_ax1_fused_ax2_fused_ax3_fused] * data_1[ax0_ax1_fused_ax2_fused_ax3_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(6272):\n            for ax3 in range(28):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 28 + ax3\n                T_divide_1 = T.Buffer((175616,), data=T_divide.data)\n                T_divide_1[cse_var_1] = data_1[cse_var_1] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[cse_var_1], T.float32(0.75))",
        "data": "7_28_32_28"
    },
    {
        "op_name": "lrn",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float powf(float, float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_divide_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_divide = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_divide_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1600; ++ax0_ax1_fused_ax2_fused) {\n    float tensor[36];\n    for (int32_t ax3 = 0; ax3 < 36; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 36) + ax3);\n      tensor[ax3] = 0.000000e+00f;\n      tensor[ax3] = (tensor[ax3] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n    },\n    for (int32_t ax3_1 = 0; ax3_1 < 36; ++ax3_1) {\n      int32_t cse_var_2 = ((ax0_ax1_fused_ax2_fused * 36) + ax3_1);\n      ((float*)T_divide_1)[cse_var_2] = (((float*)data_1)[cse_var_2] / powf((2.000000e+00f + (1.000000e-04f * tensor[ax3_1])), 7.500000e-01f));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])), 7.500000e-01f));\n},\n\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 40, 8, 36), \"float32\"), T_divide: T.Buffer((5, 40, 8, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1600):\n            tensor = T.allocate([36], \"float32\", \"global\")\n            tensor_1 = T.Buffer((36,), data=tensor)\n            data_1 = T.Buffer((57600,), data=data.data)\n            for ax3 in range(36):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 36 + ax3\n                tensor_1[ax3] = T.float32(0)\n                tensor_1[ax3] = tensor_1[ax3] + data_1[cse_var_1] * data_1[cse_var_1]\n            for ax3 in range(36):\n                cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused * 36 + ax3\n                T_divide_1 = T.Buffer((57600,), data=T_divide.data)\n                T_divide_1[cse_var_2] = data_1[cse_var_2] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[ax3], T.float32(0.75))",
        "data": "5_40_8_36"
    },
    {
        "op_name": "lrn",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float powf(float, float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_divide_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_divide = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_divide_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1536; ++ax0_ax1_fused_ax2_fused) {\n    float tensor[36];\n    for (int32_t ax3 = 0; ax3 < 36; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 36) + ax3);\n      tensor[ax3] = 0.000000e+00f;\n      tensor[ax3] = (tensor[ax3] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n    },\n    for (int32_t ax3_1 = 0; ax3_1 < 36; ++ax3_1) {\n      int32_t cse_var_2 = ((ax0_ax1_fused_ax2_fused * 36) + ax3_1);\n      ((float*)T_divide_1)[cse_var_2] = (((float*)data_1)[cse_var_2] / powf((2.000000e+00f + (1.000000e-04f * tensor[ax3_1])), 7.500000e-01f));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])), 7.500000e-01f));\n},\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 8, 32, 36), \"float32\"), T_divide: T.Buffer((6, 8, 32, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1536):\n            tensor = T.allocate([36], \"float32\", \"global\")\n            tensor_1 = T.Buffer((36,), data=tensor)\n            data_1 = T.Buffer((55296,), data=data.data)\n            for ax3 in range(36):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 36 + ax3\n                tensor_1[ax3] = T.float32(0)\n                tensor_1[ax3] = tensor_1[ax3] + data_1[cse_var_1] * data_1[cse_var_1]\n            for ax3 in range(36):\n                cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused * 36 + ax3\n                T_divide_1 = T.Buffer((55296,), data=T_divide.data)\n                T_divide_1[cse_var_2] = data_1[cse_var_2] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[ax3], T.float32(0.75))",
        "data": "6_8_32_36"
    },
    {
        "op_name": "lrn",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float powf(float, float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_divide_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_divide = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_divide_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 16; ++ax0_ax1_fused) {\n    float tensor[40];\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 40; ++ax3) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 320) + (ax2 * 40)) + ax3);\n        tensor[ax3] = 0.000000e+00f;\n        tensor[ax3] = (tensor[ax3] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n      },\n      for (int32_t ax3_1 = 0; ax3_1 < 40; ++ax3_1) {\n        int32_t cse_var_2 = (((ax0_ax1_fused * 320) + (ax2 * 40)) + ax3_1);\n        ((float*)T_divide_1)[cse_var_2] = (((float*)data_1)[cse_var_2] / powf((2.000000e+00f + (1.000000e-04f * tensor[ax3_1])), 7.500000e-01f));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])), 7.500000e-01f));\n},\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 8, 8, 40), \"float32\"), T_divide: T.Buffer((2, 8, 8, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(16):\n            tensor = T.allocate([40], \"float32\", \"global\")\n            for ax2 in range(8):\n                tensor_1 = T.Buffer((40,), data=tensor)\n                data_1 = T.Buffer((5120,), data=data.data)\n                for ax3 in range(40):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 320 + ax2 * 40 + ax3\n                    tensor_1[ax3] = T.float32(0)\n                    tensor_1[ax3] = tensor_1[ax3] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax3 in range(40):\n                    cse_var_2: T.int32 = ax0_ax1_fused * 320 + ax2 * 40 + ax3\n                    T_divide_1 = T.Buffer((5120,), data=T_divide.data)\n                    T_divide_1[cse_var_2] = data_1[cse_var_2] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[ax3], T.float32(0.75))",
        "data": "2_8_8_40"
    },
    {
        "op_name": "lrn",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float powf(float, float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_divide_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_divide = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_divide_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 9216; ++ax0_ax1_fused_ax2_fused) {\n    float tensor[28];\n    for (int32_t ax3 = 0; ax3 < 28; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 28) + ax3);\n      tensor[ax3] = 0.000000e+00f;\n      tensor[ax3] = (tensor[ax3] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n    },\n    for (int32_t ax3_1 = 0; ax3_1 < 28; ++ax3_1) {\n      int32_t cse_var_2 = ((ax0_ax1_fused_ax2_fused * 28) + ax3_1);\n      ((float*)T_divide_1)[cse_var_2] = (((float*)data_1)[cse_var_2] / powf((2.000000e+00f + (1.000000e-04f * tensor[ax3_1])), 7.500000e-01f));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  T_divide[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])), 7.500000e-01f));\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 32, 32, 28), \"float32\"), T_divide: T.Buffer((9, 32, 32, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(9216):\n            tensor = T.allocate([28], \"float32\", \"global\")\n            tensor_1 = T.Buffer((28,), data=tensor)\n            data_1 = T.Buffer((258048,), data=data.data)\n            for ax3 in range(28):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 28 + ax3\n                tensor_1[ax3] = T.float32(0)\n                tensor_1[ax3] = tensor_1[ax3] + data_1[cse_var_1] * data_1[cse_var_1]\n            for ax3 in range(28):\n                cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused * 28 + ax3\n                T_divide_1 = T.Buffer((258048,), data=T_divide.data)\n                T_divide_1[cse_var_2] = data_1[cse_var_2] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[ax3], T.float32(0.75))",
        "data": "9_32_32_28"
    },
    {
        "op_name": "mirror_pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t MirrorPadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* MirrorPadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* MirrorPadInput_1 = (((DLTensor*)MirrorPadInput)[0].data);\n  void* default_function_MirrorPadInput_shape = (((DLTensor*)MirrorPadInput)[0].shape);\n  void* default_function_MirrorPadInput_strides = (((DLTensor*)MirrorPadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_MirrorPadInput_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 945; ++i0_i1_fused) {\n    int32_t cse_var_2 = (i0_i1_fused / 35);\n    int32_t cse_var_1 = (i0_i1_fused % 35);\n    ((float*)MirrorPadInput_1)[i0_i1_fused] = ((float*)data_1)[((((875 <= i0_i1_fused) ? (48 - cse_var_2) : ((i0_i1_fused < 35) ? 0 : (cse_var_2 - 1))) * 32) + ((cse_var_1 == 34) ? 31 : ((cse_var_1 < 2) ? (1 - cse_var_1) : (cse_var_1 - 2))))];\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 945) {\n    MirrorPadInput[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[((((875 <= ((((int)blockIdx.x) * 32) + ((int)threadIdx.x))) ? (48 - (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 35)) : ((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 35) ? 0 : ((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 35) - 1))) * 32) + (((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 35) == 34) ? (65 - (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 35)) : (((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 35) < 2) ? (1 - (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 35)) : ((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 35) - 2))))];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((24, 32), \"float32\"), MirrorPadInput: T.Buffer((27, 35), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(945):\n            cse_var_2: T.int32 = i0_i1_fused // 35\n            cse_var_1: T.int32 = i0_i1_fused % 35\n            MirrorPadInput_1 = T.Buffer((945,), data=MirrorPadInput.data)\n            data_1 = T.Buffer((768,), data=data.data)\n            MirrorPadInput_1[i0_i1_fused] = data_1[T.if_then_else(875 <= i0_i1_fused, 48 - cse_var_2, T.if_then_else(i0_i1_fused < 35, 0, cse_var_2 - 1)) * 32 + T.if_then_else(cse_var_1 == 34, 31, T.if_then_else(cse_var_1 < 2, 1 - cse_var_1, cse_var_1 - 2))]",
        "data": "24_32"
    },
    {
        "op_name": "mirror_pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t MirrorPadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* MirrorPadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* MirrorPadInput_1 = (((DLTensor*)MirrorPadInput)[0].data);\n  void* default_function_MirrorPadInput_shape = (((DLTensor*)MirrorPadInput)[0].shape);\n  void* default_function_MirrorPadInput_strides = (((DLTensor*)MirrorPadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_MirrorPadInput_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 837; ++i0_i1_fused) {\n    int32_t cse_var_2 = (i0_i1_fused / 27);\n    int32_t cse_var_1 = (i0_i1_fused % 27);\n    ((float*)MirrorPadInput_1)[i0_i1_fused] = ((float*)data_1)[((((783 <= i0_i1_fused) ? (56 - cse_var_2) : ((i0_i1_fused < 27) ? 0 : (cse_var_2 - 1))) * 24) + ((cse_var_1 == 26) ? 23 : ((cse_var_1 < 2) ? (1 - cse_var_1) : (cse_var_1 - 2))))];\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(38) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(38) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 38) + ((int)threadIdx.x)) < 837) {\n    MirrorPadInput[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] = data[((((783 <= ((((int)blockIdx.x) * 38) + ((int)threadIdx.x))) ? (56 - (((((int)blockIdx.x) * 38) + ((int)threadIdx.x)) / 27)) : ((((((int)blockIdx.x) * 38) + ((int)threadIdx.x)) < 27) ? 0 : ((((((int)blockIdx.x) * 38) + ((int)threadIdx.x)) / 27) - 1))) * 24) + (((((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) % 27) == 26) ? (49 - (((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) % 27)) : (((((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) % 27) < 2) ? (1 - (((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) % 27)) : ((((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) % 27) - 2))))];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((28, 24), \"float32\"), MirrorPadInput: T.Buffer((31, 27), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(837):\n            cse_var_2: T.int32 = i0_i1_fused // 27\n            cse_var_1: T.int32 = i0_i1_fused % 27\n            MirrorPadInput_1 = T.Buffer((837,), data=MirrorPadInput.data)\n            data_1 = T.Buffer((672,), data=data.data)\n            MirrorPadInput_1[i0_i1_fused] = data_1[T.if_then_else(783 <= i0_i1_fused, 56 - cse_var_2, T.if_then_else(i0_i1_fused < 27, 0, cse_var_2 - 1)) * 24 + T.if_then_else(cse_var_1 == 26, 23, T.if_then_else(cse_var_1 < 2, 1 - cse_var_1, cse_var_1 - 2))]",
        "data": "28_24"
    },
    {
        "op_name": "mirror_pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t MirrorPadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* MirrorPadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* MirrorPadInput_1 = (((DLTensor*)MirrorPadInput)[0].data);\n  void* default_function_MirrorPadInput_shape = (((DLTensor*)MirrorPadInput)[0].shape);\n  void* default_function_MirrorPadInput_strides = (((DLTensor*)MirrorPadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_MirrorPadInput_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 27; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      ((float*)MirrorPadInput_1)[((i0 * 11) + i1)] = ((float*)data_1)[((((25 <= i0) ? (48 - i0) : ((i0 < 1) ? (0 - i0) : (i0 - 1))) * 8) + ((i1 == 10) ? (17 - i1) : ((i1 < 2) ? (1 - i1) : (i1 - 2))))];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  MirrorPadInput[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = data[((((275 <= ((((int)blockIdx.x) * 3) + ((int)threadIdx.x))) ? (48 - (((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) / 11)) : ((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) < 11) ? 0 : ((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) / 11) - 1))) * 8) + (((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 11) == 10) ? (17 - (((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 11)) : (((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 11) < 2) ? (1 - (((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 11)) : ((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 11) - 2))))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((24, 8), \"float32\"), MirrorPadInput: T.Buffer((27, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(27):\n            for i1 in range(11):\n                MirrorPadInput_1 = T.Buffer((297,), data=MirrorPadInput.data)\n                data_1 = T.Buffer((192,), data=data.data)\n                MirrorPadInput_1[i0 * 11 + i1] = data_1[T.if_then_else(25 <= i0, 48 - i0, T.if_then_else(i0 < 1, 0 - i0, i0 - 1)) * 8 + T.if_then_else(i1 == 10, 17 - i1, T.if_then_else(i1 < 2, 1 - i1, i1 - 2))]",
        "data": "24_8"
    },
    {
        "op_name": "mirror_pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t MirrorPadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* MirrorPadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* MirrorPadInput_1 = (((DLTensor*)MirrorPadInput)[0].data);\n  void* default_function_MirrorPadInput_shape = (((DLTensor*)MirrorPadInput)[0].shape);\n  void* default_function_MirrorPadInput_strides = (((DLTensor*)MirrorPadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_MirrorPadInput_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      ((float*)MirrorPadInput_1)[((i0 * 19) + i1)] = ((float*)data_1)[((((5 <= i0) ? (8 - i0) : ((i0 < 1) ? (0 - i0) : (i0 - 1))) * 16) + ((i1 == 18) ? (33 - i1) : ((i1 < 2) ? (1 - i1) : (i1 - 2))))];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(22) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(22) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 22) + ((int)threadIdx.x)) < 133) {\n    MirrorPadInput[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))] = data[((((95 <= ((((int)blockIdx.x) * 22) + ((int)threadIdx.x))) ? (8 - (((((int)blockIdx.x) * 22) + ((int)threadIdx.x)) / 19)) : ((((((int)blockIdx.x) * 22) + ((int)threadIdx.x)) < 19) ? 0 : ((((((int)blockIdx.x) * 22) + ((int)threadIdx.x)) / 19) - 1))) * 16) + (((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 19) == 18) ? (33 - (((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 19)) : (((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 19) < 2) ? (1 - (((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 19)) : ((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 19) - 2))))];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 16), \"float32\"), MirrorPadInput: T.Buffer((7, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(7):\n            for i1 in range(19):\n                MirrorPadInput_1 = T.Buffer((133,), data=MirrorPadInput.data)\n                data_1 = T.Buffer((64,), data=data.data)\n                MirrorPadInput_1[i0 * 19 + i1] = data_1[T.if_then_else(5 <= i0, 8 - i0, T.if_then_else(i0 < 1, 0 - i0, i0 - 1)) * 16 + T.if_then_else(i1 == 18, 33 - i1, T.if_then_else(i1 < 2, 1 - i1, i1 - 2))]",
        "data": "4_16"
    },
    {
        "op_name": "mirror_pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t MirrorPadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* MirrorPadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* MirrorPadInput_1 = (((DLTensor*)MirrorPadInput)[0].data);\n  void* default_function_MirrorPadInput_shape = (((DLTensor*)MirrorPadInput)[0].shape);\n  void* default_function_MirrorPadInput_strides = (((DLTensor*)MirrorPadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_MirrorPadInput_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 39; ++i0) {\n    for (int32_t i1 = 0; i1 < 23; ++i1) {\n      ((float*)MirrorPadInput_1)[((i0 * 23) + i1)] = ((float*)data_1)[((((37 <= i0) ? (72 - i0) : ((i0 < 1) ? (0 - i0) : (i0 - 1))) * 20) + ((i1 == 22) ? (41 - i1) : ((i1 < 2) ? (1 - i1) : (i1 - 2))))];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) < 897) {\n    MirrorPadInput[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((851 <= ((((int)blockIdx.x) * 64) + ((int)threadIdx.x))) ? (72 - (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 23)) : ((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) < 23) ? 0 : ((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 23) - 1))) * 20) + (((((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) % 23) == 22) ? (41 - (((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) % 23)) : (((((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) % 23) < 2) ? (1 - (((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) % 23)) : ((((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) % 23) - 2))))];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((36, 20), \"float32\"), MirrorPadInput: T.Buffer((39, 23), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(39):\n            for i1 in range(23):\n                MirrorPadInput_1 = T.Buffer((897,), data=MirrorPadInput.data)\n                data_1 = T.Buffer((720,), data=data.data)\n                MirrorPadInput_1[i0 * 23 + i1] = data_1[T.if_then_else(37 <= i0, 72 - i0, T.if_then_else(i0 < 1, 0 - i0, i0 - 1)) * 20 + T.if_then_else(i1 == 22, 41 - i1, T.if_then_else(i1 < 2, 1 - i1, i1 - 2))]",
        "data": "36_20"
    },
    {
        "op_name": "mirror_pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t MirrorPadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* MirrorPadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* MirrorPadInput_1 = (((DLTensor*)MirrorPadInput)[0].data);\n  void* default_function_MirrorPadInput_shape = (((DLTensor*)MirrorPadInput)[0].shape);\n  void* default_function_MirrorPadInput_strides = (((DLTensor*)MirrorPadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_MirrorPadInput_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      ((float*)MirrorPadInput_1)[((i0 * 11) + i1)] = ((float*)data_1)[((((13 <= i0) ? (24 - i0) : ((i0 < 1) ? (0 - i0) : (i0 - 1))) * 8) + ((i1 == 10) ? (17 - i1) : ((i1 < 2) ? (1 - i1) : (i1 - 2))))];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 165) {\n    MirrorPadInput[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[((((143 <= ((((int)blockIdx.x) * 32) + ((int)threadIdx.x))) ? (24 - (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 11)) : ((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 11) ? 0 : ((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 11) - 1))) * 8) + (((((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 11) == 10) ? (17 - (((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 11)) : (((((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 11) < 2) ? (1 - (((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 11)) : ((((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 11) - 2))))];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 8), \"float32\"), MirrorPadInput: T.Buffer((15, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(15):\n            for i1 in range(11):\n                MirrorPadInput_1 = T.Buffer((165,), data=MirrorPadInput.data)\n                data_1 = T.Buffer((96,), data=data.data)\n                MirrorPadInput_1[i0 * 11 + i1] = data_1[T.if_then_else(13 <= i0, 24 - i0, T.if_then_else(i0 < 1, 0 - i0, i0 - 1)) * 8 + T.if_then_else(i1 == 10, 17 - i1, T.if_then_else(i1 < 2, 1 - i1, i1 - 2))]",
        "data": "12_8"
    },
    {
        "op_name": "mirror_pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t MirrorPadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* MirrorPadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* MirrorPadInput_1 = (((DLTensor*)MirrorPadInput)[0].data);\n  void* default_function_MirrorPadInput_shape = (((DLTensor*)MirrorPadInput)[0].shape);\n  void* default_function_MirrorPadInput_strides = (((DLTensor*)MirrorPadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_MirrorPadInput_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 1505; ++i0_i1_fused) {\n    int32_t cse_var_2 = (i0_i1_fused / 35);\n    int32_t cse_var_1 = (i0_i1_fused % 35);\n    ((float*)MirrorPadInput_1)[i0_i1_fused] = ((float*)data_1)[((((1435 <= i0_i1_fused) ? (80 - cse_var_2) : ((i0_i1_fused < 35) ? 0 : (cse_var_2 - 1))) * 32) + ((cse_var_1 == 34) ? 31 : ((cse_var_1 < 2) ? (1 - cse_var_1) : (cse_var_1 - 2))))];\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  MirrorPadInput[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] = data[((((41 <= ((int)blockIdx.x)) ? (80 - ((int)blockIdx.x)) : ((((int)blockIdx.x) < 1) ? (0 - ((int)blockIdx.x)) : (((int)blockIdx.x) - 1))) * 32) + ((((int)threadIdx.x) == 34) ? 31 : ((((int)threadIdx.x) < 2) ? (1 - ((int)threadIdx.x)) : (((int)threadIdx.x) - 2))))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((40, 32), \"float32\"), MirrorPadInput: T.Buffer((43, 35), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(1505):\n            cse_var_2: T.int32 = i0_i1_fused // 35\n            cse_var_1: T.int32 = i0_i1_fused % 35\n            MirrorPadInput_1 = T.Buffer((1505,), data=MirrorPadInput.data)\n            data_1 = T.Buffer((1280,), data=data.data)\n            MirrorPadInput_1[i0_i1_fused] = data_1[T.if_then_else(1435 <= i0_i1_fused, 80 - cse_var_2, T.if_then_else(i0_i1_fused < 35, 0, cse_var_2 - 1)) * 32 + T.if_then_else(cse_var_1 == 34, 31, T.if_then_else(cse_var_1 < 2, 1 - cse_var_1, cse_var_1 - 2))]",
        "data": "40_32"
    },
    {
        "op_name": "mirror_pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t MirrorPadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* MirrorPadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* MirrorPadInput_1 = (((DLTensor*)MirrorPadInput)[0].data);\n  void* default_function_MirrorPadInput_shape = (((DLTensor*)MirrorPadInput)[0].shape);\n  void* default_function_MirrorPadInput_strides = (((DLTensor*)MirrorPadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_MirrorPadInput_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 27; ++i0) {\n    for (int32_t i1 = 0; i1 < 27; ++i1) {\n      ((float*)MirrorPadInput_1)[((i0 * 27) + i1)] = ((float*)data_1)[((((25 <= i0) ? (48 - i0) : ((i0 < 1) ? (0 - i0) : (i0 - 1))) * 24) + ((i1 == 26) ? (49 - i1) : ((i1 < 2) ? (1 - i1) : (i1 - 2))))];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(26) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(26) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 26) + ((int)threadIdx.x)) < 729) {\n    MirrorPadInput[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))] = data[((((675 <= ((((int)blockIdx.x) * 26) + ((int)threadIdx.x))) ? (48 - (((((int)blockIdx.x) * 26) + ((int)threadIdx.x)) / 27)) : ((((((int)blockIdx.x) * 26) + ((int)threadIdx.x)) < 27) ? 0 : ((((((int)blockIdx.x) * 26) + ((int)threadIdx.x)) / 27) - 1))) * 24) + (((((((int)blockIdx.x) * 26) + ((int)threadIdx.x)) % 27) == 26) ? (49 - (((((int)blockIdx.x) * 26) + ((int)threadIdx.x)) % 27)) : (((((((int)blockIdx.x) * 26) + ((int)threadIdx.x)) % 27) < 2) ? (1 - (((((int)blockIdx.x) * 26) + ((int)threadIdx.x)) % 27)) : ((((((int)blockIdx.x) * 26) + ((int)threadIdx.x)) % 27) - 2))))];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((24, 24), \"float32\"), MirrorPadInput: T.Buffer((27, 27), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(27):\n            for i1 in range(27):\n                MirrorPadInput_1 = T.Buffer((729,), data=MirrorPadInput.data)\n                data_1 = T.Buffer((576,), data=data.data)\n                MirrorPadInput_1[i0 * 27 + i1] = data_1[T.if_then_else(25 <= i0, 48 - i0, T.if_then_else(i0 < 1, 0 - i0, i0 - 1)) * 24 + T.if_then_else(i1 == 26, 49 - i1, T.if_then_else(i1 < 2, 1 - i1, i1 - 2))]",
        "data": "24_24"
    },
    {
        "op_name": "mirror_pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t MirrorPadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* MirrorPadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* MirrorPadInput_1 = (((DLTensor*)MirrorPadInput)[0].data);\n  void* default_function_MirrorPadInput_shape = (((DLTensor*)MirrorPadInput)[0].shape);\n  void* default_function_MirrorPadInput_strides = (((DLTensor*)MirrorPadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_MirrorPadInput_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 27; ++i0) {\n    for (int32_t i1 = 0; i1 < 39; ++i1) {\n      ((float*)MirrorPadInput_1)[((i0 * 39) + i1)] = ((float*)data_1)[((((25 <= i0) ? (48 - i0) : ((i0 < 1) ? (0 - i0) : (i0 - 1))) * 36) + ((i1 == 38) ? (73 - i1) : ((i1 < 2) ? (1 - i1) : (i1 - 2))))];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(39) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(39) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  MirrorPadInput[((((int)blockIdx.x) * 39) + ((int)threadIdx.x))] = data[((((25 <= ((int)blockIdx.x)) ? (48 - ((int)blockIdx.x)) : ((((int)blockIdx.x) < 1) ? (0 - ((int)blockIdx.x)) : (((int)blockIdx.x) - 1))) * 36) + ((((int)threadIdx.x) == 38) ? 35 : ((((int)threadIdx.x) < 2) ? (1 - ((int)threadIdx.x)) : (((int)threadIdx.x) - 2))))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((24, 36), \"float32\"), MirrorPadInput: T.Buffer((27, 39), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(27):\n            for i1 in range(39):\n                MirrorPadInput_1 = T.Buffer((1053,), data=MirrorPadInput.data)\n                data_1 = T.Buffer((864,), data=data.data)\n                MirrorPadInput_1[i0 * 39 + i1] = data_1[T.if_then_else(25 <= i0, 48 - i0, T.if_then_else(i0 < 1, 0 - i0, i0 - 1)) * 36 + T.if_then_else(i1 == 38, 73 - i1, T.if_then_else(i1 < 2, 1 - i1, i1 - 2))]",
        "data": "24_36"
    },
    {
        "op_name": "mirror_pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t MirrorPadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* MirrorPadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* MirrorPadInput_1 = (((DLTensor*)MirrorPadInput)[0].data);\n  void* default_function_MirrorPadInput_shape = (((DLTensor*)MirrorPadInput)[0].shape);\n  void* default_function_MirrorPadInput_strides = (((DLTensor*)MirrorPadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_MirrorPadInput_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 31; ++i0) {\n    for (int32_t i1 = 0; i1 < 23; ++i1) {\n      ((float*)MirrorPadInput_1)[((i0 * 23) + i1)] = ((float*)data_1)[((((29 <= i0) ? (56 - i0) : ((i0 < 1) ? (0 - i0) : (i0 - 1))) * 20) + ((i1 == 22) ? (41 - i1) : ((i1 < 2) ? (1 - i1) : (i1 - 2))))];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) < 713) {\n    MirrorPadInput[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((667 <= ((((int)blockIdx.x) * 64) + ((int)threadIdx.x))) ? (56 - (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 23)) : ((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) < 23) ? 0 : ((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 23) - 1))) * 20) + (((((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) % 23) == 22) ? (41 - (((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) % 23)) : (((((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) % 23) < 2) ? (1 - (((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) % 23)) : ((((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) % 23) - 2))))];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((28, 20), \"float32\"), MirrorPadInput: T.Buffer((31, 23), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(31):\n            for i1 in range(23):\n                MirrorPadInput_1 = T.Buffer((713,), data=MirrorPadInput.data)\n                data_1 = T.Buffer((560,), data=data.data)\n                MirrorPadInput_1[i0 * 23 + i1] = data_1[T.if_then_else(29 <= i0, 56 - i0, T.if_then_else(i0 < 1, 0 - i0, i0 - 1)) * 20 + T.if_then_else(i1 == 22, 41 - i1, T.if_then_else(i1 < 2, 1 - i1, i1 - 2))]",
        "data": "28_20"
    },
    {
        "op_name": "pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t PadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* PadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* PadInput_1 = (((DLTensor*)PadInput)[0].data);\n  void* default_function_PadInput_shape = (((DLTensor*)PadInput)[0].shape);\n  void* default_function_PadInput_strides = (((DLTensor*)PadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_PadInput_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 35; ++i0) {\n    for (int32_t i1 = 0; i1 < 23; ++i1) {\n      ((float*)PadInput_1)[((i0 * 23) + i1)] = (((((1 <= i0) && (i0 < 33)) && (2 <= i1)) && (i1 < 22)) ? ((float*)data_1)[(((i0 * 20) + i1) - 22)] : 0.000000e+00f);\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(23) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(23) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  PadInput[((((int)blockIdx.x) * 23) + ((int)threadIdx.x))] = (((((1 <= ((int)blockIdx.x)) && (((int)blockIdx.x) < 33)) && (2 <= ((int)threadIdx.x))) && (((int)threadIdx.x) < 22)) ? data[(((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) - 22)] : 0.000000e+00f);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((32, 20), \"float32\"), PadInput: T.Buffer((35, 23), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(35):\n            for i1 in range(23):\n                PadInput_1 = T.Buffer((805,), data=PadInput.data)\n                data_1 = T.Buffer((640,), data=data.data)\n                PadInput_1[i0 * 23 + i1] = T.if_then_else(1 <= i0 and i0 < 33 and 2 <= i1 and i1 < 22, data_1[i0 * 20 + i1 - 22], T.float32(0))",
        "data": "32_20"
    },
    {
        "op_name": "pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t PadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* PadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* PadInput_1 = (((DLTensor*)PadInput)[0].data);\n  void* default_function_PadInput_shape = (((DLTensor*)PadInput)[0].shape);\n  void* default_function_PadInput_strides = (((DLTensor*)PadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_PadInput_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 585; ++i0_i1_fused) {\n    int32_t cse_var_1 = (i0_i1_fused % 15);\n    ((float*)PadInput_1)[i0_i1_fused] = (((((15 <= i0_i1_fused) && (i0_i1_fused < 555)) && (2 <= cse_var_1)) && (cse_var_1 < 14)) ? ((float*)data_1)[((((i0_i1_fused / 15) * 12) + cse_var_1) - 14)] : 0.000000e+00f);\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(39) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(39) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  PadInput[((((int)blockIdx.x) * 39) + ((int)threadIdx.x))] = (((((5 <= ((((int)blockIdx.x) * 13) + (((int)threadIdx.x) / 3))) && (((((int)blockIdx.x) * 13) + (((int)threadIdx.x) / 3)) < 185)) && (2 <= (((((int)blockIdx.x) * 9) + ((int)threadIdx.x)) % 15))) && ((((((int)blockIdx.x) * 9) + ((int)threadIdx.x)) % 15) < 14)) ? data[((((((((int)blockIdx.x) * 13) + (((int)threadIdx.x) / 3)) / 5) * 12) + (((((int)blockIdx.x) * 9) + ((int)threadIdx.x)) % 15)) - 14)] : 0.000000e+00f);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((36, 12), \"float32\"), PadInput: T.Buffer((39, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(585):\n            cse_var_1: T.int32 = i0_i1_fused % 15\n            PadInput_1 = T.Buffer((585,), data=PadInput.data)\n            data_1 = T.Buffer((432,), data=data.data)\n            PadInput_1[i0_i1_fused] = T.if_then_else(15 <= i0_i1_fused and i0_i1_fused < 555 and 2 <= cse_var_1 and cse_var_1 < 14, data_1[i0_i1_fused // 15 * 12 + cse_var_1 - 14], T.float32(0))",
        "data": "36_12"
    },
    {
        "op_name": "pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t PadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* PadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* PadInput_1 = (((DLTensor*)PadInput)[0].data);\n  void* default_function_PadInput_shape = (((DLTensor*)PadInput)[0].shape);\n  void* default_function_PadInput_strides = (((DLTensor*)PadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_PadInput_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      ((float*)PadInput_1)[((i0 * 19) + i1)] = (((((1 <= i0) && (i0 < 5)) && (2 <= i1)) && (i1 < 18)) ? ((float*)data_1)[(((i0 * 16) + i1) - 18)] : 0.000000e+00f);\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) < 133) {\n    PadInput[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = (((((19 <= ((((int)blockIdx.x) * 6) + ((int)threadIdx.x))) && (((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) < 95)) && (2 <= (((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 19))) && ((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 19) < 18)) ? data[((((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) / 19) * 16) + (((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 19)) - 18)] : 0.000000e+00f);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 16), \"float32\"), PadInput: T.Buffer((7, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(7):\n            for i1 in range(19):\n                PadInput_1 = T.Buffer((133,), data=PadInput.data)\n                data_1 = T.Buffer((64,), data=data.data)\n                PadInput_1[i0 * 19 + i1] = T.if_then_else(1 <= i0 and i0 < 5 and 2 <= i1 and i1 < 18, data_1[i0 * 16 + i1 - 18], T.float32(0))",
        "data": "4_16"
    },
    {
        "op_name": "pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t PadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* PadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* PadInput_1 = (((DLTensor*)PadInput)[0].data);\n  void* default_function_PadInput_shape = (((DLTensor*)PadInput)[0].shape);\n  void* default_function_PadInput_strides = (((DLTensor*)PadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_PadInput_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 1085; ++i0_i1_fused) {\n    int32_t cse_var_1 = (i0_i1_fused % 35);\n    ((float*)PadInput_1)[i0_i1_fused] = (((((35 <= i0_i1_fused) && (i0_i1_fused < 1015)) && (2 <= cse_var_1)) && (cse_var_1 < 34)) ? ((float*)data_1)[((((i0_i1_fused / 35) * 32) + cse_var_1) - 34)] : 0.000000e+00f);\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(31) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(31) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  PadInput[((((int)blockIdx.x) * 31) + ((int)threadIdx.x))] = (((((35 <= ((((int)blockIdx.x) * 31) + ((int)threadIdx.x))) && (((((int)blockIdx.x) * 31) + ((int)threadIdx.x)) < 1015)) && (2 <= (((((int)blockIdx.x) * 31) + ((int)threadIdx.x)) % 35))) && ((((((int)blockIdx.x) * 31) + ((int)threadIdx.x)) % 35) < 34)) ? data[((((((((int)blockIdx.x) * 31) + ((int)threadIdx.x)) / 35) * 32) + (((((int)blockIdx.x) * 31) + ((int)threadIdx.x)) % 35)) - 34)] : 0.000000e+00f);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((28, 32), \"float32\"), PadInput: T.Buffer((31, 35), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(1085):\n            cse_var_1: T.int32 = i0_i1_fused % 35\n            PadInput_1 = T.Buffer((1085,), data=PadInput.data)\n            data_1 = T.Buffer((896,), data=data.data)\n            PadInput_1[i0_i1_fused] = T.if_then_else(35 <= i0_i1_fused and i0_i1_fused < 1015 and 2 <= cse_var_1 and cse_var_1 < 34, data_1[i0_i1_fused // 35 * 32 + cse_var_1 - 34], T.float32(0))",
        "data": "28_32"
    },
    {
        "op_name": "pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t PadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* PadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* PadInput_1 = (((DLTensor*)PadInput)[0].data);\n  void* default_function_PadInput_shape = (((DLTensor*)PadInput)[0].shape);\n  void* default_function_PadInput_strides = (((DLTensor*)PadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_PadInput_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 35; ++i0) {\n    for (int32_t i1 = 0; i1 < 35; ++i1) {\n      ((float*)PadInput_1)[((i0 * 35) + i1)] = (((((1 <= i0) && (i0 < 33)) && (2 <= i1)) && (i1 < 34)) ? ((float*)data_1)[(((i0 * 32) + i1) - 34)] : 0.000000e+00f);\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) < 1225) {\n    PadInput[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = (((((35 <= ((((int)blockIdx.x) * 6) + ((int)threadIdx.x))) && (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 3)) < 385)) && (2 <= (((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 35))) && ((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 35) < 34)) ? data[((((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) / 35) * 32) + (((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 35)) - 34)] : 0.000000e+00f);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((32, 32), \"float32\"), PadInput: T.Buffer((35, 35), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(35):\n            for i1 in range(35):\n                PadInput_1 = T.Buffer((1225,), data=PadInput.data)\n                data_1 = T.Buffer((1024,), data=data.data)\n                PadInput_1[i0 * 35 + i1] = T.if_then_else(1 <= i0 and i0 < 33 and 2 <= i1 and i1 < 34, data_1[i0 * 32 + i1 - 34], T.float32(0))",
        "data": "32_32"
    },
    {
        "op_name": "pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t PadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* PadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* PadInput_1 = (((DLTensor*)PadInput)[0].data);\n  void* default_function_PadInput_shape = (((DLTensor*)PadInput)[0].shape);\n  void* default_function_PadInput_strides = (((DLTensor*)PadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_PadInput_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      ((float*)PadInput_1)[((i0 * 7) + i1)] = (((((1 <= i0) && (i0 < 13)) && (2 <= i1)) && (i1 < 6)) ? ((float*)data_1)[(((i0 * 4) + i1) - 6)] : 0.000000e+00f);\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  PadInput[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = (((((1 <= ((int)blockIdx.x)) && (((int)blockIdx.x) < 13)) && (2 <= ((int)threadIdx.x))) && (((int)threadIdx.x) < 6)) ? data[(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) - 6)] : 0.000000e+00f);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 4), \"float32\"), PadInput: T.Buffer((15, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(15):\n            for i1 in range(7):\n                PadInput_1 = T.Buffer((105,), data=PadInput.data)\n                data_1 = T.Buffer((48,), data=data.data)\n                PadInput_1[i0 * 7 + i1] = T.if_then_else(1 <= i0 and i0 < 13 and 2 <= i1 and i1 < 6, data_1[i0 * 4 + i1 - 6], T.float32(0))",
        "data": "12_4"
    },
    {
        "op_name": "pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t PadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* PadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* PadInput_1 = (((DLTensor*)PadInput)[0].data);\n  void* default_function_PadInput_shape = (((DLTensor*)PadInput)[0].shape);\n  void* default_function_PadInput_strides = (((DLTensor*)PadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_PadInput_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 285; ++i0_i1_fused) {\n    int32_t cse_var_1 = (i0_i1_fused % 15);\n    ((float*)PadInput_1)[i0_i1_fused] = (((((15 <= i0_i1_fused) && (i0_i1_fused < 255)) && (2 <= cse_var_1)) && (cse_var_1 < 14)) ? ((float*)data_1)[((((i0_i1_fused / 15) * 12) + cse_var_1) - 14)] : 0.000000e+00f);\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  PadInput[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = (((((5 <= ((int)blockIdx.x)) && (((int)blockIdx.x) < 85)) && (2 <= (((((int)blockIdx.x) % 5) * 3) + ((int)threadIdx.x)))) && ((((((int)blockIdx.x) % 5) * 3) + ((int)threadIdx.x)) < 14)) ? data[(((((((int)blockIdx.x) / 5) * 12) + ((((int)blockIdx.x) % 5) * 3)) + ((int)threadIdx.x)) - 14)] : 0.000000e+00f);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 12), \"float32\"), PadInput: T.Buffer((19, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(285):\n            cse_var_1: T.int32 = i0_i1_fused % 15\n            PadInput_1 = T.Buffer((285,), data=PadInput.data)\n            data_1 = T.Buffer((192,), data=data.data)\n            PadInput_1[i0_i1_fused] = T.if_then_else(15 <= i0_i1_fused and i0_i1_fused < 255 and 2 <= cse_var_1 and cse_var_1 < 14, data_1[i0_i1_fused // 15 * 12 + cse_var_1 - 14], T.float32(0))",
        "data": "16_12"
    },
    {
        "op_name": "pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t PadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* PadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* PadInput_1 = (((DLTensor*)PadInput)[0].data);\n  void* default_function_PadInput_shape = (((DLTensor*)PadInput)[0].shape);\n  void* default_function_PadInput_strides = (((DLTensor*)PadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_PadInput_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 35; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      ((float*)PadInput_1)[((i0 * 11) + i1)] = (((((1 <= i0) && (i0 < 33)) && (2 <= i1)) && (i1 < 10)) ? ((float*)data_1)[(((i0 * 8) + i1) - 10)] : 0.000000e+00f);\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  PadInput[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = (((((11 <= ((((int)blockIdx.x) * 7) + ((int)threadIdx.x))) && (((((int)blockIdx.x) * 7) + ((int)threadIdx.x)) < 363)) && (2 <= (((((int)blockIdx.x) * 7) + ((int)threadIdx.x)) % 11))) && ((((((int)blockIdx.x) * 7) + ((int)threadIdx.x)) % 11) < 10)) ? data[((((((((int)blockIdx.x) * 7) + ((int)threadIdx.x)) / 11) * 8) + (((((int)blockIdx.x) * 7) + ((int)threadIdx.x)) % 11)) - 10)] : 0.000000e+00f);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((32, 8), \"float32\"), PadInput: T.Buffer((35, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(35):\n            for i1 in range(11):\n                PadInput_1 = T.Buffer((385,), data=PadInput.data)\n                data_1 = T.Buffer((256,), data=data.data)\n                PadInput_1[i0 * 11 + i1] = T.if_then_else(1 <= i0 and i0 < 33 and 2 <= i1 and i1 < 10, data_1[i0 * 8 + i1 - 10], T.float32(0))",
        "data": "32_8"
    },
    {
        "op_name": "pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t PadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* PadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* PadInput_1 = (((DLTensor*)PadInput)[0].data);\n  void* default_function_PadInput_shape = (((DLTensor*)PadInput)[0].shape);\n  void* default_function_PadInput_strides = (((DLTensor*)PadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_PadInput_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 27; ++i0) {\n    for (int32_t i1 = 0; i1 < 39; ++i1) {\n      ((float*)PadInput_1)[((i0 * 39) + i1)] = (((((1 <= i0) && (i0 < 25)) && (2 <= i1)) && (i1 < 38)) ? ((float*)data_1)[(((i0 * 36) + i1) - 38)] : 0.000000e+00f);\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 1053) {\n    PadInput[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (((((39 <= ((((int)blockIdx.x) * 32) + ((int)threadIdx.x))) && (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 975)) && (2 <= (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 39))) && ((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 39) < 38)) ? data[((((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 39) * 36) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 39)) - 38)] : 0.000000e+00f);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((24, 36), \"float32\"), PadInput: T.Buffer((27, 39), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(27):\n            for i1 in range(39):\n                PadInput_1 = T.Buffer((1053,), data=PadInput.data)\n                data_1 = T.Buffer((864,), data=data.data)\n                PadInput_1[i0 * 39 + i1] = T.if_then_else(1 <= i0 and i0 < 25 and 2 <= i1 and i1 < 38, data_1[i0 * 36 + i1 - 38], T.float32(0))",
        "data": "24_36"
    },
    {
        "op_name": "pad",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t PadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* PadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* PadInput_1 = (((DLTensor*)PadInput)[0].data);\n  void* default_function_PadInput_shape = (((DLTensor*)PadInput)[0].shape);\n  void* default_function_PadInput_strides = (((DLTensor*)PadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_PadInput_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 741; ++i0_i1_fused) {\n    int32_t cse_var_1 = (i0_i1_fused % 19);\n    ((float*)PadInput_1)[i0_i1_fused] = (((((19 <= i0_i1_fused) && (i0_i1_fused < 703)) && (2 <= cse_var_1)) && (cse_var_1 < 18)) ? ((float*)data_1)[((((i0_i1_fused / 19) * 16) + cse_var_1) - 18)] : 0.000000e+00f);\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) < 741) {\n    PadInput[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = (((((19 <= ((((int)blockIdx.x) * 20) + ((int)threadIdx.x))) && (((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) < 703)) && (2 <= ((((int)blockIdx.x) + ((int)threadIdx.x)) % 19))) && (((((int)blockIdx.x) + ((int)threadIdx.x)) % 19) < 18)) ? data[((((((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) / 19) * 16) + ((((int)blockIdx.x) + ((int)threadIdx.x)) % 19)) - 18)] : 0.000000e+00f);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((36, 16), \"float32\"), PadInput: T.Buffer((39, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(741):\n            cse_var_1: T.int32 = i0_i1_fused % 19\n            PadInput_1 = T.Buffer((741,), data=PadInput.data)\n            data_1 = T.Buffer((576,), data=data.data)\n            PadInput_1[i0_i1_fused] = T.if_then_else(19 <= i0_i1_fused and i0_i1_fused < 703 and 2 <= cse_var_1 and cse_var_1 < 18, data_1[i0_i1_fused // 19 * 16 + cse_var_1 - 18], T.float32(0))",
        "data": "36_16"
    },
    {
        "op_name": "pool1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    float pad_temp[232];\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 29; ++ax2) {\n        pad_temp[((ax1 * 29) + ax2)] = ((1 <= ax2) ? ((float*)data_1)[((((ax0 * 224) + (ax1 * 28)) + ax2) - 1)] : -3.402823e+38f);\n      },\n    },\n    for (int32_t ax1_1 = 0; ax1_1 < 8; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 14; ++ax2_1) {\n        ((float*)pool_max_1)[(((ax0 * 112) + (ax1_1 * 14)) + ax2_1)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          int32_t cse_var_1 = (((ax0 * 112) + (ax1_1 * 14)) + ax2_1);\n          float v_ = ((float*)pool_max_1)[cse_var_1];\n          float v__1 = pad_temp[(((ax1_1 * 29) + (ax2_1 * 2)) + rv0)];\n          ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], ((1 <= (((((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 14) * 2) + rv0)) ? data[((((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 2)) + rv0) - 1)] : -3.402823e+38f));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 8, 28), \"float32\"), pool_max: T.Buffer((8, 8, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(8):\n            pad_temp = T.allocate([232], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((232,), data=pad_temp)\n            for ax1, ax2 in T.grid(8, 29):\n                data_1 = T.Buffer((1792,), data=data.data)\n                pad_temp_1[ax1 * 29 + ax2] = T.if_then_else(1 <= ax2, data_1[ax0 * 224 + ax1 * 28 + ax2 - 1], T.float32(-3.4028234663852886e+38))\n            for ax1, ax2 in T.grid(8, 14):\n                pool_max_1 = T.Buffer((896,), data=pool_max.data)\n                pool_max_1[ax0 * 112 + ax1 * 14 + ax2] = T.float32(-3.4028234663852886e+38)\n                for rv0 in range(3):\n                    cse_var_1: T.int32 = ax0 * 112 + ax1 * 14 + ax2\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax1 * 29 + ax2 * 2 + rv0])",
        "data": "8_8_28"
    },
    {
        "op_name": "pool1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    float pad_temp[72];\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2_s = 0; ax2_s < 9; ++ax2_s) {\n        pad_temp[((ax1 * 9) + ax2_s)] = ((1 <= ax2_s) ? ((float*)data_1)[((((ax0 * 64) + (ax1 * 8)) + ax2_s) - 1)] : -3.402823e+38f);\n      },\n    },\n    for (int32_t ax1_1 = 0; ax1_1 < 8; ++ax1_1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        ((float*)pool_max_1)[(((ax0 * 32) + (ax1_1 * 4)) + ax2)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          int32_t cse_var_1 = (((ax0 * 32) + (ax1_1 * 4)) + ax2);\n          float v_ = ((float*)pool_max_1)[cse_var_1];\n          float v__1 = pad_temp[(((ax1_1 * 9) + (ax2 * 2)) + rv0)];\n          ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((int)threadIdx.x)] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((int)threadIdx.x)] = max(pool_max[((int)threadIdx.x)], ((1 <= (((((int)threadIdx.x) & 3) * 2) + rv0)) ? data[(((((int)threadIdx.x) * 2) + rv0) - 1)] : -3.402823e+38f));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 8, 8), \"float32\"), pool_max: T.Buffer((2, 8, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(2):\n            pad_temp = T.allocate([72], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((72,), data=pad_temp)\n            for ax1, ax2_s in T.grid(8, 9):\n                data_1 = T.Buffer((128,), data=data.data)\n                pad_temp_1[ax1 * 9 + ax2_s] = T.if_then_else(1 <= ax2_s, data_1[ax0 * 64 + ax1 * 8 + ax2_s - 1], T.float32(-3.4028234663852886e+38))\n            for ax1, ax2 in T.grid(8, 4):\n                pool_max_1 = T.Buffer((64,), data=pool_max.data)\n                pool_max_1[ax0 * 32 + ax1 * 4 + ax2] = T.float32(-3.4028234663852886e+38)\n                for rv0 in range(3):\n                    cse_var_1: T.int32 = ax0 * 32 + ax1 * 4 + ax2\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax1 * 9 + ax2 * 2 + rv0])",
        "data": "2_8_8"
    },
    {
        "op_name": "pool1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    float pad_temp[52];\n    for (int32_t ax1 = 0; ax1 < 4; ++ax1) {\n      for (int32_t ax2_s = 0; ax2_s < 13; ++ax2_s) {\n        pad_temp[((ax1 * 13) + ax2_s)] = ((1 <= ax2_s) ? ((float*)data_1)[((((ax0 * 48) + (ax1 * 12)) + ax2_s) - 1)] : -3.402823e+38f);\n      },\n    },\n    for (int32_t ax1_1 = 0; ax1_1 < 4; ++ax1_1) {\n      for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n        ((float*)pool_max_1)[(((ax0 * 24) + (ax1_1 * 6)) + ax2)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          int32_t cse_var_1 = (((ax0 * 24) + (ax1_1 * 6)) + ax2);\n          float v_ = ((float*)pool_max_1)[cse_var_1];\n          float v__1 = pad_temp[(((ax1_1 * 13) + (ax2 * 2)) + rv0)];\n          ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((int)threadIdx.x)] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((int)threadIdx.x)] = max(pool_max[((int)threadIdx.x)], ((1 <= (((((int)threadIdx.x) % 6) * 2) + rv0)) ? data[(((((int)threadIdx.x) * 2) + rv0) - 1)] : -3.402823e+38f));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 4, 12), \"float32\"), pool_max: T.Buffer((2, 4, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(2):\n            pad_temp = T.allocate([52], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((52,), data=pad_temp)\n            for ax1, ax2_s in T.grid(4, 13):\n                data_1 = T.Buffer((96,), data=data.data)\n                pad_temp_1[ax1 * 13 + ax2_s] = T.if_then_else(1 <= ax2_s, data_1[ax0 * 48 + ax1 * 12 + ax2_s - 1], T.float32(-3.4028234663852886e+38))\n            for ax1, ax2 in T.grid(4, 6):\n                pool_max_1 = T.Buffer((48,), data=pool_max.data)\n                pool_max_1[ax0 * 24 + ax1 * 6 + ax2] = T.float32(-3.4028234663852886e+38)\n                for rv0 in range(3):\n                    cse_var_1: T.int32 = ax0 * 24 + ax1 * 6 + ax2\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax1 * 13 + ax2 * 2 + rv0])",
        "data": "2_4_12"
    },
    {
        "op_name": "pool1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  float pad_temp[3];\n  for (int32_t ax1 = 0; ax1 < 36; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n      for (int32_t ax2_s = 0; ax2_s < 3; ++ax2_s) {\n        int32_t cse_var_1 = (ax2 * 2);\n        pad_temp[ax2_s] = ((1 <= (cse_var_1 + ax2_s)) ? ((float*)data_1)[((((ax1 * 28) + cse_var_1) + ax2_s) - 1)] : -3.402823e+38f);\n      },\n      ((float*)pool_max_1)[((ax1 * 14) + ax2)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        int32_t cse_var_2 = ((ax1 * 14) + ax2);\n        float v_ = ((float*)pool_max_1)[cse_var_2];\n        float v__1 = pad_temp[rv0];\n        ((float*)pool_max_1)[cse_var_2] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 63) {\n    pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  },\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 63) {\n      pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ((1 <= (((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 14) * 2) + rv0)) ? data[((((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 2)) + rv0) - 1)] : -3.402823e+38f));\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 36, 28), \"float32\"), pool_max: T.Buffer((1, 36, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        pad_temp = T.allocate([3], \"float32\", \"global\")\n        for ax1, ax2 in T.grid(36, 14):\n            pad_temp_1 = T.Buffer((3,), data=pad_temp, align=8)\n            for ax2_s in range(3):\n                cse_var_1: T.int32 = ax2 * 2\n                data_1 = T.Buffer((1008,), data=data.data)\n                pad_temp_1[ax2_s] = T.if_then_else(1 <= cse_var_1 + ax2_s, data_1[ax1 * 28 + cse_var_1 + ax2_s - 1], T.float32(-3.4028234663852886e+38))\n            pool_max_1 = T.Buffer((504,), data=pool_max.data)\n            pool_max_1[ax1 * 14 + ax2] = T.float32(-3.4028234663852886e+38)\n            for rv0 in range(3):\n                cse_var_2: T.int32 = ax1 * 14 + ax2\n                pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[rv0])",
        "data": "1_36_28"
    },
    {
        "op_name": "pool1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    float pad_temp[180];\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      for (int32_t ax2_s = 0; ax2_s < 9; ++ax2_s) {\n        pad_temp[((ax1 * 9) + ax2_s)] = ((1 <= ax2_s) ? ((float*)data_1)[((((ax0 * 160) + (ax1 * 8)) + ax2_s) - 1)] : -3.402823e+38f);\n      },\n    },\n    for (int32_t ax1_1 = 0; ax1_1 < 20; ++ax1_1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        ((float*)pool_max_1)[(((ax0 * 80) + (ax1_1 * 4)) + ax2)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          int32_t cse_var_1 = (((ax0 * 80) + (ax1_1 * 4)) + ax2);\n          float v_ = ((float*)pool_max_1)[cse_var_1];\n          float v__1 = pad_temp[(((ax1_1 * 9) + (ax2 * 2)) + rv0)];\n          ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], ((1 <= ((((((int)blockIdx.x) & 1) * 4) + (((int)threadIdx.x) * 2)) + rv0)) ? data[((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) * 2)) + rv0) - 1)] : -3.402823e+38f));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 20, 8), \"float32\"), pool_max: T.Buffer((2, 20, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(2):\n            pad_temp = T.allocate([180], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((180,), data=pad_temp)\n            for ax1, ax2_s in T.grid(20, 9):\n                data_1 = T.Buffer((320,), data=data.data)\n                pad_temp_1[ax1 * 9 + ax2_s] = T.if_then_else(1 <= ax2_s, data_1[ax0 * 160 + ax1 * 8 + ax2_s - 1], T.float32(-3.4028234663852886e+38))\n            for ax1, ax2 in T.grid(20, 4):\n                pool_max_1 = T.Buffer((160,), data=pool_max.data)\n                pool_max_1[ax0 * 80 + ax1 * 4 + ax2] = T.float32(-3.4028234663852886e+38)\n                for rv0 in range(3):\n                    cse_var_1: T.int32 = ax0 * 80 + ax1 * 4 + ax2\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax1 * 9 + ax2 * 2 + rv0])",
        "data": "2_20_8"
    },
    {
        "op_name": "pool1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 96; ++ax0_ax1_fused) {\n    float pad_temp[3];\n    for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n      for (int32_t ax2_s = 0; ax2_s < 3; ++ax2_s) {\n        int32_t cse_var_1 = (ax2 * 2);\n        pad_temp[ax2_s] = ((1 <= (cse_var_1 + ax2_s)) ? ((float*)data_1)[((((ax0_ax1_fused * 4) + cse_var_1) + ax2_s) - 1)] : -3.402823e+38f);\n      },\n      ((float*)pool_max_1)[((ax0_ax1_fused * 2) + ax2)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        int32_t cse_var_2 = ((ax0_ax1_fused * 2) + ax2);\n        float v_ = ((float*)pool_max_1)[cse_var_2];\n        float v__1 = pad_temp[rv0];\n        ((float*)pool_max_1)[cse_var_2] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ((1 <= (((((int)threadIdx.x) & 1) * 2) + rv0)) ? data[((((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 2)) + rv0) - 1)] : -3.402823e+38f));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 12, 4), \"float32\"), pool_max: T.Buffer((8, 12, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(96):\n            pad_temp = T.allocate([3], \"float32\", \"global\")\n            for ax2 in range(2):\n                pad_temp_1 = T.Buffer((3,), data=pad_temp, align=8)\n                for ax2_s in range(3):\n                    cse_var_1: T.int32 = ax2 * 2\n                    data_1 = T.Buffer((384,), data=data.data)\n                    pad_temp_1[ax2_s] = T.if_then_else(1 <= cse_var_1 + ax2_s, data_1[ax0_ax1_fused * 4 + cse_var_1 + ax2_s - 1], T.float32(-3.4028234663852886e+38))\n                pool_max_1 = T.Buffer((192,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 2 + ax2] = T.float32(-3.4028234663852886e+38)\n                for rv0 in range(3):\n                    cse_var_2: T.int32 = ax0_ax1_fused * 2 + ax2\n                    pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[rv0])",
        "data": "8_12_4"
    },
    {
        "op_name": "pool1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1856, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 29; ++ax2) {\n        ((float*)pad_temp)[((ax1 * 29) + ax2)] = ((1 <= ax2) ? ((float*)data_1)[((((ax0 * 448) + (ax1 * 28)) + ax2) - 1)] : -3.402823e+38f);\n      },\n    },\n    for (int32_t ax1_1 = 0; ax1_1 < 16; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 14; ++ax2_1) {\n        ((float*)pool_max_1)[(((ax0 * 224) + (ax1_1 * 14)) + ax2_1)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          int32_t cse_var_1 = (((ax0 * 224) + (ax1_1 * 14)) + ax2_1);\n          float v_ = ((float*)pool_max_1)[cse_var_1];\n          float v__1 = ((float*)pad_temp)[(((ax1_1 * 29) + (ax2_1 * 2)) + rv0)];\n          ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], ((1 <= (((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 14) * 2) + rv0)) ? data[((((((int)blockIdx.x) * 96) + (((int)threadIdx.x) * 2)) + rv0) - 1)] : -3.402823e+38f));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 16, 28), \"float32\"), pool_max: T.Buffer((3, 16, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(3):\n            pad_temp = T.allocate([464], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((464,), data=pad_temp)\n            for ax1, ax2 in T.grid(16, 29):\n                data_1 = T.Buffer((1344,), data=data.data)\n                pad_temp_1[ax1 * 29 + ax2] = T.if_then_else(1 <= ax2, data_1[ax0 * 448 + ax1 * 28 + ax2 - 1], T.float32(-3.4028234663852886e+38))\n            for ax1, ax2 in T.grid(16, 14):\n                pool_max_1 = T.Buffer((672,), data=pool_max.data)\n                pool_max_1[ax0 * 224 + ax1 * 14 + ax2] = T.float32(-3.4028234663852886e+38)\n                for rv0 in range(3):\n                    cse_var_1: T.int32 = ax0 * 224 + ax1 * 14 + ax2\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax1 * 29 + ax2 * 2 + rv0])",
        "data": "3_16_28"
    },
    {
        "op_name": "pool1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    float pad_temp[3];\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n        for (int32_t ax2_s = 0; ax2_s < 3; ++ax2_s) {\n          int32_t cse_var_1 = (ax2 * 2);\n          pad_temp[ax2_s] = ((1 <= (cse_var_1 + ax2_s)) ? ((float*)data_1)[(((((ax0 * 320) + (ax1 * 40)) + cse_var_1) + ax2_s) - 1)] : -3.402823e+38f);\n        },\n        ((float*)pool_max_1)[(((ax0 * 160) + (ax1 * 20)) + ax2)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          int32_t cse_var_2 = (((ax0 * 160) + (ax1 * 20)) + ax2);\n          float v_ = ((float*)pool_max_1)[cse_var_2];\n          float v__1 = pad_temp[rv0];\n          ((float*)pool_max_1)[cse_var_2] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], ((1 <= (((((int)threadIdx.x) % 20) * 2) + rv0)) ? data[((((((int)blockIdx.x) * 80) + (((int)threadIdx.x) * 2)) + rv0) - 1)] : -3.402823e+38f));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 8, 40), \"float32\"), pool_max: T.Buffer((7, 8, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(7):\n            pad_temp = T.allocate([3], \"float32\", \"global\")\n            for ax1, ax2 in T.grid(8, 20):\n                pad_temp_1 = T.Buffer((3,), data=pad_temp, align=8)\n                for ax2_s in range(3):\n                    cse_var_1: T.int32 = ax2 * 2\n                    data_1 = T.Buffer((2240,), data=data.data)\n                    pad_temp_1[ax2_s] = T.if_then_else(1 <= cse_var_1 + ax2_s, data_1[ax0 * 320 + ax1 * 40 + cse_var_1 + ax2_s - 1], T.float32(-3.4028234663852886e+38))\n                pool_max_1 = T.Buffer((1120,), data=pool_max.data)\n                pool_max_1[ax0 * 160 + ax1 * 20 + ax2] = T.float32(-3.4028234663852886e+38)\n                for rv0 in range(3):\n                    cse_var_2: T.int32 = ax0 * 160 + ax1 * 20 + ax2\n                    pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[rv0])",
        "data": "7_8_40"
    },
    {
        "op_name": "pool1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    float pad_temp[29];\n    for (int32_t ax1 = 0; ax1 < 24; ++ax1) {\n      for (int32_t ax2_s = 0; ax2_s < 29; ++ax2_s) {\n        pad_temp[ax2_s] = ((1 <= ax2_s) ? ((float*)data_1)[((((ax0 * 672) + (ax1 * 28)) + ax2_s) - 1)] : -3.402823e+38f);\n      },\n      for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n        ((float*)pool_max_1)[(((ax0 * 336) + (ax1 * 14)) + ax2)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          int32_t cse_var_1 = (((ax0 * 336) + (ax1 * 14)) + ax2);\n          float v_ = ((float*)pool_max_1)[cse_var_1];\n          float v__1 = pad_temp[((ax2 * 2) + rv0)];\n          ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], ((1 <= (((((int)threadIdx.x) % 14) * 2) + rv0)) ? data[((((((int)blockIdx.x) * 56) + (((int)threadIdx.x) * 2)) + rv0) - 1)] : -3.402823e+38f));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 24, 28), \"float32\"), pool_max: T.Buffer((7, 24, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(7):\n            pad_temp = T.allocate([29], \"float32\", \"global\")\n            for ax1 in range(24):\n                pad_temp_1 = T.Buffer((29,), data=pad_temp)\n                for ax2_s in range(29):\n                    data_1 = T.Buffer((4704,), data=data.data)\n                    pad_temp_1[ax2_s] = T.if_then_else(1 <= ax2_s, data_1[ax0 * 672 + ax1 * 28 + ax2_s - 1], T.float32(-3.4028234663852886e+38))\n                for ax2 in range(14):\n                    pool_max_1 = T.Buffer((2352,), data=pool_max.data)\n                    pool_max_1[ax0 * 336 + ax1 * 14 + ax2] = T.float32(-3.4028234663852886e+38)\n                    for rv0 in range(3):\n                        cse_var_1: T.int32 = ax0 * 336 + ax1 * 14 + ax2\n                        pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 2 + rv0])",
        "data": "7_24_28"
    },
    {
        "op_name": "pool1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  float pad_temp[3];\n  for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n      for (int32_t ax2_s = 0; ax2_s < 3; ++ax2_s) {\n        int32_t cse_var_1 = (ax2 * 2);\n        pad_temp[ax2_s] = ((1 <= (cse_var_1 + ax2_s)) ? ((float*)data_1)[((((ax1 * 36) + cse_var_1) + ax2_s) - 1)] : -3.402823e+38f);\n      },\n      ((float*)pool_max_1)[((ax1 * 18) + ax2)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        int32_t cse_var_2 = ((ax1 * 18) + ax2);\n        float v_ = ((float*)pool_max_1)[cse_var_2];\n        float v__1 = pad_temp[rv0];\n        ((float*)pool_max_1)[cse_var_2] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))], ((1 <= (((((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) % 18) * 2) + rv0)) ? data[((((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 2)) + rv0) - 1)] : -3.402823e+38f));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 12, 36), \"float32\"), pool_max: T.Buffer((1, 12, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        pad_temp = T.allocate([3], \"float32\", \"global\")\n        for ax1, ax2 in T.grid(12, 18):\n            pad_temp_1 = T.Buffer((3,), data=pad_temp, align=8)\n            for ax2_s in range(3):\n                cse_var_1: T.int32 = ax2 * 2\n                data_1 = T.Buffer((432,), data=data.data)\n                pad_temp_1[ax2_s] = T.if_then_else(1 <= cse_var_1 + ax2_s, data_1[ax1 * 36 + cse_var_1 + ax2_s - 1], T.float32(-3.4028234663852886e+38))\n            pool_max_1 = T.Buffer((216,), data=pool_max.data)\n            pool_max_1[ax1 * 18 + ax2] = T.float32(-3.4028234663852886e+38)\n            for rv0 in range(3):\n                cse_var_2: T.int32 = ax1 * 18 + ax2\n                pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[rv0])",
        "data": "1_12_36"
    },
    {
        "op_name": "pool2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1944; ++ax0_ax1_fused_ax2_fused) {\n    float pad_temp[9];\n    for (int32_t ax3 = 0; ax3 < 48; ++ax3) {\n      for (int32_t ax2_ax3_fused_s = 0; ax2_ax3_fused_s < 9; ++ax2_ax3_fused_s) {\n        int32_t cse_var_3 = (ax2_ax3_fused_s / 3);\n        int32_t cse_var_2 = (ax3 * 2);\n        int32_t cse_var_1 = (ax2_ax3_fused_s % 3);\n        pad_temp[ax2_ax3_fused_s] = (((1 <= (((ax0_ax1_fused_ax2_fused % 18) * 2) + cse_var_3)) && (1 <= (cse_var_2 + cse_var_1))) ? ((float*)data_1)[(((((ax0_ax1_fused_ax2_fused * 192) + (cse_var_3 * 96)) + cse_var_2) + cse_var_1) - 97)] : -3.402823e+38f);\n      },\n      ((float*)pool_max_1)[((ax0_ax1_fused_ax2_fused * 48) + ax3)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n          int32_t cse_var_4 = ((ax0_ax1_fused_ax2_fused * 48) + ax3);\n          float v_ = ((float*)pool_max_1)[cse_var_4];\n          float v__1 = pad_temp[((rv0 * 3) + rv1)];\n          ((float*)pool_max_1)[cse_var_4] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], (((1 <= ((((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) % 54) / 3) * 2) + rv0)) && (1 <= (((((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) % 48) * 2) + rv1))) ? data[((((((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) / 3) * 192) + (rv0 * 96)) + ((((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) % 48) * 2)) + rv1) - 97)] : -3.402823e+38f));\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 36, 36, 96), \"float32\"), pool_max: T.Buffer((3, 36, 18, 48), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1944):\n            pad_temp = T.allocate([9], \"float32\", \"global\")\n            for ax3 in range(48):\n                pad_temp_1 = T.Buffer((9,), data=pad_temp, align=32)\n                for ax2_ax3_fused_s in range(9):\n                    cse_var_3: T.int32 = ax2_ax3_fused_s // 3\n                    cse_var_2: T.int32 = ax3 * 2\n                    cse_var_1: T.int32 = ax2_ax3_fused_s % 3\n                    data_1 = T.Buffer((373248,), data=data.data)\n                    pad_temp_1[ax2_ax3_fused_s] = T.if_then_else(1 <= ax0_ax1_fused_ax2_fused % 18 * 2 + cse_var_3 and 1 <= cse_var_2 + cse_var_1, data_1[ax0_ax1_fused_ax2_fused * 192 + cse_var_3 * 96 + cse_var_2 + cse_var_1 - 97], T.float32(-3.4028234663852886e+38))\n                pool_max_1 = T.Buffer((93312,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused_ax2_fused * 48 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_4: T.int32 = ax0_ax1_fused_ax2_fused * 48 + ax3\n                    pool_max_1[cse_var_4] = T.max(pool_max_1[cse_var_4], pad_temp_1[rv0 * 3 + rv1])",
        "data": "3_36_36_96"
    },
    {
        "op_name": "pool2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1728; ++ax0_ax1_fused_ax2_fused) {\n    float pad_temp[9];\n    for (int32_t ax3 = 0; ax3 < 6; ++ax3) {\n      for (int32_t ax1_ax2_fused_ax3_fused_s = 0; ax1_ax2_fused_ax3_fused_s < 9; ++ax1_ax2_fused_ax3_fused_s) {\n        int32_t cse_var_3 = (ax1_ax2_fused_ax3_fused_s / 3);\n        int32_t cse_var_2 = (ax3 * 2);\n        int32_t cse_var_1 = (ax1_ax2_fused_ax3_fused_s % 3);\n        pad_temp[ax1_ax2_fused_ax3_fused_s] = (((1 <= (((ax0_ax1_fused_ax2_fused % 6) * 2) + cse_var_3)) && (1 <= (cse_var_2 + cse_var_1))) ? ((float*)data_1)[(((((ax0_ax1_fused_ax2_fused * 24) + (cse_var_3 * 12)) + cse_var_2) + cse_var_1) - 13)] : -3.402823e+38f);\n      },\n      ((float*)pool_max_1)[((ax0_ax1_fused_ax2_fused * 6) + ax3)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n          int32_t cse_var_4 = ((ax0_ax1_fused_ax2_fused * 6) + ax3);\n          float v_ = ((float*)pool_max_1)[cse_var_4];\n          float v__1 = pad_temp[((rv0 * 3) + rv1)];\n          ((float*)pool_max_1)[cse_var_4] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], (((1 <= ((((((((int)blockIdx.x) * 14) + (((int)threadIdx.x) >> 1)) % 18) / 3) * 2) + rv0)) && (1 <= (((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 6) * 2) + rv1))) ? data[((((((((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) / 3) * 24) + (rv0 * 12)) + ((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 6) * 2)) + rv1) - 13)] : -3.402823e+38f));\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 36, 12, 12), \"float32\"), pool_max: T.Buffer((8, 36, 6, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1728):\n            pad_temp = T.allocate([9], \"float32\", \"global\")\n            for ax3 in range(6):\n                pad_temp_1 = T.Buffer((9,), data=pad_temp, align=32)\n                for ax1_ax2_fused_ax3_fused_s in range(9):\n                    cse_var_3: T.int32 = ax1_ax2_fused_ax3_fused_s // 3\n                    cse_var_2: T.int32 = ax3 * 2\n                    cse_var_1: T.int32 = ax1_ax2_fused_ax3_fused_s % 3\n                    data_1 = T.Buffer((41472,), data=data.data)\n                    pad_temp_1[ax1_ax2_fused_ax3_fused_s] = T.if_then_else(1 <= ax0_ax1_fused_ax2_fused % 6 * 2 + cse_var_3 and 1 <= cse_var_2 + cse_var_1, data_1[ax0_ax1_fused_ax2_fused * 24 + cse_var_3 * 12 + cse_var_2 + cse_var_1 - 13], T.float32(-3.4028234663852886e+38))\n                pool_max_1 = T.Buffer((10368,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused_ax2_fused * 6 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_4: T.int32 = ax0_ax1_fused_ax2_fused * 6 + ax3\n                    pool_max_1[cse_var_4] = T.max(pool_max_1[cse_var_4], pad_temp_1[rv0 * 3 + rv1])",
        "data": "8_36_12_12"
    },
    {
        "op_name": "pool2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2304; ++ax0_ax1_fused_ax2_fused) {\n    float pad_temp[9];\n    for (int32_t ax3 = 0; ax3 < 54; ++ax3) {\n      for (int32_t ax2_ax3_fused_s = 0; ax2_ax3_fused_s < 9; ++ax2_ax3_fused_s) {\n        int32_t cse_var_3 = (ax2_ax3_fused_s / 3);\n        int32_t cse_var_2 = (ax3 * 2);\n        int32_t cse_var_1 = (ax2_ax3_fused_s % 3);\n        pad_temp[ax2_ax3_fused_s] = (((1 <= (((ax0_ax1_fused_ax2_fused & 15) * 2) + cse_var_3)) && (1 <= (cse_var_2 + cse_var_1))) ? ((float*)data_1)[(((((ax0_ax1_fused_ax2_fused * 216) + (cse_var_3 * 108)) + cse_var_2) + cse_var_1) - 109)] : -3.402823e+38f);\n      },\n      ((float*)pool_max_1)[((ax0_ax1_fused_ax2_fused * 54) + ax3)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n          int32_t cse_var_4 = ((ax0_ax1_fused_ax2_fused * 54) + ax3);\n          float v_ = ((float*)pool_max_1)[cse_var_4];\n          float v__1 = pad_temp[((rv0 * 3) + rv1)];\n          ((float*)pool_max_1)[cse_var_4] = ((v_) > (v__1) ? (v_) : (v__1));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], (((1 <= ((((((((int)blockIdx.x) % 18) * 8) + (((int)threadIdx.x) / 6)) / 9) * 2) + rv0)) && (1 <= (((((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) % 54) * 2) + rv1))) ? data[((((((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) / 6)) / 9) * 216) + (rv0 * 108)) + ((((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) % 54) * 2)) + rv1) - 109)] : -3.402823e+38f));\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 16, 32, 108), \"float32\"), pool_max: T.Buffer((9, 16, 16, 54), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2304):\n            pad_temp = T.allocate([9], \"float32\", \"global\")\n            for ax3 in range(54):\n                pad_temp_1 = T.Buffer((9,), data=pad_temp, align=32)\n                for ax2_ax3_fused_s in range(9):\n                    cse_var_3: T.int32 = ax2_ax3_fused_s // 3\n                    cse_var_2: T.int32 = ax3 * 2\n                    cse_var_1: T.int32 = ax2_ax3_fused_s % 3\n                    data_1 = T.Buffer((497664,), data=data.data)\n                    pad_temp_1[ax2_ax3_fused_s] = T.if_then_else(1 <= ax0_ax1_fused_ax2_fused % 16 * 2 + cse_var_3 and 1 <= cse_var_2 + cse_var_1, data_1[ax0_ax1_fused_ax2_fused * 216 + cse_var_3 * 108 + cse_var_2 + cse_var_1 - 109], T.float32(-3.4028234663852886e+38))\n                pool_max_1 = T.Buffer((124416,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused_ax2_fused * 54 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_4: T.int32 = ax0_ax1_fused_ax2_fused * 54 + ax3\n                    pool_max_1[cse_var_4] = T.max(pool_max_1[cse_var_4], pad_temp_1[rv0 * 3 + rv1])",
        "data": "9_16_32_108"
    },
    {
        "op_name": "pool2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 120; ++ax0_ax1_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1300, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t ax2 = 0; ax2 < 25; ++ax2) {\n      for (int32_t ax3_s = 0; ax3_s < 13; ++ax3_s) {\n        ((float*)pad_temp)[((ax2 * 13) + ax3_s)] = (((1 <= ax2) && (1 <= ax3_s)) ? ((float*)data_1)[((((ax0_ax1_fused * 288) + (ax2 * 12)) + ax3_s) - 13)] : -3.402823e+38f);\n      },\n    },\n    for (int32_t ax2_1 = 0; ax2_1 < 12; ++ax2_1) {\n      for (int32_t ax3 = 0; ax3 < 6; ++ax3) {\n        ((float*)pool_max_1)[(((ax0_ax1_fused * 72) + (ax2_1 * 6)) + ax3)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            int32_t cse_var_1 = (((ax0_ax1_fused * 72) + (ax2_1 * 6)) + ax3);\n            float v_ = ((float*)pool_max_1)[cse_var_1];\n            float v__1 = ((float*)pad_temp)[((((ax2_1 * 26) + (rv0 * 13)) + (ax3 * 2)) + rv1)];\n            ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], (((1 <= (((((((int)blockIdx.x) * 9) + (((int)threadIdx.x) / 6)) % 12) * 2) + rv0)) && (1 <= (((((int)threadIdx.x) % 6) * 2) + rv1))) ? data[((((((((int)blockIdx.x) * 216) + ((((int)threadIdx.x) / 6) * 24)) + (rv0 * 12)) + ((((int)threadIdx.x) % 6) * 2)) + rv1) - 13)] : -3.402823e+38f));\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 20, 24, 12), \"float32\"), pool_max: T.Buffer((6, 20, 12, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(120):\n            pad_temp = T.allocate([325], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((325,), data=pad_temp)\n            for ax2, ax3_s in T.grid(25, 13):\n                data_1 = T.Buffer((34560,), data=data.data)\n                pad_temp_1[ax2 * 13 + ax3_s] = T.if_then_else(1 <= ax2 and 1 <= ax3_s, data_1[ax0_ax1_fused * 288 + ax2 * 12 + ax3_s - 13], T.float32(-3.4028234663852886e+38))\n            for ax2, ax3 in T.grid(12, 6):\n                pool_max_1 = T.Buffer((8640,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 72 + ax2 * 6 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 72 + ax2 * 6 + ax3\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 26 + rv0 * 13 + ax3 * 2 + rv1])",
        "data": "6_20_24_12"
    },
    {
        "op_name": "pool2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 140; ++ax0_ax1_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)6292, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 121; ++ax3) {\n        ((float*)pad_temp)[((ax2 * 121) + ax3)] = (((1 <= ax2) && (1 <= ax3)) ? ((float*)data_1)[((((ax0_ax1_fused * 1440) + (ax2 * 120)) + ax3) - 121)] : -3.402823e+38f);\n      },\n    },\n    for (int32_t ax2_1 = 0; ax2_1 < 6; ++ax2_1) {\n      for (int32_t ax3_1 = 0; ax3_1 < 60; ++ax3_1) {\n        ((float*)pool_max_1)[(((ax0_ax1_fused * 360) + (ax2_1 * 60)) + ax3_1)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            int32_t cse_var_1 = (((ax0_ax1_fused * 360) + (ax2_1 * 60)) + ax3_1);\n            float v_ = ((float*)pool_max_1)[cse_var_1];\n            float v__1 = ((float*)pad_temp)[((((ax2_1 * 242) + (rv0 * 121)) + (ax3_1 * 2)) + rv1)];\n            ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], (((1 <= ((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 10)) % 36) / 6) * 2) + rv0)) && (1 <= (((((((int)blockIdx.x) * 50) + ((int)threadIdx.x)) % 60) * 2) + rv1))) ? data[((((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 10)) / 6) * 240) + (rv0 * 120)) + ((((((int)blockIdx.x) * 50) + ((int)threadIdx.x)) % 60) * 2)) + rv1) - 121)] : -3.402823e+38f));\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 20, 12, 120), \"float32\"), pool_max: T.Buffer((7, 20, 6, 60), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(140):\n            pad_temp = T.allocate([1573], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((1573,), data=pad_temp)\n            for ax2, ax3 in T.grid(13, 121):\n                data_1 = T.Buffer((201600,), data=data.data)\n                pad_temp_1[ax2 * 121 + ax3] = T.if_then_else(1 <= ax2 and 1 <= ax3, data_1[ax0_ax1_fused * 1440 + ax2 * 120 + ax3 - 121], T.float32(-3.4028234663852886e+38))\n            for ax2, ax3 in T.grid(6, 60):\n                pool_max_1 = T.Buffer((50400,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 360 + ax2 * 60 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 360 + ax2 * 60 + ax3\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 242 + rv0 * 121 + ax3 * 2 + rv1])",
        "data": "7_20_12_120"
    },
    {
        "op_name": "pool2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 240; ++ax0_ax1_fused) {\n    float pad_temp[219];\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      for (int32_t ax2_1 = 0; ax2_1 < 3; ++ax2_1) {\n        for (int32_t ax3 = 0; ax3 < 73; ++ax3) {\n          pad_temp[((ax2_1 * 73) + ax3)] = (((1 <= ((ax2 * 2) + ax2_1)) && (1 <= ax3)) ? ((float*)data_1)[(((((ax0_ax1_fused * 2880) + (ax2 * 144)) + (ax2_1 * 72)) + ax3) - 73)] : -3.402823e+38f);\n        },\n      },\n      for (int32_t ax3_1 = 0; ax3_1 < 36; ++ax3_1) {\n        ((float*)pool_max_1)[(((ax0_ax1_fused * 720) + (ax2 * 36)) + ax3_1)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            int32_t cse_var_1 = (((ax0_ax1_fused * 720) + (ax2 * 36)) + ax3_1);\n            float v_ = ((float*)pool_max_1)[cse_var_1];\n            float v__1 = pad_temp[(((rv0 * 73) + (ax3_1 * 2)) + rv1)];\n            ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], (((1 <= ((((((((int)blockIdx.x) * 25) + (((int)threadIdx.x) >> 1)) % 360) / 18) * 2) + rv0)) && (1 <= (((((((int)blockIdx.x) * 14) + ((int)threadIdx.x)) % 36) * 2) + rv1))) ? data[((((((((((int)blockIdx.x) * 25) + (((int)threadIdx.x) >> 1)) / 18) * 144) + (rv0 * 72)) + ((((((int)blockIdx.x) * 14) + ((int)threadIdx.x)) % 36) * 2)) + rv1) - 73)] : -3.402823e+38f));\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 40, 40, 72), \"float32\"), pool_max: T.Buffer((6, 40, 20, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(240):\n            pad_temp = T.allocate([219], \"float32\", \"global\")\n            for ax2 in range(20):\n                pad_temp_1 = T.Buffer((219,), data=pad_temp)\n                for ax2_1, ax3 in T.grid(3, 73):\n                    data_1 = T.Buffer((691200,), data=data.data)\n                    pad_temp_1[ax2_1 * 73 + ax3] = T.if_then_else(1 <= ax2 * 2 + ax2_1 and 1 <= ax3, data_1[ax0_ax1_fused * 2880 + ax2 * 144 + ax2_1 * 72 + ax3 - 73], T.float32(-3.4028234663852886e+38))\n                for ax3 in range(36):\n                    pool_max_1 = T.Buffer((172800,), data=pool_max.data)\n                    pool_max_1[ax0_ax1_fused * 720 + ax2 * 36 + ax3] = T.float32(-3.4028234663852886e+38)\n                    for rv0, rv1 in T.grid(3, 3):\n                        cse_var_1: T.int32 = ax0_ax1_fused * 720 + ax2 * 36 + ax3\n                        pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[rv0 * 73 + ax3 * 2 + rv1])",
        "data": "6_40_40_72"
    },
    {
        "op_name": "pool2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 180; ++ax0_ax1_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1460, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 73; ++ax3) {\n        ((float*)pad_temp)[((ax2 * 73) + ax3)] = (((1 <= ax2) && (1 <= ax3)) ? ((float*)data_1)[((((ax0_ax1_fused * 288) + (ax2 * 72)) + ax3) - 73)] : -3.402823e+38f);\n      },\n    },\n    for (int32_t ax2_1 = 0; ax2_1 < 2; ++ax2_1) {\n      for (int32_t ax3_1 = 0; ax3_1 < 36; ++ax3_1) {\n        ((float*)pool_max_1)[(((ax0_ax1_fused * 72) + (ax2_1 * 36)) + ax3_1)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            int32_t cse_var_1 = (((ax0_ax1_fused * 72) + (ax2_1 * 36)) + ax3_1);\n            float v_ = ((float*)pool_max_1)[cse_var_1];\n            float v__1 = ((float*)pad_temp)[((((ax2_1 * 146) + (rv0 * 73)) + (ax3_1 * 2)) + rv1)];\n            ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], (((1 <= ((((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) / 12)) % 6) / 3) * 2) + rv0)) && (1 <= (((((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) % 36) * 2) + rv1))) ? data[((((((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) / 12)) / 3) * 144) + (rv0 * 72)) + ((((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) % 36) * 2)) + rv1) - 73)] : -3.402823e+38f));\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 20, 4, 72), \"float32\"), pool_max: T.Buffer((9, 20, 2, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(180):\n            pad_temp = T.allocate([365], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((365,), data=pad_temp)\n            for ax2, ax3 in T.grid(5, 73):\n                data_1 = T.Buffer((51840,), data=data.data)\n                pad_temp_1[ax2 * 73 + ax3] = T.if_then_else(1 <= ax2 and 1 <= ax3, data_1[ax0_ax1_fused * 288 + ax2 * 72 + ax3 - 73], T.float32(-3.4028234663852886e+38))\n            for ax2, ax3 in T.grid(2, 36):\n                pool_max_1 = T.Buffer((12960,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 72 + ax2 * 36 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 72 + ax2 * 36 + ax3\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 146 + rv0 * 73 + ax3 * 2 + rv1])",
        "data": "9_20_4_72"
    },
    {
        "op_name": "pool2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 144; ++ax0_ax1_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)13940, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t ax2 = 0; ax2 < 41; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 85; ++ax3) {\n        ((float*)pad_temp)[((ax2 * 85) + ax3)] = (((1 <= ax2) && (1 <= ax3)) ? ((float*)data_1)[((((ax0_ax1_fused * 3360) + (ax2 * 84)) + ax3) - 85)] : -3.402823e+38f);\n      },\n    },\n    for (int32_t ax2_1 = 0; ax2_1 < 20; ++ax2_1) {\n      for (int32_t ax3_1 = 0; ax3_1 < 42; ++ax3_1) {\n        ((float*)pool_max_1)[(((ax0_ax1_fused * 840) + (ax2_1 * 42)) + ax3_1)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            int32_t cse_var_1 = (((ax0_ax1_fused * 840) + (ax2_1 * 42)) + ax3_1);\n            float v_ = ((float*)pool_max_1)[cse_var_1];\n            float v__1 = ((float*)pad_temp)[((((ax2_1 * 170) + (rv0 * 85)) + (ax3_1 * 2)) + rv1)];\n            ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], (((1 <= ((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 21)) % 40) >> 1) * 2) + rv0)) && (1 <= (((((((int)blockIdx.x) * 21) + ((int)threadIdx.x)) % 42) * 2) + rv1))) ? data[((((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 21)) >> 1) * 168) + (rv0 * 84)) + ((((((int)blockIdx.x) * 21) + ((int)threadIdx.x)) % 42) * 2)) + rv1) - 85)] : -3.402823e+38f));\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 36, 40, 84), \"float32\"), pool_max: T.Buffer((4, 36, 20, 42), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(144):\n            pad_temp = T.allocate([3485], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((3485,), data=pad_temp)\n            for ax2, ax3 in T.grid(41, 85):\n                data_1 = T.Buffer((483840,), data=data.data)\n                pad_temp_1[ax2 * 85 + ax3] = T.if_then_else(1 <= ax2 and 1 <= ax3, data_1[ax0_ax1_fused * 3360 + ax2 * 84 + ax3 - 85], T.float32(-3.4028234663852886e+38))\n            for ax2, ax3 in T.grid(20, 42):\n                pool_max_1 = T.Buffer((120960,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 840 + ax2 * 42 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 840 + ax2 * 42 + ax3\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 170 + rv0 * 85 + ax3 * 2 + rv1])",
        "data": "4_36_40_84"
    },
    {
        "op_name": "pool2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 216; ++ax0_ax1_fused) {\n    float pad_temp[9];\n    for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 24; ++ax3) {\n        for (int32_t ax2_1 = 0; ax2_1 < 3; ++ax2_1) {\n          for (int32_t ax3_s = 0; ax3_s < 3; ++ax3_s) {\n            int32_t cse_var_1 = (ax3 * 2);\n            pad_temp[((ax2_1 * 3) + ax3_s)] = (((1 <= ((ax2 * 2) + ax2_1)) && (1 <= (cse_var_1 + ax3_s))) ? ((float*)data_1)[((((((ax0_ax1_fused * 1536) + (ax2 * 96)) + (ax2_1 * 48)) + cse_var_1) + ax3_s) - 49)] : -3.402823e+38f);\n          },\n        },\n        ((float*)pool_max_1)[(((ax0_ax1_fused * 384) + (ax2 * 24)) + ax3)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            int32_t cse_var_2 = (((ax0_ax1_fused * 384) + (ax2 * 24)) + ax3);\n            float v_ = ((float*)pool_max_1)[cse_var_2];\n            float v__1 = pad_temp[((rv0 * 3) + rv1)];\n            ((float*)pool_max_1)[cse_var_2] = ((v_) > (v__1) ? (v_) : (v__1));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], (((1 <= ((((((((int)blockIdx.x) % 6) * 8) + (((int)threadIdx.x) >> 3)) / 3) * 2) + rv0)) && (1 <= (((((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) % 24) * 2) + rv1))) ? data[((((((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3) * 96) + (rv0 * 48)) + ((((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) % 24) * 2)) + rv1) - 49)] : -3.402823e+38f));\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 24, 32, 48), \"float32\"), pool_max: T.Buffer((9, 24, 16, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(216):\n            pad_temp = T.allocate([9], \"float32\", \"global\")\n            for ax2, ax3 in T.grid(16, 24):\n                pad_temp_1 = T.Buffer((9,), data=pad_temp, align=32)\n                for ax2_1, ax3_s in T.grid(3, 3):\n                    cse_var_1: T.int32 = ax3 * 2\n                    data_1 = T.Buffer((331776,), data=data.data)\n                    pad_temp_1[ax2_1 * 3 + ax3_s] = T.if_then_else(1 <= ax2 * 2 + ax2_1 and 1 <= cse_var_1 + ax3_s, data_1[ax0_ax1_fused * 1536 + ax2 * 96 + ax2_1 * 48 + cse_var_1 + ax3_s - 49], T.float32(-3.4028234663852886e+38))\n                pool_max_1 = T.Buffer((82944,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 384 + ax2 * 24 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_2: T.int32 = ax0_ax1_fused * 384 + ax2 * 24 + ax3\n                    pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[rv0 * 3 + rv1])",
        "data": "9_24_32_48"
    },
    {
        "op_name": "pool2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1308, 2, 32);\n  if (pad_temp == NULL) {\n    return -1;\n  },\n  for (int32_t ax1 = 0; ax1 < 4; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n      for (int32_t ax2_1 = 0; ax2_1 < 3; ++ax2_1) {\n        for (int32_t ax3 = 0; ax3 < 109; ++ax3) {\n          ((float*)pad_temp)[((ax2_1 * 109) + ax3)] = (((1 <= ((ax2 * 2) + ax2_1)) && (1 <= ax3)) ? ((float*)data_1)[(((((ax1 * 3456) + (ax2 * 216)) + (ax2_1 * 108)) + ax3) - 109)] : -3.402823e+38f);\n        },\n      },\n      for (int32_t ax3_1 = 0; ax3_1 < 54; ++ax3_1) {\n        ((float*)pool_max_1)[(((ax1 * 864) + (ax2 * 54)) + ax3_1)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            int32_t cse_var_1 = (((ax1 * 864) + (ax2 * 54)) + ax3_1);\n            float v_ = ((float*)pool_max_1)[cse_var_1];\n            float v__1 = ((float*)pad_temp)[(((rv0 * 109) + (ax3_1 * 2)) + rv1)];\n            ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n          },\n        },\n      },\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], (((1 <= ((((((((int)blockIdx.x) % 24) * 2) + (((int)threadIdx.x) / 18)) / 3) * 2) + rv0)) && (1 <= (((((((int)blockIdx.x) * 36) + ((int)threadIdx.x)) % 54) * 2) + rv1))) ? data[((((((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 18)) / 3) * 216) + (rv0 * 108)) + ((((((int)blockIdx.x) * 36) + ((int)threadIdx.x)) % 54) * 2)) + rv1) - 109)] : -3.402823e+38f));\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 4, 32, 108), \"float32\"), pool_max: T.Buffer((1, 4, 16, 54), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        pad_temp = T.allocate([327], \"float32\", \"global\")\n        for ax1, ax2 in T.grid(4, 16):\n            pad_temp_1 = T.Buffer((327,), data=pad_temp)\n            for ax2_1, ax3 in T.grid(3, 109):\n                data_1 = T.Buffer((13824,), data=data.data)\n                pad_temp_1[ax2_1 * 109 + ax3] = T.if_then_else(1 <= ax2 * 2 + ax2_1 and 1 <= ax3, data_1[ax1 * 3456 + ax2 * 216 + ax2_1 * 108 + ax3 - 109], T.float32(-3.4028234663852886e+38))\n            for ax3 in range(54):\n                pool_max_1 = T.Buffer((3456,), data=pool_max.data)\n                pool_max_1[ax1 * 864 + ax2 * 54 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_1: T.int32 = ax1 * 864 + ax2 * 54 + ax3\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[rv0 * 109 + ax3 * 2 + rv1])",
        "data": "1_4_32_108"
    },
    {
        "op_name": "pool3d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 32; ++ax0_ax1_fused) {\n    float pad_temp[27];\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 10; ++ax3) {\n        for (int32_t ax4 = 0; ax4 < 8; ++ax4) {\n          for (int32_t ax2_1 = 0; ax2_1 < 3; ++ax2_1) {\n            for (int32_t ax3_1 = 0; ax3_1 < 3; ++ax3_1) {\n              for (int32_t ax4_s = 0; ax4_s < 3; ++ax4_s) {\n                int32_t cse_var_1 = (ax4 * 2);\n                pad_temp[(((ax2_1 * 9) + (ax3_1 * 3)) + ax4_s)] = ((((1 <= ((ax2 * 2) + ax2_1)) && (1 <= ((ax3 * 2) + ax3_1))) && (1 <= (cse_var_1 + ax4_s))) ? ((float*)data_1)[((((((((ax0_ax1_fused * 2560) + (ax2 * 640)) + (ax2_1 * 320)) + (ax3 * 32)) + (ax3_1 * 16)) + cse_var_1) + ax4_s) - 337)] : -3.402823e+38f);\n              },\n            },\n          },\n          ((float*)pool_max_1)[((((ax0_ax1_fused * 320) + (ax2 * 80)) + (ax3 * 8)) + ax4)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n                int32_t cse_var_2 = ((((ax0_ax1_fused * 320) + (ax2 * 80)) + (ax3 * 8)) + ax4);\n                float v_ = ((float*)pool_max_1)[cse_var_2];\n                float v__1 = pad_temp[(((rv0 * 9) + (rv1 * 3)) + rv2)];\n                ((float*)pool_max_1)[cse_var_2] = ((v_) > (v__1) ? (v_) : (v__1));\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ((((1 <= ((((((((int)blockIdx.x) % 5) * 4) + (((int)threadIdx.x) >> 4)) / 5) * 2) + rv0)) && (1 <= (((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) % 10) * 2) + rv1))) && (1 <= (((((int)threadIdx.x) & 7) * 2) + rv2))) ? data[((((((((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) / 5) * 640) + (rv0 * 320)) + ((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) % 10) * 32)) + (rv1 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv2) - 337)] : -3.402823e+38f));\n      },\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 8, 8, 20, 16), \"float32\"), pool_max: T.Buffer((4, 8, 4, 10, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(32):\n            pad_temp = T.allocate([27], \"float32\", \"global\")\n            for ax2, ax3, ax4 in T.grid(4, 10, 8):\n                pad_temp_1 = T.Buffer((27,), data=pad_temp)\n                for ax2_1, ax3_1, ax4_s in T.grid(3, 3, 3):\n                    cse_var_1: T.int32 = ax4 * 2\n                    data_1 = T.Buffer((81920,), data=data.data)\n                    pad_temp_1[ax2_1 * 9 + ax3_1 * 3 + ax4_s] = T.if_then_else(1 <= ax2 * 2 + ax2_1 and 1 <= ax3 * 2 + ax3_1 and 1 <= cse_var_1 + ax4_s, data_1[ax0_ax1_fused * 2560 + ax2 * 640 + ax2_1 * 320 + ax3 * 32 + ax3_1 * 16 + cse_var_1 + ax4_s - 337], T.float32(-3.4028234663852886e+38))\n                pool_max_1 = T.Buffer((10240,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 320 + ax2 * 80 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_2: T.int32 = ax0_ax1_fused * 320 + ax2 * 80 + ax3 * 8 + ax4\n                    pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[rv0 * 9 + rv1 * 3 + rv2])",
        "data": "4_8_8_20_16"
    },
    {
        "op_name": "pool3d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 108; ++ax0_ax1_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)4284, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n      for (int32_t ax2_1 = 0; ax2_1 < 3; ++ax2_1) {\n        for (int32_t ax3 = 0; ax3 < 21; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n            ((float*)pad_temp)[(((ax2_1 * 357) + (ax3 * 17)) + ax4)] = ((((1 <= ((ax2 * 2) + ax2_1)) && (1 <= ax3)) && (1 <= ax4)) ? ((float*)data_1)[((((((ax0_ax1_fused * 11520) + (ax2 * 640)) + (ax2_1 * 320)) + (ax3 * 16)) + ax4) - 337)] : -3.402823e+38f);\n          },\n        },\n      },\n      for (int32_t ax3_1 = 0; ax3_1 < 10; ++ax3_1) {\n        for (int32_t ax4_1 = 0; ax4_1 < 8; ++ax4_1) {\n          ((float*)pool_max_1)[((((ax0_ax1_fused * 1440) + (ax2 * 80)) + (ax3_1 * 8)) + ax4_1)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n                int32_t cse_var_1 = ((((ax0_ax1_fused * 1440) + (ax2 * 80)) + (ax3_1 * 8)) + ax4_1);\n                float v_ = ((float*)pool_max_1)[cse_var_1];\n                float v__1 = ((float*)pad_temp)[(((((rv0 * 357) + (ax3_1 * 34)) + (rv1 * 17)) + (ax4_1 * 2)) + rv2)];\n                ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n              },\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], ((((1 <= ((((((((int)blockIdx.x) % 30) * 3) + (((int)threadIdx.x) >> 4)) / 5) * 2) + rv0)) && (1 <= (((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 3)) % 10) * 2) + rv1))) && (1 <= (((((int)threadIdx.x) & 7) * 2) + rv2))) ? data[((((((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 4)) / 5) * 640) + (rv0 * 320)) + ((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 3)) % 10) * 32)) + (rv1 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv2) - 337)] : -3.402823e+38f));\n      },\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 36, 36, 20, 16), \"float32\"), pool_max: T.Buffer((3, 36, 18, 10, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(108):\n            pad_temp = T.allocate([1071], \"float32\", \"global\")\n            for ax2 in range(18):\n                pad_temp_1 = T.Buffer((1071,), data=pad_temp)\n                for ax2_1, ax3, ax4 in T.grid(3, 21, 17):\n                    data_1 = T.Buffer((1244160,), data=data.data)\n                    pad_temp_1[ax2_1 * 357 + ax3 * 17 + ax4] = T.if_then_else(1 <= ax2 * 2 + ax2_1 and 1 <= ax3 and 1 <= ax4, data_1[ax0_ax1_fused * 11520 + ax2 * 640 + ax2_1 * 320 + ax3 * 16 + ax4 - 337], T.float32(-3.4028234663852886e+38))\n                for ax3, ax4 in T.grid(10, 8):\n                    pool_max_1 = T.Buffer((155520,), data=pool_max.data)\n                    pool_max_1[ax0_ax1_fused * 1440 + ax2 * 80 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                    for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                        cse_var_1: T.int32 = ax0_ax1_fused * 1440 + ax2 * 80 + ax3 * 8 + ax4\n                        pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[rv0 * 357 + ax3 * 34 + rv1 * 17 + ax4 * 2 + rv2])",
        "data": "3_36_36_20_16"
    },
    {
        "op_name": "pool3d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 84; ++ax0_ax1_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)17748, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 29; ++ax3) {\n        for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n          ((float*)pad_temp)[(((ax2 * 493) + (ax3 * 17)) + ax4)] = ((((1 <= ax2) && (1 <= ax3)) && (1 <= ax4)) ? ((float*)data_1)[(((((ax0_ax1_fused * 3584) + (ax2 * 448)) + (ax3 * 16)) + ax4) - 465)] : -3.402823e+38f);\n        },\n      },\n    },\n    for (int32_t ax2_1 = 0; ax2_1 < 4; ++ax2_1) {\n      for (int32_t ax3_1 = 0; ax3_1 < 14; ++ax3_1) {\n        for (int32_t ax4_1 = 0; ax4_1 < 8; ++ax4_1) {\n          ((float*)pool_max_1)[((((ax0_ax1_fused * 448) + (ax2_1 * 112)) + (ax3_1 * 8)) + ax4_1)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n                int32_t cse_var_1 = ((((ax0_ax1_fused * 448) + (ax2_1 * 112)) + (ax3_1 * 8)) + ax4_1);\n                float v_ = ((float*)pool_max_1)[cse_var_1];\n                float v__1 = ((float*)pad_temp)[((((((ax2_1 * 986) + (rv0 * 493)) + (ax3_1 * 34)) + (rv1 * 17)) + (ax4_1 * 2)) + rv2)];\n                ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n              },\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], ((((1 <= ((((((int)blockIdx.x) & 7) >> 1) * 2) + rv0)) && (1 <= ((((((int)blockIdx.x) & 1) * 14) + ((((int)threadIdx.x) >> 3) * 2)) + rv1))) && (1 <= (((((int)threadIdx.x) & 7) * 2) + rv2))) ? data[(((((((((((int)blockIdx.x) >> 1) * 896) + (rv0 * 448)) + ((((int)blockIdx.x) & 1) * 224)) + ((((int)threadIdx.x) >> 3) * 32)) + (rv1 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv2) - 465)] : -3.402823e+38f));\n      },\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 28, 8, 28, 16), \"float32\"), pool_max: T.Buffer((3, 28, 4, 14, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(84):\n            pad_temp = T.allocate([4437], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((4437,), data=pad_temp)\n            for ax2, ax3, ax4 in T.grid(9, 29, 17):\n                data_1 = T.Buffer((301056,), data=data.data)\n                pad_temp_1[ax2 * 493 + ax3 * 17 + ax4] = T.if_then_else(1 <= ax2 and 1 <= ax3 and 1 <= ax4, data_1[ax0_ax1_fused * 3584 + ax2 * 448 + ax3 * 16 + ax4 - 465], T.float32(-3.4028234663852886e+38))\n            for ax2, ax3, ax4 in T.grid(4, 14, 8):\n                pool_max_1 = T.Buffer((37632,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 448 + ax2 * 112 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 448 + ax2 * 112 + ax3 * 8 + ax4\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 986 + rv0 * 493 + ax3 * 34 + rv1 * 17 + ax4 * 2 + rv2])",
        "data": "3_28_8_28_16"
    },
    {
        "op_name": "pool3d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 108; ++ax0_ax1_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)22100, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 25; ++ax3) {\n        for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n          ((float*)pad_temp)[(((ax2 * 425) + (ax3 * 17)) + ax4)] = ((((1 <= ax2) && (1 <= ax3)) && (1 <= ax4)) ? ((float*)data_1)[(((((ax0_ax1_fused * 4608) + (ax2 * 384)) + (ax3 * 16)) + ax4) - 401)] : -3.402823e+38f);\n        },\n      },\n    },\n    for (int32_t ax2_1 = 0; ax2_1 < 6; ++ax2_1) {\n      for (int32_t ax3_1 = 0; ax3_1 < 12; ++ax3_1) {\n        for (int32_t ax4_1 = 0; ax4_1 < 8; ++ax4_1) {\n          ((float*)pool_max_1)[((((ax0_ax1_fused * 576) + (ax2_1 * 96)) + (ax3_1 * 8)) + ax4_1)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n                int32_t cse_var_1 = ((((ax0_ax1_fused * 576) + (ax2_1 * 96)) + (ax3_1 * 8)) + ax4_1);\n                float v_ = ((float*)pool_max_1)[cse_var_1];\n                float v__1 = ((float*)pad_temp)[((((((ax2_1 * 850) + (rv0 * 425)) + (ax3_1 * 34)) + (rv1 * 17)) + (ax4_1 * 2)) + rv2)];\n                ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n              },\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ((((1 <= ((((((((int)blockIdx.x) % 9) * 2) + (((int)threadIdx.x) >> 5)) / 3) * 2) + rv0)) && (1 <= (((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) % 12) * 2) + rv1))) && (1 <= (((((int)threadIdx.x) & 7) * 2) + rv2))) ? data[((((((((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5)) / 3) * 768) + (rv0 * 384)) + ((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) % 12) * 32)) + (rv1 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv2) - 401)] : -3.402823e+38f));\n      },\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 36, 12, 24, 16), \"float32\"), pool_max: T.Buffer((3, 36, 6, 12, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(108):\n            pad_temp = T.allocate([5525], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((5525,), data=pad_temp)\n            for ax2, ax3, ax4 in T.grid(13, 25, 17):\n                data_1 = T.Buffer((497664,), data=data.data)\n                pad_temp_1[ax2 * 425 + ax3 * 17 + ax4] = T.if_then_else(1 <= ax2 and 1 <= ax3 and 1 <= ax4, data_1[ax0_ax1_fused * 4608 + ax2 * 384 + ax3 * 16 + ax4 - 401], T.float32(-3.4028234663852886e+38))\n            for ax2, ax3, ax4 in T.grid(6, 12, 8):\n                pool_max_1 = T.Buffer((62208,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 576 + ax2 * 96 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 576 + ax2 * 96 + ax3 * 8 + ax4\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 850 + rv0 * 425 + ax3 * 34 + rv1 * 17 + ax4 * 2 + rv2])",
        "data": "3_36_12_24_16"
    },
    {
        "op_name": "pool3d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 32; ++ax0_ax1_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)5780, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 5; ++ax3) {\n        for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n          ((float*)pad_temp)[(((ax2 * 85) + (ax3 * 17)) + ax4)] = ((((1 <= ax2) && (1 <= ax3)) && (1 <= ax4)) ? ((float*)data_1)[(((((ax0_ax1_fused * 1024) + (ax2 * 64)) + (ax3 * 16)) + ax4) - 81)] : -3.402823e+38f);\n        },\n      },\n    },\n    for (int32_t ax2_1 = 0; ax2_1 < 8; ++ax2_1) {\n      for (int32_t ax3_1 = 0; ax3_1 < 2; ++ax3_1) {\n        for (int32_t ax4_1 = 0; ax4_1 < 8; ++ax4_1) {\n          ((float*)pool_max_1)[((((ax0_ax1_fused * 128) + (ax2_1 * 16)) + (ax3_1 * 8)) + ax4_1)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n                int32_t cse_var_1 = ((((ax0_ax1_fused * 128) + (ax2_1 * 16)) + (ax3_1 * 8)) + ax4_1);\n                float v_ = ((float*)pool_max_1)[cse_var_1];\n                float v__1 = ((float*)pad_temp)[((((((ax2_1 * 170) + (rv0 * 85)) + (ax3_1 * 34)) + (rv1 * 17)) + (ax4_1 * 2)) + rv2)];\n                ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n              },\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ((((1 <= ((((((int)blockIdx.x) & 3) * 4) + ((((int)threadIdx.x) >> 4) * 2)) + rv0)) && (1 <= ((((((int)threadIdx.x) & 15) >> 3) * 2) + rv1))) && (1 <= (((((int)threadIdx.x) & 7) * 2) + rv2))) ? data[((((((((((int)blockIdx.x) * 256) + ((((int)threadIdx.x) >> 4) * 128)) + (rv0 * 64)) + (((((int)threadIdx.x) & 15) >> 3) * 32)) + (rv1 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv2) - 81)] : -3.402823e+38f));\n      },\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 16, 16, 4, 16), \"float32\"), pool_max: T.Buffer((2, 16, 8, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(32):\n            pad_temp = T.allocate([1445], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((1445,), data=pad_temp)\n            for ax2, ax3, ax4 in T.grid(17, 5, 17):\n                data_1 = T.Buffer((32768,), data=data.data)\n                pad_temp_1[ax2 * 85 + ax3 * 17 + ax4] = T.if_then_else(1 <= ax2 and 1 <= ax3 and 1 <= ax4, data_1[ax0_ax1_fused * 1024 + ax2 * 64 + ax3 * 16 + ax4 - 81], T.float32(-3.4028234663852886e+38))\n            for ax2, ax3, ax4 in T.grid(8, 2, 8):\n                pool_max_1 = T.Buffer((4096,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 128 + ax2 * 16 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 128 + ax2 * 16 + ax3 * 8 + ax4\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 170 + rv0 * 85 + ax3 * 34 + rv1 * 17 + ax4 * 2 + rv2])",
        "data": "2_16_16_4_16"
    },
    {
        "op_name": "pool3d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3072; ++ax0_ax1_fused_ax2_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)2652, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 13; ++ax3) {\n        for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n          ((float*)pad_temp)[(((ax2 * 221) + (ax3 * 17)) + ax4)] = ((((1 <= (((ax0_ax1_fused_ax2_fused % 12) * 2) + ax2)) && (1 <= ax3)) && (1 <= ax4)) ? ((float*)data_1)[(((((ax0_ax1_fused_ax2_fused * 384) + (ax2 * 192)) + (ax3 * 16)) + ax4) - 209)] : -3.402823e+38f);\n        },\n      },\n    },\n    for (int32_t ax3_1 = 0; ax3_1 < 6; ++ax3_1) {\n      for (int32_t ax4_1 = 0; ax4_1 < 8; ++ax4_1) {\n        ((float*)pool_max_1)[(((ax0_ax1_fused_ax2_fused * 48) + (ax3_1 * 8)) + ax4_1)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n              int32_t cse_var_1 = (((ax0_ax1_fused_ax2_fused * 48) + (ax3_1 * 8)) + ax4_1);\n              float v_ = ((float*)pool_max_1)[cse_var_1];\n              float v__1 = ((float*)pad_temp)[(((((rv0 * 221) + (ax3_1 * 34)) + (rv1 * 17)) + (ax4_1 * 2)) + rv2)];\n              ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], ((((1 <= (((((int)blockIdx.x) % 12) * 2) + rv0)) && (1 <= (((((int)threadIdx.x) >> 3) * 2) + rv1))) && (1 <= (((((int)threadIdx.x) & 7) * 2) + rv2))) ? data[(((((((((int)blockIdx.x) * 384) + (rv0 * 192)) + ((((int)threadIdx.x) >> 3) * 32)) + (rv1 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv2) - 209)] : -3.402823e+38f));\n      },\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 32, 24, 12, 16), \"float32\"), pool_max: T.Buffer((8, 32, 12, 6, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3072):\n            pad_temp = T.allocate([663], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((663,), data=pad_temp)\n            for ax2, ax3, ax4 in T.grid(3, 13, 17):\n                data_1 = T.Buffer((1179648,), data=data.data)\n                pad_temp_1[ax2 * 221 + ax3 * 17 + ax4] = T.if_then_else(1 <= ax0_ax1_fused_ax2_fused % 12 * 2 + ax2 and 1 <= ax3 and 1 <= ax4, data_1[ax0_ax1_fused_ax2_fused * 384 + ax2 * 192 + ax3 * 16 + ax4 - 209], T.float32(-3.4028234663852886e+38))\n            for ax3, ax4 in T.grid(6, 8):\n                pool_max_1 = T.Buffer((147456,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused_ax2_fused * 48 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 48 + ax3 * 8 + ax4\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[rv0 * 221 + ax3 * 34 + rv1 * 17 + ax4 * 2 + rv2])",
        "data": "8_32_24_12_16"
    },
    {
        "op_name": "pool3d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 720; ++ax0_ax1_fused_ax2_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)2652, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 13; ++ax3) {\n        for (int32_t ax4_s = 0; ax4_s < 17; ++ax4_s) {\n          ((float*)pad_temp)[(((ax2 * 221) + (ax3 * 17)) + ax4_s)] = ((((1 <= (((ax0_ax1_fused_ax2_fused % 6) * 2) + ax2)) && (1 <= ax3)) && (1 <= ax4_s)) ? ((float*)data_1)[(((((ax0_ax1_fused_ax2_fused * 384) + (ax2 * 192)) + (ax3 * 16)) + ax4_s) - 209)] : -3.402823e+38f);\n        },\n      },\n    },\n    for (int32_t ax3_1 = 0; ax3_1 < 6; ++ax3_1) {\n      for (int32_t ax4 = 0; ax4 < 8; ++ax4) {\n        ((float*)pool_max_1)[(((ax0_ax1_fused_ax2_fused * 48) + (ax3_1 * 8)) + ax4)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n              int32_t cse_var_1 = (((ax0_ax1_fused_ax2_fused * 48) + (ax3_1 * 8)) + ax4);\n              float v_ = ((float*)pool_max_1)[cse_var_1];\n              float v__1 = ((float*)pad_temp)[(((((rv0 * 221) + (ax3_1 * 34)) + (rv1 * 17)) + (ax4 * 2)) + rv2)];\n              ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ((((1 <= ((((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) % 18) / 3) * 2) + rv0)) && (1 <= (((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) % 6) * 2) + rv1))) && (1 <= (((((int)threadIdx.x) & 7) * 2) + rv2))) ? data[((((((((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) / 3) * 384) + (rv0 * 192)) + ((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) % 6) * 32)) + (rv1 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv2) - 209)] : -3.402823e+38f));\n      },\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 24, 12, 12, 16), \"float32\"), pool_max: T.Buffer((5, 24, 6, 6, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(720):\n            pad_temp = T.allocate([663], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((663,), data=pad_temp)\n            for ax2, ax3, ax4_s in T.grid(3, 13, 17):\n                data_1 = T.Buffer((276480,), data=data.data)\n                pad_temp_1[ax2 * 221 + ax3 * 17 + ax4_s] = T.if_then_else(1 <= ax0_ax1_fused_ax2_fused % 6 * 2 + ax2 and 1 <= ax3 and 1 <= ax4_s, data_1[ax0_ax1_fused_ax2_fused * 384 + ax2 * 192 + ax3 * 16 + ax4_s - 209], T.float32(-3.4028234663852886e+38))\n            for ax3, ax4 in T.grid(6, 8):\n                pool_max_1 = T.Buffer((34560,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused_ax2_fused * 48 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 48 + ax3 * 8 + ax4\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[rv0 * 221 + ax3 * 34 + rv1 * 17 + ax4 * 2 + rv2])",
        "data": "5_24_12_12_16"
    },
    {
        "op_name": "pool3d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1008; ++ax0_ax1_fused_ax2_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)8364, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 41; ++ax3) {\n        for (int32_t ax4_s = 0; ax4_s < 17; ++ax4_s) {\n          ((float*)pad_temp)[(((ax2 * 697) + (ax3 * 17)) + ax4_s)] = ((((1 <= (((ax0_ax1_fused_ax2_fused % 6) * 2) + ax2)) && (1 <= ax3)) && (1 <= ax4_s)) ? ((float*)data_1)[(((((ax0_ax1_fused_ax2_fused * 1280) + (ax2 * 640)) + (ax3 * 16)) + ax4_s) - 657)] : -3.402823e+38f);\n        },\n      },\n    },\n    for (int32_t ax3_1 = 0; ax3_1 < 20; ++ax3_1) {\n      for (int32_t ax4 = 0; ax4 < 8; ++ax4) {\n        ((float*)pool_max_1)[(((ax0_ax1_fused_ax2_fused * 160) + (ax3_1 * 8)) + ax4)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n              int32_t cse_var_1 = (((ax0_ax1_fused_ax2_fused * 160) + (ax3_1 * 8)) + ax4);\n              float v_ = ((float*)pool_max_1)[cse_var_1];\n              float v__1 = ((float*)pad_temp)[(((((rv0 * 697) + (ax3_1 * 34)) + (rv1 * 17)) + (ax4 * 2)) + rv2)];\n              ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], ((((1 <= ((((((((int)blockIdx.x) % 20) * 3) + (((int)threadIdx.x) >> 4)) / 10) * 2) + rv0)) && (1 <= (((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 3)) % 20) * 2) + rv1))) && (1 <= (((((int)threadIdx.x) & 7) * 2) + rv2))) ? data[((((((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 4)) / 10) * 1280) + (rv0 * 640)) + ((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 3)) % 20) * 32)) + (rv1 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv2) - 657)] : -3.402823e+38f));\n      },\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 28, 12, 40, 16), \"float32\"), pool_max: T.Buffer((6, 28, 6, 20, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1008):\n            pad_temp = T.allocate([2091], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((2091,), data=pad_temp)\n            for ax2, ax3, ax4_s in T.grid(3, 41, 17):\n                data_1 = T.Buffer((1290240,), data=data.data)\n                pad_temp_1[ax2 * 697 + ax3 * 17 + ax4_s] = T.if_then_else(1 <= ax0_ax1_fused_ax2_fused % 6 * 2 + ax2 and 1 <= ax3 and 1 <= ax4_s, data_1[ax0_ax1_fused_ax2_fused * 1280 + ax2 * 640 + ax3 * 16 + ax4_s - 657], T.float32(-3.4028234663852886e+38))\n            for ax3, ax4 in T.grid(20, 8):\n                pool_max_1 = T.Buffer((161280,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused_ax2_fused * 160 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 160 + ax3 * 8 + ax4\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[rv0 * 697 + ax3 * 34 + rv1 * 17 + ax4 * 2 + rv2])",
        "data": "6_28_12_40_16"
    },
    {
        "op_name": "pool3d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 640; ++ax0_ax1_fused_ax2_fused) {\n    float pad_temp[153];\n    for (int32_t ax3 = 0; ax3 < 6; ++ax3) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        for (int32_t ax3_1 = 0; ax3_1 < 3; ++ax3_1) {\n          for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n            pad_temp[(((ax2 * 51) + (ax3_1 * 17)) + ax4)] = ((((1 <= (((ax0_ax1_fused_ax2_fused % 20) * 2) + ax2)) && (1 <= ((ax3 * 2) + ax3_1))) && (1 <= ax4)) ? ((float*)data_1)[((((((ax0_ax1_fused_ax2_fused * 384) + (ax2 * 192)) + (ax3 * 32)) + (ax3_1 * 16)) + ax4) - 209)] : -3.402823e+38f);\n          },\n        },\n      },\n      for (int32_t ax4_1 = 0; ax4_1 < 8; ++ax4_1) {\n        ((float*)pool_max_1)[(((ax0_ax1_fused_ax2_fused * 48) + (ax3 * 8)) + ax4_1)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n              int32_t cse_var_1 = (((ax0_ax1_fused_ax2_fused * 48) + (ax3 * 8)) + ax4_1);\n              float v_ = ((float*)pool_max_1)[cse_var_1];\n              float v__1 = pad_temp[((((rv0 * 51) + (rv1 * 17)) + (ax4_1 * 2)) + rv2)];\n              ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n            },\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ((((1 <= ((((((((int)blockIdx.x) % 30) * 2) + (((int)threadIdx.x) >> 4)) / 3) * 2) + rv0)) && (1 <= (((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) % 6) * 2) + rv1))) && (1 <= (((((int)threadIdx.x) & 7) * 2) + rv2))) ? data[((((((((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) / 3) * 384) + (rv0 * 192)) + ((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) % 6) * 32)) + (rv1 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv2) - 209)] : -3.402823e+38f));\n      },\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 16, 40, 12, 16), \"float32\"), pool_max: T.Buffer((2, 16, 20, 6, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(640):\n            pad_temp = T.allocate([153], \"float32\", \"global\")\n            for ax3 in range(6):\n                pad_temp_1 = T.Buffer((153,), data=pad_temp)\n                for ax2, ax3_1, ax4 in T.grid(3, 3, 17):\n                    data_1 = T.Buffer((245760,), data=data.data)\n                    pad_temp_1[ax2 * 51 + ax3_1 * 17 + ax4] = T.if_then_else(1 <= ax0_ax1_fused_ax2_fused % 20 * 2 + ax2 and 1 <= ax3 * 2 + ax3_1 and 1 <= ax4, data_1[ax0_ax1_fused_ax2_fused * 384 + ax2 * 192 + ax3 * 32 + ax3_1 * 16 + ax4 - 209], T.float32(-3.4028234663852886e+38))\n                for ax4 in range(8):\n                    pool_max_1 = T.Buffer((30720,), data=pool_max.data)\n                    pool_max_1[ax0_ax1_fused_ax2_fused * 48 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                    for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                        cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 48 + ax3 * 8 + ax4\n                        pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[rv0 * 51 + rv1 * 17 + ax4 * 2 + rv2])",
        "data": "2_16_40_12_16"
    },
    {
        "op_name": "pool3d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_pool_max_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 224; ++ax0_ax1_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)24276, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t ax2 = 0; ax2 < 21; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 17; ++ax3) {\n        for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n          ((float*)pad_temp)[(((ax2 * 289) + (ax3 * 17)) + ax4)] = ((((1 <= ax2) && (1 <= ax3)) && (1 <= ax4)) ? ((float*)data_1)[(((((ax0_ax1_fused * 5120) + (ax2 * 256)) + (ax3 * 16)) + ax4) - 273)] : -3.402823e+38f);\n        },\n      },\n    },\n    for (int32_t ax2_1 = 0; ax2_1 < 10; ++ax2_1) {\n      for (int32_t ax3_1 = 0; ax3_1 < 8; ++ax3_1) {\n        for (int32_t ax4_1 = 0; ax4_1 < 8; ++ax4_1) {\n          ((float*)pool_max_1)[((((ax0_ax1_fused * 640) + (ax2_1 * 64)) + (ax3_1 * 8)) + ax4_1)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n                int32_t cse_var_1 = ((((ax0_ax1_fused * 640) + (ax2_1 * 64)) + (ax3_1 * 8)) + ax4_1);\n                float v_ = ((float*)pool_max_1)[cse_var_1];\n                float v__1 = ((float*)pad_temp)[((((((ax2_1 * 578) + (rv0 * 289)) + (ax3_1 * 34)) + (rv1 * 17)) + (ax4_1 * 2)) + rv2)];\n                ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n              },\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ((((1 <= (((((int)blockIdx.x) % 10) * 2) + rv0)) && (1 <= (((((int)threadIdx.x) >> 3) * 2) + rv1))) && (1 <= (((((int)threadIdx.x) & 7) * 2) + rv2))) ? data[(((((((((int)blockIdx.x) * 512) + (rv0 * 256)) + ((((int)threadIdx.x) >> 3) * 32)) + (rv1 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv2) - 273)] : -3.402823e+38f));\n      },\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 32, 20, 16, 16), \"float32\"), pool_max: T.Buffer((7, 32, 10, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(224):\n            pad_temp = T.allocate([6069], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((6069,), data=pad_temp)\n            for ax2, ax3, ax4 in T.grid(21, 17, 17):\n                data_1 = T.Buffer((1146880,), data=data.data)\n                pad_temp_1[ax2 * 289 + ax3 * 17 + ax4] = T.if_then_else(1 <= ax2 and 1 <= ax3 and 1 <= ax4, data_1[ax0_ax1_fused * 5120 + ax2 * 256 + ax3 * 16 + ax4 - 273], T.float32(-3.4028234663852886e+38))\n            for ax2, ax3, ax4 in T.grid(10, 8, 8):\n                pool_max_1 = T.Buffer((143360,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 640 + ax2 * 64 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 640 + ax2 * 64 + ax3 * 8 + ax4\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 578 + rv0 * 289 + ax3 * 34 + rv1 * 17 + ax4 * 2 + rv2])",
        "data": "7_32_20_16_16"
    },
    {
        "op_name": "relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 28; ++i2) {\n        int32_t cse_var_1 = (((i0 * 1792) + (i1 * 112)) + (i2 * 4));\n        int32_t4 v_ = int32_t4((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3));\n        float4 v__1 = float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3]);\n        float4 v__2 = (float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f);\n        *(float4*)(((float*)compute_1) + cse_var_1) = ((v__1) > (v__2) ? (v__1) : (v__2));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 0.000000e+00f);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 16, 28, 4), \"float32\"), compute: T.Buffer((4, 16, 28, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(16, 28):\n                cse_var_1: T.int32 = i0 * 1792 + i1 * 112 + i2 * 4\n                compute_1 = T.Buffer((7168,), data=compute.data)\n                data_1 = T.Buffer((7168,), data=data.data)\n                compute_1[cse_var_1:cse_var_1 + 4] = T.max(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(0), 4))",
        "data": "4_16_28_4"
    },
    {
        "op_name": "relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 324; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 24; ++i2) {\n      for (int32_t i3 = 0; i3 < 32; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 768) + (i2 * 32)) + i3);\n        float v_ = ((float*)data_1)[cse_var_1];\n        ((float*)compute_1)[cse_var_1] = ((v_) > (0.000000e+00f) ? (v_) : (0.000000e+00f));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 0.000000e+00f);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 36, 24, 32), \"float32\"), compute: T.Buffer((9, 36, 24, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(324):\n            for i2, i3 in T.grid(24, 32):\n                cse_var_1: T.int32 = i0_i1_fused * 768 + i2 * 32 + i3\n                compute_1 = T.Buffer((248832,), data=compute.data)\n                data_1 = T.Buffer((248832,), data=data.data)\n                compute_1[cse_var_1] = T.max(data_1[cse_var_1], T.float32(0))",
        "data": "9_36_24_32"
    },
    {
        "op_name": "relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 72; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 36; ++i2) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 864) + (i2 * 24)) + i3);\n        float v_ = ((float*)data_1)[cse_var_1];\n        ((float*)compute_1)[cse_var_1] = ((v_) > (0.000000e+00f) ? (v_) : (0.000000e+00f));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 0.000000e+00f);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 24, 36, 24), \"float32\"), compute: T.Buffer((3, 24, 36, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(72):\n            for i2, i3 in T.grid(36, 24):\n                cse_var_1: T.int32 = i0_i1_fused * 864 + i2 * 24 + i3\n                compute_1 = T.Buffer((62208,), data=compute.data)\n                data_1 = T.Buffer((62208,), data=data.data)\n                compute_1[cse_var_1] = T.max(data_1[cse_var_1], T.float32(0))",
        "data": "3_24_36_24"
    },
    {
        "op_name": "relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 32; ++i2) {\n        int32_t cse_var_1 = (((i0 * 5120) + (i1 * 256)) + (i2 * 8));\n        int32_t8 v_ = int32_t8((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7));\n        float8 v__1 = float8(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7]);\n        float8 v__2 = (float8)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f);\n        *(float8*)(((float*)compute_1) + cse_var_1) = ((v__1) > (v__2) ? (v__1) : (v__2));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 0.000000e+00f);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 20, 32, 8), \"float32\"), compute: T.Buffer((10, 20, 32, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(20, 32):\n                cse_var_1: T.int32 = i0 * 5120 + i1 * 256 + i2 * 8\n                compute_1 = T.Buffer((51200,), data=compute.data)\n                data_1 = T.Buffer((51200,), data=data.data)\n                compute_1[cse_var_1:cse_var_1 + 8] = T.max(data_1[cse_var_1:cse_var_1 + 8], T.Broadcast(T.float32(0), 8))",
        "data": "10_20_32_8"
    },
    {
        "op_name": "relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 160; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 28; ++i2) {\n      for (int32_t i3 = 0; i3 < 28; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 784) + (i2 * 28)) + i3);\n        float v_ = ((float*)data_1)[cse_var_1];\n        ((float*)compute_1)[cse_var_1] = ((v_) > (0.000000e+00f) ? (v_) : (0.000000e+00f));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 0.000000e+00f);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 32, 28, 28), \"float32\"), compute: T.Buffer((5, 32, 28, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(160):\n            for i2, i3 in T.grid(28, 28):\n                cse_var_1: T.int32 = i0_i1_fused * 784 + i2 * 28 + i3\n                compute_1 = T.Buffer((125440,), data=compute.data)\n                data_1 = T.Buffer((125440,), data=data.data)\n                compute_1[cse_var_1] = T.max(data_1[cse_var_1], T.float32(0))",
        "data": "5_32_28_28"
    },
    {
        "op_name": "relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 240; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 40; ++i2) {\n      int32_t cse_var_1 = ((i0_i1_fused * 480) + (i2 * 12));\n      int32_t12 v_ = int32_t12((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7), (cse_var_1)+(1*8), (cse_var_1)+(1*9), (cse_var_1)+(1*10), (cse_var_1)+(1*11));\n      float12 v__1 = float12(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb]);\n      float12 v__2 = (float12)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f);\n      *(float12*)(((float*)compute_1) + cse_var_1) = ((v__1) > (v__2) ? (v__1) : (v__2));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 0.000000e+00f);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 40, 40, 12), \"float32\"), compute: T.Buffer((6, 40, 40, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(240):\n            for i2 in range(40):\n                cse_var_1: T.int32 = i0_i1_fused * 480 + i2 * 12\n                compute_1 = T.Buffer((115200,), data=compute.data)\n                data_1 = T.Buffer((115200,), data=data.data)\n                compute_1[cse_var_1:cse_var_1 + 12] = T.max(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(0), 12))",
        "data": "6_40_40_12"
    },
    {
        "op_name": "relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 160; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 24; ++i2) {\n      for (int32_t i3 = 0; i3 < 28; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 672) + (i2 * 28)) + i3);\n        float v_ = ((float*)data_1)[cse_var_1];\n        ((float*)compute_1)[cse_var_1] = ((v_) > (0.000000e+00f) ? (v_) : (0.000000e+00f));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 0.000000e+00f);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 32, 24, 28), \"float32\"), compute: T.Buffer((5, 32, 24, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(160):\n            for i2, i3 in T.grid(24, 28):\n                cse_var_1: T.int32 = i0_i1_fused * 672 + i2 * 28 + i3\n                compute_1 = T.Buffer((107520,), data=compute.data)\n                data_1 = T.Buffer((107520,), data=data.data)\n                compute_1[cse_var_1] = T.max(data_1[cse_var_1], T.float32(0))",
        "data": "5_32_24_28"
    },
    {
        "op_name": "relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 40; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        int32_t cse_var_1 = (((i0 * 3200) + (i1 * 80)) + (i2 * 4));\n        int32_t4 v_ = int32_t4((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3));\n        float4 v__1 = float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3]);\n        float4 v__2 = (float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f);\n        *(float4*)(((float*)compute_1) + cse_var_1) = ((v__1) > (v__2) ? (v__1) : (v__2));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 0.000000e+00f);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 40, 20, 4), \"float32\"), compute: T.Buffer((5, 40, 20, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(40, 20):\n                cse_var_1: T.int32 = i0 * 3200 + i1 * 80 + i2 * 4\n                compute_1 = T.Buffer((16000,), data=compute.data)\n                data_1 = T.Buffer((16000,), data=data.data)\n                compute_1[cse_var_1:cse_var_1 + 4] = T.max(data_1[cse_var_1:cse_var_1 + 4], T.Broadcast(T.float32(0), 4))",
        "data": "5_40_20_4"
    },
    {
        "op_name": "relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i1 = 0; i1 < 8; ++i1) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      for (int32_t i3 = 0; i3 < 8; ++i3) {\n        int32_t cse_var_1 = (((i1 * 64) + (i2 * 8)) + i3);\n        float v_ = ((float*)data_1)[cse_var_1];\n        ((float*)compute_1)[cse_var_1] = ((v_) > (0.000000e+00f) ? (v_) : (0.000000e+00f));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], 0.000000e+00f);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 8, 8, 8), \"float32\"), compute: T.Buffer((1, 8, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i1, i2, i3 in T.grid(8, 8, 8):\n            cse_var_1: T.int32 = i1 * 64 + i2 * 8 + i3\n            compute_1 = T.Buffer((512,), data=compute.data)\n            data_1 = T.Buffer((512,), data=data.data)\n            compute_1[cse_var_1] = T.max(data_1[cse_var_1], T.float32(0))",
        "data": "1_8_8_8"
    },
    {
        "op_name": "relu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 240; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      int32_t cse_var_1 = ((i0_i1_fused * 192) + (i2 * 12));\n      int32_t12 v_ = int32_t12((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7), (cse_var_1)+(1*8), (cse_var_1)+(1*9), (cse_var_1)+(1*10), (cse_var_1)+(1*11));\n      float12 v__1 = float12(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb]);\n      float12 v__2 = (float12)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f);\n      *(float12*)(((float*)compute_1) + cse_var_1) = ((v__1) > (v__2) ? (v__1) : (v__2));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], 0.000000e+00f);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 40, 16, 12), \"float32\"), compute: T.Buffer((6, 40, 16, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(240):\n            for i2 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused * 192 + i2 * 12\n                compute_1 = T.Buffer((46080,), data=compute.data)\n                data_1 = T.Buffer((46080,), data=data.data)\n                compute_1[cse_var_1:cse_var_1 + 12] = T.max(data_1[cse_var_1:cse_var_1 + 12], T.Broadcast(T.float32(0), 12))",
        "data": "6_40_16_12"
    },
    {
        "op_name": "scale_shift_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused < 3; ++b_outer_outer_outer_c_outer_outer_outer_fused) {\n    for (int32_t c_outer_outer_inner = 0; c_outer_outer_inner < 2; ++c_outer_outer_inner) {\n      for (int32_t j_outer_outer_inner = 0; j_outer_outer_inner < 2; ++j_outer_outer_inner) {\n        for (int32_t c_inner = 0; c_inner < 2; ++c_inner) {\n          for (int32_t i_inner = 0; i_inner < 4; ++i_inner) {\n            for (int32_t j_inner = 0; j_inner < 2; ++j_inner) {\n              int32_t cse_var_2 = ((c_outer_outer_inner * 2) + c_inner);\n              int32_t cse_var_1 = ((((((b_outer_outer_outer_c_outer_outer_outer_fused * 64) + (c_outer_outer_inner * 32)) + (c_inner * 16)) + (i_inner * 4)) + (j_outer_outer_inner * 2)) + j_inner);\n              ((float*)ScaleShift_1)[cse_var_1] = ((((float*)data_1)[cse_var_1] * ((float*)Scale_1)[cse_var_2]) + ((float*)Shift_1)[cse_var_2]);\n            },\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * Scale[((((int)blockIdx.x) & 7) >> 1)]) + Shift[((((int)blockIdx.x) & 7) >> 1)]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 4, 4, 4), \"float32\"), Scale: T.Buffer((4,), \"float32\"), Shift: T.Buffer((4,), \"float32\"), ScaleShift: T.Buffer((3, 4, 4, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused in T.parallel(3):\n            for c_outer_outer_inner, j_outer_outer_inner, c_inner, i_inner, j_inner in T.grid(2, 2, 2, 4, 2):\n                cse_var_2: T.int32 = c_outer_outer_inner * 2 + c_inner\n                cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused * 64 + c_outer_outer_inner * 32 + c_inner * 16 + i_inner * 4 + j_outer_outer_inner * 2 + j_inner\n                ScaleShift_1 = T.Buffer((192,), data=ScaleShift.data)\n                data_1 = T.Buffer((192,), data=data.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale[cse_var_2] + Shift[cse_var_2]",
        "data": "3_4_4_4",
        "Scale": "4",
        "Shift": "4"
    },
    {
        "op_name": "scale_shift_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused < 3072; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused) {\n    for (int32_t i_outer_inner = 0; i_outer_inner < 2; ++i_outer_inner) {\n      for (int32_t j_outer_inner = 0; j_outer_inner < 2; ++j_outer_inner) {\n        int32_t cse_var_4 = (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 3);\n        int32_t cse_var_3 = ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 1536) / 384);\n        int32_t cse_var_2 = ((cse_var_3 * 3) + cse_var_4);\n        int32_t cse_var_1 = (((((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused / 1536) * 30720) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 12) / 3) * 7680)) + (cse_var_3 * 1920)) + (cse_var_4 * 640)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 384) / 48) * 80)) + (i_outer_inner * 40)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 48) / 12) * 10)) + (j_outer_inner * 5));\n        int32_t5 v_ = int32_t5((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4));\n        *(float5*)(((float*)ScaleShift_1) + cse_var_1) = (((float5(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4])) * ((float5)(((float*)Scale_1)[cse_var_2], ((float*)Scale_1)[cse_var_2], ((float*)Scale_1)[cse_var_2], ((float*)Scale_1)[cse_var_2], ((float*)Scale_1)[cse_var_2]))) + ((float5)(((float*)Shift_1)[cse_var_2], ((float*)Shift_1)[cse_var_2], ((float*)Shift_1)[cse_var_2], ((float*)Shift_1)[cse_var_2], ((float*)Shift_1)[cse_var_2])));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[((((int)blockIdx.x) % 120) / 10)]) + Shift[((((int)blockIdx.x) % 120) / 10)]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 12, 16, 40), \"float32\"), Scale: T.Buffer((12,), \"float32\"), Shift: T.Buffer((12,), \"float32\"), ScaleShift: T.Buffer((8, 12, 16, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused in T.parallel(3072):\n            for i_outer_inner, j_outer_inner in T.grid(2, 2):\n                cse_var_4: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 3\n                cse_var_3: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 1536 // 384\n                cse_var_2: T.int32 = cse_var_3 * 3 + cse_var_4\n                cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused // 1536 * 30720 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 12 // 3 * 7680 + cse_var_3 * 1920 + cse_var_4 * 640 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 384 // 48 * 80 + i_outer_inner * 40 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 48 // 12 * 10 + j_outer_inner * 5\n                ScaleShift_1 = T.Buffer((61440,), data=ScaleShift.data)\n                data_1 = T.Buffer((61440,), data=data.data)\n                ScaleShift_1[cse_var_1:cse_var_1 + 5] = data_1[cse_var_1:cse_var_1 + 5] * T.Broadcast(Scale[cse_var_2], 5) + T.Broadcast(Shift[cse_var_2], 5)",
        "data": "8_12_16_40",
        "Scale": "12",
        "Shift": "12"
    },
    {
        "op_name": "scale_shift_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_c_fused_i_fused_j_fused = 0; b_c_fused_i_fused_j_fused < 3072; ++b_c_fused_i_fused_j_fused) {\n    int32_t cse_var_1 = ((b_c_fused_i_fused_j_fused % 1536) >> 6);\n    ((float*)ScaleShift_1)[b_c_fused_i_fused_j_fused] = ((((float*)data_1)[b_c_fused_i_fused_j_fused] * ((float*)Scale_1)[cse_var_1]) + ((float*)Shift_1)[cse_var_1]);\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[(((int)blockIdx.x) % 24)]) + Shift[(((int)blockIdx.x) % 24)]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 24, 16, 4), \"float32\"), Scale: T.Buffer((24,), \"float32\"), Shift: T.Buffer((24,), \"float32\"), ScaleShift: T.Buffer((2, 24, 16, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_c_fused_i_fused_j_fused in T.parallel(3072):\n            cse_var_1: T.int32 = b_c_fused_i_fused_j_fused % 1536 // 64\n            ScaleShift_1 = T.Buffer((3072,), data=ScaleShift.data)\n            data_1 = T.Buffer((3072,), data=data.data)\n            ScaleShift_1[b_c_fused_i_fused_j_fused] = data_1[b_c_fused_i_fused_j_fused] * Scale[cse_var_1] + Shift[cse_var_1]",
        "data": "2_24_16_4",
        "Scale": "24",
        "Shift": "24"
    },
    {
        "op_name": "scale_shift_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused < 144; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused) {\n    for (int32_t i_outer_outer_inner = 0; i_outer_outer_inner < 8; ++i_outer_outer_inner) {\n      int32_t cse_var_2 = (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused & 3);\n      int32_t cse_var_1 = (((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused / 48) * 384) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 12) * 32)) + (i_outer_outer_inner * 4)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 48) / 12));\n      ((float*)ScaleShift_1)[cse_var_1] = ((((float*)data_1)[cse_var_1] * ((float*)Scale_1)[cse_var_2]) + ((float*)Shift_1)[cse_var_2]);\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 3)) & 15) >> 2)]) + Shift[((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 3)) & 15) >> 2)]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 4, 8, 4), \"float32\"), Scale: T.Buffer((4,), \"float32\"), Shift: T.Buffer((4,), \"float32\"), ScaleShift: T.Buffer((9, 4, 8, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused in T.parallel(144):\n            for i_outer_outer_inner in range(8):\n                cse_var_2: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 4\n                cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused // 48 * 384 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 12 * 32 + i_outer_outer_inner * 4 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 48 // 12\n                ScaleShift_1 = T.Buffer((1152,), data=ScaleShift.data)\n                data_1 = T.Buffer((1152,), data=data.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale[cse_var_2] + Shift[cse_var_2]",
        "data": "9_4_8_4",
        "Scale": "4",
        "Shift": "4"
    },
    {
        "op_name": "scale_shift_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 640; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t c_outer_inner = 0; c_outer_inner < 2; ++c_outer_inner) {\n      for (int32_t b_inner = 0; b_inner < 2; ++b_inner) {\n        int32_t cse_var_4 = (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 320);\n        int32_t cse_var_3 = ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 160) >> 5);\n        int32_t cse_var_2 = (((cse_var_4 * 10) + (cse_var_3 * 2)) + c_outer_inner);\n        int32_t cse_var_1 = ((((((b_inner * 1280) + (cse_var_4 * 640)) + (cse_var_3 * 128)) + (c_outer_inner * 64)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 320) / 160) * 32)) + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 31));\n        ((float*)ScaleShift_1)[cse_var_1] = ((((float*)data_1)[cse_var_1] * ((float*)Scale_1)[cse_var_2]) + ((float*)Shift_1)[cse_var_2]);\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) & 31) * 5) + (((int)threadIdx.x) >> 3)) >> 3)]) + Shift[((((((int)blockIdx.x) & 31) * 5) + (((int)threadIdx.x) >> 3)) >> 3)]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 20, 4, 16), \"float32\"), Scale: T.Buffer((20,), \"float32\"), Shift: T.Buffer((20,), \"float32\"), ScaleShift: T.Buffer((2, 20, 4, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(640):\n            for c_outer_inner, b_inner in T.grid(2, 2):\n                cse_var_4: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 320\n                cse_var_3: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 160 // 32\n                cse_var_2: T.int32 = cse_var_4 * 10 + cse_var_3 * 2 + c_outer_inner\n                cse_var_1: T.int32 = b_inner * 1280 + cse_var_4 * 640 + cse_var_3 * 128 + c_outer_inner * 64 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 320 // 160 * 32 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 32\n                ScaleShift_1 = T.Buffer((2560,), data=ScaleShift.data)\n                data_1 = T.Buffer((2560,), data=data.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale[cse_var_2] + Shift[cse_var_2]",
        "data": "2_20_4_16",
        "Scale": "20",
        "Shift": "20"
    },
    {
        "op_name": "scale_shift_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 4800; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t c_outer_inner = 0; c_outer_inner < 3; ++c_outer_inner) {\n      for (int32_t j_outer_inner = 0; j_outer_inner < 2; ++j_outer_inner) {\n        int32_t cse_var_2 = ((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 96) >> 3) * 3) + c_outer_inner);\n        int32_t cse_var_1 = (((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 480) >> 3) * 960) + (c_outer_inner * 320)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 480) * 32)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 7) * 4)) + (j_outer_inner * 2));\n        int32_t2 v_ = int32_t2((cse_var_1)+(1*0), (cse_var_1)+(1*1));\n        *(float2*)(((float*)ScaleShift_1) + cse_var_1) = (((float2(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1])) * ((float2)(((float*)Scale_1)[cse_var_2], ((float*)Scale_1)[cse_var_2]))) + ((float2)(((float*)Shift_1)[cse_var_2], ((float*)Shift_1)[cse_var_2])));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[((((int)blockIdx.x) % 180) / 5)]) + Shift[((((int)blockIdx.x) % 180) / 5)]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 36, 20, 16), \"float32\"), Scale: T.Buffer((36,), \"float32\"), Shift: T.Buffer((36,), \"float32\"), ScaleShift: T.Buffer((5, 36, 20, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(4800):\n            for c_outer_inner, j_outer_inner in T.grid(3, 2):\n                cse_var_2: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 96 // 8 * 3 + c_outer_inner\n                cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 480 // 8 * 960 + c_outer_inner * 320 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 480 * 32 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 8 * 4 + j_outer_inner * 2\n                ScaleShift_1 = T.Buffer((57600,), data=ScaleShift.data)\n                data_1 = T.Buffer((57600,), data=data.data)\n                ScaleShift_1[cse_var_1:cse_var_1 + 2] = data_1[cse_var_1:cse_var_1 + 2] * T.Broadcast(Scale[cse_var_2], 2) + T.Broadcast(Shift[cse_var_2], 2)",
        "data": "5_36_20_16",
        "Scale": "36",
        "Shift": "36"
    },
    {
        "op_name": "scale_shift_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused < 48; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused) {\n    for (int32_t c_outer_inner = 0; c_outer_inner < 6; ++c_outer_inner) {\n      for (int32_t j_outer_inner = 0; j_outer_inner < 4; ++j_outer_inner) {\n        for (int32_t b_inner = 0; b_inner < 3; ++b_inner) {\n          for (int32_t i_inner = 0; i_inner < 14; ++i_inner) {\n            int32_t cse_var_3 = (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused / 12);\n            int32_t cse_var_2 = ((cse_var_3 * 6) + c_outer_inner);\n            int32_t cse_var_1 = ((((((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 6) >> 1) * 64512) + (b_inner * 21504)) + (cse_var_3 * 5376)) + (c_outer_inner * 896)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused & 1) * 448)) + (i_inner * 32)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 12) / 6) * 16)) + (j_outer_inner * 4));\n            int32_t4 v_ = int32_t4((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3));\n            *(float4*)(((float*)ScaleShift_1) + cse_var_1) = (((float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3])) * ((float4)(((float*)Scale_1)[cse_var_2], ((float*)Scale_1)[cse_var_2], ((float*)Scale_1)[cse_var_2], ((float*)Scale_1)[cse_var_2]))) + ((float4)(((float*)Shift_1)[cse_var_2], ((float*)Shift_1)[cse_var_2], ((float*)Shift_1)[cse_var_2], ((float*)Shift_1)[cse_var_2])));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * Scale[((((int)blockIdx.x) % 672) / 28)]) + Shift[((((int)blockIdx.x) % 672) / 28)]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 24, 28, 32), \"float32\"), Scale: T.Buffer((24,), \"float32\"), Shift: T.Buffer((24,), \"float32\"), ScaleShift: T.Buffer((9, 24, 28, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused in T.parallel(48):\n            for c_outer_inner, j_outer_inner, b_inner, i_inner in T.grid(6, 4, 3, 14):\n                cse_var_3: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused // 12\n                cse_var_2: T.int32 = cse_var_3 * 6 + c_outer_inner\n                cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 6 // 2 * 64512 + b_inner * 21504 + cse_var_3 * 5376 + c_outer_inner * 896 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 2 * 448 + i_inner * 32 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 12 // 6 * 16 + j_outer_inner * 4\n                ScaleShift_1 = T.Buffer((193536,), data=ScaleShift.data)\n                data_1 = T.Buffer((193536,), data=data.data)\n                ScaleShift_1[cse_var_1:cse_var_1 + 4] = data_1[cse_var_1:cse_var_1 + 4] * T.Broadcast(Scale[cse_var_2], 4) + T.Broadcast(Shift[cse_var_2], 4)",
        "data": "9_24_28_32",
        "Scale": "24",
        "Shift": "24"
    },
    {
        "op_name": "scale_shift_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused < 320; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused) {\n    for (int32_t j_outer_inner = 0; j_outer_inner < 3; ++j_outer_inner) {\n      for (int32_t c_inner = 0; c_inner < 2; ++c_inner) {\n        for (int32_t i_inner = 0; i_inner < 8; ++i_inner) {\n          int32_t cse_var_4 = ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 160) / 40);\n          int32_t cse_var_3 = ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused & 7) >> 1);\n          int32_t cse_var_2 = (((cse_var_4 * 8) + (cse_var_3 * 2)) + c_inner);\n          int32_t cse_var_1 = (((((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused / 160) * 30720) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 40) >> 3) * 6144)) + (cse_var_4 * 1536)) + (cse_var_3 * 384)) + (c_inner * 192)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused & 1) * 96)) + (i_inner * 12)) + (j_outer_inner * 4));\n          int32_t4 v_ = int32_t4((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3));\n          *(float4*)(((float*)ScaleShift_1) + cse_var_1) = (((float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3])) * ((float4)(((float*)Scale_1)[cse_var_2], ((float*)Scale_1)[cse_var_2], ((float*)Scale_1)[cse_var_2], ((float*)Scale_1)[cse_var_2]))) + ((float4)(((float*)Shift_1)[cse_var_2], ((float*)Shift_1)[cse_var_2], ((float*)Shift_1)[cse_var_2], ((float*)Shift_1)[cse_var_2])));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3)) % 768) / 24)]) + Shift[((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3)) % 768) / 24)]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 32, 16, 12), \"float32\"), Scale: T.Buffer((32,), \"float32\"), Shift: T.Buffer((32,), \"float32\"), ScaleShift: T.Buffer((10, 32, 16, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused in T.parallel(320):\n            for j_outer_inner, c_inner, i_inner in T.grid(3, 2, 8):\n                cse_var_4: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 160 // 40\n                cse_var_3: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 8 // 2\n                cse_var_2: T.int32 = cse_var_4 * 8 + cse_var_3 * 2 + c_inner\n                cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused // 160 * 30720 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 40 // 8 * 6144 + cse_var_4 * 1536 + cse_var_3 * 384 + c_inner * 192 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 2 * 96 + i_inner * 12 + j_outer_inner * 4\n                ScaleShift_1 = T.Buffer((61440,), data=ScaleShift.data)\n                data_1 = T.Buffer((61440,), data=data.data)\n                ScaleShift_1[cse_var_1:cse_var_1 + 4] = data_1[cse_var_1:cse_var_1 + 4] * T.Broadcast(Scale[cse_var_2], 4) + T.Broadcast(Shift[cse_var_2], 4)",
        "data": "10_32_16_12",
        "Scale": "32",
        "Shift": "32"
    },
    {
        "op_name": "scale_shift_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused < 224; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused) {\n    for (int32_t b_outer_inner = 0; b_outer_inner < 3; ++b_outer_inner) {\n      for (int32_t c_outer_inner = 0; c_outer_inner < 2; ++c_outer_inner) {\n        for (int32_t i_inner = 0; i_inner < 7; ++i_inner) {\n          int32_t cse_var_3 = (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused >> 4);\n          int32_t cse_var_2 = ((cse_var_3 * 2) + c_outer_inner);\n          int32_t cse_var_1 = ((((((b_outer_inner * 12544) + (cse_var_3 * 896)) + (c_outer_inner * 448)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused & 15) >> 2) * 112)) + (i_inner * 16)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused & 3) * 4));\n          int32_t4 v_ = int32_t4((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3));\n          *(float4*)(((float*)ScaleShift_1) + cse_var_1) = (((float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3])) * ((float4)(((float*)Scale_1)[cse_var_2], ((float*)Scale_1)[cse_var_2], ((float*)Scale_1)[cse_var_2], ((float*)Scale_1)[cse_var_2]))) + ((float4)(((float*)Shift_1)[cse_var_2], ((float*)Shift_1)[cse_var_2], ((float*)Shift_1)[cse_var_2], ((float*)Shift_1)[cse_var_2])));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[((((int)blockIdx.x) % 196) / 7)]) + Shift[((((int)blockIdx.x) % 196) / 7)]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 28, 28, 16), \"float32\"), Scale: T.Buffer((28,), \"float32\"), Shift: T.Buffer((28,), \"float32\"), ScaleShift: T.Buffer((3, 28, 28, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused in T.parallel(224):\n            for b_outer_inner, c_outer_inner, i_inner in T.grid(3, 2, 7):\n                cse_var_3: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused // 16\n                cse_var_2: T.int32 = cse_var_3 * 2 + c_outer_inner\n                cse_var_1: T.int32 = b_outer_inner * 12544 + cse_var_3 * 896 + c_outer_inner * 448 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused % 16 // 4 * 112 + i_inner * 16 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused % 4 * 4\n                ScaleShift_1 = T.Buffer((37632,), data=ScaleShift.data)\n                data_1 = T.Buffer((37632,), data=data.data)\n                ScaleShift_1[cse_var_1:cse_var_1 + 4] = data_1[cse_var_1:cse_var_1 + 4] * T.Broadcast(Scale[cse_var_2], 4) + T.Broadcast(Shift[cse_var_2], 4)",
        "data": "3_28_28_16",
        "Scale": "28",
        "Shift": "28"
    },
    {
        "op_name": "scale_shift_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused < 1800; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused) {\n    for (int32_t j_outer_inner = 0; j_outer_inner < 14; ++j_outer_inner) {\n      for (int32_t c_inner = 0; c_inner < 2; ++c_inner) {\n        int32_t cse_var_4 = (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused / 900);\n        int32_t cse_var_3 = ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 180) / 36);\n        int32_t cse_var_2 = (((cse_var_4 * 10) + (cse_var_3 * 2)) + c_inner);\n        int32_t cse_var_1 = ((((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 900) / 180) * 20160) + (cse_var_4 * 10080)) + (cse_var_3 * 2016)) + (c_inner * 1008)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 36) * 28)) + (j_outer_inner * 2));\n        int32_t2 v_ = int32_t2((cse_var_1)+(1*0), (cse_var_1)+(1*1));\n        *(float2*)(((float*)ScaleShift_1) + cse_var_1) = (((float2(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1])) * ((float2)(((float*)Scale_1)[cse_var_2], ((float*)Scale_1)[cse_var_2]))) + ((float2)(((float*)Shift_1)[cse_var_2], ((float*)Shift_1)[cse_var_2])));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) * 25) + (((int)threadIdx.x) >> 1)) % 10080) / 504)]) + Shift[((((((int)blockIdx.x) * 25) + (((int)threadIdx.x) >> 1)) % 10080) / 504)]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 20, 36, 28), \"float32\"), Scale: T.Buffer((20,), \"float32\"), Shift: T.Buffer((20,), \"float32\"), ScaleShift: T.Buffer((5, 20, 36, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused in T.parallel(1800):\n            for j_outer_inner, c_inner in T.grid(14, 2):\n                cse_var_4: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused // 900\n                cse_var_3: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 180 // 36\n                cse_var_2: T.int32 = cse_var_4 * 10 + cse_var_3 * 2 + c_inner\n                cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 900 // 180 * 20160 + cse_var_4 * 10080 + cse_var_3 * 2016 + c_inner * 1008 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused % 36 * 28 + j_outer_inner * 2\n                ScaleShift_1 = T.Buffer((100800,), data=ScaleShift.data)\n                data_1 = T.Buffer((100800,), data=data.data)\n                ScaleShift_1[cse_var_1:cse_var_1 + 2] = data_1[cse_var_1:cse_var_1 + 2] * T.Broadcast(Scale[cse_var_2], 2) + T.Broadcast(Shift[cse_var_2], 2)",
        "data": "5_20_36_28",
        "Scale": "20",
        "Shift": "20"
    },
    {
        "op_name": "prelu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 144; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 24; ++i2) {\n      for (int32_t i3 = 0; i3 < 28; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 672) + (i2 * 28)) + i3);\n        ((float*)compute_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? ((float*)data_1)[cse_var_1] : (((float*)data_1)[cse_var_1] * ((float*)Scale_1)[i3]));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 28)]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 16, 24, 28), \"float32\"), Scale: T.Buffer((28,), \"float32\"), compute: T.Buffer((9, 16, 24, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(144):\n            for i2, i3 in T.grid(24, 28):\n                cse_var_1: T.int32 = i0_i1_fused * 672 + i2 * 28 + i3\n                compute_1 = T.Buffer((96768,), data=compute.data)\n                data_1 = T.Buffer((96768,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[i3])",
        "data": "9_16_24_28",
        "Scale": "28"
    },
    {
        "op_name": "prelu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 100; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 32; ++i2) {\n      for (int32_t i3 = 0; i3 < 32; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 1024) + (i2 * 32)) + i3);\n        ((float*)compute_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? ((float*)data_1)[cse_var_1] : (((float*)data_1)[cse_var_1] * ((float*)Scale_1)[i3]));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[(((int)threadIdx.x) & 31)]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 20, 32, 32), \"float32\"), Scale: T.Buffer((32,), \"float32\"), compute: T.Buffer((5, 20, 32, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(100):\n            for i2, i3 in T.grid(32, 32):\n                cse_var_1: T.int32 = i0_i1_fused * 1024 + i2 * 32 + i3\n                compute_1 = T.Buffer((102400,), data=compute.data)\n                data_1 = T.Buffer((102400,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[i3])",
        "data": "5_20_32_32",
        "Scale": "32"
    },
    {
        "op_name": "prelu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 252; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 36; ++i2) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 864) + (i2 * 24)) + i3);\n        ((float*)compute_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? ((float*)data_1)[cse_var_1] : (((float*)data_1)[cse_var_1] * ((float*)Scale_1)[i3]));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 24)]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 28, 36, 24), \"float32\"), Scale: T.Buffer((24,), \"float32\"), compute: T.Buffer((9, 28, 36, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(252):\n            for i2, i3 in T.grid(36, 24):\n                cse_var_1: T.int32 = i0_i1_fused * 864 + i2 * 24 + i3\n                compute_1 = T.Buffer((217728,), data=compute.data)\n                data_1 = T.Buffer((217728,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[i3])",
        "data": "9_28_36_24",
        "Scale": "24"
    },
    {
        "op_name": "prelu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1440; ++i0_i1_fused_i2_fused) {\n    int32_t cse_var_1 = (i0_i1_fused_i2_fused * 4);\n    int32_t4 v_ = int32_t4((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3));\n    *(float4*)(((float*)compute_1) + cse_var_1) = ((((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f)) < (float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3]))) ? (float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3])) : ((float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3])) * *(float4*)(((float*)Scale_1) + 0)));\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] * Scale[(((int)threadIdx.x) & 3)]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 36, 4, 4), \"float32\"), Scale: T.Buffer((4,), \"float32\"), compute: T.Buffer((10, 36, 4, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(1440):\n            cse_var_1: T.int32 = i0_i1_fused_i2_fused * 4\n            compute_1 = T.Buffer((5760,), data=compute.data)\n            data_1 = T.Buffer((5760,), data=data.data)\n            compute_1[cse_var_1:cse_var_1 + 4] = T.Select(T.Broadcast(T.float32(0), 4) < data_1[cse_var_1:cse_var_1 + 4], data_1[cse_var_1:cse_var_1 + 4], data_1[cse_var_1:cse_var_1 + 4] * Scale[0:4])",
        "data": "10_36_4_4",
        "Scale": "4"
    },
    {
        "op_name": "prelu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 576; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 24; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 24) + i3);\n      ((float*)compute_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? ((float*)data_1)[cse_var_1] : (((float*)data_1)[cse_var_1] * ((float*)Scale_1)[i3]));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 24)]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 8, 36, 24), \"float32\"), Scale: T.Buffer((24,), \"float32\"), compute: T.Buffer((2, 8, 36, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(576):\n            for i3 in range(24):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 24 + i3\n                compute_1 = T.Buffer((13824,), data=compute.data)\n                data_1 = T.Buffer((13824,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[i3])",
        "data": "2_8_36_24",
        "Scale": "24"
    },
    {
        "op_name": "prelu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 72; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 28; ++i2) {\n      for (int32_t i3 = 0; i3 < 36; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 1008) + (i2 * 36)) + i3);\n        ((float*)compute_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? ((float*)data_1)[cse_var_1] : (((float*)data_1)[cse_var_1] * ((float*)Scale_1)[i3]));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) % 36)]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 12, 28, 36), \"float32\"), Scale: T.Buffer((36,), \"float32\"), compute: T.Buffer((6, 12, 28, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(72):\n            for i2, i3 in T.grid(28, 36):\n                cse_var_1: T.int32 = i0_i1_fused * 1008 + i2 * 36 + i3\n                compute_1 = T.Buffer((72576,), data=compute.data)\n                data_1 = T.Buffer((72576,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[i3])",
        "data": "6_12_28_36",
        "Scale": "36"
    },
    {
        "op_name": "prelu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5376; ++i0_i1_fused_i2_fused) {\n    int32_t cse_var_1 = (i0_i1_fused_i2_fused * 16);\n    int32_t16 v_ = int32_t16((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7), (cse_var_1)+(1*8), (cse_var_1)+(1*9), (cse_var_1)+(1*10), (cse_var_1)+(1*11), (cse_var_1)+(1*12), (cse_var_1)+(1*13), (cse_var_1)+(1*14), (cse_var_1)+(1*15));\n    *(float16*)(((float*)compute_1) + cse_var_1) = ((((float16)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f)) < (float16(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb],((float*)data_1)[v_.sc],((float*)data_1)[v_.sd],((float*)data_1)[v_.se],((float*)data_1)[v_.sf]))) ? (float16(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb],((float*)data_1)[v_.sc],((float*)data_1)[v_.sd],((float*)data_1)[v_.se],((float*)data_1)[v_.sf])) : ((float16(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb],((float*)data_1)[v_.sc],((float*)data_1)[v_.sd],((float*)data_1)[v_.se],((float*)data_1)[v_.sf])) * *(float16*)(((float*)Scale_1) + 0)));\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[(((int)threadIdx.x) & 15)]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 24, 32, 16), \"float32\"), Scale: T.Buffer((16,), \"float32\"), compute: T.Buffer((7, 24, 32, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(5376):\n            cse_var_1: T.int32 = i0_i1_fused_i2_fused * 16\n            compute_1 = T.Buffer((86016,), data=compute.data)\n            data_1 = T.Buffer((86016,), data=data.data)\n            compute_1[cse_var_1:cse_var_1 + 16] = T.Select(T.Broadcast(T.float32(0), 16) < data_1[cse_var_1:cse_var_1 + 16], data_1[cse_var_1:cse_var_1 + 16], data_1[cse_var_1:cse_var_1 + 16] * Scale[0:16])",
        "data": "7_24_32_16",
        "Scale": "16"
    },
    {
        "op_name": "prelu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 240; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 28; ++i2) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 672) + (i2 * 24)) + i3);\n        ((float*)compute_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? ((float*)data_1)[cse_var_1] : (((float*)data_1)[cse_var_1] * ((float*)Scale_1)[i3]));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) % 24)]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 40, 28, 24), \"float32\"), Scale: T.Buffer((24,), \"float32\"), compute: T.Buffer((6, 40, 28, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(240):\n            for i2, i3 in T.grid(28, 24):\n                cse_var_1: T.int32 = i0_i1_fused * 672 + i2 * 24 + i3\n                compute_1 = T.Buffer((161280,), data=compute.data)\n                data_1 = T.Buffer((161280,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[i3])",
        "data": "6_40_28_24",
        "Scale": "24"
    },
    {
        "op_name": "prelu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5600; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 32; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 32) + i3);\n      ((float*)compute_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? ((float*)data_1)[cse_var_1] : (((float*)data_1)[cse_var_1] * ((float*)Scale_1)[i3]));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) & 31)]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 40, 20, 32), \"float32\"), Scale: T.Buffer((32,), \"float32\"), compute: T.Buffer((7, 40, 20, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(5600):\n            for i3 in range(32):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 32 + i3\n                compute_1 = T.Buffer((179200,), data=compute.data)\n                data_1 = T.Buffer((179200,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[i3])",
        "data": "7_40_20_32",
        "Scale": "32"
    },
    {
        "op_name": "prelu",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 40; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 40; ++i2) {\n      for (int32_t i3 = 0; i3 < 36; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 1440) + (i2 * 36)) + i3);\n        ((float*)compute_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? ((float*)data_1)[cse_var_1] : (((float*)data_1)[cse_var_1] * ((float*)Scale_1)[i3]));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 28) + ((int)threadIdx.x)) % 36)]));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 40, 40, 36), \"float32\"), Scale: T.Buffer((36,), \"float32\"), compute: T.Buffer((1, 40, 40, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(40):\n            for i2, i3 in T.grid(40, 36):\n                cse_var_1: T.int32 = i0_i1_fused * 1440 + i2 * 36 + i3\n                compute_1 = T.Buffer((57600,), data=compute.data)\n                data_1 = T.Buffer((57600,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[i3])",
        "data": "1_40_40_36",
        "Scale": "36"
    },
    {
        "op_name": "scale_shift_nchwc",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused < 576; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused) {\n    for (int32_t cc_outer_inner = 0; cc_outer_inner < 2; ++cc_outer_inner) {\n      for (int32_t j_outer_inner = 0; j_outer_inner < 4; ++j_outer_inner) {\n        int32_t cse_var_3 = ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused & 1) * 14);\n        int32_t cse_var_2 = ((cc_outer_inner * 28) + cse_var_3);\n        int32_t cse_var_1 = ((((((cc_outer_inner * 32256) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused >> 5) * 1792)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused & 3) >> 1) * 896)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused & 31) >> 2) * 112)) + (j_outer_inner * 28)) + cse_var_3);\n        int32_t14 v_ = int32_t14((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7), (cse_var_1)+(1*8), (cse_var_1)+(1*9), (cse_var_1)+(1*10), (cse_var_1)+(1*11), (cse_var_1)+(1*12), (cse_var_1)+(1*13));\n        int32_t14 v__1 = int32_t14((cse_var_2)+(1*0), (cse_var_2)+(1*1), (cse_var_2)+(1*2), (cse_var_2)+(1*3), (cse_var_2)+(1*4), (cse_var_2)+(1*5), (cse_var_2)+(1*6), (cse_var_2)+(1*7), (cse_var_2)+(1*8), (cse_var_2)+(1*9), (cse_var_2)+(1*10), (cse_var_2)+(1*11), (cse_var_2)+(1*12), (cse_var_2)+(1*13));\n        *(float14*)(((float*)ScaleShift_1) + cse_var_1) = (((float14(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb],((float*)data_1)[v_.sc],((float*)data_1)[v_.sd])) * (float14(((float*)Scale_1)[v__1.s0],((float*)Scale_1)[v__1.s1],((float*)Scale_1)[v__1.s2],((float*)Scale_1)[v__1.s3],((float*)Scale_1)[v__1.s4],((float*)Scale_1)[v__1.s5],((float*)Scale_1)[v__1.s6],((float*)Scale_1)[v__1.s7],((float*)Scale_1)[v__1.s8],((float*)Scale_1)[v__1.s9],((float*)Scale_1)[v__1.sa],((float*)Scale_1)[v__1.sb],((float*)Scale_1)[v__1.sc],((float*)Scale_1)[v__1.sd]))) + (float14(((float*)Shift_1)[v__1.s0],((float*)Shift_1)[v__1.s1],((float*)Shift_1)[v__1.s2],((float*)Shift_1)[v__1.s3],((float*)Shift_1)[v__1.s4],((float*)Shift_1)[v__1.s5],((float*)Shift_1)[v__1.s6],((float*)Shift_1)[v__1.s7],((float*)Shift_1)[v__1.s8],((float*)Shift_1)[v__1.s9],((float*)Shift_1)[v__1.sa],((float*)Shift_1)[v__1.sb],((float*)Shift_1)[v__1.sc],((float*)Shift_1)[v__1.sd])));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) / 504) * 28) + (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 28))]) + Shift[(((((int)blockIdx.x) / 504) * 28) + (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 28))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 2, 36, 32, 28), \"float32\"), Scale: T.Buffer((2, 28), \"float32\"), Shift: T.Buffer((2, 28), \"float32\"), ScaleShift: T.Buffer((1, 2, 36, 32, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused in T.parallel(576):\n            for cc_outer_inner, j_outer_inner in T.grid(2, 4):\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 2 * 14\n                cse_var_2: T.int32 = cc_outer_inner * 28 + cse_var_3\n                cse_var_1: T.int32 = cc_outer_inner * 32256 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused // 32 * 1792 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 4 // 2 * 896 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 32 // 4 * 112 + j_outer_inner * 28 + cse_var_3\n                ScaleShift_1 = T.Buffer((64512,), data=ScaleShift.data)\n                data_1 = T.Buffer((64512,), data=data.data)\n                Scale_1 = T.Buffer((56,), data=Scale.data)\n                Shift_1 = T.Buffer((56,), data=Shift.data)\n                ScaleShift_1[cse_var_1:cse_var_1 + 14] = data_1[cse_var_1:cse_var_1 + 14] * Scale_1[cse_var_2:cse_var_2 + 14] + Shift_1[cse_var_2:cse_var_2 + 14]",
        "data": "1_2_36_32_28",
        "Scale": "2_28",
        "Shift": "2_28"
    },
    {
        "op_name": "scale_shift_nchwc",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused < 10800; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused) {\n    for (int32_t j_outer_inner = 0; j_outer_inner < 4; ++j_outer_inner) {\n      int32_t cse_var_4 = (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused % 72);\n      int32_t cse_var_3 = (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused % 360) / 72) * 8);\n      int32_t cse_var_2 = (((cse_var_4 / 36) * 40) + cse_var_3);\n      int32_t cse_var_1 = ((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused / 1800) * 57600) + (cse_var_4 * 800)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused % 1800) / 360) * 160)) + (j_outer_inner * 40)) + cse_var_3);\n      int32_t8 v_ = int32_t8((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7));\n      int32_t8 v__1 = int32_t8((cse_var_2)+(1*0), (cse_var_2)+(1*1), (cse_var_2)+(1*2), (cse_var_2)+(1*3), (cse_var_2)+(1*4), (cse_var_2)+(1*5), (cse_var_2)+(1*6), (cse_var_2)+(1*7));\n      *(float8*)(((float*)ScaleShift_1) + cse_var_1) = (((float8(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7])) * (float8(((float*)Scale_1)[v__1.s0],((float*)Scale_1)[v__1.s1],((float*)Scale_1)[v__1.s2],((float*)Scale_1)[v__1.s3],((float*)Scale_1)[v__1.s4],((float*)Scale_1)[v__1.s5],((float*)Scale_1)[v__1.s6],((float*)Scale_1)[v__1.s7]))) + (float8(((float*)Shift_1)[v__1.s0],((float*)Shift_1)[v__1.s1],((float*)Shift_1)[v__1.s2],((float*)Shift_1)[v__1.s3],((float*)Shift_1)[v__1.s4],((float*)Shift_1)[v__1.s5],((float*)Shift_1)[v__1.s6],((float*)Shift_1)[v__1.s7])));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) % 900) / 450) * 40) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 40))]) + Shift[((((((int)blockIdx.x) % 900) / 450) * 40) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 40))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 2, 36, 20, 40), \"float32\"), Scale: T.Buffer((2, 40), \"float32\"), Shift: T.Buffer((2, 40), \"float32\"), ScaleShift: T.Buffer((6, 2, 36, 20, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused in T.parallel(10800):\n            for j_outer_inner in range(4):\n                cse_var_4: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused % 72\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused % 360 // 72 * 8\n                cse_var_2: T.int32 = cse_var_4 // 36 * 40 + cse_var_3\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused // 1800 * 57600 + cse_var_4 * 800 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused % 1800 // 360 * 160 + j_outer_inner * 40 + cse_var_3\n                ScaleShift_1 = T.Buffer((345600,), data=ScaleShift.data)\n                data_1 = T.Buffer((345600,), data=data.data)\n                Scale_1 = T.Buffer((80,), data=Scale.data)\n                Shift_1 = T.Buffer((80,), data=Shift.data)\n                ScaleShift_1[cse_var_1:cse_var_1 + 8] = data_1[cse_var_1:cse_var_1 + 8] * Scale_1[cse_var_2:cse_var_2 + 8] + Shift_1[cse_var_2:cse_var_2 + 8]",
        "data": "6_2_36_20_40",
        "Scale": "2_40",
        "Shift": "2_40"
    },
    {
        "op_name": "scale_shift_nchwc",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused < 1792; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused) {\n    for (int32_t b_inner = 0; b_inner < 3; ++b_inner) {\n      for (int32_t j_inner = 0; j_inner < 4; ++j_inner) {\n        int32_t cse_var_4 = (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 896);\n        int32_t cse_var_3 = ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused & 7) * 4);\n        int32_t cse_var_2 = (((cse_var_4 / 448) * 32) + cse_var_3);\n        int32_t cse_var_1 = ((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused / 896) * 43008) + (b_inner * 14336)) + ((cse_var_4 >> 3) * 128)) + (j_inner * 32)) + cse_var_3);\n        int32_t4 v_ = int32_t4((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3));\n        int32_t4 v__1 = int32_t4((cse_var_2)+(1*0), (cse_var_2)+(1*1), (cse_var_2)+(1*2), (cse_var_2)+(1*3));\n        *(float4*)(((float*)ScaleShift_1) + cse_var_1) = (((float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3])) * (float4(((float*)Scale_1)[v__1.s0],((float*)Scale_1)[v__1.s1],((float*)Scale_1)[v__1.s2],((float*)Scale_1)[v__1.s3]))) + (float4(((float*)Shift_1)[v__1.s0],((float*)Shift_1)[v__1.s1],((float*)Shift_1)[v__1.s2],((float*)Shift_1)[v__1.s3])));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) % 224) / 112) * 32) + (((int)threadIdx.x) & 31))]) + Shift[((((((int)blockIdx.x) % 224) / 112) * 32) + (((int)threadIdx.x) & 31))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 2, 28, 8, 32), \"float32\"), Scale: T.Buffer((2, 32), \"float32\"), Shift: T.Buffer((2, 32), \"float32\"), ScaleShift: T.Buffer((6, 2, 28, 8, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused in T.parallel(1792):\n            for b_inner, j_inner in T.grid(3, 4):\n                cse_var_4: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 896\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 8 * 4\n                cse_var_2: T.int32 = cse_var_4 // 448 * 32 + cse_var_3\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused // 896 * 43008 + b_inner * 14336 + cse_var_4 // 8 * 128 + j_inner * 32 + cse_var_3\n                ScaleShift_1 = T.Buffer((86016,), data=ScaleShift.data)\n                data_1 = T.Buffer((86016,), data=data.data)\n                Scale_1 = T.Buffer((64,), data=Scale.data)\n                Shift_1 = T.Buffer((64,), data=Shift.data)\n                ScaleShift_1[cse_var_1:cse_var_1 + 4] = data_1[cse_var_1:cse_var_1 + 4] * Scale_1[cse_var_2:cse_var_2 + 4] + Shift_1[cse_var_2:cse_var_2 + 4]",
        "data": "6_2_28_8_32",
        "Scale": "2_32",
        "Shift": "2_32"
    },
    {
        "op_name": "scale_shift_nchwc",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused < 1120; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused) {\n    for (int32_t i_outer_outer_inner = 0; i_outer_outer_inner < 2; ++i_outer_outer_inner) {\n      int32_t cse_var_2 = ((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused % 160) / 80) * 20) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused & 1) * 10));\n      int32_t cse_var_1 = ((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused >> 3) * 160) + (i_outer_outer_inner * 80)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused & 7) * 10));\n      int32_t10 v_ = int32_t10((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7), (cse_var_1)+(1*8), (cse_var_1)+(1*9));\n      int32_t10 v__1 = int32_t10((cse_var_2)+(1*0), (cse_var_2)+(1*1), (cse_var_2)+(1*2), (cse_var_2)+(1*3), (cse_var_2)+(1*4), (cse_var_2)+(1*5), (cse_var_2)+(1*6), (cse_var_2)+(1*7), (cse_var_2)+(1*8), (cse_var_2)+(1*9));\n      *(float10*)(((float*)ScaleShift_1) + cse_var_1) = (((float10(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9])) * (float10(((float*)Scale_1)[v__1.s0],((float*)Scale_1)[v__1.s1],((float*)Scale_1)[v__1.s2],((float*)Scale_1)[v__1.s3],((float*)Scale_1)[v__1.s4],((float*)Scale_1)[v__1.s5],((float*)Scale_1)[v__1.s6],((float*)Scale_1)[v__1.s7],((float*)Scale_1)[v__1.s8],((float*)Scale_1)[v__1.s9]))) + (float10(((float*)Shift_1)[v__1.s0],((float*)Shift_1)[v__1.s1],((float*)Shift_1)[v__1.s2],((float*)Shift_1)[v__1.s3],((float*)Shift_1)[v__1.s4],((float*)Shift_1)[v__1.s5],((float*)Shift_1)[v__1.s6],((float*)Shift_1)[v__1.s7],((float*)Shift_1)[v__1.s8],((float*)Shift_1)[v__1.s9])));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) & 63) >> 5) * 20) + (((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 20))]) + Shift[((((((int)blockIdx.x) & 63) >> 5) * 20) + (((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 20))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 2, 20, 4, 20), \"float32\"), Scale: T.Buffer((2, 20), \"float32\"), Shift: T.Buffer((2, 20), \"float32\"), ScaleShift: T.Buffer((7, 2, 20, 4, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused in T.parallel(1120):\n            for i_outer_outer_inner in range(2):\n                cse_var_2: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused % 160 // 80 * 20 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused % 2 * 10\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused // 8 * 160 + i_outer_outer_inner * 80 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused % 8 * 10\n                ScaleShift_1 = T.Buffer((22400,), data=ScaleShift.data)\n                data_1 = T.Buffer((22400,), data=data.data)\n                Scale_1 = T.Buffer((40,), data=Scale.data)\n                Shift_1 = T.Buffer((40,), data=Shift.data)\n                ScaleShift_1[cse_var_1:cse_var_1 + 10] = data_1[cse_var_1:cse_var_1 + 10] * Scale_1[cse_var_2:cse_var_2 + 10] + Shift_1[cse_var_2:cse_var_2 + 10]",
        "data": "7_2_20_4_20",
        "Scale": "2_20",
        "Shift": "2_20"
    },
    {
        "op_name": "scale_shift_nchwc",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused_b_inner_fused_cc_inner_fused_i_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused_b_inner_fused_cc_inner_fused_i_inner_fused < 864; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused_b_inner_fused_cc_inner_fused_i_inner_fused) {\n    for (int32_t j_inner = 0; j_inner < 4; ++j_inner) {\n      int32_t cse_var_3 = (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused_b_inner_fused_cc_inner_fused_i_inner_fused % 432) / 216) * 6);\n      int32_t cse_var_2 = ((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused_b_inner_fused_cc_inner_fused_i_inner_fused % 72) / 36) * 12) + cse_var_3);\n      int32_t cse_var_1 = (((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused_b_inner_fused_cc_inner_fused_i_inner_fused / 432) * 10368) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused_b_inner_fused_cc_inner_fused_i_inner_fused % 216) / 36) * 1728)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused_b_inner_fused_cc_inner_fused_i_inner_fused & 3) * 432)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused_b_inner_fused_cc_inner_fused_i_inner_fused % 36) >> 2) * 48)) + (j_inner * 12)) + cse_var_3);\n      int32_t6 v_ = int32_t6((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5));\n      int32_t6 v__1 = int32_t6((cse_var_2)+(1*0), (cse_var_2)+(1*1), (cse_var_2)+(1*2), (cse_var_2)+(1*3), (cse_var_2)+(1*4), (cse_var_2)+(1*5));\n      *(float6*)(((float*)ScaleShift_1) + cse_var_1) = (((float6(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5])) * (float6(((float*)Scale_1)[v__1.s0],((float*)Scale_1)[v__1.s1],((float*)Scale_1)[v__1.s2],((float*)Scale_1)[v__1.s3],((float*)Scale_1)[v__1.s4],((float*)Scale_1)[v__1.s5]))) + (float6(((float*)Shift_1)[v__1.s0],((float*)Shift_1)[v__1.s1],((float*)Shift_1)[v__1.s2],((float*)Shift_1)[v__1.s3],((float*)Shift_1)[v__1.s4],((float*)Shift_1)[v__1.s5])));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) & 63) >> 5) * 12) + (((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 12))]) + Shift[((((((int)blockIdx.x) & 63) >> 5) * 12) + (((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 12))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 2, 4, 36, 12), \"float32\"), Scale: T.Buffer((2, 12), \"float32\"), Shift: T.Buffer((2, 12), \"float32\"), ScaleShift: T.Buffer((6, 2, 4, 36, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused_b_inner_fused_cc_inner_fused_i_inner_fused in T.parallel(864):\n            for j_inner in range(4):\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused_b_inner_fused_cc_inner_fused_i_inner_fused % 432 // 216 * 6\n                cse_var_2: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused_b_inner_fused_cc_inner_fused_i_inner_fused % 72 // 36 * 12 + cse_var_3\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused_b_inner_fused_cc_inner_fused_i_inner_fused // 432 * 10368 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused_b_inner_fused_cc_inner_fused_i_inner_fused % 216 // 36 * 1728 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused_b_inner_fused_cc_inner_fused_i_inner_fused % 4 * 432 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused_b_inner_fused_cc_inner_fused_i_inner_fused % 36 // 4 * 48 + j_inner * 12 + cse_var_3\n                ScaleShift_1 = T.Buffer((20736,), data=ScaleShift.data)\n                data_1 = T.Buffer((20736,), data=data.data)\n                Scale_1 = T.Buffer((24,), data=Scale.data)\n                Shift_1 = T.Buffer((24,), data=Shift.data)\n                ScaleShift_1[cse_var_1:cse_var_1 + 6] = data_1[cse_var_1:cse_var_1 + 6] * Scale_1[cse_var_2:cse_var_2 + 6] + Shift_1[cse_var_2:cse_var_2 + 6]",
        "data": "6_2_4_36_12",
        "Scale": "2_12",
        "Shift": "2_12"
    },
    {
        "op_name": "scale_shift_nchwc",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused < 5760; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused) {\n    for (int32_t i_outer_inner = 0; i_outer_inner < 2; ++i_outer_inner) {\n      for (int32_t cb_outer_inner = 0; cb_outer_inner < 2; ++cb_outer_inner) {\n        for (int32_t b_inner = 0; b_inner < 3; ++b_inner) {\n          for (int32_t i_inner = 0; i_inner < 2; ++i_inner) {\n            int32_t cse_var_3 = (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 1920);\n            int32_t cse_var_2 = ((((cse_var_3 / 960) * 40) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 20) * 2)) + cb_outer_inner);\n            int32_t cse_var_1 = ((((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused / 1920) * 46080) + (b_inner * 15360)) + ((cse_var_3 / 160) * 1280)) + (i_outer_inner * 640)) + (i_inner * 320)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 160) * 2)) + cb_outer_inner);\n            ((float*)ScaleShift_1)[cse_var_1] = ((((float*)data_1)[cse_var_1] * ((float*)Scale_1)[cse_var_2]) + ((float*)Shift_1)[cse_var_2]);\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) % 240) / 120) * 40) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 40))]) + Shift[((((((int)blockIdx.x) % 240) / 120) * 40) + (((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 40))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 2, 24, 8, 40), \"float32\"), Scale: T.Buffer((2, 40), \"float32\"), Shift: T.Buffer((2, 40), \"float32\"), ScaleShift: T.Buffer((9, 2, 24, 8, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused in T.parallel(5760):\n            for i_outer_inner, cb_outer_inner, b_inner, i_inner in T.grid(2, 2, 3, 2):\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 1920\n                cse_var_2: T.int32 = cse_var_3 // 960 * 40 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 20 * 2 + cb_outer_inner\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused // 1920 * 46080 + b_inner * 15360 + cse_var_3 // 160 * 1280 + i_outer_inner * 640 + i_inner * 320 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 160 * 2 + cb_outer_inner\n                ScaleShift_1 = T.Buffer((138240,), data=ScaleShift.data)\n                data_1 = T.Buffer((138240,), data=data.data)\n                Scale_1 = T.Buffer((80,), data=Scale.data)\n                Shift_1 = T.Buffer((80,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]",
        "data": "9_2_24_8_40",
        "Scale": "2_40",
        "Shift": "2_40"
    },
    {
        "op_name": "scale_shift_nchwc",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 480; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t cb_outer_outer_inner = 0; cb_outer_outer_inner < 9; ++cb_outer_outer_inner) {\n      for (int32_t i_inner = 0; i_inner < 8; ++i_inner) {\n        for (int32_t j_inner = 0; j_inner < 2; ++j_inner) {\n          int32_t cse_var_3 = (cb_outer_outer_inner * 4);\n          int32_t cse_var_2 = ((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 10) / 5) * 36) + cse_var_3);\n          int32_t cse_var_1 = (((((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 240) * 138240) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 40) / 5) * 17280)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 240) / 80) * 5760)) + (i_inner * 720)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 80) / 40) * 360)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5) * 72)) + (j_inner * 36)) + cse_var_3);\n          int32_t4 v_ = int32_t4((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3));\n          int32_t4 v__1 = int32_t4((cse_var_2)+(1*0), (cse_var_2)+(1*1), (cse_var_2)+(1*2), (cse_var_2)+(1*3));\n          *(float4*)(((float*)ScaleShift_1) + cse_var_1) = (((float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3])) * (float4(((float*)Scale_1)[v__1.s0],((float*)Scale_1)[v__1.s1],((float*)Scale_1)[v__1.s2],((float*)Scale_1)[v__1.s3]))) + (float4(((float*)Shift_1)[v__1.s0],((float*)Shift_1)[v__1.s1],((float*)Shift_1)[v__1.s2],((float*)Shift_1)[v__1.s3])));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) % 768) / 384) * 36) + (((((int)blockIdx.x) * 9) + ((int)threadIdx.x)) % 36))]) + Shift[((((((int)blockIdx.x) % 768) / 384) * 36) + (((((int)blockIdx.x) * 9) + ((int)threadIdx.x)) % 36))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 2, 24, 20, 36), \"float32\"), Scale: T.Buffer((2, 36), \"float32\"), Shift: T.Buffer((2, 36), \"float32\"), ScaleShift: T.Buffer((8, 2, 24, 20, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(480):\n            for cb_outer_outer_inner, i_inner, j_inner in T.grid(9, 8, 2):\n                cse_var_3: T.int32 = cb_outer_outer_inner * 4\n                cse_var_2: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 10 // 5 * 36 + cse_var_3\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 240 * 138240 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 40 // 5 * 17280 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 240 // 80 * 5760 + i_inner * 720 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 80 // 40 * 360 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5 * 72 + j_inner * 36 + cse_var_3\n                ScaleShift_1 = T.Buffer((276480,), data=ScaleShift.data)\n                data_1 = T.Buffer((276480,), data=data.data)\n                Scale_1 = T.Buffer((72,), data=Scale.data)\n                Shift_1 = T.Buffer((72,), data=Shift.data)\n                ScaleShift_1[cse_var_1:cse_var_1 + 4] = data_1[cse_var_1:cse_var_1 + 4] * Scale_1[cse_var_2:cse_var_2 + 4] + Shift_1[cse_var_2:cse_var_2 + 4]",
        "data": "8_2_24_20_36",
        "Scale": "2_36",
        "Shift": "2_36"
    },
    {
        "op_name": "scale_shift_nchwc",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused < 1920; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused) {\n    for (int32_t i_outer_inner = 0; i_outer_inner < 3; ++i_outer_inner) {\n      for (int32_t j_outer_inner = 0; j_outer_inner < 4; ++j_outer_inner) {\n        for (int32_t b_inner = 0; b_inner < 3; ++b_inner) {\n          int32_t cse_var_4 = ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 320) / 160);\n          int32_t cse_var_3 = (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 10) >> 1) * 4);\n          int32_t cse_var_2 = ((cse_var_4 * 20) + cse_var_3);\n          int32_t cse_var_1 = ((((((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused & 1) * 138240) + (b_inner * 46080)) + (cse_var_4 * 23040)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused / 320) * 3840)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 160) / 80) * 1920)) + (i_outer_inner * 640)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 80) / 10) * 80)) + (j_outer_inner * 20)) + cse_var_3);\n          int32_t4 v_ = int32_t4((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3));\n          int32_t4 v__1 = int32_t4((cse_var_2)+(1*0), (cse_var_2)+(1*1), (cse_var_2)+(1*2), (cse_var_2)+(1*3));\n          *(float4*)(((float*)ScaleShift_1) + cse_var_1) = (((float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3])) * (float4(((float*)Scale_1)[v__1.s0],((float*)Scale_1)[v__1.s1],((float*)Scale_1)[v__1.s2],((float*)Scale_1)[v__1.s3]))) + (float4(((float*)Shift_1)[v__1.s0],((float*)Shift_1)[v__1.s1],((float*)Shift_1)[v__1.s2],((float*)Shift_1)[v__1.s3])));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] * Scale[((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 18)) % 2560) / 1280) * 20) + (((((int)blockIdx.x) * 14) + ((int)threadIdx.x)) % 20))]) + Shift[((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 18)) % 2560) / 1280) * 20) + (((((int)blockIdx.x) * 14) + ((int)threadIdx.x)) % 20))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 2, 36, 32, 20), \"float32\"), Scale: T.Buffer((2, 20), \"float32\"), Shift: T.Buffer((2, 20), \"float32\"), ScaleShift: T.Buffer((6, 2, 36, 32, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused in T.parallel(1920):\n            for i_outer_inner, j_outer_inner, b_inner in T.grid(3, 4, 3):\n                cse_var_4: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 320 // 160\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 10 // 2 * 4\n                cse_var_2: T.int32 = cse_var_4 * 20 + cse_var_3\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 2 * 138240 + b_inner * 46080 + cse_var_4 * 23040 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused // 320 * 3840 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 160 // 80 * 1920 + i_outer_inner * 640 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 80 // 10 * 80 + j_outer_inner * 20 + cse_var_3\n                ScaleShift_1 = T.Buffer((276480,), data=ScaleShift.data)\n                data_1 = T.Buffer((276480,), data=data.data)\n                Scale_1 = T.Buffer((40,), data=Scale.data)\n                Shift_1 = T.Buffer((40,), data=Shift.data)\n                ScaleShift_1[cse_var_1:cse_var_1 + 4] = data_1[cse_var_1:cse_var_1 + 4] * Scale_1[cse_var_2:cse_var_2 + 4] + Shift_1[cse_var_2:cse_var_2 + 4]",
        "data": "6_2_36_32_20",
        "Scale": "2_20",
        "Shift": "2_20"
    },
    {
        "op_name": "scale_shift_nchwc",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused < 3200; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused) {\n    for (int32_t cb_outer_inner = 0; cb_outer_inner < 8; ++cb_outer_inner) {\n      for (int32_t j_inner = 0; j_inner < 5; ++j_inner) {\n        int32_t cse_var_3 = (cb_outer_inner * 4);\n        int32_t cse_var_2 = ((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused & 31) >> 4) * 32) + cse_var_3);\n        int32_t cse_var_1 = (((((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 160) >> 4) * 51200) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused / 320) * 5120)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused & 15) >> 2) * 1280)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 320) / 160) * 640)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused & 3) * 160)) + (j_inner * 32)) + cse_var_3);\n        int32_t4 v_ = int32_t4((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3));\n        int32_t4 v__1 = int32_t4((cse_var_2)+(1*0), (cse_var_2)+(1*1), (cse_var_2)+(1*2), (cse_var_2)+(1*3));\n        *(float4*)(((float*)ScaleShift_1) + cse_var_1) = (((float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3])) * (float4(((float*)Scale_1)[v__1.s0],((float*)Scale_1)[v__1.s1],((float*)Scale_1)[v__1.s2],((float*)Scale_1)[v__1.s3]))) + (float4(((float*)Shift_1)[v__1.s0],((float*)Shift_1)[v__1.s1],((float*)Shift_1)[v__1.s2],((float*)Shift_1)[v__1.s3])));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) & 2047) >> 10) * 32) + (((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) & 31))]) + Shift[((((((int)blockIdx.x) & 2047) >> 10) * 32) + (((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) & 31))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 2, 40, 40, 32), \"float32\"), Scale: T.Buffer((2, 32), \"float32\"), Shift: T.Buffer((2, 32), \"float32\"), ScaleShift: T.Buffer((5, 2, 40, 40, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused in T.parallel(3200):\n            for cb_outer_inner, j_inner in T.grid(8, 5):\n                cse_var_3: T.int32 = cb_outer_inner * 4\n                cse_var_2: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 32 // 16 * 32 + cse_var_3\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 160 // 16 * 51200 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused // 320 * 5120 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 16 // 4 * 1280 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 320 // 160 * 640 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 4 * 160 + j_inner * 32 + cse_var_3\n                ScaleShift_1 = T.Buffer((512000,), data=ScaleShift.data)\n                data_1 = T.Buffer((512000,), data=data.data)\n                Scale_1 = T.Buffer((64,), data=Scale.data)\n                Shift_1 = T.Buffer((64,), data=Shift.data)\n                ScaleShift_1[cse_var_1:cse_var_1 + 4] = data_1[cse_var_1:cse_var_1 + 4] * Scale_1[cse_var_2:cse_var_2 + 4] + Shift_1[cse_var_2:cse_var_2 + 4]",
        "data": "5_2_40_40_32",
        "Scale": "2_32",
        "Shift": "2_32"
    },
    {
        "op_name": "scale_shift_nchwc",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_Scale_strides == NULL)) {\n  },\n  if (!(default_function_Shift_strides == NULL)) {\n  },\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused < 2880; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused) {\n    for (int32_t j_inner = 0; j_inner < 2; ++j_inner) {\n      int32_t cse_var_4 = (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused & 3);\n      int32_t cse_var_3 = ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 960) / 480);\n      int32_t cse_var_2 = ((cse_var_3 * 4) + cse_var_4);\n      int32_t cse_var_1 = ((((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused / 960) * 1920) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 240) / 80) * 640)) + (cse_var_3 * 320)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 80) >> 2) * 16)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 480) / 240) * 8)) + (j_inner * 4)) + cse_var_4);\n      ((float*)ScaleShift_1)[cse_var_1] = ((((float*)data_1)[cse_var_1] * ((float*)Scale_1)[cse_var_2]) + ((float*)Shift_1)[cse_var_2]);\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] * Scale[((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 20)) & 31) >> 4) * 4) + (((int)threadIdx.x) & 3))]) + Shift[((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 20)) & 31) >> 4) * 4) + (((int)threadIdx.x) & 3))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 2, 20, 4, 4), \"float32\"), Scale: T.Buffer((2, 4), \"float32\"), Shift: T.Buffer((2, 4), \"float32\"), ScaleShift: T.Buffer((9, 2, 20, 4, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused in T.parallel(2880):\n            for j_inner in range(2):\n                cse_var_4: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 4\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 960 // 480\n                cse_var_2: T.int32 = cse_var_3 * 4 + cse_var_4\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused // 960 * 1920 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 240 // 80 * 640 + cse_var_3 * 320 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 80 // 4 * 16 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 480 // 240 * 8 + j_inner * 4 + cse_var_4\n                ScaleShift_1 = T.Buffer((5760,), data=ScaleShift.data)\n                data_1 = T.Buffer((5760,), data=data.data)\n                Scale_1 = T.Buffer((8,), data=Scale.data)\n                Shift_1 = T.Buffer((8,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]",
        "data": "9_2_20_4_4",
        "Scale": "2_4",
        "Shift": "2_4"
    },
    {
        "op_name": "softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1440; ++i0_i1_fused_i2_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_exp[40];\n    float T_softmax_expsum[1];\n    T_softmax_maxelem[0] = -3.402823e+38f;\n    for (int32_t k = 0; k < 40; ++k) {\n      float v_ = T_softmax_maxelem[0];\n      float v__1 = ((float*)data_1)[((i0_i1_fused_i2_fused * 40) + k)];\n      T_softmax_maxelem[0] = ((v_) > (v__1) ? (v_) : (v__1));\n    },\n    for (int32_t i3 = 0; i3 < 40; ++i3) {\n      T_softmax_exp[i3] = expf((((float*)data_1)[((i0_i1_fused_i2_fused * 40) + i3)] - T_softmax_maxelem[0]));\n    },\n    for (int32_t i3_1 = 0; i3_1 < 40; ++i3_1) {\n      T_softmax_expsum[0] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 40; ++k_1) {\n        T_softmax_expsum[0] = (T_softmax_expsum[0] + T_softmax_exp[k_1]);\n      },\n      ((float*)T_softmax_norm_1)[((i0_i1_fused_i2_fused * 40) + i3_1)] = (T_softmax_exp[i3_1] / T_softmax_expsum[0]);\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  __shared__ float T_softmax_expsum[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k_outer = 0; k_outer < 2; ++k_outer) {\n    if (((k_outer * 4) + (((int)threadIdx.x) >> 3)) < 5) {\n      normal_reduce_temp0[0] = (normal_reduce_temp0[0] + __expf((data[(((((int)blockIdx.x) * 40) + (k_outer * 32)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)])));\n    },\n  },\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    T_softmax_expsum[0] = red_buf0[0];\n  },\n  __syncthreads();\n  for (int i3_outer = 0; i3_outer < 2; ++i3_outer) {\n    if (((i3_outer * 4) + (((int)threadIdx.x) >> 3)) < 5) {\n      T_softmax_norm[(((((int)blockIdx.x) * 40) + (i3_outer * 32)) + ((int)threadIdx.x))] = (__expf((data[(((((int)blockIdx.x) * 40) + (i3_outer * 32)) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)])) / T_softmax_expsum[0]);\n    },\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 40; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 40)) + k)]);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 24, 12, 40), \"float32\"), T_softmax_norm: T.Buffer((5, 24, 12, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(1440):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_exp = T.allocate([40], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n            T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n            data_1 = T.Buffer((57600,), data=data.data)\n            for k in range(40):\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused_i2_fused * 40 + k])\n            T_softmax_exp_1 = T.Buffer((40,), data=T_softmax_exp)\n            for i3 in range(40):\n                T_softmax_exp_1[i3] = T.exp(data_1[i0_i1_fused_i2_fused * 40 + i3] - T_softmax_maxelem_1[0])\n            for i3 in range(40):\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(40):\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T_softmax_exp_1[k]\n                T_softmax_norm_1 = T.Buffer((57600,), data=T_softmax_norm.data)\n                T_softmax_norm_1[i0_i1_fused_i2_fused * 40 + i3] = T_softmax_exp_1[i3] / T_softmax_expsum_1[0]",
        "data": "5_24_12_40"
    },
    {
        "op_name": "softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 8; ++i0_i1_fused) {\n    float T_softmax_maxelem[36];\n    void* T_softmax_exp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)4032, 2, 32);\n    if (T_softmax_exp == NULL) {\n      return -1;\n    },\n    for (int32_t i2 = 0; i2 < 36; ++i2) {\n      T_softmax_maxelem[i2] = -3.402823e+38f;\n      for (int32_t k = 0; k < 28; ++k) {\n        float v_ = T_softmax_maxelem[i2];\n        float v__1 = ((float*)data_1)[(((i0_i1_fused * 1008) + (i2 * 28)) + k)];\n        T_softmax_maxelem[i2] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n    for (int32_t i2_1 = 0; i2_1 < 36; ++i2_1) {\n      for (int32_t i3 = 0; i3 < 28; ++i3) {\n        int32_t cse_var_1 = (i2_1 * 28);\n        ((float*)T_softmax_exp)[(cse_var_1 + i3)] = expf((((float*)data_1)[(((i0_i1_fused * 1008) + cse_var_1) + i3)] - T_softmax_maxelem[i2_1]));\n      },\n    },\n    for (int32_t i2_2 = 0; i2_2 < 36; ++i2_2) {\n      T_softmax_maxelem[i2_2] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 28; ++k_1) {\n        T_softmax_maxelem[i2_2] = (T_softmax_maxelem[i2_2] + ((float*)T_softmax_exp)[((i2_2 * 28) + k_1)]);\n      },\n    },\n    for (int32_t i2_3 = 0; i2_3 < 36; ++i2_3) {\n      for (int32_t i3_1 = 0; i3_1 < 28; ++i3_1) {\n        int32_t cse_var_2 = (i2_3 * 28);\n        ((float*)T_softmax_norm_1)[(((i0_i1_fused * 1008) + cse_var_2) + i3_1)] = (((float*)T_softmax_exp)[(cse_var_2 + i3_1)] / T_softmax_maxelem[i2_3]);\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_exp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 28; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  T_softmax_norm[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (__expf((data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 2)) / 7)])) / T_softmax_expsum[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 2)) / 7)]);\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 28; ++k) {\n    T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 8, 36, 28), \"float32\"), T_softmax_norm: T.Buffer((1, 8, 36, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(8):\n            T_softmax_maxelem = T.allocate([36], \"float32\", \"global\")\n            T_softmax_exp = T.allocate([1008], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((36,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((8064,), data=data.data)\n            for i2 in range(36):\n                T_softmax_maxelem_1[i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(28):\n                    T_softmax_maxelem_1[i2] = T.max(T_softmax_maxelem_1[i2], data_1[i0_i1_fused * 1008 + i2 * 28 + k])\n            T_softmax_exp_1 = T.Buffer((1008,), data=T_softmax_exp)\n            for i2, i3 in T.grid(36, 28):\n                cse_var_1: T.int32 = i2 * 28\n                T_softmax_exp_1[cse_var_1 + i3] = T.exp(data_1[i0_i1_fused * 1008 + cse_var_1 + i3] - T_softmax_maxelem_1[i2])\n            T_softmax_maxelem_2 = T.Buffer((36,), data=T_softmax_maxelem)\n            for i2 in range(36):\n                T_softmax_maxelem_2[i2] = T.float32(0)\n                for k in range(28):\n                    T_softmax_maxelem_2[i2] = T_softmax_maxelem_2[i2] + T_softmax_exp_1[i2 * 28 + k]\n            for i2, i3 in T.grid(36, 28):\n                cse_var_2: T.int32 = i2 * 28\n                T_softmax_norm_1 = T.Buffer((8064,), data=T_softmax_norm.data)\n                T_softmax_norm_1[i0_i1_fused * 1008 + cse_var_2 + i3] = T_softmax_exp_1[cse_var_2 + i3] / T_softmax_maxelem_2[i2]",
        "data": "1_8_36_28"
    },
    {
        "op_name": "softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  void* T_softmax_exp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)345600, 2, 32);\n  if (T_softmax_exp == NULL) {\n    return -1;\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 7200; ++i0_i1_fused_i2_fused) {\n    float T_softmax_maxelem[1];\n    for (int32_t i3 = 0; i3 < 12; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 12) + i3);\n      T_softmax_maxelem[0] = -3.402823e+38f;\n      for (int32_t k = 0; k < 12; ++k) {\n        float v_ = T_softmax_maxelem[0];\n        float v__1 = ((float*)data_1)[((i0_i1_fused_i2_fused * 12) + k)];\n        T_softmax_maxelem[0] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n      ((float*)T_softmax_exp)[cse_var_1] = expf((((float*)data_1)[cse_var_1] - T_softmax_maxelem[0]));\n    },\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 200; ++i0_i1_fused) {\n    float T_softmax_expsum[36];\n    for (int32_t i2 = 0; i2 < 36; ++i2) {\n      T_softmax_expsum[i2] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 12; ++k_1) {\n        T_softmax_expsum[i2] = (T_softmax_expsum[i2] + ((float*)T_softmax_exp)[(((i0_i1_fused * 432) + (i2 * 12)) + k_1)]);\n      },\n    },\n    for (int32_t i2_1 = 0; i2_1 < 36; ++i2_1) {\n      int32_t cse_var_2 = ((i0_i1_fused * 432) + (i2_1 * 12));\n      int32_t12 v__2 = int32_t12((cse_var_2)+(1*0), (cse_var_2)+(1*1), (cse_var_2)+(1*2), (cse_var_2)+(1*3), (cse_var_2)+(1*4), (cse_var_2)+(1*5), (cse_var_2)+(1*6), (cse_var_2)+(1*7), (cse_var_2)+(1*8), (cse_var_2)+(1*9), (cse_var_2)+(1*10), (cse_var_2)+(1*11));\n      *(float12*)(((float*)T_softmax_norm_1) + cse_var_2) = ((float12(((float*)T_softmax_exp)[v__2.s0],((float*)T_softmax_exp)[v__2.s1],((float*)T_softmax_exp)[v__2.s2],((float*)T_softmax_exp)[v__2.s3],((float*)T_softmax_exp)[v__2.s4],((float*)T_softmax_exp)[v__2.s5],((float*)T_softmax_exp)[v__2.s6],((float*)T_softmax_exp)[v__2.s7],((float*)T_softmax_exp)[v__2.s8],((float*)T_softmax_exp)[v__2.s9],((float*)T_softmax_exp)[v__2.sa],((float*)T_softmax_exp)[v__2.sb])) / ((float12)(T_softmax_expsum[i2_1], T_softmax_expsum[i2_1], T_softmax_expsum[i2_1], T_softmax_expsum[i2_1], T_softmax_expsum[i2_1], T_softmax_expsum[i2_1], T_softmax_expsum[i2_1], T_softmax_expsum[i2_1], T_softmax_expsum[i2_1], T_softmax_expsum[i2_1], T_softmax_expsum[i2_1], T_softmax_expsum[i2_1])));\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_exp) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 12; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 12; ++k) {\n    T_softmax_expsum[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  T_softmax_norm[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__expf((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 3)])) / T_softmax_expsum[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 3)]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 40, 36, 12), \"float32\"), T_softmax_norm: T.Buffer((5, 40, 36, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        T_softmax_exp = T.allocate([86400], \"float32\", \"global\")\n        T_softmax_exp_1 = T.Buffer((86400,), data=T_softmax_exp)\n        for i0_i1_fused_i2_fused in T.parallel(7200):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            for i3 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 12 + i3\n                T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n                T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((86400,), data=data.data)\n                for k in range(12):\n                    T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused_i2_fused * 12 + k])\n                T_softmax_exp_1[cse_var_1] = T.exp(data_1[cse_var_1] - T_softmax_maxelem_1[0])\n        for i0_i1_fused in T.parallel(200):\n            T_softmax_expsum = T.allocate([36], \"float32\", \"global\")\n            T_softmax_expsum_1 = T.Buffer((36,), data=T_softmax_expsum)\n            for i2 in range(36):\n                T_softmax_expsum_1[i2] = T.float32(0)\n                for k in range(12):\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[i0_i1_fused * 432 + i2 * 12 + k]\n            for i2 in range(36):\n                cse_var_2: T.int32 = i0_i1_fused * 432 + i2 * 12\n                T_softmax_norm_1 = T.Buffer((86400,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_2:cse_var_2 + 12] = T_softmax_exp_1[cse_var_2:cse_var_2 + 12] / T.Broadcast(T_softmax_expsum_1[i2], 12)",
        "data": "5_40_36_12"
    },
    {
        "op_name": "softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  void* T_softmax_exp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)516096, 2, 32);\n  if (T_softmax_exp == NULL) {\n    return -1;\n  },\n  void* T_softmax_expsum = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)16128, 2, 32);\n  if (T_softmax_expsum == NULL) {\n    return -1;\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 168; ++i0_i1_fused) {\n    float T_softmax_maxelem[24];\n    for (int32_t i2 = 0; i2 < 24; ++i2) {\n      T_softmax_maxelem[i2] = -3.402823e+38f;\n      for (int32_t k = 0; k < 32; ++k) {\n        float v_ = T_softmax_maxelem[i2];\n        float v__1 = ((float*)data_1)[(((i0_i1_fused * 768) + (i2 * 32)) + k)];\n        T_softmax_maxelem[i2] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n    for (int32_t i2_1 = 0; i2_1 < 24; ++i2_1) {\n      for (int32_t i3 = 0; i3 < 32; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 768) + (i2_1 * 32)) + i3);\n        ((float*)T_softmax_exp)[cse_var_1] = expf((((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1]));\n      },\n    },\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4032; ++i0_i1_fused_i2_fused) {\n    ((float*)T_softmax_expsum)[i0_i1_fused_i2_fused] = 0.000000e+00f;\n    for (int32_t k_1 = 0; k_1 < 32; ++k_1) {\n      ((float*)T_softmax_expsum)[i0_i1_fused_i2_fused] = (((float*)T_softmax_expsum)[i0_i1_fused_i2_fused] + ((float*)T_softmax_exp)[((i0_i1_fused_i2_fused * 32) + k_1)]);\n    },\n  },\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4032; ++i0_i1_fused_i2_fused_1) {\n    for (int32_t i3_1 = 0; i3_1 < 32; ++i3_1) {\n      int32_t cse_var_2 = ((i0_i1_fused_i2_fused_1 * 32) + i3_1);\n      ((float*)T_softmax_norm_1)[cse_var_2] = (((float*)T_softmax_exp)[cse_var_2] / ((float*)T_softmax_expsum)[i0_i1_fused_i2_fused_1]);\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_expsum) != 0) {\n    return -1;\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_exp) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  T_softmax_norm[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__expf((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))])) / T_softmax_expsum[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]);\n},\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 32; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 32)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 32; ++k) {\n    T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 2048) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 24, 24, 32), \"float32\"), T_softmax_norm: T.Buffer((7, 24, 24, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        T_softmax_exp = T.allocate([129024], \"float32\", \"global\")\n        T_softmax_expsum = T.allocate([4032], \"float32\", \"global\")\n        T_softmax_exp_1 = T.Buffer((129024,), data=T_softmax_exp)\n        for i0_i1_fused in T.parallel(168):\n            T_softmax_maxelem = T.allocate([24], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((24,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((129024,), data=data.data)\n            for i2 in range(24):\n                T_softmax_maxelem_1[i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(32):\n                    T_softmax_maxelem_1[i2] = T.max(T_softmax_maxelem_1[i2], data_1[i0_i1_fused * 768 + i2 * 32 + k])\n            for i2, i3 in T.grid(24, 32):\n                cse_var_1: T.int32 = i0_i1_fused * 768 + i2 * 32 + i3\n                T_softmax_exp_1[cse_var_1] = T.exp(data_1[cse_var_1] - T_softmax_maxelem_1[i2])\n        T_softmax_expsum_1 = T.Buffer((4032,), data=T_softmax_expsum)\n        for i0_i1_fused_i2_fused in T.parallel(4032):\n            T_softmax_expsum_1[i0_i1_fused_i2_fused] = T.float32(0)\n            for k in range(32):\n                T_softmax_expsum_1[i0_i1_fused_i2_fused] = T_softmax_expsum_1[i0_i1_fused_i2_fused] + T_softmax_exp_1[i0_i1_fused_i2_fused * 32 + k]\n        for i0_i1_fused_i2_fused in T.parallel(4032):\n            for i3 in range(32):\n                cse_var_2: T.int32 = i0_i1_fused_i2_fused * 32 + i3\n                T_softmax_norm_1 = T.Buffer((129024,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_2] = T_softmax_exp_1[cse_var_2] / T_softmax_expsum_1[i0_i1_fused_i2_fused]",
        "data": "7_24_24_32"
    },
    {
        "op_name": "softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 360; ++i0_i1_fused) {\n    float T_softmax_maxelem[24];\n    void* T_softmax_exp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)2304, 2, 32);\n    if (T_softmax_exp == NULL) {\n      return -1;\n    },\n    float T_softmax_expsum[1];\n    for (int32_t i2 = 0; i2 < 24; ++i2) {\n      T_softmax_maxelem[i2] = -3.402823e+38f;\n      for (int32_t k = 0; k < 24; ++k) {\n        float v_ = T_softmax_maxelem[i2];\n        float v__1 = ((float*)data_1)[(((i0_i1_fused * 576) + (i2 * 24)) + k)];\n        T_softmax_maxelem[i2] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n    for (int32_t i2_1 = 0; i2_1 < 24; ++i2_1) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_1 = (i2_1 * 24);\n        ((float*)T_softmax_exp)[(cse_var_1 + i3)] = expf((((float*)data_1)[(((i0_i1_fused * 576) + cse_var_1) + i3)] - T_softmax_maxelem[i2_1]));\n      },\n    },\n    for (int32_t i2_2 = 0; i2_2 < 24; ++i2_2) {\n      for (int32_t i3_1 = 0; i3_1 < 24; ++i3_1) {\n        int32_t cse_var_2 = (i2_2 * 24);\n        T_softmax_expsum[0] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 24; ++k_1) {\n          T_softmax_expsum[0] = (T_softmax_expsum[0] + ((float*)T_softmax_exp)[(cse_var_2 + k_1)]);\n        },\n        ((float*)T_softmax_norm_1)[(((i0_i1_fused * 576) + cse_var_2) + i3_1)] = (((float*)T_softmax_exp)[(cse_var_2 + i3_1)] / T_softmax_expsum[0]);\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_exp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(30) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 24; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(30) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  T_softmax_norm[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = (__expf((data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 6)) >> 2)])) / T_softmax_expsum[(((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 6)) >> 2)]);\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 24; ++k) {\n    T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 40, 24, 24), \"float32\"), T_softmax_norm: T.Buffer((9, 40, 24, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(360):\n            T_softmax_maxelem = T.allocate([24], \"float32\", \"global\")\n            T_softmax_exp = T.allocate([576], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((24,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((207360,), data=data.data)\n            for i2 in range(24):\n                T_softmax_maxelem_1[i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(24):\n                    T_softmax_maxelem_1[i2] = T.max(T_softmax_maxelem_1[i2], data_1[i0_i1_fused * 576 + i2 * 24 + k])\n            T_softmax_exp_1 = T.Buffer((576,), data=T_softmax_exp)\n            for i2, i3 in T.grid(24, 24):\n                cse_var_1: T.int32 = i2 * 24\n                T_softmax_exp_1[cse_var_1 + i3] = T.exp(data_1[i0_i1_fused * 576 + cse_var_1 + i3] - T_softmax_maxelem_1[i2])\n            for i2, i3 in T.grid(24, 24):\n                cse_var_2: T.int32 = i2 * 24\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(24):\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T_softmax_exp_1[cse_var_2 + k]\n                T_softmax_norm_1 = T.Buffer((207360,), data=T_softmax_norm.data)\n                T_softmax_norm_1[i0_i1_fused * 576 + cse_var_2 + i3] = T_softmax_exp_1[cse_var_2 + i3] / T_softmax_expsum_1[0]",
        "data": "9_40_24_24"
    },
    {
        "op_name": "softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1920; ++i0_i1_fused_i2_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_exp[20];\n    float T_softmax_expsum[1];\n    T_softmax_maxelem[0] = -3.402823e+38f;\n    for (int32_t k = 0; k < 20; ++k) {\n      float v_ = T_softmax_maxelem[0];\n      float v__1 = ((float*)data_1)[((i0_i1_fused_i2_fused * 20) + k)];\n      T_softmax_maxelem[0] = ((v_) > (v__1) ? (v_) : (v__1));\n    },\n    for (int32_t i3 = 0; i3 < 20; ++i3) {\n      T_softmax_exp[i3] = expf((((float*)data_1)[((i0_i1_fused_i2_fused * 20) + i3)] - T_softmax_maxelem[0]));\n    },\n    T_softmax_expsum[0] = 0.000000e+00f;\n    for (int32_t k_1 = 0; k_1 < 20; ++k_1) {\n      T_softmax_expsum[0] = (T_softmax_expsum[0] + T_softmax_exp[k_1]);\n    },\n    for (int32_t i3_1 = 0; i3_1 < 20; ++i3_1) {\n      ((float*)T_softmax_norm_1)[((i0_i1_fused_i2_fused * 20) + i3_1)] = (T_softmax_exp[i3_1] / T_softmax_expsum[0]);\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 20; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 80) + (((int)threadIdx.x) * 20)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 20; ++k) {\n    T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__expf((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)])) / T_softmax_expsum[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 12, 32, 20), \"float32\"), T_softmax_norm: T.Buffer((5, 12, 32, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(1920):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_exp = T.allocate([20], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n            T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n            data_1 = T.Buffer((38400,), data=data.data)\n            for k in range(20):\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused_i2_fused * 20 + k])\n            T_softmax_exp_1 = T.Buffer((20,), data=T_softmax_exp)\n            for i3 in range(20):\n                T_softmax_exp_1[i3] = T.exp(data_1[i0_i1_fused_i2_fused * 20 + i3] - T_softmax_maxelem_1[0])\n            T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n            T_softmax_expsum_1[0] = T.float32(0)\n            for k in range(20):\n                T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T_softmax_exp_1[k]\n            for i3 in range(20):\n                T_softmax_norm_1 = T.Buffer((38400,), data=T_softmax_norm.data)\n                T_softmax_norm_1[i0_i1_fused_i2_fused * 20 + i3] = T_softmax_exp_1[i3] / T_softmax_expsum_1[0]",
        "data": "5_12_32_20"
    },
    {
        "op_name": "softmax",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 6048; ++i0_i1_fused_i2_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_exp[20];\n    float T_softmax_expsum[1];\n    T_softmax_maxelem[0] = -3.402823e+38f;\n    for (int32_t k = 0; k < 20; ++k) {\n      float v_ = T_softmax_maxelem[0];\n      float v__1 = ((float*)data_1)[((i0_i1_fused_i2_fused * 20) + k)];\n      T_softmax_maxelem[0] = ((v_) > (v__1) ? (v_) : (v__1));\n    },\n    for (int32_t i3 = 0; i3 < 20; ++i3) {\n      T_softmax_exp[i3] = expf((((float*)data_1)[((i0_i1_fused_i2_fused * 20) + i3)] - T_softmax_maxelem[0]));\n    },\n    T_softmax_expsum[0] = 0.000000e+00f;\n    for (int32_t k_1 = 0; k_1 < 20; ++k_1) {\n      T_softmax_expsum[0] = (T_softmax_expsum[0] + T_softmax_exp[k_1]);\n    },\n    for (int32_t i3_1 = 0; i3_1 < 20; ++i3_1) {\n      ((float*)T_softmax_norm_1)[((i0_i1_fused_i2_fused * 20) + i3_1)] = (T_softmax_exp[i3_1] / T_softmax_expsum[0]);\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 20; ++k) {\n    T_softmax_expsum[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 720) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 20; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 20)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__expf((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)])) / T_softmax_expsum[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 36, 28, 20), \"float32\"), T_softmax_norm: T.Buffer((6, 36, 28, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(6048):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_exp = T.allocate([20], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n            T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n            data_1 = T.Buffer((120960,), data=data.data)\n            for k in range(20):\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused_i2_fused * 20 + k])\n            T_softmax_exp_1 = T.Buffer((20,), data=T_softmax_exp)\n            for i3 in range(20):\n                T_softmax_exp_1[i3] = T.exp(data_1[i0_i1_fused_i2_fused * 20 + i3] - T_softmax_maxelem_1[0])\n            T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n            T_softmax_expsum_1[0] = T.float32(0)\n            for k in range(20):\n                T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T_softmax_exp_1[k]\n            for i3 in range(20):\n                T_softmax_norm_1 = T.Buffer((120960,), data=T_softmax_norm.data)\n                T_softmax_norm_1[i0_i1_fused_i2_fused * 20 + i3] = T_softmax_exp_1[i3] / T_softmax_expsum_1[0]",
        "data": "6_36_28_20"
    },
    {
        "op_name": "softmax_common",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 160; ++i0_i1_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      T_softmax_maxelem[0] = -3.402823e+38f;\n      for (int32_t k = 0; k < 8; ++k) {\n        float v_ = T_softmax_maxelem[0];\n        float v__1 = ((float*)data_1)[(((i0_i1_fused * 64) + (i2 * 8)) + k)];\n        T_softmax_maxelem[0] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n      T_softmax_expsum[0] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 8; ++k_1) {\n        int32_t cse_var_1 = (((i0_i1_fused * 64) + (i2 * 8)) + k_1);\n          float v__2 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n          float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n          int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__5 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n        float v__7 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n        float v__9 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n        float v__11 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n        float v__13 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n        float v__15 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n        float v__17 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n        float v__19 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n        float v__21 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n        float v__23 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n        float v__25 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n        float v__27 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n        float v__29 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n        float v__31 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n        float v__33 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n        float v__35 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n        float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__38 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        T_softmax_expsum[0] = (T_softmax_expsum[0] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n      },\n      for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n        int32_t cse_var_2 = (((i0_i1_fused * 64) + (i2 * 8)) + i3_s);\n          float v__39 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n          float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n          int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__42 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n        float v__44 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n        float v__46 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n        float v__48 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n        float v__50 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n        float v__52 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n        float v__54 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n        float v__56 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n        float v__58 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n        float v__60 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n        float v__62 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n        float v__64 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n        float v__66 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n        float v__68 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n        float v__70 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n        float v__72 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n        float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__75 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        ((float*)T_softmax_norm_1)[cse_var_2] = (((v__74) > (v__75) ? (v__74) : (v__75)) / T_softmax_expsum[0]);\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 8; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 8)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 8; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) * 8)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))])) / T_softmax_expsum[((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 16, 8, 8), \"float32\"), T_softmax_norm: T.Buffer((10, 16, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(160):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            for i2 in range(8):\n                T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n                T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((10240,), data=data.data)\n                for k in range(8):\n                    T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused * 64 + i2 * 8 + k])\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(8):\n                    cse_var_1: T.int32 = i0_i1_fused * 64 + i2 * 8 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0])\n                for i3_s in range(8):\n                    cse_var_2: T.int32 = i0_i1_fused * 64 + i2 * 8 + i3_s\n                    T_softmax_norm_1 = T.Buffer((10240,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]",
        "data": "10_16_8_8"
    },
    {
        "op_name": "softmax_common",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 72; ++i0_i1_fused) {\n    float T_softmax_maxelem[24];\n    float T_softmax_expsum[1];\n    for (int32_t i2 = 0; i2 < 24; ++i2) {\n      T_softmax_maxelem[i2] = -3.402823e+38f;\n      for (int32_t k = 0; k < 4; ++k) {\n        float v_ = T_softmax_maxelem[i2];\n        float v__1 = ((float*)data_1)[(((i0_i1_fused * 96) + (i2 * 4)) + k)];\n        T_softmax_maxelem[i2] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n    for (int32_t i2_1 = 0; i2_1 < 24; ++i2_1) {\n      for (int32_t i3 = 0; i3 < 4; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 96) + (i2_1 * 4)) + i3);\n        T_softmax_expsum[0] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 4; ++k_1) {\n          int32_t cse_var_2 = (((i0_i1_fused * 96) + (i2_1 * 4)) + k_1);\n            float v__2 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n            float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n            int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          float v__5 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n          float v__7 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n          float v__9 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n          float v__11 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n          float v__13 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n          float v__15 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n          float v__17 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n          float v__19 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n          float v__21 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n          float v__23 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n          float v__25 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n          float v__27 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n          float v__29 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n          float v__31 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n          float v__33 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n          float v__35 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n          float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n          float v__38 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          T_softmax_expsum[0] = (T_softmax_expsum[0] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n        },\n          float v__39 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n          float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n          int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__42 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n        float v__44 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n        float v__46 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n        float v__48 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n        float v__50 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n        float v__52 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n        float v__54 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n        float v__56 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n        float v__58 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n        float v__60 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n        float v__62 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n        float v__64 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n        float v__66 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n        float v__68 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n        float v__70 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n        float v__72 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n        float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__75 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        ((float*)T_softmax_norm_1)[cse_var_1] = (((v__74) > (v__75) ? (v__74) : (v__75)) / T_softmax_expsum[0]);\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 4; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 4; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))])) / T_softmax_expsum[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 24, 24, 4), \"float32\"), T_softmax_norm: T.Buffer((3, 24, 24, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(72):\n            T_softmax_maxelem = T.allocate([24], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((24,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((6912,), data=data.data)\n            for i2 in range(24):\n                T_softmax_maxelem_1[i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(4):\n                    T_softmax_maxelem_1[i2] = T.max(T_softmax_maxelem_1[i2], data_1[i0_i1_fused * 96 + i2 * 4 + k])\n            for i2, i3 in T.grid(24, 4):\n                cse_var_1: T.int32 = i0_i1_fused * 96 + i2 * 4 + i3\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(4):\n                    cse_var_2: T.int32 = i0_i1_fused * 96 + i2 * 4 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[i2])\n                T_softmax_norm_1 = T.Buffer((6912,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[i2]) / T_softmax_expsum_1[0]",
        "data": "3_24_24_4"
    },
    {
        "op_name": "softmax_common",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 160; ++i0_i1_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      T_softmax_maxelem[0] = -3.402823e+38f;\n      for (int32_t k = 0; k < 32; ++k) {\n        float v_ = T_softmax_maxelem[0];\n        float v__1 = ((float*)data_1)[(((i0_i1_fused * 384) + (i2 * 32)) + k)];\n        T_softmax_maxelem[0] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n      T_softmax_expsum[0] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 32; ++k_1) {\n        int32_t cse_var_1 = (((i0_i1_fused * 384) + (i2 * 32)) + k_1);\n          float v__2 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n          float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n          int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__5 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n        float v__7 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n        float v__9 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n        float v__11 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n        float v__13 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n        float v__15 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n        float v__17 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n        float v__19 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n        float v__21 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n        float v__23 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n        float v__25 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n        float v__27 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n        float v__29 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n        float v__31 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n        float v__33 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n        float v__35 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n        float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__38 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        T_softmax_expsum[0] = (T_softmax_expsum[0] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n      },\n      for (int32_t i3 = 0; i3 < 32; ++i3) {\n        int32_t cse_var_2 = (((i0_i1_fused * 384) + (i2 * 32)) + i3);\n          float v__39 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n          float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n          int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__42 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n        float v__44 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n        float v__46 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n        float v__48 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n        float v__50 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n        float v__52 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n        float v__54 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n        float v__56 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n        float v__58 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n        float v__60 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n        float v__62 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n        float v__64 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n        float v__66 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n        float v__68 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n        float v__70 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n        float v__72 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n        float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__75 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        ((float*)T_softmax_norm_1)[cse_var_2] = (((v__74) > (v__75) ? (v__74) : (v__75)) / T_softmax_expsum[0]);\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 32; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) * 32)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 32; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)])) / T_softmax_expsum[((int)blockIdx.x)]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 16, 12, 32), \"float32\"), T_softmax_norm: T.Buffer((10, 16, 12, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(160):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            for i2 in range(12):\n                T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n                T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((61440,), data=data.data)\n                for k in range(32):\n                    T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused * 384 + i2 * 32 + k])\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(32):\n                    cse_var_1: T.int32 = i0_i1_fused * 384 + i2 * 32 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0])\n                for i3 in range(32):\n                    cse_var_2: T.int32 = i0_i1_fused * 384 + i2 * 32 + i3\n                    T_softmax_norm_1 = T.Buffer((61440,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]",
        "data": "10_16_12_32"
    },
    {
        "op_name": "softmax_common",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  void* T_softmax_maxelem = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)16000, 2, 32);\n  if (T_softmax_maxelem == NULL) {\n    return -1;\n  },\n  void* T_softmax_expsum = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)16000, 2, 32);\n  if (T_softmax_expsum == NULL) {\n    return -1;\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4000; ++i0_i1_fused_i2_fused) {\n    ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused] = -3.402823e+38f;\n    for (int32_t k = 0; k < 32; ++k) {\n      float v_ = ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused];\n      float v__1 = ((float*)data_1)[((i0_i1_fused_i2_fused * 32) + k)];\n      ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n    },\n  },\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4000; ++i0_i1_fused_i2_fused_1) {\n    ((float*)T_softmax_expsum)[i0_i1_fused_i2_fused_1] = 0.000000e+00f;\n    for (int32_t k_1 = 0; k_1 < 32; ++k_1) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused_1 * 32) + k_1);\n        float v__2 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n        float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n        int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__5 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n      float v__7 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n      float v__9 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n      float v__11 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n      float v__13 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n      float v__15 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n      float v__17 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n      float v__19 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n      float v__21 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n      float v__23 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n      float v__25 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n      float v__27 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n      float v__29 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n      float v__31 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n      float v__33 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n      float v__35 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n      float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__38 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      ((float*)T_softmax_expsum)[i0_i1_fused_i2_fused_1] = (((float*)T_softmax_expsum)[i0_i1_fused_i2_fused_1] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n    },\n  },\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 4000; ++i0_i1_fused_i2_fused_2) {\n    for (int32_t i3 = 0; i3 < 32; ++i3) {\n      int32_t cse_var_2 = ((i0_i1_fused_i2_fused_2 * 32) + i3);\n        float v__39 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n        float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n        int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__42 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n      float v__44 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n      float v__46 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n      float v__48 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n      float v__50 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n      float v__52 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n      float v__54 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n      float v__56 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n      float v__58 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n      float v__60 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n      float v__62 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n      float v__64 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n      float v__66 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n      float v__68 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n      float v__70 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n      float v__72 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n      float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__75 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_2];\n      ((float*)T_softmax_norm_1)[cse_var_2] = (((v__74) > (v__75) ? (v__74) : (v__75)) / ((float*)T_softmax_expsum)[i0_i1_fused_i2_fused_2]);\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_expsum) != 0) {\n    return -1;\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_maxelem) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 32; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 32)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))])) / T_softmax_expsum[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5))]);\n},\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 32; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) * 32)) + k)]);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 20, 20, 32), \"float32\"), T_softmax_norm: T.Buffer((10, 20, 20, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        T_softmax_maxelem = T.allocate([4000], \"float32\", \"global\")\n        T_softmax_expsum = T.allocate([4000], \"float32\", \"global\")\n        T_softmax_maxelem_1 = T.Buffer((4000,), data=T_softmax_maxelem)\n        data_1 = T.Buffer((128000,), data=data.data)\n        for i0_i1_fused_i2_fused in T.parallel(4000):\n            T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.float32(-3.4028234663852886e+38)\n            for k in range(32):\n                T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.max(T_softmax_maxelem_1[i0_i1_fused_i2_fused], data_1[i0_i1_fused_i2_fused * 32 + k])\n        T_softmax_expsum_1 = T.Buffer((4000,), data=T_softmax_expsum)\n        for i0_i1_fused_i2_fused in T.parallel(4000):\n            T_softmax_expsum_1[i0_i1_fused_i2_fused] = T.float32(0)\n            for k in range(32):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 32 + k\n                T_softmax_expsum_1[i0_i1_fused_i2_fused] = T_softmax_expsum_1[i0_i1_fused_i2_fused] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(4000):\n            for i3 in range(32):\n                cse_var_2: T.int32 = i0_i1_fused_i2_fused * 32 + i3\n                T_softmax_norm_1 = T.Buffer((128000,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[i0_i1_fused_i2_fused]) / T_softmax_expsum_1[i0_i1_fused_i2_fused]",
        "data": "10_20_20_32"
    },
    {
        "op_name": "softmax_common",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  void* T_softmax_maxelem = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)9600, 2, 32);\n  if (T_softmax_maxelem == NULL) {\n    return -1;\n  },\n  void* T_softmax_expsum = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)9600, 2, 32);\n  if (T_softmax_expsum == NULL) {\n    return -1;\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2400; ++i0_i1_fused_i2_fused) {\n    ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused] = -3.402823e+38f;\n    for (int32_t k = 0; k < 24; ++k) {\n      float v_ = ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused];\n      float v__1 = ((float*)data_1)[((i0_i1_fused_i2_fused * 24) + k)];\n      ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n    },\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 120; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      ((float*)T_softmax_expsum)[((i0_i1_fused * 20) + i2)] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 24; ++k_1) {\n        int32_t cse_var_2 = ((i0_i1_fused * 20) + i2);\n        int32_t cse_var_1 = (((i0_i1_fused * 480) + (i2 * 24)) + k_1);\n          float v__2 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n          int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__5 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n        float v__7 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n        float v__9 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n        float v__11 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n        float v__13 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n        float v__15 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n        float v__17 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n        float v__19 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n        float v__21 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n        float v__23 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n        float v__25 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n        float v__27 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n        float v__29 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n        float v__31 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n        float v__33 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n        float v__35 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n        float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__38 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        ((float*)T_softmax_expsum)[cse_var_2] = (((float*)T_softmax_expsum)[cse_var_2] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n      },\n    },\n  },\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2400; ++i0_i1_fused_i2_fused_1) {\n    for (int32_t i3 = 0; i3 < 24; ++i3) {\n      int32_t cse_var_3 = ((i0_i1_fused_i2_fused_1 * 24) + i3);\n        float v__39 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n        float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n        int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__42 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n      float v__44 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n      float v__46 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n      float v__48 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n      float v__50 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n      float v__52 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n      float v__54 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n      float v__56 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n      float v__58 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n      float v__60 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n      float v__62 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n      float v__64 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n      float v__66 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n      float v__68 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n      float v__70 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n      float v__72 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n      float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__75 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused_1];\n      ((float*)T_softmax_norm_1)[cse_var_3] = (((v__74) > (v__75) ? (v__74) : (v__75)) / ((float*)T_softmax_expsum)[i0_i1_fused_i2_fused_1]);\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_expsum) != 0) {\n    return -1;\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_maxelem) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 24; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 24; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)])) / T_softmax_expsum[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 40, 20, 24), \"float32\"), T_softmax_norm: T.Buffer((3, 40, 20, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        T_softmax_maxelem = T.allocate([2400], \"float32\", \"global\")\n        T_softmax_expsum = T.allocate([2400], \"float32\", \"global\")\n        T_softmax_maxelem_1 = T.Buffer((2400,), data=T_softmax_maxelem)\n        data_1 = T.Buffer((57600,), data=data.data)\n        for i0_i1_fused_i2_fused in T.parallel(2400):\n            T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.float32(-3.4028234663852886e+38)\n            for k in range(24):\n                T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.max(T_softmax_maxelem_1[i0_i1_fused_i2_fused], data_1[i0_i1_fused_i2_fused * 24 + k])\n        T_softmax_expsum_1 = T.Buffer((2400,), data=T_softmax_expsum)\n        for i0_i1_fused in T.parallel(120):\n            for i2 in range(20):\n                T_softmax_expsum_1[i0_i1_fused * 20 + i2] = T.float32(0)\n                for k in range(24):\n                    cse_var_2: T.int32 = i0_i1_fused * 20 + i2\n                    cse_var_1: T.int32 = i0_i1_fused * 480 + i2 * 24 + k\n                    T_softmax_expsum_1[cse_var_2] = T_softmax_expsum_1[cse_var_2] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(2400):\n            for i3 in range(24):\n                cse_var_3: T.int32 = i0_i1_fused_i2_fused * 24 + i3\n                T_softmax_norm_1 = T.Buffer((57600,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_3] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_3] - T_softmax_maxelem_1[i0_i1_fused_i2_fused]) / T_softmax_expsum_1[i0_i1_fused_i2_fused]",
        "data": "3_40_20_24"
    },
    {
        "op_name": "softmax_common",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4704; ++i0_i1_fused_i2_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    T_softmax_maxelem[0] = -3.402823e+38f;\n    for (int32_t k = 0; k < 28; ++k) {\n      float v_ = T_softmax_maxelem[0];\n      float v__1 = ((float*)data_1)[((i0_i1_fused_i2_fused * 28) + k)];\n      T_softmax_maxelem[0] = ((v_) > (v__1) ? (v_) : (v__1));\n    },\n    T_softmax_expsum[0] = 0.000000e+00f;\n    for (int32_t k_1 = 0; k_1 < 28; ++k_1) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 28) + k_1);\n        float v__2 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n        float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n        int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__5 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n      float v__7 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n      float v__9 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n      float v__11 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n      float v__13 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n      float v__15 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n      float v__17 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n      float v__19 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n      float v__21 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n      float v__23 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n      float v__25 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n      float v__27 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n      float v__29 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n      float v__31 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n      float v__33 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n      float v__35 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n      float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__38 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[0];\n      T_softmax_expsum[0] = (T_softmax_expsum[0] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n    },\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      int32_t cse_var_2 = ((i0_i1_fused_i2_fused * 28) + i3);\n        float v__39 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n        float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n        int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__42 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n      float v__44 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n      float v__46 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n      float v__48 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n      float v__50 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n      float v__52 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n      float v__54 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n      float v__56 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n      float v__58 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n      float v__60 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n      float v__62 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n      float v__64 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n      float v__66 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n      float v__68 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n      float v__70 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n      float v__72 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n      float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__75 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[0];\n      ((float*)T_softmax_norm_1)[cse_var_2] = (((v__74) > (v__75) ? (v__74) : (v__75)) / T_softmax_expsum[0]);\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))])) / T_softmax_expsum[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28))]);\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 28; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)]);\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 28; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 896) + (((int)threadIdx.x) * 28)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 28, 28, 28), \"float32\"), T_softmax_norm: T.Buffer((6, 28, 28, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(4704):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n            T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n            data_1 = T.Buffer((131712,), data=data.data)\n            for k in range(28):\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused_i2_fused * 28 + k])\n            T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n            T_softmax_expsum_1[0] = T.float32(0)\n            for k in range(28):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 28 + k\n                T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0])\n            for i3 in range(28):\n                cse_var_2: T.int32 = i0_i1_fused_i2_fused * 28 + i3\n                T_softmax_norm_1 = T.Buffer((131712,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]",
        "data": "6_28_28_28"
    },
    {
        "op_name": "softmax_common",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  void* T_softmax_maxelem = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)17920, 2, 32);\n  if (T_softmax_maxelem == NULL) {\n    return -1;\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4480; ++i0_i1_fused_i2_fused) {\n    ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused] = -3.402823e+38f;\n    for (int32_t k = 0; k < 24; ++k) {\n      float v_ = ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused];\n      float v__1 = ((float*)data_1)[((i0_i1_fused_i2_fused * 24) + k)];\n      ((float*)T_softmax_maxelem)[i0_i1_fused_i2_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n    },\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 112; ++i0_i1_fused) {\n    float T_softmax_expsum[40];\n    for (int32_t i2 = 0; i2 < 40; ++i2) {\n      T_softmax_expsum[i2] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 24; ++k_1) {\n        int32_t cse_var_2 = ((i0_i1_fused * 40) + i2);\n        int32_t cse_var_1 = (((i0_i1_fused * 960) + (i2 * 24)) + k_1);\n          float v__2 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n          float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n          int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__5 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n        float v__7 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n        float v__9 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n        float v__11 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n        float v__13 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n        float v__15 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n        float v__17 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n        float v__19 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n        float v__21 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n        float v__23 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n        float v__25 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n        float v__27 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n        float v__29 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n        float v__31 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n        float v__33 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n        float v__35 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n        float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__38 = ((float*)data_1)[cse_var_1] - ((float*)T_softmax_maxelem)[cse_var_2];\n        T_softmax_expsum[i2] = (T_softmax_expsum[i2] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n      },\n    },\n    for (int32_t i2_1 = 0; i2_1 < 40; ++i2_1) {\n      for (int32_t i3 = 0; i3 < 24; ++i3) {\n        int32_t cse_var_4 = ((i0_i1_fused * 40) + i2_1);\n        int32_t cse_var_3 = (((i0_i1_fused * 960) + (i2_1 * 24)) + i3);\n          float v__39 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n          float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n          int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__42 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n        float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n        float v__44 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n        float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n        float v__46 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n        float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n        float v__48 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n        float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n        float v__50 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n        float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n        float v__52 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n        float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n        float v__54 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n        float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n        float v__56 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n        float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n        float v__58 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n        float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n        float v__60 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n        float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n        float v__62 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n        float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n        float v__64 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n        float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n        float v__66 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n        float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n        float v__68 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n        float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n        float v__70 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n        float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n        float v__72 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n        float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n        float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__75 = ((float*)data_1)[cse_var_3] - ((float*)T_softmax_maxelem)[cse_var_4];\n        ((float*)T_softmax_norm_1)[cse_var_3] = (((v__74) > (v__75) ? (v__74) : (v__75)) / T_softmax_expsum[i2_1]);\n      },\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_maxelem) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 24; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 24)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)])) / T_softmax_expsum[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 3)]);\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 24; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 24)) + k)]);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 28, 40, 24), \"float32\"), T_softmax_norm: T.Buffer((4, 28, 40, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        T_softmax_maxelem = T.allocate([4480], \"float32\", \"global\")\n        T_softmax_maxelem_1 = T.Buffer((4480,), data=T_softmax_maxelem)\n        data_1 = T.Buffer((107520,), data=data.data)\n        for i0_i1_fused_i2_fused in T.parallel(4480):\n            T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.float32(-3.4028234663852886e+38)\n            for k in range(24):\n                T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.max(T_softmax_maxelem_1[i0_i1_fused_i2_fused], data_1[i0_i1_fused_i2_fused * 24 + k])\n        for i0_i1_fused in T.parallel(112):\n            T_softmax_expsum = T.allocate([40], \"float32\", \"global\")\n            T_softmax_expsum_1 = T.Buffer((40,), data=T_softmax_expsum)\n            for i2 in range(40):\n                T_softmax_expsum_1[i2] = T.float32(0)\n                for k in range(24):\n                    cse_var_2: T.int32 = i0_i1_fused * 40 + i2\n                    cse_var_1: T.int32 = i0_i1_fused * 960 + i2 * 24 + k\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2])\n            for i2, i3 in T.grid(40, 24):\n                cse_var_4: T.int32 = i0_i1_fused * 40 + i2\n                cse_var_3: T.int32 = i0_i1_fused * 960 + i2 * 24 + i3\n                T_softmax_norm_1 = T.Buffer((107520,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_3] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4]) / T_softmax_expsum_1[i2]",
        "data": "4_28_40_24"
    },
    {
        "op_name": "softmax_common",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 4; ++i0_i1_fused) {\n    float T_softmax_maxelem[36];\n    float T_softmax_expsum[36];\n    for (int32_t i2 = 0; i2 < 36; ++i2) {\n      T_softmax_maxelem[i2] = -3.402823e+38f;\n      for (int32_t k = 0; k < 40; ++k) {\n        float v_ = T_softmax_maxelem[i2];\n        float v__1 = ((float*)data_1)[(((i0_i1_fused * 1440) + (i2 * 40)) + k)];\n        T_softmax_maxelem[i2] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n    for (int32_t i2_1 = 0; i2_1 < 36; ++i2_1) {\n      T_softmax_expsum[i2_1] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 40; ++k_1) {\n        int32_t cse_var_1 = (((i0_i1_fused * 1440) + (i2_1 * 40)) + k_1);\n          float v__2 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n          float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n          int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__5 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n        float v__7 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n        float v__9 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n        float v__11 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n        float v__13 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n        float v__15 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n        float v__17 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n        float v__19 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n        float v__21 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n        float v__23 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n        float v__25 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n        float v__27 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n        float v__29 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n        float v__31 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n        float v__33 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n        float v__35 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n        float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__38 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n      },\n    },\n    for (int32_t i2_2 = 0; i2_2 < 36; ++i2_2) {\n      for (int32_t i3 = 0; i3 < 40; ++i3) {\n        int32_t cse_var_2 = (((i0_i1_fused * 1440) + (i2_2 * 40)) + i3);\n          float v__39 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n          float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n          int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__42 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n        float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n        float v__44 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n        float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n        float v__46 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n        float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n        float v__48 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n        float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n        float v__50 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n        float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n        float v__52 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n        float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n        float v__54 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n        float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n        float v__56 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n        float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n        float v__58 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n        float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n        float v__60 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n        float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n        float v__62 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n        float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n        float v__64 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n        float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n        float v__66 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n        float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n        float v__68 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n        float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n        float v__70 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n        float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n        float v__72 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n        float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n        float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__75 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_2];\n        ((float*)T_softmax_norm_1)[cse_var_2] = (((v__74) > (v__75) ? (v__74) : (v__75)) / T_softmax_expsum[i2_2]);\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k_outer = 0; k_outer < 20; ++k_outer) {\n    normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[(((((int)blockIdx.x) * 40) + (k_outer * 2)) + ((int)threadIdx.x))]);\n  },\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    T_softmax_maxelem[((int)blockIdx.x)] = red_buf0[0];\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 40; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 40)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)])) / T_softmax_expsum[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 5)]);\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 4, 36, 40), \"float32\"), T_softmax_norm: T.Buffer((1, 4, 36, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(4):\n            T_softmax_maxelem = T.allocate([36], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([36], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((36,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((5760,), data=data.data)\n            for i2 in range(36):\n                T_softmax_maxelem_1[i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(40):\n                    T_softmax_maxelem_1[i2] = T.max(T_softmax_maxelem_1[i2], data_1[i0_i1_fused * 1440 + i2 * 40 + k])\n            T_softmax_expsum_1 = T.Buffer((36,), data=T_softmax_expsum)\n            for i2 in range(36):\n                T_softmax_expsum_1[i2] = T.float32(0)\n                for k in range(40):\n                    cse_var_1: T.int32 = i0_i1_fused * 1440 + i2 * 40 + k\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[i2])\n            for i2, i3 in T.grid(36, 40):\n                cse_var_2: T.int32 = i0_i1_fused * 1440 + i2 * 40 + i3\n                T_softmax_norm_1 = T.Buffer((5760,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[i2]) / T_softmax_expsum_1[i2]",
        "data": "1_4_36_40"
    },
    {
        "op_name": "softmax_common",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 216; ++i0_i1_fused) {\n    float T_softmax_maxelem[36];\n    float T_softmax_expsum[1];\n    for (int32_t i2 = 0; i2 < 36; ++i2) {\n      T_softmax_maxelem[i2] = -3.402823e+38f;\n      for (int32_t k = 0; k < 16; ++k) {\n        float v_ = T_softmax_maxelem[i2];\n        float v__1 = ((float*)data_1)[(((i0_i1_fused * 576) + (i2 * 16)) + k)];\n        T_softmax_maxelem[i2] = ((v_) > (v__1) ? (v_) : (v__1));\n      },\n    },\n    for (int32_t i2_1 = 0; i2_1 < 36; ++i2_1) {\n      T_softmax_expsum[0] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 16; ++k_1) {\n        int32_t cse_var_1 = (((i0_i1_fused * 576) + (i2_1 * 16)) + k_1);\n          float v__2 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n          float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n          int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__5 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n        float v__7 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n        float v__9 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n        float v__11 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n        float v__13 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n        float v__15 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n        float v__17 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n        float v__19 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n        float v__21 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n        float v__23 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n        float v__25 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n        float v__27 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n        float v__29 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n        float v__31 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n        float v__33 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n        float v__35 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n        float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__38 = ((float*)data_1)[cse_var_1] - T_softmax_maxelem[i2_1];\n        T_softmax_expsum[0] = (T_softmax_expsum[0] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n      },\n      for (int32_t i3_s = 0; i3_s < 16; ++i3_s) {\n        int32_t cse_var_2 = (((i0_i1_fused * 576) + (i2_1 * 16)) + i3_s);\n          float v__39 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n          float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n          int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__42 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n        float v__44 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n        float v__46 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n        float v__48 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n        float v__50 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n        float v__52 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n        float v__54 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n        float v__56 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n        float v__58 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n        float v__60 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n        float v__62 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n        float v__64 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n        float v__66 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n        float v__68 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n        float v__70 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n        float v__72 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n        float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__75 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[i2_1];\n        ((float*)T_softmax_norm_1)[cse_var_2] = (((v__74) > (v__75) ? (v__74) : (v__75)) / T_softmax_expsum[0]);\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 16; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  },\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))])) / T_softmax_expsum[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4))]);\n},\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 16; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 144) + (((int)threadIdx.x) * 16)) + k)]);\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 24, 36, 16), \"float32\"), T_softmax_norm: T.Buffer((9, 24, 36, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(216):\n            T_softmax_maxelem = T.allocate([36], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((36,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((124416,), data=data.data)\n            for i2 in range(36):\n                T_softmax_maxelem_1[i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(16):\n                    T_softmax_maxelem_1[i2] = T.max(T_softmax_maxelem_1[i2], data_1[i0_i1_fused * 576 + i2 * 16 + k])\n            for i2 in range(36):\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(16):\n                    cse_var_1: T.int32 = i0_i1_fused * 576 + i2 * 16 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[i2])\n                for i3_s in range(16):\n                    cse_var_2: T.int32 = i0_i1_fused * 576 + i2 * 16 + i3_s\n                    T_softmax_norm_1 = T.Buffer((124416,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[i2]) / T_softmax_expsum_1[0]",
        "data": "9_24_36_16"
    },
    {
        "op_name": "space_to_depth",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t space_to_depth_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* space_to_depth = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* space_to_depth_1 = (((DLTensor*)space_to_depth)[0].data);\n  void* default_function_space_to_depth_shape = (((DLTensor*)space_to_depth)[0].shape);\n  void* default_function_space_to_depth_strides = (((DLTensor*)space_to_depth)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_space_to_depth_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 1296; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3 = 0; i3 < 28; ++i3) {\n        int32_t cse_var_1 = (i0_i1_fused % 144);\n        ((float*)space_to_depth_1)[(((i0_i1_fused * 336) + (i2 * 28)) + i3)] = ((float*)data_1)[(((((((i0_i1_fused / 144) * 48384) + ((cse_var_1 % 36) * 1344)) + (i2 * 112)) + ((cse_var_1 / 72) * 56)) + (i3 * 2)) + ((cse_var_1 % 72) / 36))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = data[((((((((((int)blockIdx.x) / 864) * 48384) + ((((((int)blockIdx.x) % 864) / 6) % 36) * 1344)) + ((((int)blockIdx.x) % 6) * 224)) + ((((int)threadIdx.x) / 28) * 112)) + ((((((int)blockIdx.x) % 864) / 6) / 72) * 56)) + ((((int)threadIdx.x) % 28) * 2)) + ((((((int)blockIdx.x) % 864) / 6) % 72) / 36))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 36, 24, 56), \"float32\"), space_to_depth: T.Buffer((9, 144, 12, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(1296):\n            for i2, i3 in T.grid(12, 28):\n                cse_var_1: T.int32 = i0_i1_fused % 144\n                space_to_depth_1 = T.Buffer((435456,), data=space_to_depth.data)\n                data_1 = T.Buffer((435456,), data=data.data)\n                space_to_depth_1[i0_i1_fused * 336 + i2 * 28 + i3] = data_1[i0_i1_fused // 144 * 48384 + T.truncmod(cse_var_1, 36) * 1344 + i2 * 112 + T.Div(cse_var_1, 72) * 56 + i3 * 2 + T.Div(T.truncmod(cse_var_1, 72), 36)]",
        "data": "9_36_24_56"
    },
    {
        "op_name": "space_to_depth",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t space_to_depth_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* space_to_depth = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* space_to_depth_1 = (((DLTensor*)space_to_depth)[0].data);\n  void* default_function_space_to_depth_shape = (((DLTensor*)space_to_depth)[0].shape);\n  void* default_function_space_to_depth_strides = (((DLTensor*)space_to_depth)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_space_to_depth_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 256; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      int32_t cse_var_1 = (i0_i1_fused & 31);\n      int32_t8 v_ = int32_t8((((((((i0_i1_fused >> 5) * 5120) + ((cse_var_1 % 8) * 640)) + (i2 * 32)) + ((cse_var_1 / 16) * 16)) + ((cse_var_1 % 16) / 8)))+(2*0), (((((((i0_i1_fused >> 5) * 5120) + ((cse_var_1 % 8) * 640)) + (i2 * 32)) + ((cse_var_1 / 16) * 16)) + ((cse_var_1 % 16) / 8)))+(2*1), (((((((i0_i1_fused >> 5) * 5120) + ((cse_var_1 % 8) * 640)) + (i2 * 32)) + ((cse_var_1 / 16) * 16)) + ((cse_var_1 % 16) / 8)))+(2*2), (((((((i0_i1_fused >> 5) * 5120) + ((cse_var_1 % 8) * 640)) + (i2 * 32)) + ((cse_var_1 / 16) * 16)) + ((cse_var_1 % 16) / 8)))+(2*3), (((((((i0_i1_fused >> 5) * 5120) + ((cse_var_1 % 8) * 640)) + (i2 * 32)) + ((cse_var_1 / 16) * 16)) + ((cse_var_1 % 16) / 8)))+(2*4), (((((((i0_i1_fused >> 5) * 5120) + ((cse_var_1 % 8) * 640)) + (i2 * 32)) + ((cse_var_1 / 16) * 16)) + ((cse_var_1 % 16) / 8)))+(2*5), (((((((i0_i1_fused >> 5) * 5120) + ((cse_var_1 % 8) * 640)) + (i2 * 32)) + ((cse_var_1 / 16) * 16)) + ((cse_var_1 % 16) / 8)))+(2*6), (((((((i0_i1_fused >> 5) * 5120) + ((cse_var_1 % 8) * 640)) + (i2 * 32)) + ((cse_var_1 / 16) * 16)) + ((cse_var_1 % 16) / 8)))+(2*7));\n      *(float8*)(((float*)space_to_depth_1) + ((i0_i1_fused * 160) + (i2 * 8))) = (float8(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7]));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = data[((((((((((int)blockIdx.x) >> 7) * 5120) + ((((((int)blockIdx.x) & 127) >> 2) % 8) * 640)) + ((((int)blockIdx.x) & 3) * 160)) + ((((int)threadIdx.x) >> 3) * 32)) + ((((((int)blockIdx.x) & 127) >> 2) / 16) * 16)) + ((((int)threadIdx.x) & 7) * 2)) + ((((((int)blockIdx.x) & 127) >> 2) % 16) / 8))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 8, 40, 16), \"float32\"), space_to_depth: T.Buffer((8, 32, 20, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(256):\n            for i2 in range(20):\n                cse_var_1: T.int32 = i0_i1_fused % 32\n                space_to_depth_1 = T.Buffer((40960,), data=space_to_depth.data)\n                data_1 = T.Buffer((40960,), data=data.data)\n                space_to_depth_1[i0_i1_fused * 160 + i2 * 8:i0_i1_fused * 160 + i2 * 8 + 8] = data_1[i0_i1_fused // 32 * 5120 + T.truncmod(cse_var_1, 8) * 640 + i2 * 32 + T.Div(cse_var_1, 16) * 16 + T.Div(T.truncmod(cse_var_1, 16), 8):i0_i1_fused // 32 * 5120 + T.truncmod(cse_var_1, 8) * 640 + i2 * 32 + T.Div(cse_var_1, 16) * 16 + T.Div(T.truncmod(cse_var_1, 16), 8) + 16:2]",
        "data": "8_8_40_16"
    },
    {
        "op_name": "space_to_depth",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t space_to_depth_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* space_to_depth = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* space_to_depth_1 = (((DLTensor*)space_to_depth)[0].data);\n  void* default_function_space_to_depth_shape = (((DLTensor*)space_to_depth)[0].shape);\n  void* default_function_space_to_depth_strides = (((DLTensor*)space_to_depth)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_space_to_depth_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 320; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      int32_t cse_var_1 = (i0_i1_fused & 63);\n      int32_t8 v_ = int32_t8((((((((i0_i1_fused >> 6) * 6144) + ((cse_var_1 % 16) * 384)) + (i2 * 32)) + ((cse_var_1 / 32) * 16)) + ((cse_var_1 % 32) / 16)))+(2*0), (((((((i0_i1_fused >> 6) * 6144) + ((cse_var_1 % 16) * 384)) + (i2 * 32)) + ((cse_var_1 / 32) * 16)) + ((cse_var_1 % 32) / 16)))+(2*1), (((((((i0_i1_fused >> 6) * 6144) + ((cse_var_1 % 16) * 384)) + (i2 * 32)) + ((cse_var_1 / 32) * 16)) + ((cse_var_1 % 32) / 16)))+(2*2), (((((((i0_i1_fused >> 6) * 6144) + ((cse_var_1 % 16) * 384)) + (i2 * 32)) + ((cse_var_1 / 32) * 16)) + ((cse_var_1 % 32) / 16)))+(2*3), (((((((i0_i1_fused >> 6) * 6144) + ((cse_var_1 % 16) * 384)) + (i2 * 32)) + ((cse_var_1 / 32) * 16)) + ((cse_var_1 % 32) / 16)))+(2*4), (((((((i0_i1_fused >> 6) * 6144) + ((cse_var_1 % 16) * 384)) + (i2 * 32)) + ((cse_var_1 / 32) * 16)) + ((cse_var_1 % 32) / 16)))+(2*5), (((((((i0_i1_fused >> 6) * 6144) + ((cse_var_1 % 16) * 384)) + (i2 * 32)) + ((cse_var_1 / 32) * 16)) + ((cse_var_1 % 32) / 16)))+(2*6), (((((((i0_i1_fused >> 6) * 6144) + ((cse_var_1 % 16) * 384)) + (i2 * 32)) + ((cse_var_1 / 32) * 16)) + ((cse_var_1 % 32) / 16)))+(2*7));\n      *(float8*)(((float*)space_to_depth_1) + ((i0_i1_fused * 96) + (i2 * 8))) = (float8(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7]));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) / 96) * 6144) + ((((((((int)blockIdx.x) % 96) * 2) + (((int)threadIdx.x) >> 5)) / 3) % 16) * 384)) + ((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) % 12) * 32)) + ((((((((int)blockIdx.x) % 96) * 2) + (((int)threadIdx.x) >> 5)) / 3) / 32) * 16)) + ((((int)threadIdx.x) & 7) * 2)) + ((((((((int)blockIdx.x) % 96) * 2) + (((int)threadIdx.x) >> 5)) / 3) % 32) / 16))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 16, 24, 16), \"float32\"), space_to_depth: T.Buffer((5, 64, 12, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(320):\n            for i2 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused % 64\n                space_to_depth_1 = T.Buffer((30720,), data=space_to_depth.data)\n                data_1 = T.Buffer((30720,), data=data.data)\n                space_to_depth_1[i0_i1_fused * 96 + i2 * 8:i0_i1_fused * 96 + i2 * 8 + 8] = data_1[i0_i1_fused // 64 * 6144 + T.truncmod(cse_var_1, 16) * 384 + i2 * 32 + T.Div(cse_var_1, 32) * 16 + T.Div(T.truncmod(cse_var_1, 32), 16):i0_i1_fused // 64 * 6144 + T.truncmod(cse_var_1, 16) * 384 + i2 * 32 + T.Div(cse_var_1, 32) * 16 + T.Div(T.truncmod(cse_var_1, 32), 16) + 16:2]",
        "data": "5_16_24_16"
    },
    {
        "op_name": "space_to_depth",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t space_to_depth_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* space_to_depth = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* space_to_depth_1 = (((DLTensor*)space_to_depth)[0].data);\n  void* default_function_space_to_depth_shape = (((DLTensor*)space_to_depth)[0].shape);\n  void* default_function_space_to_depth_strides = (((DLTensor*)space_to_depth)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_space_to_depth_strides == NULL)) {\n  },\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        for (int32_t i3 = 0; i3 < 28; ++i3) {\n          int32_t cse_var_1 = (i0 * 1792);\n          ((float*)space_to_depth_1)[(((cse_var_1 + (i1 * 112)) + (i2 * 28)) + i3)] = ((float*)data_1)[(((((cse_var_1 + ((i1 % 4) * 448)) + (i2 * 112)) + ((i1 / 8) * 56)) + (i3 * 2)) + ((i1 % 8) / 4))];\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = data[((((((((((int)blockIdx.x) >> 5) * 1792) + ((((((int)blockIdx.x) & 31) >> 1) % 4) * 448)) + ((((int)blockIdx.x) & 1) * 224)) + ((((int)threadIdx.x) / 28) * 112)) + ((((((int)blockIdx.x) & 31) >> 1) / 8) * 56)) + ((((int)threadIdx.x) % 28) * 2)) + ((((((int)blockIdx.x) & 31) >> 1) % 8) / 4))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 4, 8, 56), \"float32\"), space_to_depth: T.Buffer((2, 16, 4, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0 in T.parallel(2):\n            for i1, i2, i3 in T.grid(16, 4, 28):\n                cse_var_1: T.int32 = i0 * 1792\n                space_to_depth_1 = T.Buffer((3584,), data=space_to_depth.data)\n                data_1 = T.Buffer((3584,), data=data.data)\n                space_to_depth_1[cse_var_1 + i1 * 112 + i2 * 28 + i3] = data_1[cse_var_1 + T.truncmod(i1, 4) * 448 + i2 * 112 + T.Div(i1, 8) * 56 + i3 * 2 + T.Div(T.truncmod(i1, 8), 4)]",
        "data": "2_4_8_56"
    },
    {
        "op_name": "space_to_depth",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t space_to_depth_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* space_to_depth = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* space_to_depth_1 = (((DLTensor*)space_to_depth)[0].data);\n  void* default_function_space_to_depth_shape = (((DLTensor*)space_to_depth)[0].shape);\n  void* default_function_space_to_depth_strides = (((DLTensor*)space_to_depth)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_space_to_depth_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 640; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 12; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused % 320) >> 2);\n      ((float*)space_to_depth_1)[((i0_i1_fused_i2_fused * 12) + i3)] = ((float*)data_1)[(((((((i0_i1_fused_i2_fused / 320) * 3840) + ((cse_var_1 % 20) * 192)) + ((i0_i1_fused_i2_fused & 3) * 48)) + ((cse_var_1 / 40) * 24)) + (i3 * 2)) + ((cse_var_1 % 40) / 20))];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) / 60) * 3840) + ((((((((int)blockIdx.x) % 60) * 4) + (((int)threadIdx.x) >> 4)) / 3) % 20) * 192)) + (((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) % 12) / 3) * 48)) + ((((((((int)blockIdx.x) % 60) * 4) + (((int)threadIdx.x) >> 4)) / 3) / 40) * 24)) + ((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 12) * 2)) + ((((((((int)blockIdx.x) % 60) * 4) + (((int)threadIdx.x) >> 4)) / 3) % 40) / 20))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 20, 8, 24), \"float32\"), space_to_depth: T.Buffer((2, 80, 4, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(640):\n            for i3 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused % 320 // 4\n                space_to_depth_1 = T.Buffer((7680,), data=space_to_depth.data)\n                data_1 = T.Buffer((7680,), data=data.data)\n                space_to_depth_1[i0_i1_fused_i2_fused * 12 + i3] = data_1[i0_i1_fused_i2_fused // 320 * 3840 + T.truncmod(cse_var_1, 20) * 192 + i0_i1_fused_i2_fused % 4 * 48 + T.Div(cse_var_1, 40) * 24 + i3 * 2 + T.Div(T.truncmod(cse_var_1, 40), 20)]",
        "data": "2_20_8_24"
    },
    {
        "op_name": "space_to_depth",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t space_to_depth_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* space_to_depth = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* space_to_depth_1 = (((DLTensor*)space_to_depth)[0].data);\n  void* default_function_space_to_depth_shape = (((DLTensor*)space_to_depth)[0].shape);\n  void* default_function_space_to_depth_strides = (((DLTensor*)space_to_depth)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_space_to_depth_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 64; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 40; ++i2) {\n      for (int32_t i3 = 0; i3 < 32; ++i3) {\n        int32_t cse_var_1 = (i0_i1_fused & 31);\n        ((float*)space_to_depth_1)[(((i0_i1_fused * 1280) + (i2 * 32)) + i3)] = ((float*)data_1)[(((((((i0_i1_fused >> 5) * 40960) + ((cse_var_1 % 8) * 5120)) + (i2 * 128)) + ((cse_var_1 / 16) * 64)) + (i3 * 2)) + ((cse_var_1 % 16) / 8))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((((((((int)blockIdx.x) / 640) * 40960) + ((((((int)blockIdx.x) % 640) / 20) % 8) * 5120)) + ((((int)blockIdx.x) % 20) * 256)) + ((((int)threadIdx.x) >> 5) * 128)) + ((((((int)blockIdx.x) % 640) / 20) / 16) * 64)) + ((((int)threadIdx.x) & 31) * 2)) + ((((((int)blockIdx.x) % 640) / 20) % 16) / 8))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 8, 80, 64), \"float32\"), space_to_depth: T.Buffer((2, 32, 40, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(64):\n            for i2, i3 in T.grid(40, 32):\n                cse_var_1: T.int32 = i0_i1_fused % 32\n                space_to_depth_1 = T.Buffer((81920,), data=space_to_depth.data)\n                data_1 = T.Buffer((81920,), data=data.data)\n                space_to_depth_1[i0_i1_fused * 1280 + i2 * 32 + i3] = data_1[i0_i1_fused // 32 * 40960 + T.truncmod(cse_var_1, 8) * 5120 + i2 * 128 + T.Div(cse_var_1, 16) * 64 + i3 * 2 + T.Div(T.truncmod(cse_var_1, 16), 8)]",
        "data": "2_8_80_64"
    },
    {
        "op_name": "space_to_depth",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t space_to_depth_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* space_to_depth = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* space_to_depth_1 = (((DLTensor*)space_to_depth)[0].data);\n  void* default_function_space_to_depth_shape = (((DLTensor*)space_to_depth)[0].shape);\n  void* default_function_space_to_depth_strides = (((DLTensor*)space_to_depth)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_space_to_depth_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3456; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 24; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused % 576) >> 2);\n      ((float*)space_to_depth_1)[((i0_i1_fused_i2_fused * 24) + i3)] = ((float*)data_1)[(((((((i0_i1_fused_i2_fused / 576) * 13824) + ((cse_var_1 % 36) * 384)) + ((i0_i1_fused_i2_fused & 3) * 96)) + ((cse_var_1 / 72) * 48)) + (i3 * 2)) + ((cse_var_1 % 72) / 36))];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) / 864) * 13824) + ((((((int)blockIdx.x) % 864) / 6) % 36) * 384)) + (((((((int)blockIdx.x) % 6) * 2) + (((int)threadIdx.x) >> 3)) / 3) * 96)) + ((((((int)blockIdx.x) % 864) / 6) / 72) * 48)) + ((((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) % 24) * 2)) + ((((((int)blockIdx.x) % 864) / 6) % 72) / 36))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 36, 8, 48), \"float32\"), space_to_depth: T.Buffer((6, 144, 4, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(3456):\n            for i3 in range(24):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused % 576 // 4\n                space_to_depth_1 = T.Buffer((82944,), data=space_to_depth.data)\n                data_1 = T.Buffer((82944,), data=data.data)\n                space_to_depth_1[i0_i1_fused_i2_fused * 24 + i3] = data_1[i0_i1_fused_i2_fused // 576 * 13824 + T.truncmod(cse_var_1, 36) * 384 + i0_i1_fused_i2_fused % 4 * 96 + T.Div(cse_var_1, 72) * 48 + i3 * 2 + T.Div(T.truncmod(cse_var_1, 72), 36)]",
        "data": "6_36_8_48"
    },
    {
        "op_name": "space_to_depth",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t space_to_depth_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* space_to_depth = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* space_to_depth_1 = (((DLTensor*)space_to_depth)[0].data);\n  void* default_function_space_to_depth_shape = (((DLTensor*)space_to_depth)[0].shape);\n  void* default_function_space_to_depth_strides = (((DLTensor*)space_to_depth)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_space_to_depth_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 448; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 40; ++i3) {\n        int32_t cse_var_1 = (i0_i1_fused & 63);\n        ((float*)space_to_depth_1)[(((i0_i1_fused * 160) + (i2 * 40)) + i3)] = ((float*)data_1)[(((((((i0_i1_fused >> 6) * 10240) + ((cse_var_1 % 16) * 640)) + (i2 * 160)) + ((cse_var_1 / 32) * 80)) + (i3 * 2)) + ((cse_var_1 % 32) / 16))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) / 160) * 10240) + ((((((((int)blockIdx.x) % 160) * 2) + (((int)threadIdx.x) >> 5)) / 5) % 16) * 640)) + (((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) % 20) / 5) * 160)) + ((((((((int)blockIdx.x) % 160) * 2) + (((int)threadIdx.x) >> 5)) / 5) / 32) * 80)) + ((((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 40) * 2)) + ((((((((int)blockIdx.x) % 160) * 2) + (((int)threadIdx.x) >> 5)) / 5) % 32) / 16))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 16, 8, 80), \"float32\"), space_to_depth: T.Buffer((7, 64, 4, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(448):\n            for i2, i3 in T.grid(4, 40):\n                cse_var_1: T.int32 = i0_i1_fused % 64\n                space_to_depth_1 = T.Buffer((71680,), data=space_to_depth.data)\n                data_1 = T.Buffer((71680,), data=data.data)\n                space_to_depth_1[i0_i1_fused * 160 + i2 * 40 + i3] = data_1[i0_i1_fused // 64 * 10240 + T.truncmod(cse_var_1, 16) * 640 + i2 * 160 + T.Div(cse_var_1, 32) * 80 + i3 * 2 + T.Div(T.truncmod(cse_var_1, 32), 16)]",
        "data": "7_16_8_80"
    },
    {
        "op_name": "space_to_depth",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t space_to_depth_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* space_to_depth = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* space_to_depth_1 = (((DLTensor*)space_to_depth)[0].data);\n  void* default_function_space_to_depth_shape = (((DLTensor*)space_to_depth)[0].shape);\n  void* default_function_space_to_depth_strides = (((DLTensor*)space_to_depth)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_space_to_depth_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 192; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 28; ++i2) {\n      int32_t cse_var_1 = (i0_i1_fused % 96);\n      int32_t16 v_ = int32_t16((((((((i0_i1_fused / 96) * 43008) + ((cse_var_1 % 24) * 1792)) + (i2 * 64)) + ((cse_var_1 / 48) * 32)) + ((cse_var_1 % 48) / 24)))+(2*0), (((((((i0_i1_fused / 96) * 43008) + ((cse_var_1 % 24) * 1792)) + (i2 * 64)) + ((cse_var_1 / 48) * 32)) + ((cse_var_1 % 48) / 24)))+(2*1), (((((((i0_i1_fused / 96) * 43008) + ((cse_var_1 % 24) * 1792)) + (i2 * 64)) + ((cse_var_1 / 48) * 32)) + ((cse_var_1 % 48) / 24)))+(2*2), (((((((i0_i1_fused / 96) * 43008) + ((cse_var_1 % 24) * 1792)) + (i2 * 64)) + ((cse_var_1 / 48) * 32)) + ((cse_var_1 % 48) / 24)))+(2*3), (((((((i0_i1_fused / 96) * 43008) + ((cse_var_1 % 24) * 1792)) + (i2 * 64)) + ((cse_var_1 / 48) * 32)) + ((cse_var_1 % 48) / 24)))+(2*4), (((((((i0_i1_fused / 96) * 43008) + ((cse_var_1 % 24) * 1792)) + (i2 * 64)) + ((cse_var_1 / 48) * 32)) + ((cse_var_1 % 48) / 24)))+(2*5), (((((((i0_i1_fused / 96) * 43008) + ((cse_var_1 % 24) * 1792)) + (i2 * 64)) + ((cse_var_1 / 48) * 32)) + ((cse_var_1 % 48) / 24)))+(2*6), (((((((i0_i1_fused / 96) * 43008) + ((cse_var_1 % 24) * 1792)) + (i2 * 64)) + ((cse_var_1 / 48) * 32)) + ((cse_var_1 % 48) / 24)))+(2*7), (((((((i0_i1_fused / 96) * 43008) + ((cse_var_1 % 24) * 1792)) + (i2 * 64)) + ((cse_var_1 / 48) * 32)) + ((cse_var_1 % 48) / 24)))+(2*8), (((((((i0_i1_fused / 96) * 43008) + ((cse_var_1 % 24) * 1792)) + (i2 * 64)) + ((cse_var_1 / 48) * 32)) + ((cse_var_1 % 48) / 24)))+(2*9), (((((((i0_i1_fused / 96) * 43008) + ((cse_var_1 % 24) * 1792)) + (i2 * 64)) + ((cse_var_1 / 48) * 32)) + ((cse_var_1 % 48) / 24)))+(2*10), (((((((i0_i1_fused / 96) * 43008) + ((cse_var_1 % 24) * 1792)) + (i2 * 64)) + ((cse_var_1 / 48) * 32)) + ((cse_var_1 % 48) / 24)))+(2*11), (((((((i0_i1_fused / 96) * 43008) + ((cse_var_1 % 24) * 1792)) + (i2 * 64)) + ((cse_var_1 / 48) * 32)) + ((cse_var_1 % 48) / 24)))+(2*12), (((((((i0_i1_fused / 96) * 43008) + ((cse_var_1 % 24) * 1792)) + (i2 * 64)) + ((cse_var_1 / 48) * 32)) + ((cse_var_1 % 48) / 24)))+(2*13), (((((((i0_i1_fused / 96) * 43008) + ((cse_var_1 % 24) * 1792)) + (i2 * 64)) + ((cse_var_1 / 48) * 32)) + ((cse_var_1 % 48) / 24)))+(2*14), (((((((i0_i1_fused / 96) * 43008) + ((cse_var_1 % 24) * 1792)) + (i2 * 64)) + ((cse_var_1 / 48) * 32)) + ((cse_var_1 % 48) / 24)))+(2*15));\n      *(float16*)(((float*)space_to_depth_1) + ((i0_i1_fused * 448) + (i2 * 16))) = (float16(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb],((float*)data_1)[v_.sc],((float*)data_1)[v_.sd],((float*)data_1)[v_.se],((float*)data_1)[v_.sf]));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) / 768) * 43008) + ((((((int)blockIdx.x) % 768) >> 3) % 24) * 1792)) + (((((((int)blockIdx.x) & 7) * 7) + (((int)threadIdx.x) >> 3)) >> 1) * 64)) + ((((((int)blockIdx.x) % 768) >> 3) / 48) * 32)) + ((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) & 15) * 2)) + ((((((int)blockIdx.x) % 768) >> 3) % 48) / 24))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 24, 56, 32), \"float32\"), space_to_depth: T.Buffer((2, 96, 28, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(192):\n            for i2 in range(28):\n                cse_var_1: T.int32 = i0_i1_fused % 96\n                space_to_depth_1 = T.Buffer((86016,), data=space_to_depth.data)\n                data_1 = T.Buffer((86016,), data=data.data)\n                space_to_depth_1[i0_i1_fused * 448 + i2 * 16:i0_i1_fused * 448 + i2 * 16 + 16] = data_1[i0_i1_fused // 96 * 43008 + T.truncmod(cse_var_1, 24) * 1792 + i2 * 64 + T.Div(cse_var_1, 48) * 32 + T.Div(T.truncmod(cse_var_1, 48), 24):i0_i1_fused // 96 * 43008 + T.truncmod(cse_var_1, 24) * 1792 + i2 * 64 + T.Div(cse_var_1, 48) * 32 + T.Div(T.truncmod(cse_var_1, 48), 24) + 32:2]",
        "data": "2_24_56_32"
    },
    {
        "op_name": "space_to_depth",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t space_to_depth_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* space_to_depth = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* space_to_depth_1 = (((DLTensor*)space_to_depth)[0].data);\n  void* default_function_space_to_depth_shape = (((DLTensor*)space_to_depth)[0].shape);\n  void* default_function_space_to_depth_strides = (((DLTensor*)space_to_depth)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_space_to_depth_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 36; ++i2) {\n      for (int32_t i3 = 0; i3 < 28; ++i3) {\n        ((float*)space_to_depth_1)[(((i0_i1_fused * 1008) + (i2 * 28)) + i3)] = ((float*)data_1)[((((((i0_i1_fused % 12) * 4032) + (i2 * 112)) + ((i0_i1_fused / 24) * 56)) + (i3 * 2)) + ((i0_i1_fused % 24) / 12))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = data[(((((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 18)) / 56) % 12) * 4032) + (((((((int)blockIdx.x) * 27) + (((int)threadIdx.x) >> 1)) % 504) / 14) * 112)) + (((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 18)) / 56) / 24) * 56)) + ((((((int)blockIdx.x) * 26) + ((int)threadIdx.x)) % 28) * 2)) + (((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 18)) / 56) % 24) / 12))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 12, 72, 56), \"float32\"), space_to_depth: T.Buffer((1, 48, 36, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(48):\n            for i2, i3 in T.grid(36, 28):\n                space_to_depth_1 = T.Buffer((48384,), data=space_to_depth.data)\n                data_1 = T.Buffer((48384,), data=data.data)\n                space_to_depth_1[i0_i1_fused * 1008 + i2 * 28 + i3] = data_1[T.truncmod(i0_i1_fused, 12) * 4032 + i2 * 112 + T.Div(i0_i1_fused, 24) * 56 + i3 * 2 + T.Div(T.truncmod(i0_i1_fused, 24), 12)]",
        "data": "1_12_72_56"
    },
    {
        "op_name": "strided_slice",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t a_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* a = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* a_1 = (((DLTensor*)a)[0].data);\n  void* default_function_a_shape = (((DLTensor*)a)[0].shape);\n  void* default_function_a_strides = (((DLTensor*)a)[0].strides);\n  int32_t dev_id = (((DLTensor*)a)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_a_strides == NULL)) {\n  },\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 15; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      ((float*)T_strided_slice_1)[((ax0_ax1_fused * 7) + ax2)] = ((float*)a_1)[(((((ax0_ax1_fused / 5) * 720) + ((ax0_ax1_fused % 5) * 20)) + ax2) + 763)];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 105) {\n    T_strided_slice[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = a[(((((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 35) * 720) + (((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 35) / 7) * 20)) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 7)) + 763)];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((32, 36, 20), \"float32\"), T_strided_slice: T.Buffer((3, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(15):\n            for ax2 in range(7):\n                T_strided_slice_1 = T.Buffer((105,), data=T_strided_slice.data)\n                a_1 = T.Buffer((23040,), data=a.data)\n                T_strided_slice_1[ax0_ax1_fused * 7 + ax2] = a_1[ax0_ax1_fused // 5 * 720 + ax0_ax1_fused % 5 * 20 + ax2 + 763]",
        "a": "32_36_20"
    },
    {
        "op_name": "strided_slice",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t a_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* a = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* a_1 = (((DLTensor*)a)[0].data);\n  void* default_function_a_shape = (((DLTensor*)a)[0].shape);\n  void* default_function_a_strides = (((DLTensor*)a)[0].strides);\n  int32_t dev_id = (((DLTensor*)a)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_a_strides == NULL)) {\n  },\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 105; ++ax0_ax1_fused_ax2_fused) {\n    ((float*)T_strided_slice_1)[ax0_ax1_fused_ax2_fused] = ((float*)a_1)[(((((ax0_ax1_fused_ax2_fused / 35) * 240) + (((ax0_ax1_fused_ax2_fused % 35) / 7) * 20)) + (ax0_ax1_fused_ax2_fused % 7)) + 283)];\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a);\nextern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  T_strided_slice[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] = a[((((((int)blockIdx.x) * 240) + ((((int)threadIdx.x) / 7) * 20)) + (((int)threadIdx.x) % 7)) + 283)];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((36, 12, 20), \"float32\"), T_strided_slice: T.Buffer((3, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(105):\n            T_strided_slice_1 = T.Buffer((105,), data=T_strided_slice.data)\n            a_1 = T.Buffer((8640,), data=a.data)\n            T_strided_slice_1[ax0_ax1_fused_ax2_fused] = a_1[ax0_ax1_fused_ax2_fused // 35 * 240 + ax0_ax1_fused_ax2_fused % 35 // 7 * 20 + ax0_ax1_fused_ax2_fused % 7 + 283]",
        "a": "36_12_20"
    },
    {
        "op_name": "strided_slice",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t a_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* a = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* a_1 = (((DLTensor*)a)[0].data);\n  void* default_function_a_shape = (((DLTensor*)a)[0].shape);\n  void* default_function_a_strides = (((DLTensor*)a)[0].strides);\n  int32_t dev_id = (((DLTensor*)a)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_a_strides == NULL)) {\n  },\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 15; ++ax0_ax1_fused) {\n    ((float*)T_strided_slice_1)[ax0_ax1_fused] = ((float*)a_1)[((((ax0_ax1_fused / 5) * 112) + ((ax0_ax1_fused % 5) * 4)) + 123)];\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a);\nextern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  T_strided_slice[((int)threadIdx.x)] = a[((((((int)threadIdx.x) / 5) * 112) + ((((int)threadIdx.x) % 5) * 4)) + 123)];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((28, 28, 4), \"float32\"), T_strided_slice: T.Buffer((3, 5, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(15):\n            T_strided_slice_1 = T.Buffer((15,), data=T_strided_slice.data)\n            a_1 = T.Buffer((3136,), data=a.data)\n            T_strided_slice_1[ax0_ax1_fused] = a_1[ax0_ax1_fused // 5 * 112 + ax0_ax1_fused % 5 * 4 + 123]",
        "a": "28_28_4"
    },
    {
        "op_name": "strided_slice",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t a_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* a = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* a_1 = (((DLTensor*)a)[0].data);\n  void* default_function_a_shape = (((DLTensor*)a)[0].shape);\n  void* default_function_a_strides = (((DLTensor*)a)[0].strides);\n  int32_t dev_id = (((DLTensor*)a)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_a_strides == NULL)) {\n  },\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 15; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      ((float*)T_strided_slice_1)[((ax0_ax1_fused * 7) + ax2)] = ((float*)a_1)[(((((ax0_ax1_fused / 5) * 192) + ((ax0_ax1_fused % 5) * 24)) + ax2) + 243)];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) / 3)) < 35) {\n    T_strided_slice[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = a[(((((((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) / 35) * 192) + (((((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) % 35) / 7) * 24)) + (((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 7)) + 243)];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((36, 8, 24), \"float32\"), T_strided_slice: T.Buffer((3, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(15):\n            for ax2 in range(7):\n                T_strided_slice_1 = T.Buffer((105,), data=T_strided_slice.data)\n                a_1 = T.Buffer((6912,), data=a.data)\n                T_strided_slice_1[ax0_ax1_fused * 7 + ax2] = a_1[ax0_ax1_fused // 5 * 192 + ax0_ax1_fused % 5 * 24 + ax2 + 243]",
        "a": "36_8_24"
    },
    {
        "op_name": "strided_slice",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t a_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* a = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* a_1 = (((DLTensor*)a)[0].data);\n  void* default_function_a_shape = (((DLTensor*)a)[0].shape);\n  void* default_function_a_strides = (((DLTensor*)a)[0].strides);\n  int32_t dev_id = (((DLTensor*)a)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_a_strides == NULL)) {\n  },\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 15; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      ((float*)T_strided_slice_1)[((ax0_ax1_fused * 7) + ax2)] = ((float*)a_1)[(((((ax0_ax1_fused / 5) * 768) + ((ax0_ax1_fused % 5) * 32)) + ax2) + 835)];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(26) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a);\nextern \"C\" __global__ void __launch_bounds__(26) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  if (((((int)blockIdx.x) * 26) + ((int)threadIdx.x)) < 105) {\n    T_strided_slice[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))] = a[(((((((((int)blockIdx.x) * 26) + ((int)threadIdx.x)) / 35) * 768) + (((((((int)blockIdx.x) * 26) + ((int)threadIdx.x)) % 35) / 7) * 32)) + (((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) % 7)) + 835)];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((40, 24, 32), \"float32\"), T_strided_slice: T.Buffer((3, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(15):\n            for ax2 in range(7):\n                T_strided_slice_1 = T.Buffer((105,), data=T_strided_slice.data)\n                a_1 = T.Buffer((30720,), data=a.data)\n                T_strided_slice_1[ax0_ax1_fused * 7 + ax2] = a_1[ax0_ax1_fused // 5 * 768 + ax0_ax1_fused % 5 * 32 + ax2 + 835]",
        "a": "40_24_32"
    },
    {
        "op_name": "strided_slice",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t a_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* a = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* a_1 = (((DLTensor*)a)[0].data);\n  void* default_function_a_shape = (((DLTensor*)a)[0].shape);\n  void* default_function_a_strides = (((DLTensor*)a)[0].strides);\n  int32_t dev_id = (((DLTensor*)a)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_a_strides == NULL)) {\n  },\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        ((float*)T_strided_slice_1)[(((ax0 * 35) + (ax1 * 7)) + ax2)] = ((float*)a_1)[((((ax0 * 1600) + (ax1 * 40)) + ax2) + 1683)];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a);\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 3)) < 35) {\n    T_strided_slice[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = a[(((((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) / 35) * 1600) + (((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 35) / 7) * 40)) + (((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 7)) + 1683)];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((8, 40, 40), \"float32\"), T_strided_slice: T.Buffer((3, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(5, 7):\n                T_strided_slice_1 = T.Buffer((105,), data=T_strided_slice.data)\n                a_1 = T.Buffer((12800,), data=a.data)\n                T_strided_slice_1[ax0 * 35 + ax1 * 7 + ax2] = a_1[ax0 * 1600 + ax1 * 40 + ax2 + 1683]",
        "a": "8_40_40"
    },
    {
        "op_name": "strided_slice",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t a_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* a = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* a_1 = (((DLTensor*)a)[0].data);\n  void* default_function_a_shape = (((DLTensor*)a)[0].shape);\n  void* default_function_a_strides = (((DLTensor*)a)[0].strides);\n  int32_t dev_id = (((DLTensor*)a)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_a_strides == NULL)) {\n  },\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 15; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      ((float*)T_strided_slice_1)[((ax0_ax1_fused * 7) + ax2)] = ((float*)a_1)[(((((ax0_ax1_fused / 5) * 240) + ((ax0_ax1_fused % 5) * 20)) + ax2) + 283)];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a);\nextern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  T_strided_slice[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] = a[((((((int)blockIdx.x) * 240) + ((((int)threadIdx.x) / 7) * 20)) + (((int)threadIdx.x) % 7)) + 283)];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((16, 12, 20), \"float32\"), T_strided_slice: T.Buffer((3, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(15):\n            for ax2 in range(7):\n                T_strided_slice_1 = T.Buffer((105,), data=T_strided_slice.data)\n                a_1 = T.Buffer((3840,), data=a.data)\n                T_strided_slice_1[ax0_ax1_fused * 7 + ax2] = a_1[ax0_ax1_fused // 5 * 240 + ax0_ax1_fused % 5 * 20 + ax2 + 283]",
        "a": "16_12_20"
    },
    {
        "op_name": "strided_slice",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t a_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* a = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* a_1 = (((DLTensor*)a)[0].data);\n  void* default_function_a_shape = (((DLTensor*)a)[0].shape);\n  void* default_function_a_strides = (((DLTensor*)a)[0].strides);\n  int32_t dev_id = (((DLTensor*)a)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_a_strides == NULL)) {\n  },\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 75; ++ax0_ax1_fused_ax2_fused) {\n    ((float*)T_strided_slice_1)[ax0_ax1_fused_ax2_fused] = ((float*)a_1)[(((((ax0_ax1_fused_ax2_fused / 25) * 128) + (((ax0_ax1_fused_ax2_fused % 25) / 5) * 8)) + (ax0_ax1_fused_ax2_fused % 5)) + 147)];\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  if (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) < 75) {\n    T_strided_slice[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = a[(((((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) / 25) * 128) + (((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 25) / 5) * 8)) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 5)) + 147)];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((16, 16, 8), \"float32\"), T_strided_slice: T.Buffer((3, 5, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(75):\n            T_strided_slice_1 = T.Buffer((75,), data=T_strided_slice.data)\n            a_1 = T.Buffer((2048,), data=a.data)\n            T_strided_slice_1[ax0_ax1_fused_ax2_fused] = a_1[ax0_ax1_fused_ax2_fused // 25 * 128 + ax0_ax1_fused_ax2_fused % 25 // 5 * 8 + ax0_ax1_fused_ax2_fused % 5 + 147]",
        "a": "16_16_8"
    },
    {
        "op_name": "strided_slice",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t a_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* a = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* a_1 = (((DLTensor*)a)[0].data);\n  void* default_function_a_shape = (((DLTensor*)a)[0].shape);\n  void* default_function_a_strides = (((DLTensor*)a)[0].strides);\n  int32_t dev_id = (((DLTensor*)a)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_a_strides == NULL)) {\n  },\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 15; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      ((float*)T_strided_slice_1)[((ax0_ax1_fused * 7) + ax2)] = ((float*)a_1)[(((((ax0_ax1_fused / 5) * 192) + ((ax0_ax1_fused % 5) * 16)) + ax2) + 227)];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(52) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a);\nextern \"C\" __global__ void __launch_bounds__(52) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  if (((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) < 105) {\n    T_strided_slice[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] = a[(((((((((int)blockIdx.x) * 52) + ((int)threadIdx.x)) / 35) * 192) + (((((((int)blockIdx.x) * 17) + ((int)threadIdx.x)) % 35) / 7) * 16)) + (((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 7)) + 227)];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((16, 12, 16), \"float32\"), T_strided_slice: T.Buffer((3, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(15):\n            for ax2 in range(7):\n                T_strided_slice_1 = T.Buffer((105,), data=T_strided_slice.data)\n                a_1 = T.Buffer((3072,), data=a.data)\n                T_strided_slice_1[ax0_ax1_fused * 7 + ax2] = a_1[ax0_ax1_fused // 5 * 192 + ax0_ax1_fused % 5 * 16 + ax2 + 227]",
        "a": "16_12_16"
    },
    {
        "op_name": "strided_slice",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t a_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* a = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* a_1 = (((DLTensor*)a)[0].data);\n  void* default_function_a_shape = (((DLTensor*)a)[0].shape);\n  void* default_function_a_strides = (((DLTensor*)a)[0].strides);\n  int32_t dev_id = (((DLTensor*)a)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_a_strides == NULL)) {\n  },\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 15; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      ((float*)T_strided_slice_1)[((ax0_ax1_fused * 7) + ax2)] = ((float*)a_1)[(((((ax0_ax1_fused / 5) * 1120) + ((ax0_ax1_fused % 5) * 28)) + ax2) + 1179)];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a);\nextern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  T_strided_slice[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))] = a[(((((((int)blockIdx.x) / 7) * 1120) + (((((((int)blockIdx.x) % 7) * 5) + ((int)threadIdx.x)) / 7) * 28)) + (((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) % 7)) + 1179)];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((12, 40, 28), \"float32\"), T_strided_slice: T.Buffer((3, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(15):\n            for ax2 in range(7):\n                T_strided_slice_1 = T.Buffer((105,), data=T_strided_slice.data)\n                a_1 = T.Buffer((13440,), data=a.data)\n                T_strided_slice_1[ax0_ax1_fused * 7 + ax2] = a_1[ax0_ax1_fused // 5 * 1120 + ax0_ax1_fused % 5 * 28 + ax2 + 1179]",
        "a": "12_40_28"
    },
    {
        "op_name": "unpack_NCHWc_to_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t packed_out_code = arg_type_ids[0];\n  int32_t output_unpack_code = arg_type_ids[1];\n  void* packed_out = (((TVMValue*)args)[0].v_handle);\n  void* output_unpack = (((TVMValue*)args)[1].v_handle);\n  void* packed_out_1 = (((DLTensor*)packed_out)[0].data);\n  void* default_function_packed_out_shape = (((DLTensor*)packed_out)[0].shape);\n  void* default_function_packed_out_strides = (((DLTensor*)packed_out)[0].strides);\n  int32_t dev_id = (((DLTensor*)packed_out)[0].device.device_id);\n  void* output_unpack_1 = (((DLTensor*)output_unpack)[0].data);\n  void* default_function_output_unpack_shape = (((DLTensor*)output_unpack)[0].shape);\n  void* default_function_output_unpack_strides = (((DLTensor*)output_unpack)[0].strides);\n  if (!(default_function_packed_out_strides == NULL)) {\n  },\n  if (!(default_function_output_unpack_strides == NULL)) {\n  },\n  for (int32_t n_c_fused = 0; n_c_fused < 16; ++n_c_fused) {\n    for (int32_t h = 0; h < 16; ++h) {\n      for (int32_t w = 0; w < 8; ++w) {\n        ((float*)output_unpack_1)[(((n_c_fused * 128) + (h * 8)) + w)] = ((float*)packed_out_1)[(((((n_c_fused >> 1) * 256) + (h * 16)) + (w * 2)) + (n_c_fused & 1))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out) {\n  output_unpack[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = packed_out[(((((((int)blockIdx.x) >> 2) * 256) + ((((int)blockIdx.x) & 1) * 128)) + (((int)threadIdx.x) * 2)) + ((((int)blockIdx.x) & 3) >> 1))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(packed_out: T.Buffer((2, 4, 16, 8, 2), \"float32\"), output_unpack: T.Buffer((2, 8, 16, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for n_c_fused in T.parallel(16):\n            for h, w in T.grid(16, 8):\n                output_unpack_1 = T.Buffer((2048,), data=output_unpack.data)\n                packed_out_1 = T.Buffer((2048,), data=packed_out.data)\n                output_unpack_1[n_c_fused * 128 + h * 8 + w] = packed_out_1[n_c_fused // 2 * 256 + h * 16 + w * 2 + n_c_fused % 2]",
        "packed_out": "2_4_16_8_2"
    },
    {
        "op_name": "unpack_NCHWc_to_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t packed_out_code = arg_type_ids[0];\n  int32_t output_unpack_code = arg_type_ids[1];\n  void* packed_out = (((TVMValue*)args)[0].v_handle);\n  void* output_unpack = (((TVMValue*)args)[1].v_handle);\n  void* packed_out_1 = (((DLTensor*)packed_out)[0].data);\n  void* default_function_packed_out_shape = (((DLTensor*)packed_out)[0].shape);\n  void* default_function_packed_out_strides = (((DLTensor*)packed_out)[0].strides);\n  int32_t dev_id = (((DLTensor*)packed_out)[0].device.device_id);\n  void* output_unpack_1 = (((DLTensor*)output_unpack)[0].data);\n  void* default_function_output_unpack_shape = (((DLTensor*)output_unpack)[0].shape);\n  void* default_function_output_unpack_strides = (((DLTensor*)output_unpack)[0].strides);\n  if (!(default_function_packed_out_strides == NULL)) {\n  },\n  if (!(default_function_output_unpack_strides == NULL)) {\n  },\n  for (int32_t n_c_fused_h_fused = 0; n_c_fused_h_fused < 1152; ++n_c_fused_h_fused) {\n    for (int32_t w = 0; w < 40; ++w) {\n      ((float*)output_unpack_1)[((n_c_fused_h_fused * 40) + w)] = ((float*)packed_out_1)[(((((n_c_fused_h_fused / 24) * 960) + ((n_c_fused_h_fused % 12) * 80)) + (w * 2)) + ((n_c_fused_h_fused % 24) / 12))];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out) {\n  output_unpack[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = packed_out[((((((int)blockIdx.x) / 15) * 960) + ((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 480) * 2)) + ((((((int)blockIdx.x) % 15) * 2) + (((int)threadIdx.x) >> 5)) / 15))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(packed_out: T.Buffer((4, 12, 12, 40, 2), \"float32\"), output_unpack: T.Buffer((4, 24, 12, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for n_c_fused_h_fused in T.parallel(1152):\n            for w in range(40):\n                output_unpack_1 = T.Buffer((46080,), data=output_unpack.data)\n                packed_out_1 = T.Buffer((46080,), data=packed_out.data)\n                output_unpack_1[n_c_fused_h_fused * 40 + w] = packed_out_1[n_c_fused_h_fused // 24 * 960 + n_c_fused_h_fused % 12 * 80 + w * 2 + n_c_fused_h_fused % 24 // 12]",
        "packed_out": "4_12_12_40_2"
    },
    {
        "op_name": "unpack_NCHWc_to_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t packed_out_code = arg_type_ids[0];\n  int32_t output_unpack_code = arg_type_ids[1];\n  void* packed_out = (((TVMValue*)args)[0].v_handle);\n  void* output_unpack = (((TVMValue*)args)[1].v_handle);\n  void* packed_out_1 = (((DLTensor*)packed_out)[0].data);\n  void* default_function_packed_out_shape = (((DLTensor*)packed_out)[0].shape);\n  void* default_function_packed_out_strides = (((DLTensor*)packed_out)[0].strides);\n  int32_t dev_id = (((DLTensor*)packed_out)[0].device.device_id);\n  void* output_unpack_1 = (((DLTensor*)output_unpack)[0].data);\n  void* default_function_output_unpack_shape = (((DLTensor*)output_unpack)[0].shape);\n  void* default_function_output_unpack_strides = (((DLTensor*)output_unpack)[0].strides);\n  if (!(default_function_packed_out_strides == NULL)) {\n  },\n  if (!(default_function_output_unpack_strides == NULL)) {\n  },\n  for (int32_t n_c_fused_h_fused_w_fused = 0; n_c_fused_h_fused_w_fused < 230400; ++n_c_fused_h_fused_w_fused) {\n    ((float*)output_unpack_1)[n_c_fused_h_fused_w_fused] = ((float*)packed_out_1)[((((n_c_fused_h_fused_w_fused / 640) * 640) + ((n_c_fused_h_fused_w_fused % 320) * 2)) + ((n_c_fused_h_fused_w_fused % 640) / 320))];\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out) {\n  output_unpack[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = packed_out[(((((((int)blockIdx.x) / 10) * 640) + ((((int)blockIdx.x) % 5) * 128)) + (((int)threadIdx.x) * 2)) + ((((int)blockIdx.x) % 10) / 5))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(packed_out: T.Buffer((10, 36, 16, 20, 2), \"float32\"), output_unpack: T.Buffer((10, 72, 16, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for n_c_fused_h_fused_w_fused in T.parallel(230400):\n            output_unpack_1 = T.Buffer((230400,), data=output_unpack.data)\n            packed_out_1 = T.Buffer((230400,), data=packed_out.data)\n            output_unpack_1[n_c_fused_h_fused_w_fused] = packed_out_1[n_c_fused_h_fused_w_fused // 640 * 640 + n_c_fused_h_fused_w_fused % 320 * 2 + n_c_fused_h_fused_w_fused % 640 // 320]",
        "packed_out": "10_36_16_20_2"
    },
    {
        "op_name": "unpack_NCHWc_to_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t packed_out_code = arg_type_ids[0];\n  int32_t output_unpack_code = arg_type_ids[1];\n  void* packed_out = (((TVMValue*)args)[0].v_handle);\n  void* output_unpack = (((TVMValue*)args)[1].v_handle);\n  void* packed_out_1 = (((DLTensor*)packed_out)[0].data);\n  void* default_function_packed_out_shape = (((DLTensor*)packed_out)[0].shape);\n  void* default_function_packed_out_strides = (((DLTensor*)packed_out)[0].strides);\n  int32_t dev_id = (((DLTensor*)packed_out)[0].device.device_id);\n  void* output_unpack_1 = (((DLTensor*)output_unpack)[0].data);\n  void* default_function_output_unpack_shape = (((DLTensor*)output_unpack)[0].shape);\n  void* default_function_output_unpack_strides = (((DLTensor*)output_unpack)[0].strides);\n  if (!(default_function_packed_out_strides == NULL)) {\n  },\n  if (!(default_function_output_unpack_strides == NULL)) {\n  },\n  for (int32_t n = 0; n < 8; ++n) {\n    for (int32_t c = 0; c < 40; ++c) {\n      for (int32_t h = 0; h < 40; ++h) {\n        int32_t cse_var_1 = (n * 12800);\n        int32_t8 v_ = int32_t8(((((cse_var_1 + ((c >> 1) * 640)) + (h * 16)) + (c & 1)))+(2*0), ((((cse_var_1 + ((c >> 1) * 640)) + (h * 16)) + (c & 1)))+(2*1), ((((cse_var_1 + ((c >> 1) * 640)) + (h * 16)) + (c & 1)))+(2*2), ((((cse_var_1 + ((c >> 1) * 640)) + (h * 16)) + (c & 1)))+(2*3), ((((cse_var_1 + ((c >> 1) * 640)) + (h * 16)) + (c & 1)))+(2*4), ((((cse_var_1 + ((c >> 1) * 640)) + (h * 16)) + (c & 1)))+(2*5), ((((cse_var_1 + ((c >> 1) * 640)) + (h * 16)) + (c & 1)))+(2*6), ((((cse_var_1 + ((c >> 1) * 640)) + (h * 16)) + (c & 1)))+(2*7));\n        *(float8*)(((float*)output_unpack_1) + ((cse_var_1 + (c * 320)) + (h * 8))) = (float8(((float*)packed_out_1)[v_.s0],((float*)packed_out_1)[v_.s1],((float*)packed_out_1)[v_.s2],((float*)packed_out_1)[v_.s3],((float*)packed_out_1)[v_.s4],((float*)packed_out_1)[v_.s5],((float*)packed_out_1)[v_.s6],((float*)packed_out_1)[v_.s7]));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out) {\n  output_unpack[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = packed_out[(((((((int)blockIdx.x) / 10) * 640) + ((((int)blockIdx.x) % 5) * 128)) + (((int)threadIdx.x) * 2)) + ((((int)blockIdx.x) % 10) / 5))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(packed_out: T.Buffer((8, 20, 40, 8, 2), \"float32\"), output_unpack: T.Buffer((8, 40, 40, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for n in T.parallel(8):\n            for c, h in T.grid(40, 40):\n                cse_var_1: T.int32 = n * 12800\n                output_unpack_1 = T.Buffer((102400,), data=output_unpack.data)\n                packed_out_1 = T.Buffer((102400,), data=packed_out.data)\n                output_unpack_1[cse_var_1 + c * 320 + h * 8:cse_var_1 + c * 320 + h * 8 + 8] = packed_out_1[cse_var_1 + c // 2 * 640 + h * 16 + c % 2:cse_var_1 + c // 2 * 640 + h * 16 + c % 2 + 16:2]",
        "packed_out": "8_20_40_8_2"
    },
    {
        "op_name": "unpack_NCHWc_to_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t packed_out_code = arg_type_ids[0];\n  int32_t output_unpack_code = arg_type_ids[1];\n  void* packed_out = (((TVMValue*)args)[0].v_handle);\n  void* output_unpack = (((TVMValue*)args)[1].v_handle);\n  void* packed_out_1 = (((DLTensor*)packed_out)[0].data);\n  void* default_function_packed_out_shape = (((DLTensor*)packed_out)[0].shape);\n  void* default_function_packed_out_strides = (((DLTensor*)packed_out)[0].strides);\n  int32_t dev_id = (((DLTensor*)packed_out)[0].device.device_id);\n  void* output_unpack_1 = (((DLTensor*)output_unpack)[0].data);\n  void* default_function_output_unpack_shape = (((DLTensor*)output_unpack)[0].shape);\n  void* default_function_output_unpack_strides = (((DLTensor*)output_unpack)[0].strides);\n  if (!(default_function_packed_out_strides == NULL)) {\n  },\n  if (!(default_function_output_unpack_strides == NULL)) {\n  },\n  for (int32_t n_c_fused = 0; n_c_fused < 320; ++n_c_fused) {\n    for (int32_t h = 0; h < 24; ++h) {\n      for (int32_t w = 0; w < 36; ++w) {\n        ((float*)output_unpack_1)[(((n_c_fused * 864) + (h * 36)) + w)] = ((float*)packed_out_1)[(((((n_c_fused >> 1) * 1728) + (h * 72)) + (w * 2)) + (n_c_fused & 1))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out) {\n  output_unpack[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = packed_out[((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 12)) / 144) * 1728) + ((((((int)blockIdx.x) * 60) + ((int)threadIdx.x)) % 864) * 2)) + ((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 12)) % 144) / 72))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(packed_out: T.Buffer((10, 16, 24, 36, 2), \"float32\"), output_unpack: T.Buffer((10, 32, 24, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for n_c_fused in T.parallel(320):\n            for h, w in T.grid(24, 36):\n                output_unpack_1 = T.Buffer((276480,), data=output_unpack.data)\n                packed_out_1 = T.Buffer((276480,), data=packed_out.data)\n                output_unpack_1[n_c_fused * 864 + h * 36 + w] = packed_out_1[n_c_fused // 2 * 1728 + h * 72 + w * 2 + n_c_fused % 2]",
        "packed_out": "10_16_24_36_2"
    },
    {
        "op_name": "unpack_NCHWc_to_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t packed_out_code = arg_type_ids[0];\n  int32_t output_unpack_code = arg_type_ids[1];\n  void* packed_out = (((TVMValue*)args)[0].v_handle);\n  void* output_unpack = (((TVMValue*)args)[1].v_handle);\n  void* packed_out_1 = (((DLTensor*)packed_out)[0].data);\n  void* default_function_packed_out_shape = (((DLTensor*)packed_out)[0].shape);\n  void* default_function_packed_out_strides = (((DLTensor*)packed_out)[0].strides);\n  int32_t dev_id = (((DLTensor*)packed_out)[0].device.device_id);\n  void* output_unpack_1 = (((DLTensor*)output_unpack)[0].data);\n  void* default_function_output_unpack_shape = (((DLTensor*)output_unpack)[0].shape);\n  void* default_function_output_unpack_strides = (((DLTensor*)output_unpack)[0].strides);\n  if (!(default_function_packed_out_strides == NULL)) {\n  },\n  if (!(default_function_output_unpack_strides == NULL)) {\n  },\n  for (int32_t n_c_fused = 0; n_c_fused < 224; ++n_c_fused) {\n    for (int32_t h = 0; h < 8; ++h) {\n      for (int32_t w = 0; w < 40; ++w) {\n        ((float*)output_unpack_1)[(((n_c_fused * 320) + (h * 40)) + w)] = ((float*)packed_out_1)[(((((n_c_fused >> 1) * 640) + (h * 80)) + (w * 2)) + (n_c_fused & 1))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out) {\n  output_unpack[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = packed_out[(((((((int)blockIdx.x) / 10) * 640) + ((((int)blockIdx.x) % 5) * 128)) + (((int)threadIdx.x) * 2)) + ((((int)blockIdx.x) % 10) / 5))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(packed_out: T.Buffer((4, 28, 8, 40, 2), \"float32\"), output_unpack: T.Buffer((4, 56, 8, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for n_c_fused in T.parallel(224):\n            for h, w in T.grid(8, 40):\n                output_unpack_1 = T.Buffer((71680,), data=output_unpack.data)\n                packed_out_1 = T.Buffer((71680,), data=packed_out.data)\n                output_unpack_1[n_c_fused * 320 + h * 40 + w] = packed_out_1[n_c_fused // 2 * 640 + h * 80 + w * 2 + n_c_fused % 2]",
        "packed_out": "4_28_8_40_2"
    },
    {
        "op_name": "unpack_NCHWc_to_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t packed_out_code = arg_type_ids[0];\n  int32_t output_unpack_code = arg_type_ids[1];\n  void* packed_out = (((TVMValue*)args)[0].v_handle);\n  void* output_unpack = (((TVMValue*)args)[1].v_handle);\n  void* packed_out_1 = (((DLTensor*)packed_out)[0].data);\n  void* default_function_packed_out_shape = (((DLTensor*)packed_out)[0].shape);\n  void* default_function_packed_out_strides = (((DLTensor*)packed_out)[0].strides);\n  int32_t dev_id = (((DLTensor*)packed_out)[0].device.device_id);\n  void* output_unpack_1 = (((DLTensor*)output_unpack)[0].data);\n  void* default_function_output_unpack_shape = (((DLTensor*)output_unpack)[0].shape);\n  void* default_function_output_unpack_strides = (((DLTensor*)output_unpack)[0].strides);\n  if (!(default_function_packed_out_strides == NULL)) {\n  },\n  if (!(default_function_output_unpack_strides == NULL)) {\n  },\n  for (int32_t n = 0; n < 7; ++n) {\n    for (int32_t c = 0; c < 8; ++c) {\n      for (int32_t h = 0; h < 4; ++h) {\n        for (int32_t w = 0; w < 20; ++w) {\n          int32_t cse_var_1 = (n * 640);\n          ((float*)output_unpack_1)[(((cse_var_1 + (c * 80)) + (h * 20)) + w)] = ((float*)packed_out_1)[((((cse_var_1 + ((c >> 1) * 160)) + (h * 40)) + (w * 2)) + (c & 1))];\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(14) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out);\nextern \"C\" __global__ void __launch_bounds__(14) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out) {\n  output_unpack[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] = packed_out[((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 1)) / 80) * 160) + ((((((int)blockIdx.x) * 14) + ((int)threadIdx.x)) % 80) * 2)) + ((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 1)) % 80) / 40))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(packed_out: T.Buffer((7, 4, 4, 20, 2), \"float32\"), output_unpack: T.Buffer((7, 8, 4, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for n in T.parallel(7):\n            for c, h, w in T.grid(8, 4, 20):\n                cse_var_1: T.int32 = n * 640\n                output_unpack_1 = T.Buffer((4480,), data=output_unpack.data)\n                packed_out_1 = T.Buffer((4480,), data=packed_out.data)\n                output_unpack_1[cse_var_1 + c * 80 + h * 20 + w] = packed_out_1[cse_var_1 + c // 2 * 160 + h * 40 + w * 2 + c % 2]",
        "packed_out": "7_4_4_20_2"
    },
    {
        "op_name": "unpack_NCHWc_to_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t packed_out_code = arg_type_ids[0];\n  int32_t output_unpack_code = arg_type_ids[1];\n  void* packed_out = (((TVMValue*)args)[0].v_handle);\n  void* output_unpack = (((TVMValue*)args)[1].v_handle);\n  void* packed_out_1 = (((DLTensor*)packed_out)[0].data);\n  void* default_function_packed_out_shape = (((DLTensor*)packed_out)[0].shape);\n  void* default_function_packed_out_strides = (((DLTensor*)packed_out)[0].strides);\n  int32_t dev_id = (((DLTensor*)packed_out)[0].device.device_id);\n  void* output_unpack_1 = (((DLTensor*)output_unpack)[0].data);\n  void* default_function_output_unpack_shape = (((DLTensor*)output_unpack)[0].shape);\n  void* default_function_output_unpack_strides = (((DLTensor*)output_unpack)[0].strides);\n  if (!(default_function_packed_out_strides == NULL)) {\n  },\n  if (!(default_function_output_unpack_strides == NULL)) {\n  },\n  for (int32_t n_c_fused_h_fused = 0; n_c_fused_h_fused < 2880; ++n_c_fused_h_fused) {\n    int32_t8 v_ = int32_t8((((((n_c_fused_h_fused / 72) * 576) + ((n_c_fused_h_fused % 36) * 16)) + ((n_c_fused_h_fused % 72) / 36)))+(2*0), (((((n_c_fused_h_fused / 72) * 576) + ((n_c_fused_h_fused % 36) * 16)) + ((n_c_fused_h_fused % 72) / 36)))+(2*1), (((((n_c_fused_h_fused / 72) * 576) + ((n_c_fused_h_fused % 36) * 16)) + ((n_c_fused_h_fused % 72) / 36)))+(2*2), (((((n_c_fused_h_fused / 72) * 576) + ((n_c_fused_h_fused % 36) * 16)) + ((n_c_fused_h_fused % 72) / 36)))+(2*3), (((((n_c_fused_h_fused / 72) * 576) + ((n_c_fused_h_fused % 36) * 16)) + ((n_c_fused_h_fused % 72) / 36)))+(2*4), (((((n_c_fused_h_fused / 72) * 576) + ((n_c_fused_h_fused % 36) * 16)) + ((n_c_fused_h_fused % 72) / 36)))+(2*5), (((((n_c_fused_h_fused / 72) * 576) + ((n_c_fused_h_fused % 36) * 16)) + ((n_c_fused_h_fused % 72) / 36)))+(2*6), (((((n_c_fused_h_fused / 72) * 576) + ((n_c_fused_h_fused % 36) * 16)) + ((n_c_fused_h_fused % 72) / 36)))+(2*7));\n    *(float8*)(((float*)output_unpack_1) + (n_c_fused_h_fused * 8)) = (float8(((float*)packed_out_1)[v_.s0],((float*)packed_out_1)[v_.s1],((float*)packed_out_1)[v_.s2],((float*)packed_out_1)[v_.s3],((float*)packed_out_1)[v_.s4],((float*)packed_out_1)[v_.s5],((float*)packed_out_1)[v_.s6],((float*)packed_out_1)[v_.s7]));\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out);\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out) {\n  output_unpack[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = packed_out[(((((((int)blockIdx.x) >> 4) * 576) + ((((int)blockIdx.x) & 7) * 72)) + (((int)threadIdx.x) * 2)) + ((((int)blockIdx.x) & 15) >> 3))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(packed_out: T.Buffer((5, 8, 36, 8, 2), \"float32\"), output_unpack: T.Buffer((5, 16, 36, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for n_c_fused_h_fused in T.parallel(2880):\n            output_unpack_1 = T.Buffer((23040,), data=output_unpack.data)\n            packed_out_1 = T.Buffer((23040,), data=packed_out.data)\n            output_unpack_1[n_c_fused_h_fused * 8:n_c_fused_h_fused * 8 + 8] = packed_out_1[n_c_fused_h_fused // 72 * 576 + n_c_fused_h_fused % 36 * 16 + n_c_fused_h_fused % 72 // 36:n_c_fused_h_fused // 72 * 576 + n_c_fused_h_fused % 36 * 16 + n_c_fused_h_fused % 72 // 36 + 16:2]",
        "packed_out": "5_8_36_8_2"
    },
    {
        "op_name": "unpack_NCHWc_to_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t packed_out_code = arg_type_ids[0];\n  int32_t output_unpack_code = arg_type_ids[1];\n  void* packed_out = (((TVMValue*)args)[0].v_handle);\n  void* output_unpack = (((TVMValue*)args)[1].v_handle);\n  void* packed_out_1 = (((DLTensor*)packed_out)[0].data);\n  void* default_function_packed_out_shape = (((DLTensor*)packed_out)[0].shape);\n  void* default_function_packed_out_strides = (((DLTensor*)packed_out)[0].strides);\n  int32_t dev_id = (((DLTensor*)packed_out)[0].device.device_id);\n  void* output_unpack_1 = (((DLTensor*)output_unpack)[0].data);\n  void* default_function_output_unpack_shape = (((DLTensor*)output_unpack)[0].shape);\n  void* default_function_output_unpack_strides = (((DLTensor*)output_unpack)[0].strides);\n  if (!(default_function_packed_out_strides == NULL)) {\n  },\n  if (!(default_function_output_unpack_strides == NULL)) {\n  },\n  for (int32_t n_c_fused = 0; n_c_fused < 720; ++n_c_fused) {\n    for (int32_t h = 0; h < 12; ++h) {\n      for (int32_t w = 0; w < 24; ++w) {\n        ((float*)output_unpack_1)[(((n_c_fused * 288) + (h * 24)) + w)] = ((float*)packed_out_1)[(((((n_c_fused >> 1) * 576) + (h * 48)) + (w * 2)) + (n_c_fused & 1))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out) {\n  output_unpack[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = packed_out[((((((int)blockIdx.x) / 9) * 576) + ((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 288) * 2)) + ((((((int)blockIdx.x) % 9) * 2) + (((int)threadIdx.x) >> 5)) / 9))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(packed_out: T.Buffer((9, 40, 12, 24, 2), \"float32\"), output_unpack: T.Buffer((9, 80, 12, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for n_c_fused in T.parallel(720):\n            for h, w in T.grid(12, 24):\n                output_unpack_1 = T.Buffer((207360,), data=output_unpack.data)\n                packed_out_1 = T.Buffer((207360,), data=packed_out.data)\n                output_unpack_1[n_c_fused * 288 + h * 24 + w] = packed_out_1[n_c_fused // 2 * 576 + h * 48 + w * 2 + n_c_fused % 2]",
        "packed_out": "9_40_12_24_2"
    },
    {
        "op_name": "unpack_NCHWc_to_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t packed_out_code = arg_type_ids[0];\n  int32_t output_unpack_code = arg_type_ids[1];\n  void* packed_out = (((TVMValue*)args)[0].v_handle);\n  void* output_unpack = (((TVMValue*)args)[1].v_handle);\n  void* packed_out_1 = (((DLTensor*)packed_out)[0].data);\n  void* default_function_packed_out_shape = (((DLTensor*)packed_out)[0].shape);\n  void* default_function_packed_out_strides = (((DLTensor*)packed_out)[0].strides);\n  int32_t dev_id = (((DLTensor*)packed_out)[0].device.device_id);\n  void* output_unpack_1 = (((DLTensor*)output_unpack)[0].data);\n  void* default_function_output_unpack_shape = (((DLTensor*)output_unpack)[0].shape);\n  void* default_function_output_unpack_strides = (((DLTensor*)output_unpack)[0].strides);\n  if (!(default_function_packed_out_strides == NULL)) {\n  },\n  if (!(default_function_output_unpack_strides == NULL)) {\n  },\n  for (int32_t n_c_fused = 0; n_c_fused < 80; ++n_c_fused) {\n    for (int32_t h = 0; h < 24; ++h) {\n      for (int32_t w = 0; w < 36; ++w) {\n        ((float*)output_unpack_1)[(((n_c_fused * 864) + (h * 36)) + w)] = ((float*)packed_out_1)[(((((n_c_fused >> 1) * 1728) + (h * 72)) + (w * 2)) + (n_c_fused & 1))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out) {\n  output_unpack[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = packed_out[(((((((int)blockIdx.x) / 36) * 1728) + ((((int)blockIdx.x) % 18) * 96)) + (((int)threadIdx.x) * 2)) + ((((int)blockIdx.x) % 36) / 18))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(packed_out: T.Buffer((2, 20, 24, 36, 2), \"float32\"), output_unpack: T.Buffer((2, 40, 24, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for n_c_fused in T.parallel(80):\n            for h, w in T.grid(24, 36):\n                output_unpack_1 = T.Buffer((69120,), data=output_unpack.data)\n                packed_out_1 = T.Buffer((69120,), data=packed_out.data)\n                output_unpack_1[n_c_fused * 864 + h * 36 + w] = packed_out_1[n_c_fused // 2 * 1728 + h * 72 + w * 2 + n_c_fused % 2]",
        "packed_out": "2_20_24_36_2"
    },
    {
        "op_name": "upsampling",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t resize_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* resize = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* resize_1 = (((DLTensor*)resize)[0].data);\n  void* default_function_resize_shape = (((DLTensor*)resize)[0].shape);\n  void* default_function_resize_strides = (((DLTensor*)resize)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_resize_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 896; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 96; ++i3) {\n      ((float*)resize_1)[((i0_i1_fused_i2_fused * 96) + i3)] = ((float*)data_1)[((((i0_i1_fused_i2_fused / 112) * 2688) + (((i0_i1_fused_i2_fused % 112) / 2) * 48)) + (i3 / 2))];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) / 168) * 2688) + ((((((((int)blockIdx.x) % 168) * 2) + (((int)threadIdx.x) >> 5)) / 3) / 2) * 48)) + ((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 96) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 4, 56, 48), \"float32\"), resize: T.Buffer((2, 4, 112, 96), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(896):\n            for i3 in range(96):\n                resize_1 = T.Buffer((86016,), data=resize.data)\n                data_1 = T.Buffer((21504,), data=data.data)\n                resize_1[i0_i1_fused_i2_fused * 96 + i3] = data_1[i0_i1_fused_i2_fused // 112 * 2688 + T.Div(i0_i1_fused_i2_fused % 112, 2) * 48 + T.Div(i3, 2)]",
        "data": "2_4_56_48"
    },
    {
        "op_name": "upsampling",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t resize_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* resize = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* resize_1 = (((DLTensor*)resize)[0].data);\n  void* default_function_resize_shape = (((DLTensor*)resize)[0].shape);\n  void* default_function_resize_strides = (((DLTensor*)resize)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_resize_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 252; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 64; ++i2) {\n      for (int32_t i3 = 0; i3 < 112; ++i3) {\n        ((float*)resize_1)[(((i0_i1_fused * 7168) + (i2 * 112)) + i3)] = ((float*)data_1)[(((i0_i1_fused * 1792) + ((i2 / 2) * 56)) + (i3 / 2))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ data, float* __restrict__ resize);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) >> 7) * 1792) + ((((((int)blockIdx.x) & 127) >> 1) / 2) * 56)) + ((((int)blockIdx.x) & 1) * 28)) + (((int)threadIdx.x) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 36, 32, 56), \"float32\"), resize: T.Buffer((7, 36, 64, 112), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(252):\n            for i2, i3 in T.grid(64, 112):\n                resize_1 = T.Buffer((1806336,), data=resize.data)\n                data_1 = T.Buffer((451584,), data=data.data)\n                resize_1[i0_i1_fused * 7168 + i2 * 112 + i3] = data_1[i0_i1_fused * 1792 + T.Div(i2, 2) * 56 + T.Div(i3, 2)]",
        "data": "7_36_32_56"
    },
    {
        "op_name": "upsampling",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t resize_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* resize = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* resize_1 = (((DLTensor*)resize)[0].data);\n  void* default_function_resize_shape = (((DLTensor*)resize)[0].shape);\n  void* default_function_resize_strides = (((DLTensor*)resize)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_resize_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 24; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 128; ++i2) {\n      for (int32_t i3 = 0; i3 < 96; ++i3) {\n        ((float*)resize_1)[(((i0_i1_fused * 12288) + (i2 * 96)) + i3)] = ((float*)data_1)[(((i0_i1_fused * 3072) + ((i2 / 2) * 48)) + (i3 / 2))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) / 192) * 3072) + ((((((((int)blockIdx.x) % 192) * 2) + (((int)threadIdx.x) >> 5)) / 3) / 2) * 48)) + ((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 96) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 24, 64, 48), \"float32\"), resize: T.Buffer((1, 24, 128, 96), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(24):\n            for i2, i3 in T.grid(128, 96):\n                resize_1 = T.Buffer((294912,), data=resize.data)\n                data_1 = T.Buffer((73728,), data=data.data)\n                resize_1[i0_i1_fused * 12288 + i2 * 96 + i3] = data_1[i0_i1_fused * 3072 + T.Div(i2, 2) * 48 + T.Div(i3, 2)]",
        "data": "1_24_64_48"
    },
    {
        "op_name": "upsampling",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t resize_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* resize = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* resize_1 = (((DLTensor*)resize)[0].data);\n  void* default_function_resize_shape = (((DLTensor*)resize)[0].shape);\n  void* default_function_resize_strides = (((DLTensor*)resize)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_resize_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 360; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 32; ++i2) {\n      for (int32_t i3 = 0; i3 < 144; ++i3) {\n        ((float*)resize_1)[(((i0_i1_fused * 4608) + (i2 * 144)) + i3)] = ((float*)data_1)[(((i0_i1_fused * 1152) + ((i2 / 2) * 72)) + (i3 / 2))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) / 72) * 1152) + ((((((((int)blockIdx.x) % 72) * 4) + (((int)threadIdx.x) >> 4)) / 9) / 2) * 72)) + ((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 144) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 40, 16, 72), \"float32\"), resize: T.Buffer((9, 40, 32, 144), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(360):\n            for i2, i3 in T.grid(32, 144):\n                resize_1 = T.Buffer((1658880,), data=resize.data)\n                data_1 = T.Buffer((414720,), data=data.data)\n                resize_1[i0_i1_fused * 4608 + i2 * 144 + i3] = data_1[i0_i1_fused * 1152 + T.Div(i2, 2) * 72 + T.Div(i3, 2)]",
        "data": "9_40_16_72"
    },
    {
        "op_name": "upsampling",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t resize_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* resize = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* resize_1 = (((DLTensor*)resize)[0].data);\n  void* default_function_resize_shape = (((DLTensor*)resize)[0].shape);\n  void* default_function_resize_strides = (((DLTensor*)resize)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_resize_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 140; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 48; ++i2) {\n      for (int32_t i3 = 0; i3 < 112; ++i3) {\n        ((float*)resize_1)[(((i0_i1_fused * 5376) + (i2 * 112)) + i3)] = ((float*)data_1)[(((i0_i1_fused * 1344) + ((i2 / 2) * 56)) + (i3 / 2))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) / 84) * 1344) + ((((((((int)blockIdx.x) % 84) * 4) + (((int)threadIdx.x) >> 4)) / 7) / 2) * 56)) + ((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 112) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 20, 24, 56), \"float32\"), resize: T.Buffer((7, 20, 48, 112), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(140):\n            for i2, i3 in T.grid(48, 112):\n                resize_1 = T.Buffer((752640,), data=resize.data)\n                data_1 = T.Buffer((188160,), data=data.data)\n                resize_1[i0_i1_fused * 5376 + i2 * 112 + i3] = data_1[i0_i1_fused * 1344 + T.Div(i2, 2) * 56 + T.Div(i3, 2)]",
        "data": "7_20_24_56"
    },
    {
        "op_name": "upsampling",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t resize_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* resize = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* resize_1 = (((DLTensor*)resize)[0].data);\n  void* default_function_resize_shape = (((DLTensor*)resize)[0].shape);\n  void* default_function_resize_strides = (((DLTensor*)resize)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_resize_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 168; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 80; ++i2) {\n      for (int32_t i3 = 0; i3 < 32; ++i3) {\n        ((float*)resize_1)[(((i0_i1_fused * 2560) + (i2 * 32)) + i3)] = ((float*)data_1)[(((i0_i1_fused * 640) + ((i2 / 2) * 16)) + (i3 / 2))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ resize);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 4)) / 160) * 640) + ((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 4)) % 160) >> 1) / 2) * 16)) + ((((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) & 31) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 24, 40, 16), \"float32\"), resize: T.Buffer((7, 24, 80, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(168):\n            for i2, i3 in T.grid(80, 32):\n                resize_1 = T.Buffer((430080,), data=resize.data)\n                data_1 = T.Buffer((107520,), data=data.data)\n                resize_1[i0_i1_fused * 2560 + i2 * 32 + i3] = data_1[i0_i1_fused * 640 + T.Div(i2, 2) * 16 + T.Div(i3, 2)]",
        "data": "7_24_40_16"
    },
    {
        "op_name": "upsampling",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t resize_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* resize = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* resize_1 = (((DLTensor*)resize)[0].data);\n  void* default_function_resize_shape = (((DLTensor*)resize)[0].shape);\n  void* default_function_resize_strides = (((DLTensor*)resize)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_resize_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 40960; ++i0_i1_fused_i2_fused) {\n    int32_t16 v_ = ((int32_t16)((((i0_i1_fused_i2_fused >> 7) * 512) + (((i0_i1_fused_i2_fused & 127) / 2) * 8)), (((i0_i1_fused_i2_fused >> 7) * 512) + (((i0_i1_fused_i2_fused & 127) / 2) * 8)), (((i0_i1_fused_i2_fused >> 7) * 512) + (((i0_i1_fused_i2_fused & 127) / 2) * 8)), (((i0_i1_fused_i2_fused >> 7) * 512) + (((i0_i1_fused_i2_fused & 127) / 2) * 8)), (((i0_i1_fused_i2_fused >> 7) * 512) + (((i0_i1_fused_i2_fused & 127) / 2) * 8)), (((i0_i1_fused_i2_fused >> 7) * 512) + (((i0_i1_fused_i2_fused & 127) / 2) * 8)), (((i0_i1_fused_i2_fused >> 7) * 512) + (((i0_i1_fused_i2_fused & 127) / 2) * 8)), (((i0_i1_fused_i2_fused >> 7) * 512) + (((i0_i1_fused_i2_fused & 127) / 2) * 8)), (((i0_i1_fused_i2_fused >> 7) * 512) + (((i0_i1_fused_i2_fused & 127) / 2) * 8)), (((i0_i1_fused_i2_fused >> 7) * 512) + (((i0_i1_fused_i2_fused & 127) / 2) * 8)), (((i0_i1_fused_i2_fused >> 7) * 512) + (((i0_i1_fused_i2_fused & 127) / 2) * 8)), (((i0_i1_fused_i2_fused >> 7) * 512) + (((i0_i1_fused_i2_fused & 127) / 2) * 8)), (((i0_i1_fused_i2_fused >> 7) * 512) + (((i0_i1_fused_i2_fused & 127) / 2) * 8)), (((i0_i1_fused_i2_fused >> 7) * 512) + (((i0_i1_fused_i2_fused & 127) / 2) * 8)), (((i0_i1_fused_i2_fused >> 7) * 512) + (((i0_i1_fused_i2_fused & 127) / 2) * 8)), (((i0_i1_fused_i2_fused >> 7) * 512) + (((i0_i1_fused_i2_fused & 127) / 2) * 8)))) + (int32_t16((0)+(1*0), (0)+(1*1), (0)+(1*2), (0)+(1*3), (0)+(1*4), (0)+(1*5), (0)+(1*6), (0)+(1*7), (0)+(1*8), (0)+(1*9), (0)+(1*10), (0)+(1*11), (0)+(1*12), (0)+(1*13), (0)+(1*14), (0)+(1*15)) / ((int32_t16)(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)));\n    *(float16*)(((float*)resize_1) + (i0_i1_fused_i2_fused * 16)) = (float16(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb],((float*)data_1)[v_.sc],((float*)data_1)[v_.sd],((float*)data_1)[v_.se],((float*)data_1)[v_.sf]));\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((int)blockIdx.x) * 16) + (((((int)threadIdx.x) >> 4) / 2) * 8)) + ((((int)threadIdx.x) & 15) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 40, 64, 8), \"float32\"), resize: T.Buffer((8, 40, 128, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(40960):\n            resize_1 = T.Buffer((655360,), data=resize.data)\n            data_1 = T.Buffer((163840,), data=data.data)\n            resize_1[i0_i1_fused_i2_fused * 16:i0_i1_fused_i2_fused * 16 + 16] = data_1[T.Broadcast(i0_i1_fused_i2_fused // 128 * 512 + T.Div(i0_i1_fused_i2_fused % 128, 2) * 8, 16) + T.Div(T.Ramp(0, 1, 16), T.Broadcast(2, 16))]",
        "data": "8_40_64_8"
    },
    {
        "op_name": "upsampling",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t resize_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* resize = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* resize_1 = (((DLTensor*)resize)[0].data);\n  void* default_function_resize_shape = (((DLTensor*)resize)[0].shape);\n  void* default_function_resize_strides = (((DLTensor*)resize)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_resize_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3840; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 64; ++i3) {\n      ((float*)resize_1)[((i0_i1_fused_i2_fused * 64) + i3)] = ((float*)data_1)[((((i0_i1_fused_i2_fused / 160) * 2560) + (((i0_i1_fused_i2_fused % 160) / 2) * 32)) + (i3 / 2))];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ data, float* __restrict__ resize);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 20)) >> 9) * 2560) + ((((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 2)) % 2560) >> 4) / 2) * 32)) + ((((((int)blockIdx.x) * 60) + ((int)threadIdx.x)) & 63) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 8, 80, 32), \"float32\"), resize: T.Buffer((3, 8, 160, 64), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(3840):\n            for i3 in range(64):\n                resize_1 = T.Buffer((245760,), data=resize.data)\n                data_1 = T.Buffer((61440,), data=data.data)\n                resize_1[i0_i1_fused_i2_fused * 64 + i3] = data_1[i0_i1_fused_i2_fused // 160 * 2560 + T.Div(i0_i1_fused_i2_fused % 160, 2) * 32 + T.Div(i3, 2)]",
        "data": "3_8_80_32"
    },
    {
        "op_name": "upsampling",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t resize_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* resize = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* resize_1 = (((DLTensor*)resize)[0].data);\n  void* default_function_resize_shape = (((DLTensor*)resize)[0].shape);\n  void* default_function_resize_strides = (((DLTensor*)resize)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_resize_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 10368; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 80; ++i3) {\n      ((float*)resize_1)[((i0_i1_fused_i2_fused * 80) + i3)] = ((float*)data_1)[((((i0_i1_fused_i2_fused / 96) * 1920) + (((i0_i1_fused_i2_fused % 96) / 2) * 40)) + (i3 / 2))];\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) / 120) * 1920) + ((((((((int)blockIdx.x) % 120) * 4) + (((int)threadIdx.x) >> 4)) / 5) / 2) * 40)) + ((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 80) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 36, 48, 40), \"float32\"), resize: T.Buffer((3, 36, 96, 80), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused_i2_fused in T.parallel(10368):\n            for i3 in range(80):\n                resize_1 = T.Buffer((829440,), data=resize.data)\n                data_1 = T.Buffer((207360,), data=data.data)\n                resize_1[i0_i1_fused_i2_fused * 80 + i3] = data_1[i0_i1_fused_i2_fused // 96 * 1920 + T.Div(i0_i1_fused_i2_fused % 96, 2) * 40 + T.Div(i3, 2)]",
        "data": "3_36_48_40"
    },
    {
        "op_name": "upsampling",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t resize_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* resize = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* resize_1 = (((DLTensor*)resize)[0].data);\n  void* default_function_resize_shape = (((DLTensor*)resize)[0].shape);\n  void* default_function_resize_strides = (((DLTensor*)resize)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_resize_strides == NULL)) {\n  },\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 400; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 64; ++i2) {\n      for (int32_t i3 = 0; i3 < 80; ++i3) {\n        ((float*)resize_1)[(((i0_i1_fused * 5120) + (i2 * 80)) + i3)] = ((float*)data_1)[(((i0_i1_fused * 1280) + ((i2 / 2) * 40)) + (i3 / 2))];\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) / 80) * 1280) + ((((((((int)blockIdx.x) % 80) * 4) + (((int)threadIdx.x) >> 4)) / 5) / 2) * 40)) + ((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 80) / 2))];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 40, 32, 40), \"float32\"), resize: T.Buffer((10, 40, 64, 80), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for i0_i1_fused in T.parallel(400):\n            for i2, i3 in T.grid(64, 80):\n                resize_1 = T.Buffer((2048000,), data=resize.data)\n                data_1 = T.Buffer((512000,), data=data.data)\n                resize_1[i0_i1_fused * 5120 + i2 * 80 + i3] = data_1[i0_i1_fused * 1280 + T.Div(i2, 2) * 40 + T.Div(i3, 2)]",
        "data": "10_40_32_40"
    },
    {
        "op_name": "rms_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t weight_code = arg_type_ids[1];\n  int32_t T_cast_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* weight = (((TVMValue*)args)[1].v_handle);\n  void* T_cast = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* weight_1 = (((DLTensor*)weight)[0].data);\n  void* default_function_weight_shape = (((DLTensor*)weight)[0].shape);\n  void* default_function_weight_strides = (((DLTensor*)weight)[0].strides);\n  void* T_cast_1 = (((DLTensor*)T_cast)[0].data);\n  void* default_function_T_cast_shape = (((DLTensor*)T_cast)[0].shape);\n  void* default_function_T_cast_strides = (((DLTensor*)T_cast)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_weight_strides == NULL)) {\n  },\n  if (!(default_function_T_cast_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 320; ++ax0_ax1_fused_ax2_fused) {\n    float T_multiply_red[8];\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_multiply_red[ax2] = 0.000000e+00f;\n      for (int32_t k1 = 0; k1 < 16; ++k1) {\n        int32_t cse_var_1 = (((k1 * 160) + ((ax0_ax1_fused_ax2_fused % 20) * 8)) + ax2);\n        T_multiply_red[ax2] = (T_multiply_red[ax2] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n      },\n    },\n    for (int32_t ax3_s = 0; ax3_s < 8; ++ax3_s) {\n      int32_t cse_var_2 = ((ax0_ax1_fused_ax2_fused * 8) + ax3_s);\n      ((float*)T_cast_1)[cse_var_2] = ((((float*)data_1)[cse_var_2] * ((float*)weight_1)[(ax0_ax1_fused_ax2_fused / 20)]) * (1.000000e+00f / sqrtf(((T_multiply_red[ax3_s] * 6.250000e-02f) + 1.000000e-05f))));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] * weight[(((int)blockIdx.x) >> 3)]) * (1.000000e+00f / sqrtf(((T_multiply_red[(((((int)blockIdx.x) & 7) * 20) + ((int)threadIdx.x))] * 6.250000e-02f) + 1.000000e-05f))));\n},\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 16; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + (data[(((k1 * 160) + (((int)blockIdx.x) * 4)) + ((int)threadIdx.x))] * data[(((k1 * 160) + (((int)blockIdx.x) * 4)) + ((int)threadIdx.x))]));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 16, 20, 8), \"float32\"), weight: T.Buffer((8,), \"float32\"), T_cast: T.Buffer((1, 16, 20, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(320):\n            T_multiply_red = T.allocate([8], \"float32\", \"global\")\n            T_multiply_red_1 = T.Buffer((8,), data=T_multiply_red, align=32)\n            data_1 = T.Buffer((2560,), data=data.data)\n            for ax2 in range(8):\n                T_multiply_red_1[ax2] = T.float32(0)\n                for k1 in range(16):\n                    cse_var_1: T.int32 = k1 * 160 + ax0_ax1_fused_ax2_fused % 20 * 8 + ax2\n                    T_multiply_red_1[ax2] = T_multiply_red_1[ax2] + data_1[cse_var_1] * data_1[cse_var_1]\n            for ax3_s in range(8):\n                cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused * 8 + ax3_s\n                T_cast_1 = T.Buffer((2560,), data=T_cast.data)\n                T_cast_1[cse_var_2] = data_1[cse_var_2] * weight[ax0_ax1_fused_ax2_fused // 20] * T.rsqrt(T_multiply_red_1[ax3_s] * T.float32(0.0625) + T.float32(1.0000000000000001e-05))",
        "data": "1_16_20_8",
        "weight": "8"
    },
    {
        "op_name": "rms_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t weight_code = arg_type_ids[1];\n  int32_t T_cast_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* weight = (((TVMValue*)args)[1].v_handle);\n  void* T_cast = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* weight_1 = (((DLTensor*)weight)[0].data);\n  void* default_function_weight_shape = (((DLTensor*)weight)[0].shape);\n  void* default_function_weight_strides = (((DLTensor*)weight)[0].strides);\n  void* T_cast_1 = (((DLTensor*)T_cast)[0].data);\n  void* default_function_T_cast_shape = (((DLTensor*)T_cast)[0].shape);\n  void* default_function_T_cast_strides = (((DLTensor*)T_cast)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_weight_strides == NULL)) {\n  },\n  if (!(default_function_T_cast_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    float T_multiply_red[20];\n    for (int32_t ax1 = 0; ax1 < 4; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n        for (int32_t ax2_1 = 0; ax2_1 < 20; ++ax2_1) {\n          T_multiply_red[ax2_1] = 0.000000e+00f;\n          for (int32_t k1 = 0; k1 < 4; ++k1) {\n            int32_t cse_var_1 = ((((ax0 * 960) + (k1 * 240)) + (ax2 * 20)) + ax2_1);\n            T_multiply_red[ax2_1] = (T_multiply_red[ax2_1] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n          },\n        },\n        for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n          int32_t cse_var_2 = ((((ax0 * 960) + (ax1 * 240)) + (ax2 * 20)) + ax3);\n          ((float*)T_cast_1)[cse_var_2] = ((((float*)data_1)[cse_var_2] * ((float*)weight_1)[ax1]) * (1.000000e+00f / sqrtf(((T_multiply_red[ax3] * 2.500000e-01f) + 1.000000e-05f))));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * weight[((((((int)blockIdx.x) % 30) * 2) + (((int)threadIdx.x) >> 4)) / 15)]) * (1.000000e+00f / sqrtf(((T_multiply_red[(((((int)blockIdx.x) / 30) * 240) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 240))] * 2.500000e-01f) + 1.000000e-05f))));\n},\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 4; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] + (data[(((((((int)blockIdx.x) / 5) * 960) + (k1 * 240)) + ((((int)blockIdx.x) % 5) * 48)) + ((int)threadIdx.x))] * data[(((((((int)blockIdx.x) / 5) * 960) + (k1 * 240)) + ((((int)blockIdx.x) % 5) * 48)) + ((int)threadIdx.x))]));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 4, 12, 20), \"float32\"), weight: T.Buffer((20,), \"float32\"), T_cast: T.Buffer((3, 4, 12, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(3):\n            T_multiply_red = T.allocate([20], \"float32\", \"global\")\n            for ax1, ax2 in T.grid(4, 12):\n                T_multiply_red_1 = T.Buffer((20,), data=T_multiply_red)\n                data_1 = T.Buffer((2880,), data=data.data)\n                for ax2_1 in range(20):\n                    T_multiply_red_1[ax2_1] = T.float32(0)\n                    for k1 in range(4):\n                        cse_var_1: T.int32 = ax0 * 960 + k1 * 240 + ax2 * 20 + ax2_1\n                        T_multiply_red_1[ax2_1] = T_multiply_red_1[ax2_1] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax3 in range(20):\n                    cse_var_2: T.int32 = ax0 * 960 + ax1 * 240 + ax2 * 20 + ax3\n                    T_cast_1 = T.Buffer((2880,), data=T_cast.data)\n                    T_cast_1[cse_var_2] = data_1[cse_var_2] * weight[ax1] * T.rsqrt(T_multiply_red_1[ax3] * T.float32(0.25) + T.float32(1.0000000000000001e-05))",
        "data": "3_4_12_20",
        "weight": "20"
    },
    {
        "op_name": "rms_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t weight_code = arg_type_ids[1];\n  int32_t T_cast_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* weight = (((TVMValue*)args)[1].v_handle);\n  void* T_cast = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* weight_1 = (((DLTensor*)weight)[0].data);\n  void* default_function_weight_shape = (((DLTensor*)weight)[0].shape);\n  void* default_function_weight_strides = (((DLTensor*)weight)[0].strides);\n  void* T_cast_1 = (((DLTensor*)T_cast)[0].data);\n  void* default_function_T_cast_shape = (((DLTensor*)T_cast)[0].shape);\n  void* default_function_T_cast_strides = (((DLTensor*)T_cast)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_weight_strides == NULL)) {\n  },\n  if (!(default_function_T_cast_strides == NULL)) {\n  },\n  void* T_multiply_red = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)35840, 2, 32);\n  if (T_multiply_red == NULL) {\n    return -1;\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 280; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 32; ++ax2) {\n      ((float*)T_multiply_red)[((ax0_ax1_fused * 32) + ax2)] = 0.000000e+00f;\n      for (int32_t k1 = 0; k1 < 28; ++k1) {\n        int32_t cse_var_2 = ((ax0_ax1_fused * 32) + ax2);\n        int32_t cse_var_1 = (((((ax0_ax1_fused / 40) * 35840) + (k1 * 1280)) + ((ax0_ax1_fused % 40) * 32)) + ax2);\n        ((float*)T_multiply_red)[cse_var_2] = (((float*)T_multiply_red)[cse_var_2] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n      },\n    },\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 7840; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 32; ++ax3) {\n      int32_t cse_var_3 = ((ax0_ax1_fused_ax2_fused * 32) + ax3);\n      ((float*)T_cast_1)[cse_var_3] = ((((float*)data_1)[cse_var_3] * ((float*)weight_1)[((ax0_ax1_fused_ax2_fused % 1120) / 40)]) * (1.000000e+00f / sqrtf(((((float*)T_multiply_red)[((((ax0_ax1_fused_ax2_fused / 1120) * 1280) + ((ax0_ax1_fused_ax2_fused % 40) * 32)) + ax3)] * 3.571429e-02f) + 1.000000e-05f))));\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, T_multiply_red) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight);\nextern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * weight[((((int)blockIdx.x) % 560) / 20)]) * (1.000000e+00f / sqrtf(((T_multiply_red[((((((int)blockIdx.x) / 560) * 1280) + ((((int)blockIdx.x) % 20) * 64)) + ((int)threadIdx.x))] * 3.571429e-02f) + 1.000000e-05f))));\n},\n\nextern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 28; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] + (data[((((((((int)blockIdx.x) * 7) + ((int)threadIdx.x)) / 1280) * 35840) + (k1 * 1280)) + (((((int)blockIdx.x) * 7) + ((int)threadIdx.x)) % 1280))] * data[((((((((int)blockIdx.x) * 7) + ((int)threadIdx.x)) / 1280) * 35840) + (k1 * 1280)) + (((((int)blockIdx.x) * 7) + ((int)threadIdx.x)) % 1280))]));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 28, 40, 32), \"float32\"), weight: T.Buffer((32,), \"float32\"), T_cast: T.Buffer((7, 28, 40, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        T_multiply_red = T.allocate([8960], \"float32\", \"global\")\n        T_multiply_red_1 = T.Buffer((8960,), data=T_multiply_red)\n        data_1 = T.Buffer((250880,), data=data.data)\n        for ax0_ax1_fused in T.parallel(280):\n            for ax2 in range(32):\n                T_multiply_red_1[ax0_ax1_fused * 32 + ax2] = T.float32(0)\n                for k1 in range(28):\n                    cse_var_2: T.int32 = ax0_ax1_fused * 32 + ax2\n                    cse_var_1: T.int32 = ax0_ax1_fused // 40 * 35840 + k1 * 1280 + ax0_ax1_fused % 40 * 32 + ax2\n                    T_multiply_red_1[cse_var_2] = T_multiply_red_1[cse_var_2] + data_1[cse_var_1] * data_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(7840):\n            for ax3 in range(32):\n                cse_var_3: T.int32 = ax0_ax1_fused_ax2_fused * 32 + ax3\n                T_cast_1 = T.Buffer((250880,), data=T_cast.data)\n                T_cast_1[cse_var_3] = data_1[cse_var_3] * weight[ax0_ax1_fused_ax2_fused % 1120 // 40] * T.rsqrt(T_multiply_red_1[ax0_ax1_fused_ax2_fused // 1120 * 1280 + ax0_ax1_fused_ax2_fused % 40 * 32 + ax3] * T.float32(0.035714285714285712) + T.float32(1.0000000000000001e-05))",
        "data": "7_28_40_32",
        "weight": "32"
    },
    {
        "op_name": "rms_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t weight_code = arg_type_ids[1];\n  int32_t T_cast_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* weight = (((TVMValue*)args)[1].v_handle);\n  void* T_cast = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* weight_1 = (((DLTensor*)weight)[0].data);\n  void* default_function_weight_shape = (((DLTensor*)weight)[0].shape);\n  void* default_function_weight_strides = (((DLTensor*)weight)[0].strides);\n  void* T_cast_1 = (((DLTensor*)T_cast)[0].data);\n  void* default_function_T_cast_shape = (((DLTensor*)T_cast)[0].shape);\n  void* default_function_T_cast_strides = (((DLTensor*)T_cast)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_weight_strides == NULL)) {\n  },\n  if (!(default_function_T_cast_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2016; ++ax0_ax1_fused_ax2_fused) {\n    float T_multiply_red[8];\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_multiply_red[ax2] = 0.000000e+00f;\n      for (int32_t k1 = 0; k1 < 24; ++k1) {\n        int32_t cse_var_1 = (((((ax0_ax1_fused_ax2_fused / 288) * 2304) + (k1 * 96)) + ((ax0_ax1_fused_ax2_fused % 12) * 8)) + ax2);\n        T_multiply_red[ax2] = (T_multiply_red[ax2] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n      },\n    },\n    for (int32_t ax3_s = 0; ax3_s < 8; ++ax3_s) {\n      int32_t cse_var_2 = ((ax0_ax1_fused_ax2_fused * 8) + ax3_s);\n      ((float*)T_cast_1)[cse_var_2] = ((((float*)data_1)[cse_var_2] * ((float*)weight_1)[((ax0_ax1_fused_ax2_fused % 288) / 12)]) * (1.000000e+00f / sqrtf(((T_multiply_red[ax3_s] * 4.166667e-02f) + 1.000000e-05f))));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] * weight[((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 3)) % 288) / 12)]) * (1.000000e+00f / sqrtf(((T_multiply_red[((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 3)) / 288) * 96) + ((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 3)) % 12) * 8)) + (((int)threadIdx.x) & 7))] * 4.166667e-02f) + 1.000000e-05f))));\n},\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 24; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + (data[(((((((int)blockIdx.x) / 12) * 2304) + (k1 * 96)) + ((((int)blockIdx.x) % 12) * 8)) + ((int)threadIdx.x))] * data[(((((((int)blockIdx.x) / 12) * 2304) + (k1 * 96)) + ((((int)blockIdx.x) % 12) * 8)) + ((int)threadIdx.x))]));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 24, 12, 8), \"float32\"), weight: T.Buffer((8,), \"float32\"), T_cast: T.Buffer((7, 24, 12, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2016):\n            T_multiply_red = T.allocate([8], \"float32\", \"global\")\n            T_multiply_red_1 = T.Buffer((8,), data=T_multiply_red, align=32)\n            data_1 = T.Buffer((16128,), data=data.data)\n            for ax2 in range(8):\n                T_multiply_red_1[ax2] = T.float32(0)\n                for k1 in range(24):\n                    cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused // 288 * 2304 + k1 * 96 + ax0_ax1_fused_ax2_fused % 12 * 8 + ax2\n                    T_multiply_red_1[ax2] = T_multiply_red_1[ax2] + data_1[cse_var_1] * data_1[cse_var_1]\n            for ax3_s in range(8):\n                cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused * 8 + ax3_s\n                T_cast_1 = T.Buffer((16128,), data=T_cast.data)\n                T_cast_1[cse_var_2] = data_1[cse_var_2] * weight[ax0_ax1_fused_ax2_fused % 288 // 12] * T.rsqrt(T_multiply_red_1[ax3_s] * T.float32(0.041666666666666664) + T.float32(1.0000000000000001e-05))",
        "data": "7_24_12_8",
        "weight": "8"
    },
    {
        "op_name": "rms_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t weight_code = arg_type_ids[1];\n  int32_t T_cast_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* weight = (((TVMValue*)args)[1].v_handle);\n  void* T_cast = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* weight_1 = (((DLTensor*)weight)[0].data);\n  void* default_function_weight_shape = (((DLTensor*)weight)[0].shape);\n  void* default_function_weight_strides = (((DLTensor*)weight)[0].strides);\n  void* T_cast_1 = (((DLTensor*)T_cast)[0].data);\n  void* default_function_T_cast_shape = (((DLTensor*)T_cast)[0].shape);\n  void* default_function_T_cast_strides = (((DLTensor*)T_cast)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_weight_strides == NULL)) {\n  },\n  if (!(default_function_T_cast_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 20; ++ax0_ax1_fused) {\n    float T_multiply_red[12];\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      for (int32_t ax2_1 = 0; ax2_1 < 12; ++ax2_1) {\n        T_multiply_red[ax2_1] = 0.000000e+00f;\n        for (int32_t k1 = 0; k1 < 4; ++k1) {\n          int32_t cse_var_1 = (((((ax0_ax1_fused >> 2) * 192) + (k1 * 48)) + (ax2 * 12)) + ax2_1);\n          T_multiply_red[ax2_1] = (T_multiply_red[ax2_1] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n        },\n      },\n      for (int32_t ax3_s = 0; ax3_s < 12; ++ax3_s) {\n        int32_t cse_var_2 = (((ax0_ax1_fused * 48) + (ax2 * 12)) + ax3_s);\n        ((float*)T_cast_1)[cse_var_2] = ((((float*)data_1)[cse_var_2] * ((float*)weight_1)[(ax0_ax1_fused & 3)]) * (1.000000e+00f / sqrtf(((T_multiply_red[ax3_s] * 2.500000e-01f) + 1.000000e-05f))));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * weight[((((int)blockIdx.x) % 24) / 6)]) * (1.000000e+00f / sqrtf(((T_multiply_red[((((((int)blockIdx.x) / 24) * 48) + ((((int)blockIdx.x) % 6) * 8)) + ((int)threadIdx.x))] * 2.500000e-01f) + 1.000000e-05f))));\n},\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 4; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] + (data[(((((int)blockIdx.x) * 192) + (k1 * 48)) + ((int)threadIdx.x))] * data[(((((int)blockIdx.x) * 192) + (k1 * 48)) + ((int)threadIdx.x))]));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 4, 4, 12), \"float32\"), weight: T.Buffer((12,), \"float32\"), T_cast: T.Buffer((5, 4, 4, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(20):\n            T_multiply_red = T.allocate([12], \"float32\", \"global\")\n            for ax2 in range(4):\n                T_multiply_red_1 = T.Buffer((12,), data=T_multiply_red, align=32)\n                data_1 = T.Buffer((960,), data=data.data)\n                for ax2_1 in range(12):\n                    T_multiply_red_1[ax2_1] = T.float32(0)\n                    for k1 in range(4):\n                        cse_var_1: T.int32 = ax0_ax1_fused // 4 * 192 + k1 * 48 + ax2 * 12 + ax2_1\n                        T_multiply_red_1[ax2_1] = T_multiply_red_1[ax2_1] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax3_s in range(12):\n                    cse_var_2: T.int32 = ax0_ax1_fused * 48 + ax2 * 12 + ax3_s\n                    T_cast_1 = T.Buffer((960,), data=T_cast.data)\n                    T_cast_1[cse_var_2] = data_1[cse_var_2] * weight[ax0_ax1_fused % 4] * T.rsqrt(T_multiply_red_1[ax3_s] * T.float32(0.25) + T.float32(1.0000000000000001e-05))",
        "data": "5_4_4_12",
        "weight": "12"
    },
    {
        "op_name": "rms_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t weight_code = arg_type_ids[1];\n  int32_t T_cast_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* weight = (((TVMValue*)args)[1].v_handle);\n  void* T_cast = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* weight_1 = (((DLTensor*)weight)[0].data);\n  void* default_function_weight_shape = (((DLTensor*)weight)[0].shape);\n  void* default_function_weight_strides = (((DLTensor*)weight)[0].strides);\n  void* T_cast_1 = (((DLTensor*)T_cast)[0].data);\n  void* default_function_T_cast_shape = (((DLTensor*)T_cast)[0].shape);\n  void* default_function_T_cast_strides = (((DLTensor*)T_cast)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_weight_strides == NULL)) {\n  },\n  if (!(default_function_T_cast_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 5760; ++ax0_ax1_fused_ax2_fused) {\n    float T_multiply_red[8];\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_multiply_red[ax2] = 0.000000e+00f;\n      for (int32_t k1 = 0; k1 < 36; ++k1) {\n        int32_t cse_var_1 = (((((ax0_ax1_fused_ax2_fused / 720) * 5760) + (k1 * 160)) + ((ax0_ax1_fused_ax2_fused % 20) * 8)) + ax2);\n        T_multiply_red[ax2] = (T_multiply_red[ax2] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n      },\n    },\n    for (int32_t ax3_s = 0; ax3_s < 8; ++ax3_s) {\n      int32_t cse_var_2 = ((ax0_ax1_fused_ax2_fused * 8) + ax3_s);\n      ((float*)T_cast_1)[cse_var_2] = ((((float*)data_1)[cse_var_2] * ((float*)weight_1)[((ax0_ax1_fused_ax2_fused % 720) / 20)]) * (1.000000e+00f / sqrtf(((T_multiply_red[ax3_s] * 2.777778e-02f) + 1.000000e-05f))));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * weight[((((((int)blockIdx.x) % 90) * 2) + (((int)threadIdx.x) >> 5)) / 5)]) * (1.000000e+00f / sqrtf(((T_multiply_red[((((((int)blockIdx.x) / 90) * 160) + ((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) % 20) * 8)) + (((int)threadIdx.x) & 7))] * 2.777778e-02f) + 1.000000e-05f))));\n},\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 36; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + (data[(((((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5)) / 5) * 5760) + (k1 * 160)) + ((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) % 20) * 8)) + (((int)threadIdx.x) & 7))] * data[(((((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5)) / 5) * 5760) + (k1 * 160)) + ((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) % 20) * 8)) + (((int)threadIdx.x) & 7))]));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 36, 20, 8), \"float32\"), weight: T.Buffer((8,), \"float32\"), T_cast: T.Buffer((8, 36, 20, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(5760):\n            T_multiply_red = T.allocate([8], \"float32\", \"global\")\n            T_multiply_red_1 = T.Buffer((8,), data=T_multiply_red, align=32)\n            data_1 = T.Buffer((46080,), data=data.data)\n            for ax2 in range(8):\n                T_multiply_red_1[ax2] = T.float32(0)\n                for k1 in range(36):\n                    cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused // 720 * 5760 + k1 * 160 + ax0_ax1_fused_ax2_fused % 20 * 8 + ax2\n                    T_multiply_red_1[ax2] = T_multiply_red_1[ax2] + data_1[cse_var_1] * data_1[cse_var_1]\n            for ax3_s in range(8):\n                cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused * 8 + ax3_s\n                T_cast_1 = T.Buffer((46080,), data=T_cast.data)\n                T_cast_1[cse_var_2] = data_1[cse_var_2] * weight[ax0_ax1_fused_ax2_fused % 720 // 20] * T.rsqrt(T_multiply_red_1[ax3_s] * T.float32(0.027777777777777776) + T.float32(1.0000000000000001e-05))",
        "data": "8_36_20_8",
        "weight": "8"
    },
    {
        "op_name": "rms_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t weight_code = arg_type_ids[1];\n  int32_t T_cast_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* weight = (((TVMValue*)args)[1].v_handle);\n  void* T_cast = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* weight_1 = (((DLTensor*)weight)[0].data);\n  void* default_function_weight_shape = (((DLTensor*)weight)[0].shape);\n  void* default_function_weight_strides = (((DLTensor*)weight)[0].strides);\n  void* T_cast_1 = (((DLTensor*)T_cast)[0].data);\n  void* default_function_T_cast_shape = (((DLTensor*)T_cast)[0].shape);\n  void* default_function_T_cast_strides = (((DLTensor*)T_cast)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_weight_strides == NULL)) {\n  },\n  if (!(default_function_T_cast_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 84; ++ax0_ax1_fused) {\n    void* T_multiply_red = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)2560, 2, 32);\n    if (T_multiply_red == NULL) {\n      return -1;\n    },\n    for (int32_t ax1 = 0; ax1 < 40; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n        ((float*)T_multiply_red)[((ax1 * 16) + ax2)] = 0.000000e+00f;\n        for (int32_t k1 = 0; k1 < 12; ++k1) {\n          int32_t cse_var_3 = (ax1 * 16);\n          int32_t cse_var_2 = (cse_var_3 + ax2);\n          int32_t cse_var_1 = (((((ax0_ax1_fused / 12) * 7680) + (k1 * 640)) + cse_var_3) + ax2);\n          ((float*)T_multiply_red)[cse_var_2] = (((float*)T_multiply_red)[cse_var_2] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n        },\n      },\n    },\n    for (int32_t ax2_1 = 0; ax2_1 < 40; ++ax2_1) {\n      for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n        int32_t cse_var_5 = (ax2_1 * 16);\n        int32_t cse_var_4 = (((ax0_ax1_fused * 640) + cse_var_5) + ax3);\n        ((float*)T_cast_1)[cse_var_4] = ((((float*)data_1)[cse_var_4] * ((float*)weight_1)[(ax0_ax1_fused % 12)]) * (1.000000e+00f / sqrtf(((((float*)T_multiply_red)[(cse_var_5 + ax3)] * 8.333333e-02f) + 1.000000e-05f))));\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, T_multiply_red) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] * weight[((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 3)) % 960) / 80)]) * (1.000000e+00f / sqrtf(((T_multiply_red[(((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 3)) / 960) * 640) + (((((int)blockIdx.x) * 56) + ((int)threadIdx.x)) % 640))] * 8.333333e-02f) + 1.000000e-05f))));\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 12; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + (data[(((((((int)blockIdx.x) / 20) * 7680) + (k1 * 640)) + ((((int)blockIdx.x) % 20) * 32)) + ((int)threadIdx.x))] * data[(((((((int)blockIdx.x) / 20) * 7680) + (k1 * 640)) + ((((int)blockIdx.x) % 20) * 32)) + ((int)threadIdx.x))]));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 12, 40, 16), \"float32\"), weight: T.Buffer((16,), \"float32\"), T_cast: T.Buffer((7, 12, 40, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(84):\n            T_multiply_red = T.allocate([640], \"float32\", \"global\")\n            T_multiply_red_1 = T.Buffer((640,), data=T_multiply_red)\n            data_1 = T.Buffer((53760,), data=data.data)\n            for ax1, ax2 in T.grid(40, 16):\n                T_multiply_red_1[ax1 * 16 + ax2] = T.float32(0)\n                for k1 in range(12):\n                    cse_var_3: T.int32 = ax1 * 16\n                    cse_var_2: T.int32 = cse_var_3 + ax2\n                    cse_var_1: T.int32 = ax0_ax1_fused // 12 * 7680 + k1 * 640 + cse_var_3 + ax2\n                    T_multiply_red_1[cse_var_2] = T_multiply_red_1[cse_var_2] + data_1[cse_var_1] * data_1[cse_var_1]\n            for ax2, ax3 in T.grid(40, 16):\n                cse_var_5: T.int32 = ax2 * 16\n                cse_var_4: T.int32 = ax0_ax1_fused * 640 + cse_var_5 + ax3\n                T_cast_1 = T.Buffer((53760,), data=T_cast.data)\n                T_cast_1[cse_var_4] = data_1[cse_var_4] * weight[ax0_ax1_fused % 12] * T.rsqrt(T_multiply_red_1[cse_var_5 + ax3] * T.float32(0.083333333333333329) + T.float32(1.0000000000000001e-05))",
        "data": "7_12_40_16",
        "weight": "16"
    },
    {
        "op_name": "rms_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t weight_code = arg_type_ids[1];\n  int32_t T_cast_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* weight = (((TVMValue*)args)[1].v_handle);\n  void* T_cast = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* weight_1 = (((DLTensor*)weight)[0].data);\n  void* default_function_weight_shape = (((DLTensor*)weight)[0].shape);\n  void* default_function_weight_strides = (((DLTensor*)weight)[0].strides);\n  void* T_cast_1 = (((DLTensor*)T_cast)[0].data);\n  void* default_function_T_cast_shape = (((DLTensor*)T_cast)[0].shape);\n  void* default_function_T_cast_strides = (((DLTensor*)T_cast)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_weight_strides == NULL)) {\n  },\n  if (!(default_function_T_cast_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1152; ++ax0_ax1_fused_ax2_fused) {\n    float T_multiply_red[36];\n    for (int32_t ax2 = 0; ax2 < 36; ++ax2) {\n      T_multiply_red[ax2] = 0.000000e+00f;\n      for (int32_t k1 = 0; k1 < 12; ++k1) {\n        int32_t cse_var_1 = (((((ax0_ax1_fused_ax2_fused / 192) * 6912) + (k1 * 576)) + ((ax0_ax1_fused_ax2_fused & 15) * 36)) + ax2);\n        T_multiply_red[ax2] = (T_multiply_red[ax2] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n      },\n    },\n    for (int32_t ax3 = 0; ax3 < 36; ++ax3) {\n      int32_t cse_var_2 = ((ax0_ax1_fused_ax2_fused * 36) + ax3);\n      ((float*)T_cast_1)[cse_var_2] = ((((float*)data_1)[cse_var_2] * ((float*)weight_1)[((ax0_ax1_fused_ax2_fused % 192) >> 4)]) * (1.000000e+00f / sqrtf(((T_multiply_red[ax3] * 8.333333e-02f) + 1.000000e-05f))));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] * weight[((((int)blockIdx.x) % 384) >> 5)]) * (1.000000e+00f / sqrtf(((T_multiply_red[((((((int)blockIdx.x) / 384) * 576) + ((((int)blockIdx.x) & 31) * 18)) + ((int)threadIdx.x))] * 8.333333e-02f) + 1.000000e-05f))));\n},\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 12; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + (data[(((((((int)blockIdx.x) / 18) * 6912) + (k1 * 576)) + ((((int)blockIdx.x) % 18) * 32)) + ((int)threadIdx.x))] * data[(((((((int)blockIdx.x) / 18) * 6912) + (k1 * 576)) + ((((int)blockIdx.x) % 18) * 32)) + ((int)threadIdx.x))]));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 12, 16, 36), \"float32\"), weight: T.Buffer((36,), \"float32\"), T_cast: T.Buffer((6, 12, 16, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1152):\n            T_multiply_red = T.allocate([36], \"float32\", \"global\")\n            T_multiply_red_1 = T.Buffer((36,), data=T_multiply_red)\n            data_1 = T.Buffer((41472,), data=data.data)\n            for ax2 in range(36):\n                T_multiply_red_1[ax2] = T.float32(0)\n                for k1 in range(12):\n                    cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused // 192 * 6912 + k1 * 576 + ax0_ax1_fused_ax2_fused % 16 * 36 + ax2\n                    T_multiply_red_1[ax2] = T_multiply_red_1[ax2] + data_1[cse_var_1] * data_1[cse_var_1]\n            for ax3 in range(36):\n                cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused * 36 + ax3\n                T_cast_1 = T.Buffer((41472,), data=T_cast.data)\n                T_cast_1[cse_var_2] = data_1[cse_var_2] * weight[ax0_ax1_fused_ax2_fused % 192 // 16] * T.rsqrt(T_multiply_red_1[ax3] * T.float32(0.083333333333333329) + T.float32(1.0000000000000001e-05))",
        "data": "6_12_16_36",
        "weight": "36"
    },
    {
        "op_name": "rms_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t weight_code = arg_type_ids[1];\n  int32_t T_cast_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* weight = (((TVMValue*)args)[1].v_handle);\n  void* T_cast = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* weight_1 = (((DLTensor*)weight)[0].data);\n  void* default_function_weight_shape = (((DLTensor*)weight)[0].shape);\n  void* default_function_weight_strides = (((DLTensor*)weight)[0].strides);\n  void* T_cast_1 = (((DLTensor*)T_cast)[0].data);\n  void* default_function_T_cast_shape = (((DLTensor*)T_cast)[0].shape);\n  void* default_function_T_cast_strides = (((DLTensor*)T_cast)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_weight_strides == NULL)) {\n  },\n  if (!(default_function_T_cast_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    void* T_multiply_red = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)2304, 2, 32);\n    if (T_multiply_red == NULL) {\n      return -1;\n    },\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 36; ++ax2) {\n        ((float*)T_multiply_red)[((ax1 * 36) + ax2)] = 0.000000e+00f;\n        for (int32_t k1 = 0; k1 < 20; ++k1) {\n          int32_t cse_var_3 = (ax1 * 36);\n          int32_t cse_var_2 = (cse_var_3 + ax2);\n          int32_t cse_var_1 = ((((ax0 * 11520) + (k1 * 576)) + cse_var_3) + ax2);\n          ((float*)T_multiply_red)[cse_var_2] = (((float*)T_multiply_red)[cse_var_2] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n        },\n      },\n    },\n    for (int32_t ax1_1 = 0; ax1_1 < 20; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 16; ++ax2_1) {\n        for (int32_t ax3 = 0; ax3 < 36; ++ax3) {\n          int32_t cse_var_5 = (ax2_1 * 36);\n          int32_t cse_var_4 = ((((ax0 * 11520) + (ax1_1 * 576)) + cse_var_5) + ax3);\n          ((float*)T_cast_1)[cse_var_4] = ((((float*)data_1)[cse_var_4] * ((float*)weight_1)[ax1_1]) * (1.000000e+00f / sqrtf(((((float*)T_multiply_red)[(cse_var_5 + ax3)] * 5.000000e-02f) + 1.000000e-05f))));\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, T_multiply_red) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * weight[((((int)blockIdx.x) % 180) / 9)]) * (1.000000e+00f / sqrtf(((T_multiply_red[((((((int)blockIdx.x) / 180) * 576) + ((((int)blockIdx.x) % 9) * 64)) + ((int)threadIdx.x))] * 5.000000e-02f) + 1.000000e-05f))));\n},\n\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 20; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] + (data[((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3)) / 72) * 11520) + (k1 * 576)) + (((((int)blockIdx.x) * 40) + ((int)threadIdx.x)) % 576))] * data[((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3)) / 72) * 11520) + (k1 * 576)) + (((((int)blockIdx.x) * 40) + ((int)threadIdx.x)) % 576))]));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 20, 16, 36), \"float32\"), weight: T.Buffer((36,), \"float32\"), T_cast: T.Buffer((10, 20, 16, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(10):\n            T_multiply_red = T.allocate([576], \"float32\", \"global\")\n            T_multiply_red_1 = T.Buffer((576,), data=T_multiply_red)\n            data_1 = T.Buffer((115200,), data=data.data)\n            for ax1, ax2 in T.grid(16, 36):\n                T_multiply_red_1[ax1 * 36 + ax2] = T.float32(0)\n                for k1 in range(20):\n                    cse_var_3: T.int32 = ax1 * 36\n                    cse_var_2: T.int32 = cse_var_3 + ax2\n                    cse_var_1: T.int32 = ax0 * 11520 + k1 * 576 + cse_var_3 + ax2\n                    T_multiply_red_1[cse_var_2] = T_multiply_red_1[cse_var_2] + data_1[cse_var_1] * data_1[cse_var_1]\n            for ax1, ax2, ax3 in T.grid(20, 16, 36):\n                cse_var_5: T.int32 = ax2 * 36\n                cse_var_4: T.int32 = ax0 * 11520 + ax1 * 576 + cse_var_5 + ax3\n                T_cast_1 = T.Buffer((115200,), data=T_cast.data)\n                T_cast_1[cse_var_4] = data_1[cse_var_4] * weight[ax1] * T.rsqrt(T_multiply_red_1[cse_var_5 + ax3] * T.float32(0.050000000000000003) + T.float32(1.0000000000000001e-05))",
        "data": "10_20_16_36",
        "weight": "36"
    },
    {
        "op_name": "rms_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t weight_code = arg_type_ids[1];\n  int32_t T_cast_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* weight = (((TVMValue*)args)[1].v_handle);\n  void* T_cast = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* weight_1 = (((DLTensor*)weight)[0].data);\n  void* default_function_weight_shape = (((DLTensor*)weight)[0].shape);\n  void* default_function_weight_strides = (((DLTensor*)weight)[0].strides);\n  void* T_cast_1 = (((DLTensor*)T_cast)[0].data);\n  void* default_function_T_cast_shape = (((DLTensor*)T_cast)[0].shape);\n  void* default_function_T_cast_strides = (((DLTensor*)T_cast)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_weight_strides == NULL)) {\n  },\n  if (!(default_function_T_cast_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 128; ++ax0_ax1_fused) {\n    float T_multiply_red[4];\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax2_1 = 0; ax2_1 < 4; ++ax2_1) {\n        T_multiply_red[ax2_1] = 0.000000e+00f;\n        for (int32_t k1 = 0; k1 < 32; ++k1) {\n          int32_t cse_var_1 = (((((ax0_ax1_fused >> 5) * 1024) + (k1 * 32)) + (ax2 * 4)) + ax2_1);\n          T_multiply_red[ax2_1] = (T_multiply_red[ax2_1] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n        },\n      },\n      for (int32_t ax3_s = 0; ax3_s < 4; ++ax3_s) {\n        int32_t cse_var_2 = (((ax0_ax1_fused * 32) + (ax2 * 4)) + ax3_s);\n        ((float*)T_cast_1)[cse_var_2] = ((((float*)data_1)[cse_var_2] * ((float*)weight_1)[(ax0_ax1_fused & 31)]) * (1.000000e+00f / sqrtf(((T_multiply_red[ax3_s] * 3.125000e-02f) + 1.000000e-05f))));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * weight[(((((int)blockIdx.x) & 15) * 2) + (((int)threadIdx.x) >> 5))]) * (1.000000e+00f / sqrtf(((T_multiply_red[(((((int)blockIdx.x) >> 4) * 32) + (((int)threadIdx.x) & 31))] * 3.125000e-02f) + 1.000000e-05f))));\n},\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 32; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + (data[(((((((int)blockIdx.x) >> 2) * 1024) + (k1 * 32)) + ((((int)blockIdx.x) & 3) * 8)) + ((int)threadIdx.x))] * data[(((((((int)blockIdx.x) >> 2) * 1024) + (k1 * 32)) + ((((int)blockIdx.x) & 3) * 8)) + ((int)threadIdx.x))]));\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 32, 8, 4), \"float32\"), weight: T.Buffer((4,), \"float32\"), T_cast: T.Buffer((4, 32, 8, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(128):\n            T_multiply_red = T.allocate([4], \"float32\", \"global\")\n            for ax2 in range(8):\n                T_multiply_red_1 = T.Buffer((4,), data=T_multiply_red, align=16)\n                data_1 = T.Buffer((4096,), data=data.data)\n                for ax2_1 in range(4):\n                    T_multiply_red_1[ax2_1] = T.float32(0)\n                    for k1 in range(32):\n                        cse_var_1: T.int32 = ax0_ax1_fused // 32 * 1024 + k1 * 32 + ax2 * 4 + ax2_1\n                        T_multiply_red_1[ax2_1] = T_multiply_red_1[ax2_1] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax3_s in range(4):\n                    cse_var_2: T.int32 = ax0_ax1_fused * 32 + ax2 * 4 + ax3_s\n                    T_cast_1 = T.Buffer((4096,), data=T_cast.data)\n                    T_cast_1[cse_var_2] = data_1[cse_var_2] * weight[ax0_ax1_fused % 32] * T.rsqrt(T_multiply_red_1[ax3_s] * T.float32(0.03125) + T.float32(1.0000000000000001e-05))",
        "data": "4_32_8_4",
        "weight": "4"
    },
    {
        "op_name": "batch_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t gamma_code = arg_type_ids[1];\n  int32_t beta_code = arg_type_ids[2];\n  int32_t moving_mean_code = arg_type_ids[3];\n  int32_t moving_var_code = arg_type_ids[4];\n  int32_t T_divide_code = arg_type_ids[5];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* gamma = (((TVMValue*)args)[1].v_handle);\n  void* beta = (((TVMValue*)args)[2].v_handle);\n  void* moving_mean = (((TVMValue*)args)[3].v_handle);\n  void* moving_var = (((TVMValue*)args)[4].v_handle);\n  void* T_divide = (((TVMValue*)args)[5].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* gamma_1 = (((DLTensor*)gamma)[0].data);\n  void* default_function_gamma_shape = (((DLTensor*)gamma)[0].shape);\n  void* default_function_gamma_strides = (((DLTensor*)gamma)[0].strides);\n  void* beta_1 = (((DLTensor*)beta)[0].data);\n  void* default_function_beta_shape = (((DLTensor*)beta)[0].shape);\n  void* default_function_beta_strides = (((DLTensor*)beta)[0].strides);\n  void* moving_mean_1 = (((DLTensor*)moving_mean)[0].data);\n  void* default_function_moving_mean_shape = (((DLTensor*)moving_mean)[0].shape);\n  void* default_function_moving_mean_strides = (((DLTensor*)moving_mean)[0].strides);\n  void* moving_var_1 = (((DLTensor*)moving_var)[0].data);\n  void* default_function_moving_var_shape = (((DLTensor*)moving_var)[0].shape);\n  void* default_function_moving_var_strides = (((DLTensor*)moving_var)[0].strides);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_gamma_strides == NULL)) {\n  },\n  if (!(default_function_beta_strides == NULL)) {\n  },\n  if (!(default_function_moving_mean_strides == NULL)) {\n  },\n  if (!(default_function_moving_var_strides == NULL)) {\n  },\n  if (!(default_function_T_divide_strides == NULL)) {\n  },\n  float T_reshape[28];\n  for (int32_t ax1 = 0; ax1 < 28; ++ax1) {\n    T_reshape[ax1] = ((float*)moving_mean_1)[ax1];\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1792; ++ax0_ax1_fused_ax2_fused) {\n    float T_reshape_1[1];\n    for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n      int32_t cse_var_2 = ((ax0_ax1_fused_ax2_fused % 896) >> 5);\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 20) + ax3);\n      T_reshape_1[0] = ((float*)moving_var_1)[cse_var_2];\n      ((float*)T_divide_1)[cse_var_1] = ((((float*)data_1)[cse_var_1] - T_reshape[cse_var_2]) / sqrtf((T_reshape_1[0] + 1.000000e-05f)));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - moving_mean[((((((int)blockIdx.x) % 320) * 7) + (((int)threadIdx.x) >> 3)) / 80)]) / sqrtf((moving_var[((((((int)blockIdx.x) % 320) * 7) + (((int)threadIdx.x) >> 3)) / 80)] + 1.000000e-05f)));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 28, 32, 20), \"float32\"), gamma: T.Buffer((28,), \"float32\"), beta: T.Buffer((28,), \"float32\"), moving_mean: T.Buffer((28,), \"float32\"), moving_var: T.Buffer((28,), \"float32\"), T_divide: T.Buffer((2, 28, 32, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        T_reshape = T.allocate([28], \"float32\", \"global\")\n        T_reshape_1 = T.Buffer((28,), data=T_reshape)\n        for ax1 in range(28):\n            T_reshape_1[ax1] = moving_mean[ax1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(1792):\n            T_reshape_2 = T.allocate([1], \"float32\", \"global\")\n            for ax3 in range(20):\n                cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused % 896 // 32\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 20 + ax3\n                T_reshape_3 = T.Buffer((1,), data=T_reshape_2, align=4)\n                T_reshape_3[0] = moving_var[cse_var_2]\n                T_divide_1 = T.Buffer((35840,), data=T_divide.data)\n                data_1 = T.Buffer((35840,), data=data.data)\n                T_divide_1[cse_var_1] = (data_1[cse_var_1] - T_reshape_1[cse_var_2]) / T.sqrt(T_reshape_3[0] + T.float32(1.0000000000000001e-05))",
        "data": "2_28_32_20",
        "gamma": "28",
        "beta": "28",
        "moving_mean": "28",
        "moving_var": "28"
    },
    {
        "op_name": "batch_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t gamma_code = arg_type_ids[1];\n  int32_t beta_code = arg_type_ids[2];\n  int32_t moving_mean_code = arg_type_ids[3];\n  int32_t moving_var_code = arg_type_ids[4];\n  int32_t T_divide_code = arg_type_ids[5];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* gamma = (((TVMValue*)args)[1].v_handle);\n  void* beta = (((TVMValue*)args)[2].v_handle);\n  void* moving_mean = (((TVMValue*)args)[3].v_handle);\n  void* moving_var = (((TVMValue*)args)[4].v_handle);\n  void* T_divide = (((TVMValue*)args)[5].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* gamma_1 = (((DLTensor*)gamma)[0].data);\n  void* default_function_gamma_shape = (((DLTensor*)gamma)[0].shape);\n  void* default_function_gamma_strides = (((DLTensor*)gamma)[0].strides);\n  void* beta_1 = (((DLTensor*)beta)[0].data);\n  void* default_function_beta_shape = (((DLTensor*)beta)[0].shape);\n  void* default_function_beta_strides = (((DLTensor*)beta)[0].strides);\n  void* moving_mean_1 = (((DLTensor*)moving_mean)[0].data);\n  void* default_function_moving_mean_shape = (((DLTensor*)moving_mean)[0].shape);\n  void* default_function_moving_mean_strides = (((DLTensor*)moving_mean)[0].strides);\n  void* moving_var_1 = (((DLTensor*)moving_var)[0].data);\n  void* default_function_moving_var_shape = (((DLTensor*)moving_var)[0].shape);\n  void* default_function_moving_var_strides = (((DLTensor*)moving_var)[0].strides);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_gamma_strides == NULL)) {\n  },\n  if (!(default_function_beta_strides == NULL)) {\n  },\n  if (!(default_function_moving_mean_strides == NULL)) {\n  },\n  if (!(default_function_moving_var_strides == NULL)) {\n  },\n  if (!(default_function_T_divide_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    float T_reshape[1];\n    float T_reshape_1[1];\n    for (int32_t ax1 = 0; ax1 < 28; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 40; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 12; ++ax3) {\n          int32_t cse_var_1 = ((((ax0 * 13440) + (ax1 * 480)) + (ax2 * 12)) + ax3);\n          T_reshape[0] = ((float*)moving_mean_1)[ax1];\n          T_reshape_1[0] = ((float*)moving_var_1)[ax1];\n          ((float*)T_divide_1)[cse_var_1] = ((((float*)data_1)[cse_var_1] - T_reshape[0]) / sqrtf((T_reshape_1[0] + 1.000000e-05f)));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - moving_mean[((((int)blockIdx.x) % 336) / 12)]) / sqrtf((moving_var[((((int)blockIdx.x) % 336) / 12)] + 1.000000e-05f)));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 28, 40, 12), \"float32\"), gamma: T.Buffer((28,), \"float32\"), beta: T.Buffer((28,), \"float32\"), moving_mean: T.Buffer((28,), \"float32\"), moving_var: T.Buffer((28,), \"float32\"), T_divide: T.Buffer((8, 28, 40, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(8):\n            T_reshape = T.allocate([1], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n            for ax1, ax2, ax3 in T.grid(28, 40, 12):\n                cse_var_1: T.int32 = ax0 * 13440 + ax1 * 480 + ax2 * 12 + ax3\n                T_reshape_2 = T.Buffer((1,), data=T_reshape, align=4)\n                T_reshape_2[0] = moving_mean[ax1]\n                T_reshape_3 = T.Buffer((1,), data=T_reshape_1, align=4)\n                T_reshape_3[0] = moving_var[ax1]\n                T_divide_1 = T.Buffer((107520,), data=T_divide.data)\n                data_1 = T.Buffer((107520,), data=data.data)\n                T_divide_1[cse_var_1] = (data_1[cse_var_1] - T_reshape_2[0]) / T.sqrt(T_reshape_3[0] + T.float32(1.0000000000000001e-05))",
        "data": "8_28_40_12",
        "gamma": "28",
        "beta": "28",
        "moving_mean": "28",
        "moving_var": "28"
    },
    {
        "op_name": "batch_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t gamma_code = arg_type_ids[1];\n  int32_t beta_code = arg_type_ids[2];\n  int32_t moving_mean_code = arg_type_ids[3];\n  int32_t moving_var_code = arg_type_ids[4];\n  int32_t T_divide_code = arg_type_ids[5];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* gamma = (((TVMValue*)args)[1].v_handle);\n  void* beta = (((TVMValue*)args)[2].v_handle);\n  void* moving_mean = (((TVMValue*)args)[3].v_handle);\n  void* moving_var = (((TVMValue*)args)[4].v_handle);\n  void* T_divide = (((TVMValue*)args)[5].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* gamma_1 = (((DLTensor*)gamma)[0].data);\n  void* default_function_gamma_shape = (((DLTensor*)gamma)[0].shape);\n  void* default_function_gamma_strides = (((DLTensor*)gamma)[0].strides);\n  void* beta_1 = (((DLTensor*)beta)[0].data);\n  void* default_function_beta_shape = (((DLTensor*)beta)[0].shape);\n  void* default_function_beta_strides = (((DLTensor*)beta)[0].strides);\n  void* moving_mean_1 = (((DLTensor*)moving_mean)[0].data);\n  void* default_function_moving_mean_shape = (((DLTensor*)moving_mean)[0].shape);\n  void* default_function_moving_mean_strides = (((DLTensor*)moving_mean)[0].strides);\n  void* moving_var_1 = (((DLTensor*)moving_var)[0].data);\n  void* default_function_moving_var_shape = (((DLTensor*)moving_var)[0].shape);\n  void* default_function_moving_var_strides = (((DLTensor*)moving_var)[0].strides);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_gamma_strides == NULL)) {\n  },\n  if (!(default_function_beta_strides == NULL)) {\n  },\n  if (!(default_function_moving_mean_strides == NULL)) {\n  },\n  if (!(default_function_moving_var_strides == NULL)) {\n  },\n  if (!(default_function_T_divide_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 200; ++ax0_ax1_fused) {\n    float T_reshape[1];\n    float T_reshape_1[1];\n    T_reshape[0] = ((float*)moving_mean_1)[(ax0_ax1_fused % 20)];\n    for (int32_t ax2 = 0; ax2 < 36; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 12; ++ax3) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 432) + (ax2 * 12)) + ax3);\n        T_reshape_1[0] = ((float*)moving_var_1)[(ax0_ax1_fused % 20)];\n        ((float*)T_divide_1)[cse_var_1] = ((((float*)data_1)[cse_var_1] - T_reshape[0]) / sqrtf((T_reshape_1[0] + 1.000000e-05f)));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - moving_mean[((((((int)blockIdx.x) % 135) * 4) + (((int)threadIdx.x) >> 4)) / 27)]) / sqrtf((moving_var[((((((int)blockIdx.x) % 135) * 4) + (((int)threadIdx.x) >> 4)) / 27)] + 1.000000e-05f)));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 20, 36, 12), \"float32\"), gamma: T.Buffer((20,), \"float32\"), beta: T.Buffer((20,), \"float32\"), moving_mean: T.Buffer((20,), \"float32\"), moving_var: T.Buffer((20,), \"float32\"), T_divide: T.Buffer((10, 20, 36, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(200):\n            T_reshape = T.allocate([1], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n            T_reshape_2 = T.Buffer((1,), data=T_reshape, align=4)\n            T_reshape_2[0] = moving_mean[ax0_ax1_fused % 20]\n            for ax2, ax3 in T.grid(36, 12):\n                cse_var_1: T.int32 = ax0_ax1_fused * 432 + ax2 * 12 + ax3\n                T_reshape_3 = T.Buffer((1,), data=T_reshape_1, align=4)\n                T_reshape_3[0] = moving_var[ax0_ax1_fused % 20]\n                T_divide_1 = T.Buffer((86400,), data=T_divide.data)\n                data_1 = T.Buffer((86400,), data=data.data)\n                T_divide_1[cse_var_1] = (data_1[cse_var_1] - T_reshape_2[0]) / T.sqrt(T_reshape_3[0] + T.float32(1.0000000000000001e-05))",
        "data": "10_20_36_12",
        "gamma": "20",
        "beta": "20",
        "moving_mean": "20",
        "moving_var": "20"
    },
    {
        "op_name": "batch_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t gamma_code = arg_type_ids[1];\n  int32_t beta_code = arg_type_ids[2];\n  int32_t moving_mean_code = arg_type_ids[3];\n  int32_t moving_var_code = arg_type_ids[4];\n  int32_t T_divide_code = arg_type_ids[5];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* gamma = (((TVMValue*)args)[1].v_handle);\n  void* beta = (((TVMValue*)args)[2].v_handle);\n  void* moving_mean = (((TVMValue*)args)[3].v_handle);\n  void* moving_var = (((TVMValue*)args)[4].v_handle);\n  void* T_divide = (((TVMValue*)args)[5].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* gamma_1 = (((DLTensor*)gamma)[0].data);\n  void* default_function_gamma_shape = (((DLTensor*)gamma)[0].shape);\n  void* default_function_gamma_strides = (((DLTensor*)gamma)[0].strides);\n  void* beta_1 = (((DLTensor*)beta)[0].data);\n  void* default_function_beta_shape = (((DLTensor*)beta)[0].shape);\n  void* default_function_beta_strides = (((DLTensor*)beta)[0].strides);\n  void* moving_mean_1 = (((DLTensor*)moving_mean)[0].data);\n  void* default_function_moving_mean_shape = (((DLTensor*)moving_mean)[0].shape);\n  void* default_function_moving_mean_strides = (((DLTensor*)moving_mean)[0].strides);\n  void* moving_var_1 = (((DLTensor*)moving_var)[0].data);\n  void* default_function_moving_var_shape = (((DLTensor*)moving_var)[0].shape);\n  void* default_function_moving_var_strides = (((DLTensor*)moving_var)[0].strides);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_gamma_strides == NULL)) {\n  },\n  if (!(default_function_beta_strides == NULL)) {\n  },\n  if (!(default_function_moving_mean_strides == NULL)) {\n  },\n  if (!(default_function_moving_var_strides == NULL)) {\n  },\n  if (!(default_function_T_divide_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    float T_reshape[24];\n    float T_reshape_1[1];\n    for (int32_t ax1 = 0; ax1 < 24; ++ax1) {\n      T_reshape[ax1] = ((float*)moving_mean_1)[ax1];\n    },\n    for (int32_t ax1_1 = 0; ax1_1 < 24; ++ax1_1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 32; ++ax3) {\n          int32_t cse_var_1 = ((((ax0 * 6144) + (ax1_1 * 256)) + (ax2 * 32)) + ax3);\n          T_reshape_1[0] = ((float*)moving_var_1)[ax1_1];\n          ((float*)T_divide_1)[cse_var_1] = ((((float*)data_1)[cse_var_1] - T_reshape[ax1_1]) / sqrtf((T_reshape_1[0] + 1.000000e-05f)));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - moving_mean[((((int)blockIdx.x) % 96) >> 2)]) / sqrtf((moving_var[((((int)blockIdx.x) % 96) >> 2)] + 1.000000e-05f)));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 24, 8, 32), \"float32\"), gamma: T.Buffer((24,), \"float32\"), beta: T.Buffer((24,), \"float32\"), moving_mean: T.Buffer((24,), \"float32\"), moving_var: T.Buffer((24,), \"float32\"), T_divide: T.Buffer((7, 24, 8, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(7):\n            T_reshape = T.allocate([24], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n            T_reshape_2 = T.Buffer((24,), data=T_reshape)\n            for ax1 in range(24):\n                T_reshape_2[ax1] = moving_mean[ax1]\n            for ax1, ax2, ax3 in T.grid(24, 8, 32):\n                cse_var_1: T.int32 = ax0 * 6144 + ax1 * 256 + ax2 * 32 + ax3\n                T_reshape_3 = T.Buffer((1,), data=T_reshape_1, align=4)\n                T_reshape_3[0] = moving_var[ax1]\n                T_divide_1 = T.Buffer((43008,), data=T_divide.data)\n                data_1 = T.Buffer((43008,), data=data.data)\n                T_divide_1[cse_var_1] = (data_1[cse_var_1] - T_reshape_2[ax1]) / T.sqrt(T_reshape_3[0] + T.float32(1.0000000000000001e-05))",
        "data": "7_24_8_32",
        "gamma": "24",
        "beta": "24",
        "moving_mean": "24",
        "moving_var": "24"
    },
    {
        "op_name": "batch_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t gamma_code = arg_type_ids[1];\n  int32_t beta_code = arg_type_ids[2];\n  int32_t moving_mean_code = arg_type_ids[3];\n  int32_t moving_var_code = arg_type_ids[4];\n  int32_t T_divide_code = arg_type_ids[5];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* gamma = (((TVMValue*)args)[1].v_handle);\n  void* beta = (((TVMValue*)args)[2].v_handle);\n  void* moving_mean = (((TVMValue*)args)[3].v_handle);\n  void* moving_var = (((TVMValue*)args)[4].v_handle);\n  void* T_divide = (((TVMValue*)args)[5].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* gamma_1 = (((DLTensor*)gamma)[0].data);\n  void* default_function_gamma_shape = (((DLTensor*)gamma)[0].shape);\n  void* default_function_gamma_strides = (((DLTensor*)gamma)[0].strides);\n  void* beta_1 = (((DLTensor*)beta)[0].data);\n  void* default_function_beta_shape = (((DLTensor*)beta)[0].shape);\n  void* default_function_beta_strides = (((DLTensor*)beta)[0].strides);\n  void* moving_mean_1 = (((DLTensor*)moving_mean)[0].data);\n  void* default_function_moving_mean_shape = (((DLTensor*)moving_mean)[0].shape);\n  void* default_function_moving_mean_strides = (((DLTensor*)moving_mean)[0].strides);\n  void* moving_var_1 = (((DLTensor*)moving_var)[0].data);\n  void* default_function_moving_var_shape = (((DLTensor*)moving_var)[0].shape);\n  void* default_function_moving_var_strides = (((DLTensor*)moving_var)[0].strides);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_gamma_strides == NULL)) {\n  },\n  if (!(default_function_beta_strides == NULL)) {\n  },\n  if (!(default_function_moving_mean_strides == NULL)) {\n  },\n  if (!(default_function_moving_var_strides == NULL)) {\n  },\n  if (!(default_function_T_divide_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    float T_reshape[32];\n    float T_reshape_1[1];\n    for (int32_t ax1 = 0; ax1 < 32; ++ax1) {\n      T_reshape[ax1] = ((float*)moving_mean_1)[ax1];\n    },\n    for (int32_t ax1_1 = 0; ax1_1 < 32; ++ax1_1) {\n      T_reshape_1[0] = ((float*)moving_var_1)[ax1_1];\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 32; ++ax3) {\n          int32_t cse_var_1 = ((((ax0 * 4096) + (ax1_1 * 128)) + (ax2 * 32)) + ax3);\n          ((float*)T_divide_1)[cse_var_1] = ((((float*)data_1)[cse_var_1] - T_reshape[ax1_1]) / sqrtf((T_reshape_1[0] + 1.000000e-05f)));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - moving_mean[((((int)blockIdx.x) & 63) >> 1)]) / sqrtf((moving_var[((((int)blockIdx.x) & 63) >> 1)] + 1.000000e-05f)));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 32, 4, 32), \"float32\"), gamma: T.Buffer((32,), \"float32\"), beta: T.Buffer((32,), \"float32\"), moving_mean: T.Buffer((32,), \"float32\"), moving_var: T.Buffer((32,), \"float32\"), T_divide: T.Buffer((10, 32, 4, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(10):\n            T_reshape = T.allocate([32], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n            T_reshape_2 = T.Buffer((32,), data=T_reshape)\n            for ax1 in range(32):\n                T_reshape_2[ax1] = moving_mean[ax1]\n            for ax1 in range(32):\n                T_reshape_3 = T.Buffer((1,), data=T_reshape_1, align=4)\n                T_reshape_3[0] = moving_var[ax1]\n                for ax2, ax3 in T.grid(4, 32):\n                    cse_var_1: T.int32 = ax0 * 4096 + ax1 * 128 + ax2 * 32 + ax3\n                    T_divide_1 = T.Buffer((40960,), data=T_divide.data)\n                    data_1 = T.Buffer((40960,), data=data.data)\n                    T_divide_1[cse_var_1] = (data_1[cse_var_1] - T_reshape_2[ax1]) / T.sqrt(T_reshape_3[0] + T.float32(1.0000000000000001e-05))",
        "data": "10_32_4_32",
        "gamma": "32",
        "beta": "32",
        "moving_mean": "32",
        "moving_var": "32"
    },
    {
        "op_name": "batch_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t gamma_code = arg_type_ids[1];\n  int32_t beta_code = arg_type_ids[2];\n  int32_t moving_mean_code = arg_type_ids[3];\n  int32_t moving_var_code = arg_type_ids[4];\n  int32_t T_divide_code = arg_type_ids[5];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* gamma = (((TVMValue*)args)[1].v_handle);\n  void* beta = (((TVMValue*)args)[2].v_handle);\n  void* moving_mean = (((TVMValue*)args)[3].v_handle);\n  void* moving_var = (((TVMValue*)args)[4].v_handle);\n  void* T_divide = (((TVMValue*)args)[5].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* gamma_1 = (((DLTensor*)gamma)[0].data);\n  void* default_function_gamma_shape = (((DLTensor*)gamma)[0].shape);\n  void* default_function_gamma_strides = (((DLTensor*)gamma)[0].strides);\n  void* beta_1 = (((DLTensor*)beta)[0].data);\n  void* default_function_beta_shape = (((DLTensor*)beta)[0].shape);\n  void* default_function_beta_strides = (((DLTensor*)beta)[0].strides);\n  void* moving_mean_1 = (((DLTensor*)moving_mean)[0].data);\n  void* default_function_moving_mean_shape = (((DLTensor*)moving_mean)[0].shape);\n  void* default_function_moving_mean_strides = (((DLTensor*)moving_mean)[0].strides);\n  void* moving_var_1 = (((DLTensor*)moving_var)[0].data);\n  void* default_function_moving_var_shape = (((DLTensor*)moving_var)[0].shape);\n  void* default_function_moving_var_strides = (((DLTensor*)moving_var)[0].strides);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_gamma_strides == NULL)) {\n  },\n  if (!(default_function_beta_strides == NULL)) {\n  },\n  if (!(default_function_moving_mean_strides == NULL)) {\n  },\n  if (!(default_function_moving_var_strides == NULL)) {\n  },\n  if (!(default_function_T_divide_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 112; ++ax0_ax1_fused) {\n    float T_reshape[1];\n    float T_reshape_1[1];\n    T_reshape[0] = ((float*)moving_mean_1)[(ax0_ax1_fused % 28)];\n    for (int32_t ax2 = 0; ax2 < 32; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 36; ++ax3) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 1152) + (ax2 * 36)) + ax3);\n        T_reshape_1[0] = ((float*)moving_var_1)[(ax0_ax1_fused % 28)];\n        ((float*)T_divide_1)[cse_var_1] = ((((float*)data_1)[cse_var_1] - T_reshape[0]) / sqrtf((T_reshape_1[0] + 1.000000e-05f)));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - moving_mean[((((int)blockIdx.x) % 504) / 18)]) / sqrtf((moving_var[((((int)blockIdx.x) % 504) / 18)] + 1.000000e-05f)));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 28, 32, 36), \"float32\"), gamma: T.Buffer((28,), \"float32\"), beta: T.Buffer((28,), \"float32\"), moving_mean: T.Buffer((28,), \"float32\"), moving_var: T.Buffer((28,), \"float32\"), T_divide: T.Buffer((4, 28, 32, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(112):\n            T_reshape = T.allocate([1], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n            T_reshape_2 = T.Buffer((1,), data=T_reshape, align=4)\n            T_reshape_2[0] = moving_mean[ax0_ax1_fused % 28]\n            for ax2, ax3 in T.grid(32, 36):\n                cse_var_1: T.int32 = ax0_ax1_fused * 1152 + ax2 * 36 + ax3\n                T_reshape_3 = T.Buffer((1,), data=T_reshape_1, align=4)\n                T_reshape_3[0] = moving_var[ax0_ax1_fused % 28]\n                T_divide_1 = T.Buffer((129024,), data=T_divide.data)\n                data_1 = T.Buffer((129024,), data=data.data)\n                T_divide_1[cse_var_1] = (data_1[cse_var_1] - T_reshape_2[0]) / T.sqrt(T_reshape_3[0] + T.float32(1.0000000000000001e-05))",
        "data": "4_28_32_36",
        "gamma": "28",
        "beta": "28",
        "moving_mean": "28",
        "moving_var": "28"
    },
    {
        "op_name": "batch_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t gamma_code = arg_type_ids[1];\n  int32_t beta_code = arg_type_ids[2];\n  int32_t moving_mean_code = arg_type_ids[3];\n  int32_t moving_var_code = arg_type_ids[4];\n  int32_t T_divide_code = arg_type_ids[5];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* gamma = (((TVMValue*)args)[1].v_handle);\n  void* beta = (((TVMValue*)args)[2].v_handle);\n  void* moving_mean = (((TVMValue*)args)[3].v_handle);\n  void* moving_var = (((TVMValue*)args)[4].v_handle);\n  void* T_divide = (((TVMValue*)args)[5].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* gamma_1 = (((DLTensor*)gamma)[0].data);\n  void* default_function_gamma_shape = (((DLTensor*)gamma)[0].shape);\n  void* default_function_gamma_strides = (((DLTensor*)gamma)[0].strides);\n  void* beta_1 = (((DLTensor*)beta)[0].data);\n  void* default_function_beta_shape = (((DLTensor*)beta)[0].shape);\n  void* default_function_beta_strides = (((DLTensor*)beta)[0].strides);\n  void* moving_mean_1 = (((DLTensor*)moving_mean)[0].data);\n  void* default_function_moving_mean_shape = (((DLTensor*)moving_mean)[0].shape);\n  void* default_function_moving_mean_strides = (((DLTensor*)moving_mean)[0].strides);\n  void* moving_var_1 = (((DLTensor*)moving_var)[0].data);\n  void* default_function_moving_var_shape = (((DLTensor*)moving_var)[0].shape);\n  void* default_function_moving_var_strides = (((DLTensor*)moving_var)[0].strides);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_gamma_strides == NULL)) {\n  },\n  if (!(default_function_beta_strides == NULL)) {\n  },\n  if (!(default_function_moving_mean_strides == NULL)) {\n  },\n  if (!(default_function_moving_var_strides == NULL)) {\n  },\n  if (!(default_function_T_divide_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 196; ++ax0_ax1_fused) {\n    float T_reshape[1];\n    float T_reshape_1[1];\n    for (int32_t ax2 = 0; ax2 < 40; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 12; ++ax3) {\n        int32_t cse_var_2 = (ax0_ax1_fused % 28);\n        int32_t cse_var_1 = (((ax0_ax1_fused * 480) + (ax2 * 12)) + ax3);\n        T_reshape[0] = ((float*)moving_mean_1)[cse_var_2];\n        T_reshape_1[0] = ((float*)moving_var_1)[cse_var_2];\n        ((float*)T_divide_1)[cse_var_1] = ((((float*)data_1)[cse_var_1] - T_reshape[0]) / sqrtf((T_reshape_1[0] + 1.000000e-05f)));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - moving_mean[((((((int)blockIdx.x) % 210) * 2) + (((int)threadIdx.x) >> 5)) / 15)]) / sqrtf((moving_var[((((((int)blockIdx.x) % 210) * 2) + (((int)threadIdx.x) >> 5)) / 15)] + 1.000000e-05f)));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 28, 40, 12), \"float32\"), gamma: T.Buffer((28,), \"float32\"), beta: T.Buffer((28,), \"float32\"), moving_mean: T.Buffer((28,), \"float32\"), moving_var: T.Buffer((28,), \"float32\"), T_divide: T.Buffer((7, 28, 40, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused in T.parallel(196):\n            T_reshape = T.allocate([1], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n            for ax2, ax3 in T.grid(40, 12):\n                cse_var_2: T.int32 = ax0_ax1_fused % 28\n                cse_var_1: T.int32 = ax0_ax1_fused * 480 + ax2 * 12 + ax3\n                T_reshape_2 = T.Buffer((1,), data=T_reshape, align=4)\n                T_reshape_2[0] = moving_mean[cse_var_2]\n                T_reshape_3 = T.Buffer((1,), data=T_reshape_1, align=4)\n                T_reshape_3[0] = moving_var[cse_var_2]\n                T_divide_1 = T.Buffer((94080,), data=T_divide.data)\n                data_1 = T.Buffer((94080,), data=data.data)\n                T_divide_1[cse_var_1] = (data_1[cse_var_1] - T_reshape_2[0]) / T.sqrt(T_reshape_3[0] + T.float32(1.0000000000000001e-05))",
        "data": "7_28_40_12",
        "gamma": "28",
        "beta": "28",
        "moving_mean": "28",
        "moving_var": "28"
    },
    {
        "op_name": "batch_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t gamma_code = arg_type_ids[1];\n  int32_t beta_code = arg_type_ids[2];\n  int32_t moving_mean_code = arg_type_ids[3];\n  int32_t moving_var_code = arg_type_ids[4];\n  int32_t T_divide_code = arg_type_ids[5];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* gamma = (((TVMValue*)args)[1].v_handle);\n  void* beta = (((TVMValue*)args)[2].v_handle);\n  void* moving_mean = (((TVMValue*)args)[3].v_handle);\n  void* moving_var = (((TVMValue*)args)[4].v_handle);\n  void* T_divide = (((TVMValue*)args)[5].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* gamma_1 = (((DLTensor*)gamma)[0].data);\n  void* default_function_gamma_shape = (((DLTensor*)gamma)[0].shape);\n  void* default_function_gamma_strides = (((DLTensor*)gamma)[0].strides);\n  void* beta_1 = (((DLTensor*)beta)[0].data);\n  void* default_function_beta_shape = (((DLTensor*)beta)[0].shape);\n  void* default_function_beta_strides = (((DLTensor*)beta)[0].strides);\n  void* moving_mean_1 = (((DLTensor*)moving_mean)[0].data);\n  void* default_function_moving_mean_shape = (((DLTensor*)moving_mean)[0].shape);\n  void* default_function_moving_mean_strides = (((DLTensor*)moving_mean)[0].strides);\n  void* moving_var_1 = (((DLTensor*)moving_var)[0].data);\n  void* default_function_moving_var_shape = (((DLTensor*)moving_var)[0].shape);\n  void* default_function_moving_var_strides = (((DLTensor*)moving_var)[0].strides);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_gamma_strides == NULL)) {\n  },\n  if (!(default_function_beta_strides == NULL)) {\n  },\n  if (!(default_function_moving_mean_strides == NULL)) {\n  },\n  if (!(default_function_moving_var_strides == NULL)) {\n  },\n  if (!(default_function_T_divide_strides == NULL)) {\n  },\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1344; ++ax0_ax1_fused_ax2_fused) {\n    float T_reshape[1];\n    float T_reshape_1[1];\n    for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n      int32_t cse_var_2 = ((ax0_ax1_fused_ax2_fused % 672) / 24);\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 16) + ax3);\n      T_reshape[0] = ((float*)moving_mean_1)[cse_var_2];\n      T_reshape_1[0] = ((float*)moving_var_1)[cse_var_2];\n      ((float*)T_divide_1)[cse_var_1] = ((((float*)data_1)[cse_var_1] - T_reshape[0]) / sqrtf((T_reshape_1[0] + 1.000000e-05f)));\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - moving_mean[((((((int)blockIdx.x) % 192) * 7) + (((int)threadIdx.x) >> 3)) / 48)]) / sqrtf((moving_var[((((((int)blockIdx.x) % 192) * 7) + (((int)threadIdx.x) >> 3)) / 48)] + 1.000000e-05f)));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 28, 24, 16), \"float32\"), gamma: T.Buffer((28,), \"float32\"), beta: T.Buffer((28,), \"float32\"), moving_mean: T.Buffer((28,), \"float32\"), moving_var: T.Buffer((28,), \"float32\"), T_divide: T.Buffer((2, 28, 24, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1344):\n            T_reshape = T.allocate([1], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n            for ax3 in range(16):\n                cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused % 672 // 24\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 16 + ax3\n                T_reshape_2 = T.Buffer((1,), data=T_reshape, align=4)\n                T_reshape_2[0] = moving_mean[cse_var_2]\n                T_reshape_3 = T.Buffer((1,), data=T_reshape_1, align=4)\n                T_reshape_3[0] = moving_var[cse_var_2]\n                T_divide_1 = T.Buffer((21504,), data=T_divide.data)\n                data_1 = T.Buffer((21504,), data=data.data)\n                T_divide_1[cse_var_1] = (data_1[cse_var_1] - T_reshape_2[0]) / T.sqrt(T_reshape_3[0] + T.float32(1.0000000000000001e-05))",
        "data": "2_28_24_16",
        "gamma": "28",
        "beta": "28",
        "moving_mean": "28",
        "moving_var": "28"
    },
    {
        "op_name": "batch_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t gamma_code = arg_type_ids[1];\n  int32_t beta_code = arg_type_ids[2];\n  int32_t moving_mean_code = arg_type_ids[3];\n  int32_t moving_var_code = arg_type_ids[4];\n  int32_t T_divide_code = arg_type_ids[5];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* gamma = (((TVMValue*)args)[1].v_handle);\n  void* beta = (((TVMValue*)args)[2].v_handle);\n  void* moving_mean = (((TVMValue*)args)[3].v_handle);\n  void* moving_var = (((TVMValue*)args)[4].v_handle);\n  void* T_divide = (((TVMValue*)args)[5].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* gamma_1 = (((DLTensor*)gamma)[0].data);\n  void* default_function_gamma_shape = (((DLTensor*)gamma)[0].shape);\n  void* default_function_gamma_strides = (((DLTensor*)gamma)[0].strides);\n  void* beta_1 = (((DLTensor*)beta)[0].data);\n  void* default_function_beta_shape = (((DLTensor*)beta)[0].shape);\n  void* default_function_beta_strides = (((DLTensor*)beta)[0].strides);\n  void* moving_mean_1 = (((DLTensor*)moving_mean)[0].data);\n  void* default_function_moving_mean_shape = (((DLTensor*)moving_mean)[0].shape);\n  void* default_function_moving_mean_strides = (((DLTensor*)moving_mean)[0].strides);\n  void* moving_var_1 = (((DLTensor*)moving_var)[0].data);\n  void* default_function_moving_var_shape = (((DLTensor*)moving_var)[0].shape);\n  void* default_function_moving_var_strides = (((DLTensor*)moving_var)[0].strides);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_gamma_strides == NULL)) {\n  },\n  if (!(default_function_beta_strides == NULL)) {\n  },\n  if (!(default_function_moving_mean_strides == NULL)) {\n  },\n  if (!(default_function_moving_var_strides == NULL)) {\n  },\n  if (!(default_function_T_divide_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    float T_reshape[12];\n    float T_reshape_1[12];\n    *(float12*)(T_reshape + 0) = *(float12*)(((float*)moving_mean_1) + 0);\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      T_reshape_1[ax1] = ((float*)moving_var_1)[ax1];\n    },\n    for (int32_t ax1_1 = 0; ax1_1 < 12; ++ax1_1) {\n      for (int32_t ax2 = 0; ax2 < 24; ++ax2) {\n        int32_t cse_var_1 = (((ax0 * 3456) + (ax1_1 * 288)) + (ax2 * 12));\n        int32_t12 v_ = int32_t12((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7), (cse_var_1)+(1*8), (cse_var_1)+(1*9), (cse_var_1)+(1*10), (cse_var_1)+(1*11));\n        *(float12*)(((float*)T_divide_1) + cse_var_1) = (((float12(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb])) - ((float12)(T_reshape[ax1_1], T_reshape[ax1_1], T_reshape[ax1_1], T_reshape[ax1_1], T_reshape[ax1_1], T_reshape[ax1_1], T_reshape[ax1_1], T_reshape[ax1_1], T_reshape[ax1_1], T_reshape[ax1_1], T_reshape[ax1_1], T_reshape[ax1_1]))) / ((float12)(sqrtf((T_reshape_1[ax1_1] + 1.000000e-05f)), sqrtf((T_reshape_1[ax1_1] + 1.000000e-05f)), sqrtf((T_reshape_1[ax1_1] + 1.000000e-05f)), sqrtf((T_reshape_1[ax1_1] + 1.000000e-05f)), sqrtf((T_reshape_1[ax1_1] + 1.000000e-05f)), sqrtf((T_reshape_1[ax1_1] + 1.000000e-05f)), sqrtf((T_reshape_1[ax1_1] + 1.000000e-05f)), sqrtf((T_reshape_1[ax1_1] + 1.000000e-05f)), sqrtf((T_reshape_1[ax1_1] + 1.000000e-05f)), sqrtf((T_reshape_1[ax1_1] + 1.000000e-05f)), sqrtf((T_reshape_1[ax1_1] + 1.000000e-05f)), sqrtf((T_reshape_1[ax1_1] + 1.000000e-05f)))));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(27) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var);\nextern \"C\" __global__ void __launch_bounds__(27) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))] - moving_mean[((((((int)blockIdx.x) & 127) * 3) + (((int)threadIdx.x) / 9)) >> 5)]) / sqrtf((moving_var[((((((int)blockIdx.x) & 127) * 3) + (((int)threadIdx.x) / 9)) >> 5)] + 1.000000e-05f)));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 12, 24, 12), \"float32\"), gamma: T.Buffer((12,), \"float32\"), beta: T.Buffer((12,), \"float32\"), moving_mean: T.Buffer((12,), \"float32\"), moving_var: T.Buffer((12,), \"float32\"), T_divide: T.Buffer((3, 12, 24, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(3):\n            T_reshape = T.allocate([12], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([12], \"float32\", \"global\")\n            T_reshape_2 = T.Buffer((12,), data=T_reshape, align=32)\n            T_reshape_2[0:12] = moving_mean[0:12]\n            T_reshape_3 = T.Buffer((12,), data=T_reshape_1, align=32)\n            for ax1 in range(12):\n                T_reshape_3[ax1] = moving_var[ax1]\n            for ax1, ax2 in T.grid(12, 24):\n                cse_var_1: T.int32 = ax0 * 3456 + ax1 * 288 + ax2 * 12\n                T_divide_1 = T.Buffer((10368,), data=T_divide.data)\n                data_1 = T.Buffer((10368,), data=data.data)\n                T_divide_1[cse_var_1:cse_var_1 + 12] = (data_1[cse_var_1:cse_var_1 + 12] - T.Broadcast(T_reshape_2[ax1], 12)) / T.Broadcast(T.sqrt(T_reshape_3[ax1] + T.float32(1.0000000000000001e-05)), 12)",
        "data": "3_12_24_12",
        "gamma": "12",
        "beta": "12",
        "moving_mean": "12",
        "moving_var": "12"
    },
    {
        "op_name": "batch_norm",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t gamma_code = arg_type_ids[1];\n  int32_t beta_code = arg_type_ids[2];\n  int32_t moving_mean_code = arg_type_ids[3];\n  int32_t moving_var_code = arg_type_ids[4];\n  int32_t T_divide_code = arg_type_ids[5];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* gamma = (((TVMValue*)args)[1].v_handle);\n  void* beta = (((TVMValue*)args)[2].v_handle);\n  void* moving_mean = (((TVMValue*)args)[3].v_handle);\n  void* moving_var = (((TVMValue*)args)[4].v_handle);\n  void* T_divide = (((TVMValue*)args)[5].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* gamma_1 = (((DLTensor*)gamma)[0].data);\n  void* default_function_gamma_shape = (((DLTensor*)gamma)[0].shape);\n  void* default_function_gamma_strides = (((DLTensor*)gamma)[0].strides);\n  void* beta_1 = (((DLTensor*)beta)[0].data);\n  void* default_function_beta_shape = (((DLTensor*)beta)[0].shape);\n  void* default_function_beta_strides = (((DLTensor*)beta)[0].strides);\n  void* moving_mean_1 = (((DLTensor*)moving_mean)[0].data);\n  void* default_function_moving_mean_shape = (((DLTensor*)moving_mean)[0].shape);\n  void* default_function_moving_mean_strides = (((DLTensor*)moving_mean)[0].strides);\n  void* moving_var_1 = (((DLTensor*)moving_var)[0].data);\n  void* default_function_moving_var_shape = (((DLTensor*)moving_var)[0].shape);\n  void* default_function_moving_var_strides = (((DLTensor*)moving_var)[0].strides);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_gamma_strides == NULL)) {\n  },\n  if (!(default_function_beta_strides == NULL)) {\n  },\n  if (!(default_function_moving_mean_strides == NULL)) {\n  },\n  if (!(default_function_moving_var_strides == NULL)) {\n  },\n  if (!(default_function_T_divide_strides == NULL)) {\n  },\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    float T_reshape[36];\n    float T_reshape_1[36];\n    for (int32_t ax1 = 0; ax1 < 36; ++ax1) {\n      T_reshape[ax1] = ((float*)moving_mean_1)[ax1];\n    },\n    for (int32_t ax1_1 = 0; ax1_1 < 36; ++ax1_1) {\n      T_reshape_1[ax1_1] = ((float*)moving_var_1)[ax1_1];\n    },\n    for (int32_t ax1_2 = 0; ax1_2 < 36; ++ax1_2) {\n      for (int32_t ax2 = 0; ax2 < 32; ++ax2) {\n        int32_t cse_var_1 = (((ax0 * 4608) + (ax1_2 * 128)) + (ax2 * 4));\n        int32_t4 v_ = int32_t4((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3));\n        *(float4*)(((float*)T_divide_1) + cse_var_1) = (((float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3])) - ((float4)(T_reshape[ax1_2], T_reshape[ax1_2], T_reshape[ax1_2], T_reshape[ax1_2]))) / ((float4)(sqrtf((T_reshape_1[ax1_2] + 1.000000e-05f)), sqrtf((T_reshape_1[ax1_2] + 1.000000e-05f)), sqrtf((T_reshape_1[ax1_2] + 1.000000e-05f)), sqrtf((T_reshape_1[ax1_2] + 1.000000e-05f)))));\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - moving_mean[((((int)blockIdx.x) % 72) >> 1)]) / sqrtf((moving_var[((((int)blockIdx.x) % 72) >> 1)] + 1.000000e-05f)));\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 36, 32, 4), \"float32\"), gamma: T.Buffer((36,), \"float32\"), beta: T.Buffer((36,), \"float32\"), moving_mean: T.Buffer((36,), \"float32\"), moving_var: T.Buffer((36,), \"float32\"), T_divide: T.Buffer((5, 36, 32, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for ax0 in T.parallel(5):\n            T_reshape = T.allocate([36], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([36], \"float32\", \"global\")\n            T_reshape_2 = T.Buffer((36,), data=T_reshape)\n            for ax1 in range(36):\n                T_reshape_2[ax1] = moving_mean[ax1]\n            T_reshape_3 = T.Buffer((36,), data=T_reshape_1)\n            for ax1 in range(36):\n                T_reshape_3[ax1] = moving_var[ax1]\n            for ax1, ax2 in T.grid(36, 32):\n                cse_var_1: T.int32 = ax0 * 4608 + ax1 * 128 + ax2 * 4\n                T_divide_1 = T.Buffer((23040,), data=T_divide.data)\n                data_1 = T.Buffer((23040,), data=data.data)\n                T_divide_1[cse_var_1:cse_var_1 + 4] = (data_1[cse_var_1:cse_var_1 + 4] - T.Broadcast(T_reshape_2[ax1], 4)) / T.Broadcast(T.sqrt(T_reshape_3[ax1] + T.float32(1.0000000000000001e-05)), 4)",
        "data": "5_36_32_4",
        "gamma": "36",
        "beta": "36",
        "moving_mean": "36",
        "moving_var": "36"
    },
    {
        "op_name": "conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t bias_code = arg_type_ids[2];\n  int32_t conv2d_nchw_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* bias = (((TVMValue*)args)[2].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* bias_1 = (((DLTensor*)bias)[0].data);\n  void* default_function_bias_shape = (((DLTensor*)bias)[0].shape);\n  void* default_function_bias_strides = (((DLTensor*)bias)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_bias_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused < 89; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)6528, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    void* conv2d_nchw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)20, 2, 32);\n    if (conv2d_nchw_local == NULL) {\n      return -1;\n    },\n    for (int32_t i1 = 0; i1 < 68; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n          ((float*)pad_temp)[(((i1 * 24) + (i2 * 8)) + i3_s)] = ((((1 <= i2) && (1 <= i3_s)) && (i3_s < 7)) ? ((float*)data_1)[((((i1 * 12) + (i2 * 6)) + i3_s) - 7)] : 0.000000e+00f);\n        },\n      },\n    },\n    ((float5*)conv2d_nchw_local)[0] = ((float5)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n    for (int32_t rc_outer = 0; rc_outer < 68; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n        for (int32_t rx_outer = 0; rx_outer < 4; ++rx_outer) {\n          int32_t5 v_ = int32_t5(((((rc_outer * 24) + (ry_outer * 8)) + rx_outer))+(1*0), ((((rc_outer * 24) + (ry_outer * 8)) + rx_outer))+(1*1), ((((rc_outer * 24) + (ry_outer * 8)) + rx_outer))+(1*2), ((((rc_outer * 24) + (ry_outer * 8)) + rx_outer))+(1*3), ((((rc_outer * 24) + (ry_outer * 8)) + rx_outer))+(1*4));\n          ((float5*)conv2d_nchw_local)[0] = (((float5*)conv2d_nchw_local)[0] + ((float5(((float*)pad_temp)[v_.s0],((float*)pad_temp)[v_.s1],((float*)pad_temp)[v_.s2],((float*)pad_temp)[v_.s3],((float*)pad_temp)[v_.s4])) * ((float5)(((float*)kernel_1)[((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 816) + (rc_outer * 12)) + (ry_outer * 4)) + rx_outer)], ((float*)kernel_1)[((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 816) + (rc_outer * 12)) + (ry_outer * 4)) + rx_outer)], ((float*)kernel_1)[((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 816) + (rc_outer * 12)) + (ry_outer * 4)) + rx_outer)], ((float*)kernel_1)[((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 816) + (rc_outer * 12)) + (ry_outer * 4)) + rx_outer)], ((float*)kernel_1)[((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 816) + (rc_outer * 12)) + (ry_outer * 4)) + rx_outer)]))));\n        },\n      },\n    },\n    *(float5*)(((float*)conv2d_nchw_1) + (nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 5)) = ((float5*)conv2d_nchw_local)[0];\n    if (TVMBackendFreeWorkspace(1, dev_id, conv2d_nchw_local) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int rc_ry_fused_rx_fused_outer = 0; rc_ry_fused_rx_fused_outer < 51; ++rc_ry_fused_rx_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (((((1 <= (((((int)threadIdx.x) >> 2) + rc_ry_fused_rx_fused_outer) % 3)) && (1 <= ((((int)blockIdx.x) % 5) + (((int)threadIdx.x) & 3)))) && (((((int)blockIdx.x) % 5) + (((int)threadIdx.x) & 3)) < 7)) ? data[((((((((rc_ry_fused_rx_fused_outer * 4) + (((int)threadIdx.x) >> 2)) / 3) * 12) + ((((((int)threadIdx.x) >> 2) + rc_ry_fused_rx_fused_outer) % 3) * 6)) + (((int)blockIdx.x) % 5)) + (((int)threadIdx.x) & 3)) - 7)] : 0.000000e+00f) * kernel[((((((int)blockIdx.x) / 5) * 816) + (rc_ry_fused_rx_fused_outer * 16)) + ((int)threadIdx.x))]));\n  },\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    conv2d_nchw[((int)blockIdx.x)] = red_buf0[0];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 68, 2, 6), \"float32\"), kernel: T.Buffer((89, 68, 3, 4), \"float32\"), bias: T.Buffer((1, 89, 1, 1), \"float32\"), conv2d_nchw: T.Buffer((1, 89, 1, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused in T.parallel(89):\n            pad_temp = T.allocate([1632], \"float32\", \"global\")\n            conv2d_nchw_local = T.allocate([1], \"float32x5\", \"local\")\n            pad_temp_1 = T.Buffer((1632,), data=pad_temp)\n            for i1, i2, i3_s in T.grid(68, 3, 8):\n                data_1 = T.Buffer((816,), data=data.data)\n                pad_temp_1[i1 * 24 + i2 * 8 + i3_s] = T.if_then_else(1 <= i2 and 1 <= i3_s and i3_s < 7, data_1[i1 * 12 + i2 * 6 + i3_s - 7], T.float32(0))\n            conv2d_nchw_local_1 = T.Buffer((1,), \"float32x5\", data=conv2d_nchw_local, scope=\"local\", align=16)\n            conv2d_nchw_local_1[0] = T.Broadcast(T.float32(0), 5)\n            for rc_outer, ry_outer, rx_outer in T.grid(68, 3, 4):\n                kernel_1 = T.Buffer((72624,), data=kernel.data)\n                conv2d_nchw_local_1[0] = conv2d_nchw_local_1[0] + pad_temp_1[rc_outer * 24 + ry_outer * 8 + rx_outer:rc_outer * 24 + ry_outer * 8 + rx_outer + 5] * T.Broadcast(kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 816 + rc_outer * 12 + ry_outer * 4 + rx_outer], 5)\n            conv2d_nchw_1 = T.Buffer((445,), data=conv2d_nchw.data)\n            conv2d_nchw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 5:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 5 + 5] = conv2d_nchw_local_1[0]",
        "data": "1_68_2_6",
        "kernel": "89_68_3_4",
        "bias": "1_89_1_1"
    },
    {
        "op_name": "conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t bias_code = arg_type_ids[2];\n  int32_t conv2d_nchw_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* bias = (((TVMValue*)args)[2].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* bias_1 = (((DLTensor*)bias)[0].data);\n  void* default_function_bias_shape = (((DLTensor*)bias)[0].shape);\n  void* default_function_bias_strides = (((DLTensor*)bias)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_bias_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 200; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)44352, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    void* conv2d_nchw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)144, 2, 32);\n    if (conv2d_nchw_local == NULL) {\n      return -1;\n    },\n    for (int32_t i0 = 0; i0 < 9; ++i0) {\n      for (int32_t i1 = 0; i1 < 28; ++i1) {\n        for (int32_t i2 = 0; i2 < 4; ++i2) {\n          for (int32_t i3_s = 0; i3_s < 11; ++i3_s) {\n            int32_t cse_var_2 = (nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 7);\n            int32_t cse_var_1 = (i2 + cse_var_2);\n            ((float*)pad_temp)[((((i0 * 1232) + (i1 * 44)) + (i2 * 11)) + i3_s)] = (((((1 <= cse_var_1) && (cse_var_1 < 10)) && (1 <= i3_s)) && (i3_s < 10)) ? ((float*)data_1)[((((((i0 * 2268) + (i1 * 81)) + (i2 * 9)) + (cse_var_2 * 9)) + i3_s) - 10)] : 0.000000e+00f);\n          },\n        },\n      },\n    },\n    for (int32_t xx_outer_inner = 0; xx_outer_inner < 5; ++xx_outer_inner) {\n      for (int32_t nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 3; ++nn_c_outer_inner_init) {\n        for (int32_t nn_c_inner_init = 0; nn_c_inner_init < 3; ++nn_c_inner_init) {\n          for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 4; ++ff_c_inner_init) {\n            ((float*)conv2d_nchw_local)[(((nn_c_outer_inner_init * 12) + (nn_c_inner_init * 4)) + ff_c_inner_init)] = 0.000000e+00f;\n          },\n        },\n      },\n      for (int32_t rc_outer = 0; rc_outer < 4; ++rc_outer) {\n        for (int32_t ry_outer = 0; ry_outer < 2; ++ry_outer) {\n          for (int32_t rx_outer = 0; rx_outer < 3; ++rx_outer) {\n            for (int32_t nn_c_outer_inner = 0; nn_c_outer_inner < 3; ++nn_c_outer_inner) {\n              for (int32_t rc_inner = 0; rc_inner < 7; ++rc_inner) {\n                for (int32_t ry_inner = 0; ry_inner < 2; ++ry_inner) {\n                  for (int32_t nn_c_inner = 0; nn_c_inner < 3; ++nn_c_inner) {\n                    for (int32_t ff_c_inner = 0; ff_c_inner < 4; ++ff_c_inner) {\n                      int32_t cse_var_3 = (((nn_c_outer_inner * 12) + (nn_c_inner * 4)) + ff_c_inner);\n                      ((float*)conv2d_nchw_local)[cse_var_3] = (((float*)conv2d_nchw_local)[cse_var_3] + (((float*)pad_temp)[((((((((nn_c_outer_inner * 3696) + (nn_c_inner * 1232)) + (rc_outer * 308)) + (rc_inner * 44)) + (ry_outer * 22)) + (ry_inner * 11)) + (xx_outer_inner * 2)) + rx_outer)] * ((float*)kernel_1)[((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused >> 3) * 1344) + (ff_c_inner * 336)) + (rc_outer * 84)) + (rc_inner * 12)) + (ry_outer * 6)) + (ry_inner * 3)) + rx_outer)]));\n                    },\n                  },\n                },\n              },\n            },\n          },\n        },\n      },\n      for (int32_t nn_inner = 0; nn_inner < 9; ++nn_inner) {\n        for (int32_t ff_inner = 0; ff_inner < 4; ++ff_inner) {\n          ((float*)conv2d_nchw_1)[(((((nn_inner * 4000) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused >> 3) * 160)) + (ff_inner * 40)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 7) * 5)) + xx_outer_inner)] = ((float*)conv2d_nchw_local)[((nn_inner * 4) + ff_inner)];\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv2d_nchw_local) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(100) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(100) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[8];\n  __shared__ float pad_temp_shared[154];\n  __shared__ float kernel_shared[5600];\n  for (int ff_c_inner_init = 0; ff_c_inner_init < 2; ++ff_c_inner_init) {\n    for (int yy_c_inner_init = 0; yy_c_inner_init < 4; ++yy_c_inner_init) {\n      conv2d_nchw_local[((ff_c_inner_init * 4) + yy_c_inner_init)] = 0.000000e+00f;\n    },\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 2; ++rc_outer_outer) {\n    for (int rx_outer_outer = 0; rx_outer_outer < 3; ++rx_outer_outer) {\n      __syncthreads();\n      for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 50) + (((int)threadIdx.x) >> 1)) < 77) {\n          pad_temp_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 100) + ((int)threadIdx.x))] = (((((1 <= ((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer + ((int)threadIdx.x)) % 11)) && (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer + ((int)threadIdx.x)) % 11) < 10)) && (1 <= (((((int)blockIdx.x) % 5) * 2) + rx_outer_outer))) && (((rx_outer_outer >> 1) + (((int)blockIdx.x) % 5)) < 5)) ? data[((((((((((int)blockIdx.x) / 5) * 2268) + (rc_outer_outer * 1134)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 100) + ((int)threadIdx.x)) / 11) * 81)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer + ((int)threadIdx.x)) % 11) * 9)) + ((((int)blockIdx.x) % 5) * 2)) + rx_outer_outer) - 10)] : 0.000000e+00f);\n        },\n      },\n      for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 < 56; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) {\n        kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 100) + ((int)threadIdx.x))] = kernel[((((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 25) + (((int)threadIdx.x) >> 2)) / 14) * 336) + (rc_outer_outer * 168)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 11) + (((int)threadIdx.x) >> 2)) % 14) * 12)) + ((((int)threadIdx.x) & 3) * 3)) + rx_outer_outer)];\n      },\n      __syncthreads();\n      for (int rc_outer_inner = 0; rc_outer_inner < 2; ++rc_outer_inner) {\n        for (int ry_outer_inner = 0; ry_outer_inner < 2; ++ry_outer_inner) {\n          for (int rc_inner = 0; rc_inner < 7; ++rc_inner) {\n            for (int ry_inner = 0; ry_inner < 2; ++ry_inner) {\n              for (int ff_c_inner = 0; ff_c_inner < 2; ++ff_c_inner) {\n                for (int yy_c_inner = 0; yy_c_inner < 4; ++yy_c_inner) {\n                  conv2d_nchw_local[((ff_c_inner * 4) + yy_c_inner)] = (conv2d_nchw_local[((ff_c_inner * 4) + yy_c_inner)] + (pad_temp_shared[((((((rc_outer_inner * 77) + (rc_inner * 11)) + ((((int)threadIdx.x) & 1) * 4)) + (ry_outer_inner * 2)) + yy_c_inner) + ry_inner)] * kernel_shared[(((((((((int)threadIdx.x) >> 1) * 112) + (ff_c_inner * 56)) + (rc_outer_inner * 28)) + (rc_inner * 4)) + (ry_outer_inner * 2)) + ry_inner)]));\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 2; ++ff_inner) {\n    for (int yy_inner = 0; yy_inner < 4; ++yy_inner) {\n      conv2d_nchw[(((((((((int)blockIdx.x) / 5) * 4000) + ((((int)threadIdx.x) >> 1) * 80)) + (ff_inner * 40)) + ((((int)threadIdx.x) & 1) * 20)) + (yy_inner * 5)) + (((int)blockIdx.x) % 5))] = conv2d_nchw_local[((ff_inner * 4) + yy_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 28, 9, 9), \"float32\"), kernel: T.Buffer((100, 28, 4, 3), \"float32\"), bias: T.Buffer((1, 100, 1, 1), \"float32\"), conv2d_nchw: T.Buffer((9, 100, 8, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(200):\n            pad_temp = T.allocate([11088], \"float32\", \"global\")\n            conv2d_nchw_local = T.allocate([36], \"float32\", \"local\")\n            pad_temp_1 = T.Buffer((11088,), data=pad_temp)\n            for i0, i1, i2, i3_s in T.grid(9, 28, 4, 11):\n                cse_var_2: T.int32 = nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 8\n                cse_var_1: T.int32 = i2 + cse_var_2\n                data_1 = T.Buffer((20412,), data=data.data)\n                pad_temp_1[i0 * 1232 + i1 * 44 + i2 * 11 + i3_s] = T.if_then_else(1 <= cse_var_1 and cse_var_1 < 10 and 1 <= i3_s and i3_s < 10, data_1[i0 * 2268 + i1 * 81 + i2 * 9 + cse_var_2 * 9 + i3_s - 10], T.float32(0))\n            for xx_outer_inner in range(5):\n                conv2d_nchw_local_1 = T.Buffer((36,), data=conv2d_nchw_local, scope=\"local\")\n                for nn_c_outer_inner_init, nn_c_inner_init, ff_c_inner_init in T.grid(3, 3, 4):\n                    conv2d_nchw_local_1[nn_c_outer_inner_init * 12 + nn_c_inner_init * 4 + ff_c_inner_init] = T.float32(0)\n                for rc_outer, ry_outer, rx_outer, nn_c_outer_inner, rc_inner, ry_inner, nn_c_inner, ff_c_inner in T.grid(4, 2, 3, 3, 7, 2, 3, 4):\n                    cse_var_3: T.int32 = nn_c_outer_inner * 12 + nn_c_inner * 4 + ff_c_inner\n                    kernel_1 = T.Buffer((33600,), data=kernel.data)\n                    conv2d_nchw_local_1[cse_var_3] = conv2d_nchw_local_1[cse_var_3] + pad_temp_1[nn_c_outer_inner * 3696 + nn_c_inner * 1232 + rc_outer * 308 + rc_inner * 44 + ry_outer * 22 + ry_inner * 11 + xx_outer_inner * 2 + rx_outer] * kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 8 * 1344 + ff_c_inner * 336 + rc_outer * 84 + rc_inner * 12 + ry_outer * 6 + ry_inner * 3 + rx_outer]\n                for nn_inner, ff_inner in T.grid(9, 4):\n                    conv2d_nchw_1 = T.Buffer((36000,), data=conv2d_nchw.data)\n                    conv2d_nchw_1[nn_inner * 4000 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 8 * 160 + ff_inner * 40 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 8 * 5 + xx_outer_inner] = conv2d_nchw_local_1[nn_inner * 4 + ff_inner]",
        "data": "9_28_9_9",
        "kernel": "100_28_4_3",
        "bias": "1_100_1_1"
    },
    {
        "op_name": "conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t bias_code = arg_type_ids[2];\n  int32_t conv2d_nchw_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* bias = (((TVMValue*)args)[2].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* bias_1 = (((DLTensor*)bias)[0].data);\n  void* default_function_bias_shape = (((DLTensor*)bias)[0].shape);\n  void* default_function_bias_strides = (((DLTensor*)bias)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_bias_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused = 0; nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused < 198; ++nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused) {\n    void* conv2d_nchw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)16, 2, 32);\n    if (conv2d_nchw_local == NULL) {\n      return -1;\n    },\n    float pad_temp[132];\n    for (int32_t ff_c_outer_outer_inner = 0; ff_c_outer_outer_inner < 2; ++ff_c_outer_outer_inner) {\n      for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 2; ++ff_c_outer_inner_init) {\n        ((float*)conv2d_nchw_local)[((ff_c_outer_outer_inner * 2) + ff_c_outer_inner_init)] = 0.000000e+00f;\n      },\n      for (int32_t rc_outer = 0; rc_outer < 7; ++rc_outer) {\n        for (int32_t i1 = 0; i1 < 11; ++i1) {\n          for (int32_t i2 = 0; i2 < 4; ++i2) {\n            for (int32_t i3_s = 0; i3_s < 3; ++i3_s) {\n              pad_temp[(((i1 * 12) + (i2 * 3)) + i3_s)] = ((((1 <= i2) && (1 <= i3_s)) && (i3_s < 2)) ? ((float*)data_1)[(((((((nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused / 22) * 308) + (rc_outer * 44)) + (i1 * 4)) + i2) + i3_s) - 2)] : 0.000000e+00f);\n            },\n          },\n        },\n        for (int32_t ry_outer = 0; ry_outer < 4; ++ry_outer) {\n          for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 2; ++ff_c_outer_inner) {\n            for (int32_t rc_inner = 0; rc_inner < 11; ++rc_inner) {\n              for (int32_t rx_inner = 0; rx_inner < 3; ++rx_inner) {\n                int32_t cse_var_3 = (rc_inner * 12);\n                int32_t cse_var_2 = (ry_outer * 3);\n                int32_t cse_var_1 = ((ff_c_outer_outer_inner * 2) + ff_c_outer_inner);\n                ((float*)conv2d_nchw_local)[cse_var_1] = (((float*)conv2d_nchw_local)[cse_var_1] + (pad_temp[((cse_var_3 + cse_var_2) + rx_inner)] * ((float*)kernel_1)[((((((((nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused % 22) * 3696) + (ff_c_outer_outer_inner * 1848)) + (ff_c_outer_inner * 924)) + (rc_outer * 132)) + cse_var_3) + cse_var_2) + rx_inner)]));\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t ff_inner = 0; ff_inner < 4; ++ff_inner) {\n      ((float*)conv2d_nchw_1)[((nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused * 4) + ff_inner)] = ((float*)conv2d_nchw_local)[ff_inner];\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv2d_nchw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(44) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(44) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[3];\n  __shared__ float pad_temp_shared[36];\n  __shared__ float kernel_shared[528];\n  for (int nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 3; ++nn_c_outer_inner_init) {\n    conv2d_nchw_local[nn_c_outer_inner_init] = 0.000000e+00f;\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 77; ++rc_outer_outer) {\n    __syncthreads();\n    if (((int)threadIdx.x) < 36) {\n      pad_temp_shared[((int)threadIdx.x)] = ((((3 <= (((int)threadIdx.x) % 12)) && (1 <= (((int)threadIdx.x) % 3))) && ((((int)threadIdx.x) % 3) < 2)) ? data[(((((((((int)blockIdx.x) >> 1) * 924) + ((((int)threadIdx.x) / 12) * 308)) + (rc_outer_outer * 4)) + ((((int)threadIdx.x) % 12) / 3)) + (((int)threadIdx.x) % 3)) - 2)] : 0.000000e+00f);\n    },\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 6; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n      *(float2*)(kernel_shared + ((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 88) + (((int)threadIdx.x) * 2))) = *(float2*)(kernel + (((((((int)blockIdx.x) & 1) * 40656) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 88) + (((int)threadIdx.x) * 2)) / 12) * 924)) + (rc_outer_outer * 12)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 4) + (((int)threadIdx.x) * 2)) % 12)));\n    },\n    __syncthreads();\n    for (int ry_outer_inner = 0; ry_outer_inner < 4; ++ry_outer_inner) {\n      for (int nn_c_outer_inner = 0; nn_c_outer_inner < 3; ++nn_c_outer_inner) {\n        for (int rx_inner = 0; rx_inner < 3; ++rx_inner) {\n          conv2d_nchw_local[nn_c_outer_inner] = (conv2d_nchw_local[nn_c_outer_inner] + (pad_temp_shared[(((nn_c_outer_inner * 12) + (ry_outer_inner * 3)) + rx_inner)] * kernel_shared[(((((int)threadIdx.x) * 12) + (ry_outer_inner * 3)) + rx_inner)]));\n        },\n      },\n    },\n  },\n  for (int nn_inner = 0; nn_inner < 3; ++nn_inner) {\n    conv2d_nchw[(((((((int)blockIdx.x) >> 1) * 264) + (nn_inner * 88)) + ((((int)blockIdx.x) & 1) * 44)) + ((int)threadIdx.x))] = conv2d_nchw_local[nn_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 77, 4, 1), \"float32\"), kernel: T.Buffer((88, 77, 4, 3), \"float32\"), bias: T.Buffer((1, 88, 1, 1), \"float32\"), conv2d_nchw: T.Buffer((9, 88, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused in T.parallel(198):\n            conv2d_nchw_local = T.allocate([4], \"float32\", \"local\")\n            pad_temp = T.allocate([132], \"float32\", \"global\")\n            conv2d_nchw_local_1 = T.Buffer((4,), data=conv2d_nchw_local, scope=\"local\", align=16)\n            for ff_c_outer_outer_inner in range(2):\n                for ff_c_outer_inner_init in range(2):\n                    conv2d_nchw_local_1[ff_c_outer_outer_inner * 2 + ff_c_outer_inner_init] = T.float32(0)\n                for rc_outer in range(7):\n                    pad_temp_1 = T.Buffer((132,), data=pad_temp)\n                    for i1, i2, i3_s in T.grid(11, 4, 3):\n                        data_1 = T.Buffer((2772,), data=data.data)\n                        pad_temp_1[i1 * 12 + i2 * 3 + i3_s] = T.if_then_else(1 <= i2 and 1 <= i3_s and i3_s < 2, data_1[nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused // 22 * 308 + rc_outer * 44 + i1 * 4 + i2 + i3_s - 2], T.float32(0))\n                    for ry_outer, ff_c_outer_inner, rc_inner, rx_inner in T.grid(4, 2, 11, 3):\n                        cse_var_3: T.int32 = rc_inner * 12\n                        cse_var_2: T.int32 = ry_outer * 3\n                        cse_var_1: T.int32 = ff_c_outer_outer_inner * 2 + ff_c_outer_inner\n                        kernel_1 = T.Buffer((81312,), data=kernel.data)\n                        conv2d_nchw_local_1[cse_var_1] = conv2d_nchw_local_1[cse_var_1] + pad_temp_1[cse_var_3 + cse_var_2 + rx_inner] * kernel_1[nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused % 22 * 3696 + ff_c_outer_outer_inner * 1848 + ff_c_outer_inner * 924 + rc_outer * 132 + cse_var_3 + cse_var_2 + rx_inner]\n            for ff_inner in range(4):\n                conv2d_nchw_1 = T.Buffer((792,), data=conv2d_nchw.data)\n                conv2d_nchw_1[nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused * 4 + ff_inner] = conv2d_nchw_local_1[ff_inner]",
        "data": "9_77_4_1",
        "kernel": "88_77_4_3",
        "bias": "1_88_1_1"
    },
    {
        "op_name": "conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t bias_code = arg_type_ids[2];\n  int32_t conv2d_nchw_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* bias = (((TVMValue*)args)[2].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* bias_1 = (((DLTensor*)bias)[0].data);\n  void* default_function_bias_shape = (((DLTensor*)bias)[0].shape);\n  void* default_function_bias_strides = (((DLTensor*)bias)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_bias_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused < 73; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)10800, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t nn_inner_init = 0; nn_inner_init < 10; ++nn_inner_init) {\n      for (int32_t yy_inner_init = 0; yy_inner_init < 2; ++yy_inner_init) {\n        *(float3*)(((float*)conv2d_nchw_1) + (((nn_inner_init * 438) + (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused * 6)) + (yy_inner_init * 3))) = ((float3)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 9; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 2; ++ry_outer) {\n        for (int32_t i0 = 0; i0 < 10; ++i0) {\n          for (int32_t i1 = 0; i1 < 9; ++i1) {\n            for (int32_t i2 = 0; i2 < 5; ++i2) {\n              for (int32_t i3_s = 0; i3_s < 6; ++i3_s) {\n                ((float*)pad_temp)[((((i0 * 270) + (i1 * 30)) + (i2 * 6)) + i3_s)] = ((((1 <= ((ry_outer * 2) + i2)) && (1 <= i3_s)) && (i3_s < 5)) ? ((float*)data_1)[(((((((i0 * 1944) + (rc_outer * 216)) + (i1 * 24)) + (ry_outer * 8)) + (i2 * 4)) + i3_s) - 5)] : 0.000000e+00f);\n              },\n            },\n          },\n        },\n        for (int32_t rc_inner = 0; rc_inner < 9; ++rc_inner) {\n          for (int32_t ry_inner = 0; ry_inner < 2; ++ry_inner) {\n            for (int32_t rx_inner = 0; rx_inner < 4; ++rx_inner) {\n              for (int32_t nn_inner = 0; nn_inner < 10; ++nn_inner) {\n                for (int32_t yy_inner = 0; yy_inner < 2; ++yy_inner) {\n                  int32_t cse_var_1 = (((nn_inner * 438) + (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused * 6)) + (yy_inner * 3));\n                  int32_t3 v_ = int32_t3((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2));\n                  int32_t3 v__1 = int32_t3(((((((nn_inner * 270) + (rc_inner * 30)) + (yy_inner * 18)) + (ry_inner * 6)) + rx_inner))+(1*0), ((((((nn_inner * 270) + (rc_inner * 30)) + (yy_inner * 18)) + (ry_inner * 6)) + rx_inner))+(1*1), ((((((nn_inner * 270) + (rc_inner * 30)) + (yy_inner * 18)) + (ry_inner * 6)) + rx_inner))+(1*2));\n                  *(float3*)(((float*)conv2d_nchw_1) + cse_var_1) = ((float3(((float*)conv2d_nchw_1)[v_.s0],((float*)conv2d_nchw_1)[v_.s1],((float*)conv2d_nchw_1)[v_.s2])) + ((float3(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1],((float*)pad_temp)[v__1.s2])) * ((float3)(((float*)kernel_1)[((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused * 1296) + (rc_outer * 144)) + (rc_inner * 16)) + (ry_outer * 8)) + (ry_inner * 4)) + rx_inner)], ((float*)kernel_1)[((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused * 1296) + (rc_outer * 144)) + (rc_inner * 16)) + (ry_outer * 8)) + (ry_inner * 4)) + rx_inner)], ((float*)kernel_1)[((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused * 1296) + (rc_outer * 144)) + (rc_inner * 16)) + (ry_outer * 8)) + (ry_inner * 4)) + rx_inner)]))));\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(146) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(146) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[1];\n  __shared__ float pad_temp_shared[90];\n  __shared__ float kernel_shared[2628];\n  conv2d_nchw_local[0] = 0.000000e+00f;\n  for (int rc_outer_outer = 0; rc_outer_outer < 9; ++rc_outer_outer) {\n    for (int ry_outer_outer = 0; ry_outer_outer < 2; ++ry_outer_outer) {\n      for (int rx_outer_outer = 0; rx_outer_outer < 2; ++rx_outer_outer) {\n        __syncthreads();\n        if (((int)threadIdx.x) < 90) {\n          pad_temp_shared[((int)threadIdx.x)] = ((((1 <= ((ry_outer_outer * 2) + ((((int)threadIdx.x) % 10) >> 1))) && (1 <= (((rx_outer_outer * 2) + (((int)blockIdx.x) % 3)) + (((int)threadIdx.x) & 1)))) && ((((rx_outer_outer * 2) + (((int)blockIdx.x) % 3)) + (((int)threadIdx.x) & 1)) < 5)) ? data[((((((((((((int)blockIdx.x) / 3) * 1944) + (rc_outer_outer * 216)) + ((((int)threadIdx.x) / 10) * 24)) + (ry_outer_outer * 8)) + (((((int)threadIdx.x) % 10) >> 1) * 4)) + (rx_outer_outer * 2)) + (((int)blockIdx.x) % 3)) + (((int)threadIdx.x) & 1)) - 5)] : 0.000000e+00f);\n        },\n        for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 18; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n          kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 146) + ((int)threadIdx.x))] = kernel[((((((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 73) + (((int)threadIdx.x) >> 1)) / 18) * 1296) + (rc_outer_outer * 144)) + (((((((int)threadIdx.x) >> 1) + ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) % 18) >> 1) * 16)) + (ry_outer_outer * 8)) + ((((((int)threadIdx.x) >> 1) + ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) & 1) * 4)) + (rx_outer_outer * 2)) + (((int)threadIdx.x) & 1))];\n        },\n        __syncthreads();\n        for (int rc_inner = 0; rc_inner < 9; ++rc_inner) {\n          for (int ry_inner = 0; ry_inner < 2; ++ry_inner) {\n            for (int rx_inner = 0; rx_inner < 2; ++rx_inner) {\n              conv2d_nchw_local[0] = (conv2d_nchw_local[0] + (pad_temp_shared[((((rc_inner * 10) + ((((int)threadIdx.x) & 1) * 6)) + (ry_inner * 2)) + rx_inner)] * kernel_shared[(((((((int)threadIdx.x) >> 1) * 36) + (rc_inner * 4)) + (ry_inner * 2)) + rx_inner)]));\n            },\n          },\n        },\n      },\n    },\n  },\n  conv2d_nchw[((((((int)blockIdx.x) / 3) * 438) + (((int)threadIdx.x) * 3)) + (((int)blockIdx.x) % 3))] = conv2d_nchw_local[0];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 81, 6, 4), \"float32\"), kernel: T.Buffer((73, 81, 4, 4), \"float32\"), bias: T.Buffer((1, 73, 1, 1), \"float32\"), conv2d_nchw: T.Buffer((10, 73, 2, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused in T.parallel(73):\n            pad_temp = T.allocate([2700], \"float32\", \"global\")\n            conv2d_nchw_1 = T.Buffer((4380,), data=conv2d_nchw.data)\n            for nn_inner_init, yy_inner_init in T.grid(10, 2):\n                conv2d_nchw_1[nn_inner_init * 438 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused * 6 + yy_inner_init * 3:nn_inner_init * 438 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused * 6 + yy_inner_init * 3 + 3] = T.Broadcast(T.float32(0), 3)\n            for rc_outer, ry_outer in T.grid(9, 2):\n                pad_temp_1 = T.Buffer((2700,), data=pad_temp)\n                for i0, i1, i2, i3_s in T.grid(10, 9, 5, 6):\n                    data_1 = T.Buffer((19440,), data=data.data)\n                    pad_temp_1[i0 * 270 + i1 * 30 + i2 * 6 + i3_s] = T.if_then_else(1 <= ry_outer * 2 + i2 and 1 <= i3_s and i3_s < 5, data_1[i0 * 1944 + rc_outer * 216 + i1 * 24 + ry_outer * 8 + i2 * 4 + i3_s - 5], T.float32(0))\n                for rc_inner, ry_inner, rx_inner, nn_inner, yy_inner in T.grid(9, 2, 4, 10, 2):\n                    cse_var_1: T.int32 = nn_inner * 438 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused * 6 + yy_inner * 3\n                    kernel_1 = T.Buffer((94608,), data=kernel.data)\n                    conv2d_nchw_1[cse_var_1:cse_var_1 + 3] = conv2d_nchw_1[cse_var_1:cse_var_1 + 3] + pad_temp_1[nn_inner * 270 + rc_inner * 30 + yy_inner * 18 + ry_inner * 6 + rx_inner:nn_inner * 270 + rc_inner * 30 + yy_inner * 18 + ry_inner * 6 + rx_inner + 3] * T.Broadcast(kernel_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused * 1296 + rc_outer * 144 + rc_inner * 16 + ry_outer * 8 + ry_inner * 4 + rx_inner], 3)",
        "data": "10_81_6_4",
        "kernel": "73_81_4_4",
        "bias": "1_73_1_1"
    },
    {
        "op_name": "conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t bias_code = arg_type_ids[2];\n  int32_t conv2d_nchw_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* bias = (((TVMValue*)args)[2].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* bias_1 = (((DLTensor*)bias)[0].data);\n  void* default_function_bias_shape = (((DLTensor*)bias)[0].shape);\n  void* default_function_bias_strides = (((DLTensor*)bias)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_bias_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused < 80; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused) {\n    void* conv2d_nchw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)40, 2, 32);\n    if (conv2d_nchw_local == NULL) {\n      return -1;\n    },\n    float pad_temp[24];\n    for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 5; ++ff_c_outer_inner_init) {\n      for (int32_t nn_c_inner_init = 0; nn_c_inner_init < 2; ++nn_c_inner_init) {\n        ((float*)conv2d_nchw_local)[((nn_c_inner_init * 5) + ff_c_outer_inner_init)] = 0.000000e+00f;\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 25; ++rc_outer) {\n      for (int32_t rx_outer = 0; rx_outer < 4; ++rx_outer) {\n        for (int32_t i0 = 0; i0 < 2; ++i0) {\n          for (int32_t i1_i2_fused_i3_fused_s = 0; i1_i2_fused_i3_fused_s < 12; ++i1_i2_fused_i3_fused_s) {\n            int32_t cse_var_3 = (nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 5);\n            int32_t cse_var_2 = (i1_i2_fused_i3_fused_s & 3);\n            int32_t cse_var_1 = (rx_outer + cse_var_3);\n            pad_temp[((i0 * 12) + i1_i2_fused_i3_fused_s)] = (((((1 <= cse_var_2) && (cse_var_2 < 3)) && (1 <= cse_var_1)) && (cse_var_1 < 7)) ? ((float*)data_1)[(((((((i0 * 900) + (rc_outer * 36)) + ((i1_i2_fused_i3_fused_s >> 2) * 12)) + (cse_var_2 * 6)) + rx_outer) + cse_var_3) - 7)] : 0.000000e+00f);\n          },\n        },\n        for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 5; ++ff_c_outer_inner) {\n          for (int32_t rc_inner = 0; rc_inner < 3; ++rc_inner) {\n            for (int32_t ry_inner = 0; ry_inner < 4; ++ry_inner) {\n              for (int32_t nn_c_inner = 0; nn_c_inner < 2; ++nn_c_inner) {\n                int32_t cse_var_4 = ((nn_c_inner * 5) + ff_c_outer_inner);\n                ((float*)conv2d_nchw_local)[cse_var_4] = (((float*)conv2d_nchw_local)[cse_var_4] + (pad_temp[(((nn_c_inner * 12) + (rc_inner * 4)) + ry_inner)] * ((float*)kernel_1)[(((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused / 5) * 6000) + (ff_c_outer_inner * 1200)) + (rc_outer * 48)) + (rc_inner * 16)) + (ry_inner * 4)) + rx_outer)]));\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 2; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 5; ++ff_inner) {\n        ((float*)conv2d_nchw_1)[((((nn_inner * 400) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused / 5) * 25)) + (ff_inner * 5)) + (nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 5))] = ((float*)conv2d_nchw_local)[((nn_inner * 5) + ff_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv2d_nchw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(80) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(80) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[1];\n  __shared__ float pad_temp_shared[40];\n  __shared__ float kernel_shared[3200];\n  conv2d_nchw_local[0] = 0.000000e+00f;\n  for (int rc_outer_outer = 0; rc_outer_outer < 15; ++rc_outer_outer) {\n    for (int rx_outer_outer = 0; rx_outer_outer < 2; ++rx_outer_outer) {\n      __syncthreads();\n      if (((int)threadIdx.x) < 40) {\n        pad_temp_shared[((int)threadIdx.x)] = (((((2 <= (((int)threadIdx.x) & 7)) && ((((int)threadIdx.x) & 7) < 6)) && (1 <= (((rx_outer_outer * 2) + (((int)blockIdx.x) % 5)) + (((int)threadIdx.x) & 1)))) && ((((rx_outer_outer * 2) + (((int)blockIdx.x) % 5)) + (((int)threadIdx.x) & 1)) < 7)) ? data[(((((((((((int)blockIdx.x) / 5) * 900) + (rc_outer_outer * 60)) + ((((int)threadIdx.x) >> 3) * 12)) + (((((int)threadIdx.x) & 7) >> 1) * 6)) + (rx_outer_outer * 2)) + (((int)blockIdx.x) % 5)) + (((int)threadIdx.x) & 1)) - 7)] : 0.000000e+00f);\n      },\n      for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 40; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n        kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 80) + ((int)threadIdx.x))] = kernel[((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 2400) + ((((int)threadIdx.x) / 40) * 1200)) + (rc_outer_outer * 80)) + (((((int)threadIdx.x) % 40) >> 1) * 4)) + (rx_outer_outer * 2)) + (((int)threadIdx.x) & 1))];\n      },\n      __syncthreads();\n      for (int rc_outer_inner = 0; rc_outer_inner < 5; ++rc_outer_inner) {\n        for (int rx_outer_inner = 0; rx_outer_inner < 2; ++rx_outer_inner) {\n          for (int ry_inner = 0; ry_inner < 4; ++ry_inner) {\n            conv2d_nchw_local[0] = (conv2d_nchw_local[0] + (pad_temp_shared[(((rc_outer_inner * 8) + (ry_inner * 2)) + rx_outer_inner)] * kernel_shared[((((((int)threadIdx.x) * 40) + (rc_outer_inner * 8)) + (ry_inner * 2)) + rx_outer_inner)]));\n          },\n        },\n      },\n    },\n  },\n  conv2d_nchw[((((((int)blockIdx.x) / 5) * 400) + (((int)threadIdx.x) * 5)) + (((int)blockIdx.x) % 5))] = conv2d_nchw_local[0];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 75, 2, 6), \"float32\"), kernel: T.Buffer((80, 75, 4, 4), \"float32\"), bias: T.Buffer((1, 80, 1, 1), \"float32\"), conv2d_nchw: T.Buffer((2, 80, 1, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused in T.parallel(80):\n            conv2d_nchw_local = T.allocate([10], \"float32\", \"local\")\n            pad_temp = T.allocate([24], \"float32\", \"global\")\n            conv2d_nchw_local_1 = T.Buffer((10,), data=conv2d_nchw_local, scope=\"local\", align=32)\n            for ff_c_outer_inner_init, nn_c_inner_init in T.grid(5, 2):\n                conv2d_nchw_local_1[nn_c_inner_init * 5 + ff_c_outer_inner_init] = T.float32(0)\n            for rc_outer, rx_outer in T.grid(25, 4):\n                pad_temp_1 = T.Buffer((24,), data=pad_temp)\n                for i0, i1_i2_fused_i3_fused_s in T.grid(2, 12):\n                    cse_var_3: T.int32 = nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 5\n                    cse_var_2: T.int32 = i1_i2_fused_i3_fused_s % 4\n                    cse_var_1: T.int32 = rx_outer + cse_var_3\n                    data_1 = T.Buffer((1800,), data=data.data)\n                    pad_temp_1[i0 * 12 + i1_i2_fused_i3_fused_s] = T.if_then_else(1 <= cse_var_2 and cse_var_2 < 3 and 1 <= cse_var_1 and cse_var_1 < 7, data_1[i0 * 900 + rc_outer * 36 + i1_i2_fused_i3_fused_s // 4 * 12 + cse_var_2 * 6 + rx_outer + cse_var_3 - 7], T.float32(0))\n                for ff_c_outer_inner, rc_inner, ry_inner, nn_c_inner in T.grid(5, 3, 4, 2):\n                    cse_var_4: T.int32 = nn_c_inner * 5 + ff_c_outer_inner\n                    kernel_1 = T.Buffer((96000,), data=kernel.data)\n                    conv2d_nchw_local_1[cse_var_4] = conv2d_nchw_local_1[cse_var_4] + pad_temp_1[nn_c_inner * 12 + rc_inner * 4 + ry_inner] * kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused // 5 * 6000 + ff_c_outer_inner * 1200 + rc_outer * 48 + rc_inner * 16 + ry_inner * 4 + rx_outer]\n            for nn_inner, ff_inner in T.grid(2, 5):\n                conv2d_nchw_1 = T.Buffer((800,), data=conv2d_nchw.data)\n                conv2d_nchw_1[nn_inner * 400 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused // 5 * 25 + ff_inner * 5 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 5] = conv2d_nchw_local_1[nn_inner * 5 + ff_inner]",
        "data": "2_75_2_6",
        "kernel": "80_75_4_4",
        "bias": "1_80_1_1"
    },
    {
        "op_name": "conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t bias_code = arg_type_ids[2];\n  int32_t conv2d_nchw_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* bias = (((TVMValue*)args)[2].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* bias_1 = (((DLTensor*)bias)[0].data);\n  void* default_function_bias_shape = (((DLTensor*)bias)[0].shape);\n  void* default_function_bias_strides = (((DLTensor*)bias)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_bias_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused < 224; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)17388, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t i1 = 0; i1 < 69; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        for (int32_t i3_s = 0; i3_s < 7; ++i3_s) {\n          ((float*)pad_temp)[(((i1 * 63) + (i2 * 7)) + i3_s)] = (((1 <= i2) && (1 <= i3_s)) ? ((float*)data_1)[((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused >> 5) * 3312) + (i1 * 48)) + (i2 * 6)) + i3_s) - 7)] : 0.000000e+00f);\n        },\n      },\n    },\n    for (int32_t ff_inner_init = 0; ff_inner_init < 3; ++ff_inner_init) {\n      for (int32_t yy_inner_init = 0; yy_inner_init < 4; ++yy_inner_init) {\n        *(float3*)(((float*)conv2d_nchw_1) + (((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused * 36) + (ff_inner_init * 12)) + (yy_inner_init * 3))) = ((float3)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 23; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n        for (int32_t rx_outer = 0; rx_outer < 3; ++rx_outer) {\n          for (int32_t rc_inner = 0; rc_inner < 3; ++rc_inner) {\n            for (int32_t ff_inner = 0; ff_inner < 3; ++ff_inner) {\n              for (int32_t yy_inner = 0; yy_inner < 4; ++yy_inner) {\n                int32_t cse_var_1 = (((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused * 36) + (ff_inner * 12)) + (yy_inner * 3));\n                int32_t3 v_ = int32_t3((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2));\n                int32_t3 v__1 = int32_t3(((((((rc_outer * 189) + (rc_inner * 63)) + (yy_inner * 14)) + (ry_outer * 7)) + rx_outer))+(2*0), ((((((rc_outer * 189) + (rc_inner * 63)) + (yy_inner * 14)) + (ry_outer * 7)) + rx_outer))+(2*1), ((((((rc_outer * 189) + (rc_inner * 63)) + (yy_inner * 14)) + (ry_outer * 7)) + rx_outer))+(2*2));\n                *(float3*)(((float*)conv2d_nchw_1) + cse_var_1) = ((float3(((float*)conv2d_nchw_1)[v_.s0],((float*)conv2d_nchw_1)[v_.s1],((float*)conv2d_nchw_1)[v_.s2])) + ((float3(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1],((float*)pad_temp)[v__1.s2])) * ((float3)(((float*)kernel_1)[(((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused & 31) * 1863) + (ff_inner * 621)) + (rc_outer * 27)) + (rc_inner * 9)) + (ry_outer * 3)) + rx_outer)], ((float*)kernel_1)[(((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused & 31) * 1863) + (ff_inner * 621)) + (rc_outer * 27)) + (rc_inner * 9)) + (ry_outer * 3)) + rx_outer)], ((float*)kernel_1)[(((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused & 31) * 1863) + (ff_inner * 621)) + (rc_outer * 27)) + (rc_inner * 9)) + (ry_outer * 3)) + rx_outer)]))));\n              },\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[28];\n  __shared__ float pad_temp_shared[567];\n  __shared__ float kernel_shared[1296];\n  for (int nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 7; ++nn_c_outer_inner_init) {\n    for (int ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 2; ++ff_c_outer_inner_init) {\n      for (int yy_c_inner_init = 0; yy_c_inner_init < 2; ++yy_c_inner_init) {\n        conv2d_nchw_local[(((nn_c_outer_inner_init * 4) + (ff_c_outer_inner_init * 2)) + yy_c_inner_init)] = 0.000000e+00f;\n      },\n    },\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 23; ++rc_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 12; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 16) + (((int)threadIdx.x) / 3)) < 189) {\n        pad_temp_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 48) + ((int)threadIdx.x))] = (((1 <= (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 7) + (((int)threadIdx.x) / 3)) % 9)) && (1 <= (((((int)blockIdx.x) % 3) * 2) + (((int)threadIdx.x) % 3)))) ? data[((((((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 16) + (((int)threadIdx.x) / 3)) / 27) * 3312) + (rc_outer_outer * 144)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 16) + (((int)threadIdx.x) / 3)) % 27) / 9) * 48)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 7) + (((int)threadIdx.x) / 3)) % 9) * 6)) + ((((int)blockIdx.x) % 3) * 2)) + (((int)threadIdx.x) % 3)) - 7)] : 0.000000e+00f);\n      },\n    },\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 < 27; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) {\n      kernel_shared[(((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 16) + (((int)threadIdx.x) / 3)) / 9) * 27) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 7) + (((int)threadIdx.x) / 3)) % 9) / 3) * 9)) + ((((((int)threadIdx.x) / 3) + ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) % 3) * 3)) + (((int)threadIdx.x) % 3))] = kernel[(((((((((int)blockIdx.x) / 3) * 29808) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 16) + (((int)threadIdx.x) / 3)) / 9) * 621)) + (rc_outer_outer * 27)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 7) + (((int)threadIdx.x) / 3)) % 9) / 3) * 9)) + ((((((int)threadIdx.x) / 3) + ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) % 3) * 3)) + (((int)threadIdx.x) % 3))];\n    },\n    __syncthreads();\n    for (int rx_outer_inner = 0; rx_outer_inner < 3; ++rx_outer_inner) {\n      for (int nn_c_outer_inner = 0; nn_c_outer_inner < 7; ++nn_c_outer_inner) {\n        for (int ff_c_outer_inner = 0; ff_c_outer_inner < 2; ++ff_c_outer_inner) {\n          for (int rc_inner = 0; rc_inner < 3; ++rc_inner) {\n            for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n              for (int yy_c_inner = 0; yy_c_inner < 2; ++yy_c_inner) {\n                conv2d_nchw_local[(((nn_c_outer_inner * 4) + (ff_c_outer_inner * 2)) + yy_c_inner)] = (conv2d_nchw_local[(((nn_c_outer_inner * 4) + (ff_c_outer_inner * 2)) + yy_c_inner)] + (pad_temp_shared[((((((nn_c_outer_inner * 81) + (rc_inner * 27)) + ((((int)threadIdx.x) & 1) * 12)) + (yy_c_inner * 6)) + (ry_inner * 3)) + rx_outer_inner)] * kernel_shared[((((((((int)threadIdx.x) >> 1) * 54) + (ff_c_outer_inner * 27)) + (rc_inner * 9)) + (ry_inner * 3)) + rx_outer_inner)]));\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int nn_inner = 0; nn_inner < 7; ++nn_inner) {\n    for (int ff_inner = 0; ff_inner < 2; ++ff_inner) {\n      for (int yy_inner = 0; yy_inner < 2; ++yy_inner) {\n        conv2d_nchw[(((((((nn_inner * 1152) + ((((int)blockIdx.x) / 3) * 576)) + ((((int)threadIdx.x) >> 1) * 24)) + (ff_inner * 12)) + ((((int)threadIdx.x) & 1) * 6)) + (yy_inner * 3)) + (((int)blockIdx.x) % 3))] = conv2d_nchw_local[(((nn_inner * 4) + (ff_inner * 2)) + yy_inner)];\n      },\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 69, 8, 6), \"float32\"), kernel: T.Buffer((96, 69, 3, 3), \"float32\"), bias: T.Buffer((1, 96, 1, 1), \"float32\"), conv2d_nchw: T.Buffer((7, 96, 4, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused in T.parallel(224):\n            pad_temp = T.allocate([4347], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((4347,), data=pad_temp)\n            for i1, i2, i3_s in T.grid(69, 9, 7):\n                data_1 = T.Buffer((23184,), data=data.data)\n                pad_temp_1[i1 * 63 + i2 * 7 + i3_s] = T.if_then_else(1 <= i2 and 1 <= i3_s, data_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused // 32 * 3312 + i1 * 48 + i2 * 6 + i3_s - 7], T.float32(0))\n            conv2d_nchw_1 = T.Buffer((8064,), data=conv2d_nchw.data)\n            for ff_inner_init, yy_inner_init in T.grid(3, 4):\n                conv2d_nchw_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused * 36 + ff_inner_init * 12 + yy_inner_init * 3:nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused * 36 + ff_inner_init * 12 + yy_inner_init * 3 + 3] = T.Broadcast(T.float32(0), 3)\n            for rc_outer, ry_outer, rx_outer, rc_inner, ff_inner, yy_inner in T.grid(23, 3, 3, 3, 3, 4):\n                cse_var_1: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused * 36 + ff_inner * 12 + yy_inner * 3\n                kernel_1 = T.Buffer((59616,), data=kernel.data)\n                conv2d_nchw_1[cse_var_1:cse_var_1 + 3] = conv2d_nchw_1[cse_var_1:cse_var_1 + 3] + pad_temp_1[rc_outer * 189 + rc_inner * 63 + yy_inner * 14 + ry_outer * 7 + rx_outer:rc_outer * 189 + rc_inner * 63 + yy_inner * 14 + ry_outer * 7 + rx_outer + 6:2] * T.Broadcast(kernel_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 32 * 1863 + ff_inner * 621 + rc_outer * 27 + rc_inner * 9 + ry_outer * 3 + rx_outer], 3)",
        "data": "7_69_8_6",
        "kernel": "96_69_3_3",
        "bias": "1_96_1_1"
    },
    {
        "op_name": "conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t bias_code = arg_type_ids[2];\n  int32_t conv2d_nchw_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* bias = (((TVMValue*)args)[2].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* bias_1 = (((DLTensor*)bias)[0].data);\n  void* default_function_bias_shape = (((DLTensor*)bias)[0].shape);\n  void* default_function_bias_strides = (((DLTensor*)bias)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_bias_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused < 174; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused) {\n    void* conv2d_nchw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)100, 2, 32);\n    if (conv2d_nchw_local == NULL) {\n      return -1;\n    },\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1024, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t yy_c_inner_init = 0; yy_c_inner_init < 5; ++yy_c_inner_init) {\n      ((float5*)conv2d_nchw_local)[yy_c_inner_init] = ((float5)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n    },\n    for (int32_t rc_outer = 0; rc_outer < 7; ++rc_outer) {\n      for (int32_t i1 = 0; i1 < 4; ++i1) {\n        for (int32_t i2 = 0; i2 < 8; ++i2) {\n          for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n            ((float*)pad_temp)[(((i1 * 64) + (i2 * 8)) + i3_s)] = (((((1 <= i2) && (i2 < 7)) && (1 <= i3_s)) && (i3_s < 7)) ? ((float*)data_1)[(((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused / 58) * 1008) + (rc_outer * 144)) + (i1 * 36)) + (i2 * 6)) + i3_s) - 7)] : 0.000000e+00f);\n          },\n        },\n      },\n      for (int32_t ry_outer = 0; ry_outer < 4; ++ry_outer) {\n        for (int32_t rx_outer = 0; rx_outer < 2; ++rx_outer) {\n          for (int32_t rc_inner = 0; rc_inner < 4; ++rc_inner) {\n            for (int32_t rx_inner = 0; rx_inner < 2; ++rx_inner) {\n              for (int32_t yy_c_inner = 0; yy_c_inner < 5; ++yy_c_inner) {\n                int32_t cse_var_1 = (rx_outer * 2);\n                int32_t5 v_ = int32_t5(((((((rc_inner * 64) + (yy_c_inner * 8)) + (ry_outer * 8)) + cse_var_1) + rx_inner))+(1*0), ((((((rc_inner * 64) + (yy_c_inner * 8)) + (ry_outer * 8)) + cse_var_1) + rx_inner))+(1*1), ((((((rc_inner * 64) + (yy_c_inner * 8)) + (ry_outer * 8)) + cse_var_1) + rx_inner))+(1*2), ((((((rc_inner * 64) + (yy_c_inner * 8)) + (ry_outer * 8)) + cse_var_1) + rx_inner))+(1*3), ((((((rc_inner * 64) + (yy_c_inner * 8)) + (ry_outer * 8)) + cse_var_1) + rx_inner))+(1*4));\n                ((float5*)conv2d_nchw_local)[yy_c_inner] = (((float5*)conv2d_nchw_local)[yy_c_inner] + ((float5(((float*)pad_temp)[v_.s0],((float*)pad_temp)[v_.s1],((float*)pad_temp)[v_.s2],((float*)pad_temp)[v_.s3],((float*)pad_temp)[v_.s4])) * ((float5)(((float*)kernel_1)[(((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 58) * 448) + (rc_outer * 64)) + (rc_inner * 16)) + (ry_outer * 4)) + cse_var_1) + rx_inner)], ((float*)kernel_1)[(((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 58) * 448) + (rc_outer * 64)) + (rc_inner * 16)) + (ry_outer * 4)) + cse_var_1) + rx_inner)], ((float*)kernel_1)[(((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 58) * 448) + (rc_outer * 64)) + (rc_inner * 16)) + (ry_outer * 4)) + cse_var_1) + rx_inner)], ((float*)kernel_1)[(((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 58) * 448) + (rc_outer * 64)) + (rc_inner * 16)) + (ry_outer * 4)) + cse_var_1) + rx_inner)], ((float*)kernel_1)[(((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 58) * 448) + (rc_outer * 64)) + (rc_inner * 16)) + (ry_outer * 4)) + cse_var_1) + rx_inner)]))));\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t yy_inner = 0; yy_inner < 5; ++yy_inner) {\n      *(float5*)(((float*)conv2d_nchw_1) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused * 25) + (yy_inner * 5))) = ((float5*)conv2d_nchw_local)[yy_inner];\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv2d_nchw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(145) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(145) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[6];\n  __shared__ float pad_temp_shared[48];\n  __shared__ float kernel_shared[464];\n  for (int nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 3; ++nn_c_outer_inner_init) {\n    for (int ff_c_inner_init = 0; ff_c_inner_init < 2; ++ff_c_inner_init) {\n      conv2d_nchw_local[((nn_c_outer_inner_init * 2) + ff_c_inner_init)] = 0.000000e+00f;\n    },\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 14; ++rc_outer_outer) {\n    for (int rx_outer_outer = 0; rx_outer_outer < 4; ++rx_outer_outer) {\n      __syncthreads();\n      if (((int)threadIdx.x) < 2) {\n        for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_s = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_s < 24; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_s) {\n          pad_temp_shared[((((int)threadIdx.x) * 24) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_s)] = (((((1 <= (ax0_ax1_fused_ax2_fused_ax3_fused_inner_s & 7)) && ((ax0_ax1_fused_ax2_fused_ax3_fused_inner_s & 7) < 7)) && (1 <= (rx_outer_outer + ((int)blockIdx.x)))) && ((rx_outer_outer + ((int)blockIdx.x)) < 7)) ? data[((((((((((((int)threadIdx.x) * 3) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_s >> 3)) >> 1) * 1008) + (rc_outer_outer * 72)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_inner_s >> 3) + ((int)threadIdx.x)) & 1) * 36)) + ((ax0_ax1_fused_ax2_fused_ax3_fused_inner_s & 7) * 6)) + rx_outer_outer) + ((int)blockIdx.x)) - 7)] : 0.000000e+00f);\n        },\n      },\n      for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 5) + (((int)threadIdx.x) / 29)) < 16) {\n          kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 145) + ((int)threadIdx.x))] = kernel[(((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 145) + ((int)threadIdx.x)) >> 3) * 448) + (rc_outer_outer * 32)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer + ((int)threadIdx.x)) & 7) * 4)) + rx_outer_outer)];\n        },\n      },\n      __syncthreads();\n      for (int rc_outer_inner = 0; rc_outer_inner < 2; ++rc_outer_inner) {\n        for (int ry_outer_inner = 0; ry_outer_inner < 4; ++ry_outer_inner) {\n          for (int nn_c_outer_inner = 0; nn_c_outer_inner < 3; ++nn_c_outer_inner) {\n            for (int ff_c_inner = 0; ff_c_inner < 2; ++ff_c_inner) {\n              conv2d_nchw_local[((nn_c_outer_inner * 2) + ff_c_inner)] = (conv2d_nchw_local[((nn_c_outer_inner * 2) + ff_c_inner)] + (pad_temp_shared[((((nn_c_outer_inner * 16) + (rc_outer_inner * 8)) + ry_outer_inner) + (((int)threadIdx.x) % 5))] * kernel_shared[(((((((int)threadIdx.x) / 5) * 16) + (ff_c_inner * 8)) + (rc_outer_inner * 4)) + ry_outer_inner)]));\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int nn_inner = 0; nn_inner < 3; ++nn_inner) {\n    for (int ff_inner = 0; ff_inner < 2; ++ff_inner) {\n      conv2d_nchw[(((((nn_inner * 1450) + ((((int)threadIdx.x) / 5) * 50)) + (ff_inner * 25)) + ((((int)threadIdx.x) % 5) * 5)) + ((int)blockIdx.x))] = conv2d_nchw_local[((nn_inner * 2) + ff_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 28, 6, 6), \"float32\"), kernel: T.Buffer((58, 28, 4, 4), \"float32\"), bias: T.Buffer((1, 58, 1, 1), \"float32\"), conv2d_nchw: T.Buffer((3, 58, 5, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused in T.parallel(174):\n            conv2d_nchw_local = T.allocate([5], \"float32x5\", \"local\")\n            pad_temp = T.allocate([256], \"float32\", \"global\")\n            conv2d_nchw_local_1 = T.Buffer((5,), \"float32x5\", data=conv2d_nchw_local, scope=\"local\")\n            for yy_c_inner_init in range(5):\n                conv2d_nchw_local_1[yy_c_inner_init] = T.Broadcast(T.float32(0), 5)\n            for rc_outer in range(7):\n                pad_temp_1 = T.Buffer((256,), data=pad_temp)\n                for i1, i2, i3_s in T.grid(4, 8, 8):\n                    data_1 = T.Buffer((3024,), data=data.data)\n                    pad_temp_1[i1 * 64 + i2 * 8 + i3_s] = T.if_then_else(1 <= i2 and i2 < 7 and 1 <= i3_s and i3_s < 7, data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused // 58 * 1008 + rc_outer * 144 + i1 * 36 + i2 * 6 + i3_s - 7], T.float32(0))\n                for ry_outer, rx_outer, rc_inner, rx_inner, yy_c_inner in T.grid(4, 2, 4, 2, 5):\n                    cse_var_1: T.int32 = rx_outer * 2\n                    kernel_1 = T.Buffer((25984,), data=kernel.data)\n                    conv2d_nchw_local_1[yy_c_inner] = conv2d_nchw_local_1[yy_c_inner] + pad_temp_1[rc_inner * 64 + yy_c_inner * 8 + ry_outer * 8 + cse_var_1 + rx_inner:rc_inner * 64 + yy_c_inner * 8 + ry_outer * 8 + cse_var_1 + rx_inner + 5] * T.Broadcast(kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 58 * 448 + rc_outer * 64 + rc_inner * 16 + ry_outer * 4 + cse_var_1 + rx_inner], 5)\n            for yy_inner in range(5):\n                conv2d_nchw_1 = T.Buffer((4350,), data=conv2d_nchw.data)\n                conv2d_nchw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused * 25 + yy_inner * 5:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused * 25 + yy_inner * 5 + 5] = conv2d_nchw_local_1[yy_inner]",
        "data": "3_28_6_6",
        "kernel": "58_28_4_4",
        "bias": "1_58_1_1"
    },
    {
        "op_name": "conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t bias_code = arg_type_ids[2];\n  int32_t conv2d_nchw_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* bias = (((TVMValue*)args)[2].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* bias_1 = (((DLTensor*)bias)[0].data);\n  void* default_function_bias_shape = (((DLTensor*)bias)[0].shape);\n  void* default_function_bias_strides = (((DLTensor*)bias)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_bias_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused < 18; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1404, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3_s = 0; i3_s < 9; ++i3_s) {\n          int32_t cse_var_1 = ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 6) / 3);\n          ((float*)pad_temp)[(((i1 * 27) + (i2 * 9)) + i3_s)] = (((((1 <= ((cse_var_1 * 2) + i2)) && ((cse_var_1 + (i2 >> 1)) < 2)) && (1 <= i3_s)) && (i3_s < 8)) ? ((float*)data_1)[((((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused / 6) * 819) + ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 3) * 273)) + (i1 * 21)) + (cse_var_1 * 14)) + (i2 * 7)) + i3_s) - 8)] : 0.000000e+00f);\n        },\n      },\n    },\n    for (int32_t ff_outer_inner_init = 0; ff_outer_inner_init < 29; ++ff_outer_inner_init) {\n      for (int32_t ff_inner_init = 0; ff_inner_init < 2; ++ff_inner_init) {\n        *(float7*)(((float*)conv2d_nchw_1) + ((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused / 6) * 2436) + ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 3) * 812)) + (ff_outer_inner_init * 28)) + (ff_inner_init * 14)) + (((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 6) / 3) * 7))) = ((float7)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 13; ++rc_outer) {\n      for (int32_t ff_outer_inner = 0; ff_outer_inner < 29; ++ff_outer_inner) {\n        for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n          for (int32_t rx_inner = 0; rx_inner < 3; ++rx_inner) {\n            for (int32_t ff_inner = 0; ff_inner < 2; ++ff_inner) {\n              int32_t cse_var_2 = ((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused / 6) * 2436) + ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 3) * 812)) + (ff_outer_inner * 28)) + (ff_inner * 14)) + (((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 6) / 3) * 7));\n              int32_t7 v_ = int32_t7((cse_var_2)+(1*0), (cse_var_2)+(1*1), (cse_var_2)+(1*2), (cse_var_2)+(1*3), (cse_var_2)+(1*4), (cse_var_2)+(1*5), (cse_var_2)+(1*6));\n              int32_t7 v__1 = int32_t7(((((rc_outer * 27) + (ry_inner * 9)) + rx_inner))+(1*0), ((((rc_outer * 27) + (ry_inner * 9)) + rx_inner))+(1*1), ((((rc_outer * 27) + (ry_inner * 9)) + rx_inner))+(1*2), ((((rc_outer * 27) + (ry_inner * 9)) + rx_inner))+(1*3), ((((rc_outer * 27) + (ry_inner * 9)) + rx_inner))+(1*4), ((((rc_outer * 27) + (ry_inner * 9)) + rx_inner))+(1*5), ((((rc_outer * 27) + (ry_inner * 9)) + rx_inner))+(1*6));\n              *(float7*)(((float*)conv2d_nchw_1) + cse_var_2) = ((float7(((float*)conv2d_nchw_1)[v_.s0],((float*)conv2d_nchw_1)[v_.s1],((float*)conv2d_nchw_1)[v_.s2],((float*)conv2d_nchw_1)[v_.s3],((float*)conv2d_nchw_1)[v_.s4],((float*)conv2d_nchw_1)[v_.s5],((float*)conv2d_nchw_1)[v_.s6])) + ((float7(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1],((float*)pad_temp)[v__1.s2],((float*)pad_temp)[v__1.s3],((float*)pad_temp)[v__1.s4],((float*)pad_temp)[v__1.s5],((float*)pad_temp)[v__1.s6])) * ((float7)(((float*)kernel_1)[(((((ff_outer_inner * 234) + (ff_inner * 117)) + (rc_outer * 9)) + (ry_inner * 3)) + rx_inner)], ((float*)kernel_1)[(((((ff_outer_inner * 234) + (ff_inner * 117)) + (rc_outer * 9)) + (ry_inner * 3)) + rx_inner)], ((float*)kernel_1)[(((((ff_outer_inner * 234) + (ff_inner * 117)) + (rc_outer * 9)) + (ry_inner * 3)) + rx_inner)], ((float*)kernel_1)[(((((ff_outer_inner * 234) + (ff_inner * 117)) + (rc_outer * 9)) + (ry_inner * 3)) + rx_inner)], ((float*)kernel_1)[(((((ff_outer_inner * 234) + (ff_inner * 117)) + (rc_outer * 9)) + (ry_inner * 3)) + rx_inner)], ((float*)kernel_1)[(((((ff_outer_inner * 234) + (ff_inner * 117)) + (rc_outer * 9)) + (ry_inner * 3)) + rx_inner)], ((float*)kernel_1)[(((((ff_outer_inner * 234) + (ff_inner * 117)) + (rc_outer * 9)) + (ry_inner * 3)) + rx_inner)]))));\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(87) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(87) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[4];\n  __shared__ float pad_temp_shared[585];\n  __shared__ float kernel_shared[6786];\n  for (int ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 2; ++ff_c_outer_inner_init) {\n    for (int yy_c_outer_inner_init = 0; yy_c_outer_inner_init < 2; ++yy_c_outer_inner_init) {\n      conv2d_nchw_local[((ff_c_outer_inner_init * 2) + yy_c_outer_inner_init)] = 0.000000e+00f;\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 7; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 29) + (((int)threadIdx.x) / 3)) < 195) {\n      pad_temp_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 87) + ((int)threadIdx.x))] = (((((1 <= (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 4) + (((int)threadIdx.x) / 3)) % 5)) && ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 4) + (((int)threadIdx.x) / 3)) % 5) < 4)) && (1 <= ((((int)blockIdx.x) % 7) + (((int)threadIdx.x) % 3)))) && (((((int)blockIdx.x) % 7) + (((int)threadIdx.x) % 3)) < 8)) ? data[(((((((((int)blockIdx.x) / 7) * 819) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 29) + (((int)threadIdx.x) / 3)) / 5) * 21)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 4) + (((int)threadIdx.x) / 3)) % 5) * 7)) + (((int)blockIdx.x) % 7)) + (((int)threadIdx.x) % 3)) - 8)] : 0.000000e+00f);\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 < 78; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) {\n    kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 87) + ((int)threadIdx.x))] = kernel[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 87) + ((int)threadIdx.x))];\n  },\n  __syncthreads();\n  for (int ry_outer_inner = 0; ry_outer_inner < 3; ++ry_outer_inner) {\n    for (int ff_c_outer_inner = 0; ff_c_outer_inner < 2; ++ff_c_outer_inner) {\n      for (int yy_c_outer_inner = 0; yy_c_outer_inner < 2; ++yy_c_outer_inner) {\n        for (int rc_inner = 0; rc_inner < 13; ++rc_inner) {\n          for (int rx_inner = 0; rx_inner < 3; ++rx_inner) {\n            conv2d_nchw_local[((ff_c_outer_inner * 2) + yy_c_outer_inner)] = (conv2d_nchw_local[((ff_c_outer_inner * 2) + yy_c_outer_inner)] + (pad_temp_shared[((((((((int)threadIdx.x) / 29) * 195) + (rc_inner * 15)) + (yy_c_outer_inner * 6)) + (ry_outer_inner * 3)) + rx_inner)] * kernel_shared[((((((((int)threadIdx.x) % 29) * 234) + (ff_c_outer_inner * 117)) + (rc_inner * 9)) + (ry_outer_inner * 3)) + rx_inner)]));\n          },\n        },\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 2; ++ff_inner) {\n    for (int yy_inner = 0; yy_inner < 2; ++yy_inner) {\n      conv2d_nchw[((((((((int)blockIdx.x) / 7) * 2436) + (((int)threadIdx.x) * 28)) + (ff_inner * 14)) + (yy_inner * 7)) + (((int)blockIdx.x) % 7))] = conv2d_nchw_local[((ff_inner * 2) + yy_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 13, 3, 7), \"float32\"), kernel: T.Buffer((58, 13, 3, 3), \"float32\"), bias: T.Buffer((1, 58, 1, 1), \"float32\"), conv2d_nchw: T.Buffer((9, 58, 2, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused in T.parallel(18):\n            pad_temp = T.allocate([351], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((351,), data=pad_temp)\n            for i1, i2, i3_s in T.grid(13, 3, 9):\n                cse_var_1: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 6 // 3\n                data_1 = T.Buffer((2457,), data=data.data)\n                pad_temp_1[i1 * 27 + i2 * 9 + i3_s] = T.if_then_else(1 <= cse_var_1 * 2 + i2 and cse_var_1 + i2 // 2 < 2 and 1 <= i3_s and i3_s < 8, data_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused // 6 * 819 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 3 * 273 + i1 * 21 + cse_var_1 * 14 + i2 * 7 + i3_s - 8], T.float32(0))\n            conv2d_nchw_1 = T.Buffer((7308,), data=conv2d_nchw.data)\n            for ff_outer_inner_init, ff_inner_init in T.grid(29, 2):\n                conv2d_nchw_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused // 6 * 2436 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 3 * 812 + ff_outer_inner_init * 28 + ff_inner_init * 14 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 6 // 3 * 7:nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused // 6 * 2436 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 3 * 812 + ff_outer_inner_init * 28 + ff_inner_init * 14 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 6 // 3 * 7 + 7] = T.Broadcast(T.float32(0), 7)\n            for rc_outer, ff_outer_inner, ry_inner, rx_inner, ff_inner in T.grid(13, 29, 3, 3, 2):\n                cse_var_2: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused // 6 * 2436 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 3 * 812 + ff_outer_inner * 28 + ff_inner * 14 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 6 // 3 * 7\n                kernel_1 = T.Buffer((6786,), data=kernel.data)\n                conv2d_nchw_1[cse_var_2:cse_var_2 + 7] = conv2d_nchw_1[cse_var_2:cse_var_2 + 7] + pad_temp_1[rc_outer * 27 + ry_inner * 9 + rx_inner:rc_outer * 27 + ry_inner * 9 + rx_inner + 7] * T.Broadcast(kernel_1[ff_outer_inner * 234 + ff_inner * 117 + rc_outer * 9 + ry_inner * 3 + rx_inner], 7)",
        "data": "9_13_3_7",
        "kernel": "58_13_3_3",
        "bias": "1_58_1_1"
    },
    {
        "op_name": "conv2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv2d_nchw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_ff_outer_fused = 0; nn_outer_ff_outer_fused < 156; ++nn_outer_ff_outer_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)13440, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    void* conv2d_nchw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)60, 2, 32);\n    if (conv2d_nchw_local == NULL) {\n      return -1;\n    },\n    for (int32_t i1 = 0; i1 < 28; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        for (int32_t i3_s = 0; i3_s < 12; ++i3_s) {\n          ((float*)pad_temp)[(((i1 * 120) + (i2 * 12)) + i3_s)] = (((((1 <= i2) && (i2 < 9)) && (1 <= i3_s)) && (i3_s < 11)) ? ((float*)data_1)[((((((nn_outer_ff_outer_fused / 26) * 2240) + (i1 * 80)) + (i2 * 10)) + i3_s) - 11)] : 0.000000e+00f);\n        },\n      },\n    },\n    for (int32_t yy_outer = 0; yy_outer < 7; ++yy_outer) {\n      for (int32_t ff_c_outer_outer_inner = 0; ff_c_outer_outer_inner < 3; ++ff_c_outer_outer_inner) {\n        ((float5*)conv2d_nchw_local)[ff_c_outer_outer_inner] = ((float5)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n        for (int32_t rx_outer = 0; rx_outer < 4; ++rx_outer) {\n          for (int32_t rc_inner = 0; rc_inner < 28; ++rc_inner) {\n            for (int32_t ry_inner = 0; ry_inner < 4; ++ry_inner) {\n              int32_t5 v_ = int32_t5((((((rc_inner * 120) + (yy_outer * 12)) + (ry_inner * 12)) + rx_outer))+(2*0), (((((rc_inner * 120) + (yy_outer * 12)) + (ry_inner * 12)) + rx_outer))+(2*1), (((((rc_inner * 120) + (yy_outer * 12)) + (ry_inner * 12)) + rx_outer))+(2*2), (((((rc_inner * 120) + (yy_outer * 12)) + (ry_inner * 12)) + rx_outer))+(2*3), (((((rc_inner * 120) + (yy_outer * 12)) + (ry_inner * 12)) + rx_outer))+(2*4));\n              ((float5*)conv2d_nchw_local)[ff_c_outer_outer_inner] = (((float5*)conv2d_nchw_local)[ff_c_outer_outer_inner] + ((float5(((float*)pad_temp)[v_.s0],((float*)pad_temp)[v_.s1],((float*)pad_temp)[v_.s2],((float*)pad_temp)[v_.s3],((float*)pad_temp)[v_.s4])) * ((float5)(((float*)kernel_1)[((((((nn_outer_ff_outer_fused % 26) * 1344) + (ff_c_outer_outer_inner * 448)) + (rc_inner * 16)) + (ry_inner * 4)) + rx_outer)], ((float*)kernel_1)[((((((nn_outer_ff_outer_fused % 26) * 1344) + (ff_c_outer_outer_inner * 448)) + (rc_inner * 16)) + (ry_inner * 4)) + rx_outer)], ((float*)kernel_1)[((((((nn_outer_ff_outer_fused % 26) * 1344) + (ff_c_outer_outer_inner * 448)) + (rc_inner * 16)) + (ry_inner * 4)) + rx_outer)], ((float*)kernel_1)[((((((nn_outer_ff_outer_fused % 26) * 1344) + (ff_c_outer_outer_inner * 448)) + (rc_inner * 16)) + (ry_inner * 4)) + rx_outer)], ((float*)kernel_1)[((((((nn_outer_ff_outer_fused % 26) * 1344) + (ff_c_outer_outer_inner * 448)) + (rc_inner * 16)) + (ry_inner * 4)) + rx_outer)]))));\n            },\n          },\n        },\n      },\n      for (int32_t ff_inner = 0; ff_inner < 3; ++ff_inner) {\n        *(float5*)(((float*)conv2d_nchw_1) + (((nn_outer_ff_outer_fused * 105) + (ff_inner * 35)) + (yy_outer * 5))) = ((float5*)conv2d_nchw_local)[ff_inner];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv2d_nchw_local) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(546) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(546) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[5];\n  __shared__ float pad_temp_shared[600];\n  __shared__ float kernel_shared[104];\n  conv2d_nchw_local[0] = 0.000000e+00f;\n  conv2d_nchw_local[1] = 0.000000e+00f;\n  conv2d_nchw_local[2] = 0.000000e+00f;\n  conv2d_nchw_local[3] = 0.000000e+00f;\n  conv2d_nchw_local[4] = 0.000000e+00f;\n  for (int rc_outer_outer = 0; rc_outer_outer < 28; ++rc_outer_outer) {\n    for (int rx_outer_outer = 0; rx_outer_outer < 2; ++rx_outer_outer) {\n      __syncthreads();\n      for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 91) + (((int)threadIdx.x) / 6)) < 100) {\n          pad_temp_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 546) + ((int)threadIdx.x))] = (((((5 <= (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 23) + (((int)threadIdx.x) >> 1)) % 50)) && ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 46) + ((int)threadIdx.x)) % 100) < 90)) && (1 <= ((rx_outer_outer * 2) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 6) + ((int)threadIdx.x)) % 10)))) && (((rx_outer_outer * 2) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 6) + ((int)threadIdx.x)) % 10)) < 11)) ? data[(((((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 273) + (((int)threadIdx.x) >> 1)) / 50) * 2240) + (rc_outer_outer * 80)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 23) + (((int)threadIdx.x) >> 1)) % 50) / 5) * 10)) + (rx_outer_outer * 2)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 6) + ((int)threadIdx.x)) % 10)) - 11)] : 0.000000e+00f);\n        },\n      },\n      if (((int)threadIdx.x) < 104) {\n        kernel_shared[((int)threadIdx.x)] = kernel[((((((((int)blockIdx.x) * 5824) + ((((int)threadIdx.x) >> 3) * 448)) + (rc_outer_outer * 16)) + (((((int)threadIdx.x) & 7) >> 1) * 4)) + (rx_outer_outer * 2)) + (((int)threadIdx.x) & 1))];\n      },\n      __syncthreads();\n      for (int ry_outer_inner = 0; ry_outer_inner < 4; ++ry_outer_inner) {\n        for (int rx_inner = 0; rx_inner < 2; ++rx_inner) {\n          conv2d_nchw_local[0] = (conv2d_nchw_local[0] + (pad_temp_shared[(((((((int)threadIdx.x) / 91) * 100) + (ry_outer_inner * 10)) + ((((int)threadIdx.x) % 7) * 10)) + rx_inner)] * kernel_shared[(((((((int)threadIdx.x) % 91) / 7) * 8) + (ry_outer_inner * 2)) + rx_inner)]));\n          conv2d_nchw_local[1] = (conv2d_nchw_local[1] + (pad_temp_shared[((((((((int)threadIdx.x) / 91) * 100) + (ry_outer_inner * 10)) + ((((int)threadIdx.x) % 7) * 10)) + rx_inner) + 2)] * kernel_shared[(((((((int)threadIdx.x) % 91) / 7) * 8) + (ry_outer_inner * 2)) + rx_inner)]));\n          conv2d_nchw_local[2] = (conv2d_nchw_local[2] + (pad_temp_shared[((((((((int)threadIdx.x) / 91) * 100) + (ry_outer_inner * 10)) + ((((int)threadIdx.x) % 7) * 10)) + rx_inner) + 4)] * kernel_shared[(((((((int)threadIdx.x) % 91) / 7) * 8) + (ry_outer_inner * 2)) + rx_inner)]));\n          conv2d_nchw_local[3] = (conv2d_nchw_local[3] + (pad_temp_shared[((((((((int)threadIdx.x) / 91) * 100) + (ry_outer_inner * 10)) + ((((int)threadIdx.x) % 7) * 10)) + rx_inner) + 6)] * kernel_shared[(((((((int)threadIdx.x) % 91) / 7) * 8) + (ry_outer_inner * 2)) + rx_inner)]));\n          conv2d_nchw_local[4] = (conv2d_nchw_local[4] + (pad_temp_shared[((((((((int)threadIdx.x) / 91) * 100) + (ry_outer_inner * 10)) + ((((int)threadIdx.x) % 7) * 10)) + rx_inner) + 8)] * kernel_shared[(((((((int)threadIdx.x) % 91) / 7) * 8) + (ry_outer_inner * 2)) + rx_inner)]));\n        },\n      },\n    },\n  },\n  conv2d_nchw[((((((int)threadIdx.x) / 91) * 2730) + (((int)blockIdx.x) * 455)) + ((((int)threadIdx.x) % 91) * 5))] = conv2d_nchw_local[0];\n  conv2d_nchw[(((((((int)threadIdx.x) / 91) * 2730) + (((int)blockIdx.x) * 455)) + ((((int)threadIdx.x) % 91) * 5)) + 1)] = conv2d_nchw_local[1];\n  conv2d_nchw[(((((((int)threadIdx.x) / 91) * 2730) + (((int)blockIdx.x) * 455)) + ((((int)threadIdx.x) % 91) * 5)) + 2)] = conv2d_nchw_local[2];\n  conv2d_nchw[(((((((int)threadIdx.x) / 91) * 2730) + (((int)blockIdx.x) * 455)) + ((((int)threadIdx.x) % 91) * 5)) + 3)] = conv2d_nchw_local[3];\n  conv2d_nchw[(((((((int)threadIdx.x) / 91) * 2730) + (((int)blockIdx.x) * 455)) + ((((int)threadIdx.x) % 91) * 5)) + 4)] = conv2d_nchw_local[4];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 28, 8, 10), \"float32\"), kernel: T.Buffer((78, 28, 4, 4), \"float32\"), conv2d_nchw: T.Buffer((6, 78, 7, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_ff_outer_fused in T.parallel(156):\n            pad_temp = T.allocate([3360], \"float32\", \"global\")\n            conv2d_nchw_local = T.allocate([3], \"float32x5\", \"local\")\n            pad_temp_1 = T.Buffer((3360,), data=pad_temp)\n            for i1, i2, i3_s in T.grid(28, 10, 12):\n                data_1 = T.Buffer((13440,), data=data.data)\n                pad_temp_1[i1 * 120 + i2 * 12 + i3_s] = T.if_then_else(1 <= i2 and i2 < 9 and 1 <= i3_s and i3_s < 11, data_1[nn_outer_ff_outer_fused // 26 * 2240 + i1 * 80 + i2 * 10 + i3_s - 11], T.float32(0))\n            for yy_outer in range(7):\n                conv2d_nchw_local_1 = T.Buffer((3,), \"float32x5\", data=conv2d_nchw_local, scope=\"local\", align=32)\n                for ff_c_outer_outer_inner in range(3):\n                    conv2d_nchw_local_1[ff_c_outer_outer_inner] = T.Broadcast(T.float32(0), 5)\n                    for rx_outer, rc_inner, ry_inner in T.grid(4, 28, 4):\n                        kernel_1 = T.Buffer((34944,), data=kernel.data)\n                        conv2d_nchw_local_1[ff_c_outer_outer_inner] = conv2d_nchw_local_1[ff_c_outer_outer_inner] + pad_temp_1[rc_inner * 120 + yy_outer * 12 + ry_inner * 12 + rx_outer:rc_inner * 120 + yy_outer * 12 + ry_inner * 12 + rx_outer + 10:2] * T.Broadcast(kernel_1[nn_outer_ff_outer_fused % 26 * 1344 + ff_c_outer_outer_inner * 448 + rc_inner * 16 + ry_inner * 4 + rx_outer], 5)\n                for ff_inner in range(3):\n                    conv2d_nchw_1 = T.Buffer((16380,), data=conv2d_nchw.data)\n                    conv2d_nchw_1[nn_outer_ff_outer_fused * 105 + ff_inner * 35 + yy_outer * 5:nn_outer_ff_outer_fused * 105 + ff_inner * 35 + yy_outer * 5 + 5] = conv2d_nchw_local_1[ff_inner]",
        "data": "6_28_8_10",
        "kernel": "78_28_4_4"
    },
    {
        "op_name": "conv2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv2d_nchw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused < 22; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)17360, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    void* conv2d_nchw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)504, 2, 32);\n    if (conv2d_nchw_local == NULL) {\n      return -1;\n    },\n    for (int32_t i0 = 0; i0 < 7; ++i0) {\n      for (int32_t i1 = 0; i1 < 31; ++i1) {\n        for (int32_t i2 = 0; i2 < 5; ++i2) {\n          for (int32_t i3_s = 0; i3_s < 4; ++i3_s) {\n            int32_t cse_var_1 = (nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused / 11);\n            ((float*)pad_temp)[((((i0 * 620) + (i1 * 20)) + (i2 * 4)) + i3_s)] = (((((1 <= ((cse_var_1 * 2) + i2)) && ((cse_var_1 + (i2 >> 1)) < 3)) && (1 <= i3_s)) && (i3_s < 3)) ? ((float*)data_1)[((((((i0 * 310) + (i1 * 10)) + (cse_var_1 * 4)) + (i2 * 2)) + i3_s) - 3)] : 0.000000e+00f);\n          },\n        },\n      },\n    },\n    for (int32_t nn_c_inner_init = 0; nn_c_inner_init < 7; ++nn_c_inner_init) {\n      for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 9; ++ff_c_inner_init) {\n        for (int32_t yy_c_inner_init = 0; yy_c_inner_init < 2; ++yy_c_inner_init) {\n          ((float*)conv2d_nchw_local)[(((nn_c_inner_init * 18) + (ff_c_inner_init * 2)) + yy_c_inner_init)] = 0.000000e+00f;\n        },\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 31; ++rc_outer) {\n      for (int32_t rx_outer = 0; rx_outer < 4; ++rx_outer) {\n        for (int32_t ry_inner = 0; ry_inner < 4; ++ry_inner) {\n          for (int32_t nn_c_inner = 0; nn_c_inner < 7; ++nn_c_inner) {\n            for (int32_t ff_c_inner = 0; ff_c_inner < 9; ++ff_c_inner) {\n              for (int32_t yy_c_inner = 0; yy_c_inner < 2; ++yy_c_inner) {\n                int32_t cse_var_3 = (ry_inner * 4);\n                int32_t cse_var_2 = (((nn_c_inner * 18) + (ff_c_inner * 2)) + yy_c_inner);\n                ((float*)conv2d_nchw_local)[cse_var_2] = (((float*)conv2d_nchw_local)[cse_var_2] + (((float*)pad_temp)[(((((nn_c_inner * 620) + (rc_outer * 20)) + (yy_c_inner * 4)) + cse_var_3) + rx_outer)] * ((float*)kernel_1)[((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused % 11) * 4464) + (ff_c_inner * 496)) + (rc_outer * 16)) + cse_var_3) + rx_outer)]));\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 7; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 9; ++ff_inner) {\n        for (int32_t yy_inner = 0; yy_inner < 2; ++yy_inner) {\n          ((float*)conv2d_nchw_1)[(((((nn_inner * 396) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused % 11) * 36)) + (ff_inner * 4)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused / 11) * 2)) + yy_inner)] = ((float*)conv2d_nchw_local)[(((nn_inner * 18) + (ff_inner * 2)) + yy_inner)];\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv2d_nchw_local) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(77) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(77) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[2];\n  __shared__ float pad_temp_shared[434];\n  __shared__ float kernel_shared[341];\n  conv2d_nchw_local[0] = 0.000000e+00f;\n  conv2d_nchw_local[1] = 0.000000e+00f;\n  for (int ry_outer_outer = 0; ry_outer_outer < 4; ++ry_outer_outer) {\n    for (int rx_outer_outer = 0; rx_outer_outer < 4; ++rx_outer_outer) {\n      __syncthreads();\n      for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 6; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 11) + (((int)threadIdx.x) / 7)) < 62) {\n          pad_temp_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 77) + ((int)threadIdx.x))] = (((((1 <= ((((((int)blockIdx.x) & 1) * 2) + ry_outer_outer) + ((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer + ((int)threadIdx.x)) & 1))) && ((((ry_outer_outer + ((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer + ((int)threadIdx.x)) & 1)) >> 1) + (((int)blockIdx.x) & 1)) < 3)) && (1 <= rx_outer_outer)) && (rx_outer_outer < 3)) ? data[(((((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 77) + ((int)threadIdx.x)) >> 1) * 10) + ((((int)blockIdx.x) & 1) * 4)) + (ry_outer_outer * 2)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer + ((int)threadIdx.x)) & 1) * 2)) + rx_outer_outer) - 3)] : 0.000000e+00f);\n        },\n      },\n      for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 < 5; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) {\n        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 7) + (((int)threadIdx.x) / 11)) < 31) {\n          kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 77) + ((int)threadIdx.x))] = kernel[((((((((int)blockIdx.x) >> 1) * 5456) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 1232)) + (((int)threadIdx.x) * 16)) + (ry_outer_outer * 4)) + rx_outer_outer)];\n        },\n      },\n      __syncthreads();\n      for (int rc_inner = 0; rc_inner < 31; ++rc_inner) {\n        conv2d_nchw_local[0] = (conv2d_nchw_local[0] + (pad_temp_shared[(((((int)threadIdx.x) / 11) * 62) + (rc_inner * 2))] * kernel_shared[(((((int)threadIdx.x) % 11) * 31) + rc_inner)]));\n        conv2d_nchw_local[1] = (conv2d_nchw_local[1] + (pad_temp_shared[((((((int)threadIdx.x) / 11) * 62) + (rc_inner * 2)) + 1)] * kernel_shared[(((((int)threadIdx.x) % 11) * 31) + rc_inner)]));\n      },\n    },\n  },\n  conv2d_nchw[(((((((int)threadIdx.x) / 11) * 396) + ((((int)blockIdx.x) >> 1) * 44)) + ((((int)threadIdx.x) % 11) * 4)) + ((((int)blockIdx.x) & 1) * 2))] = conv2d_nchw_local[0];\n  conv2d_nchw[((((((((int)threadIdx.x) / 11) * 396) + ((((int)blockIdx.x) >> 1) * 44)) + ((((int)threadIdx.x) % 11) * 4)) + ((((int)blockIdx.x) & 1) * 2)) + 1)] = conv2d_nchw_local[1];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 31, 5, 2), \"float32\"), kernel: T.Buffer((99, 31, 4, 4), \"float32\"), conv2d_nchw: T.Buffer((7, 99, 4, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused in T.parallel(22):\n            pad_temp = T.allocate([4340], \"float32\", \"global\")\n            conv2d_nchw_local = T.allocate([126], \"float32\", \"local\")\n            pad_temp_1 = T.Buffer((4340,), data=pad_temp)\n            for i0, i1, i2, i3_s in T.grid(7, 31, 5, 4):\n                cse_var_1: T.int32 = nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused // 11\n                data_1 = T.Buffer((2170,), data=data.data)\n                pad_temp_1[i0 * 620 + i1 * 20 + i2 * 4 + i3_s] = T.if_then_else(1 <= cse_var_1 * 2 + i2 and cse_var_1 + i2 // 2 < 3 and 1 <= i3_s and i3_s < 3, data_1[i0 * 310 + i1 * 10 + cse_var_1 * 4 + i2 * 2 + i3_s - 3], T.float32(0))\n            conv2d_nchw_local_1 = T.Buffer((126,), data=conv2d_nchw_local, scope=\"local\")\n            for nn_c_inner_init, ff_c_inner_init, yy_c_inner_init in T.grid(7, 9, 2):\n                conv2d_nchw_local_1[nn_c_inner_init * 18 + ff_c_inner_init * 2 + yy_c_inner_init] = T.float32(0)\n            for rc_outer, rx_outer, ry_inner, nn_c_inner, ff_c_inner, yy_c_inner in T.grid(31, 4, 4, 7, 9, 2):\n                cse_var_3: T.int32 = ry_inner * 4\n                cse_var_2: T.int32 = nn_c_inner * 18 + ff_c_inner * 2 + yy_c_inner\n                kernel_1 = T.Buffer((49104,), data=kernel.data)\n                conv2d_nchw_local_1[cse_var_2] = conv2d_nchw_local_1[cse_var_2] + pad_temp_1[nn_c_inner * 620 + rc_outer * 20 + yy_c_inner * 4 + cse_var_3 + rx_outer] * kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused % 11 * 4464 + ff_c_inner * 496 + rc_outer * 16 + cse_var_3 + rx_outer]\n            for nn_inner, ff_inner, yy_inner in T.grid(7, 9, 2):\n                conv2d_nchw_1 = T.Buffer((2772,), data=conv2d_nchw.data)\n                conv2d_nchw_1[nn_inner * 396 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused % 11 * 36 + ff_inner * 4 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused // 11 * 2 + yy_inner] = conv2d_nchw_local_1[nn_inner * 18 + ff_inner * 2 + yy_inner]",
        "data": "7_31_5_2",
        "kernel": "99_31_4_4"
    },
    {
        "op_name": "conv2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv2d_nchw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused < 77; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)7440, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t i0 = 0; i0 < 3; ++i0) {\n      for (int32_t i1 = 0; i1 < 31; ++i1) {\n        for (int32_t i2 = 0; i2 < 5; ++i2) {\n          for (int32_t i3_s = 0; i3_s < 4; ++i3_s) {\n            ((float*)pad_temp)[((((i0 * 620) + (i1 * 20)) + (i2 * 4)) + i3_s)] = ((((1 <= i2) && (i2 < 4)) && (1 <= i3_s)) ? ((float*)data_1)[(((((i0 * 279) + (i1 * 9)) + (i2 * 3)) + i3_s) - 4)] : 0.000000e+00f);\n          },\n        },\n      },\n    },\n    for (int32_t nn_outer_outer_inner = 0; nn_outer_outer_inner < 3; ++nn_outer_outer_inner) {\n      for (int32_t yy_inner_init = 0; yy_inner_init < 2; ++yy_inner_init) {\n        ((float*)conv2d_nchw_1)[(((nn_outer_outer_inner * 154) + (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused * 2)) + yy_inner_init)] = 0.000000e+00f;\n      },\n      for (int32_t rc_outer = 0; rc_outer < 31; ++rc_outer) {\n        for (int32_t rx_outer = 0; rx_outer < 2; ++rx_outer) {\n          for (int32_t ry_inner = 0; ry_inner < 4; ++ry_inner) {\n            for (int32_t rx_inner = 0; rx_inner < 2; ++rx_inner) {\n              for (int32_t yy_inner = 0; yy_inner < 2; ++yy_inner) {\n                int32_t cse_var_3 = (ry_inner * 4);\n                int32_t cse_var_2 = (rx_outer * 2);\n                int32_t cse_var_1 = (((nn_outer_outer_inner * 154) + (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused * 2)) + yy_inner);\n                ((float*)conv2d_nchw_1)[cse_var_1] = (((float*)conv2d_nchw_1)[cse_var_1] + (((float*)pad_temp)[((((((nn_outer_outer_inner * 620) + (rc_outer * 20)) + (yy_inner * 4)) + cse_var_3) + cse_var_2) + rx_inner)] * ((float*)kernel_1)[(((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused * 496) + (rc_outer * 16)) + cse_var_3) + cse_var_2) + rx_inner)]));\n              },\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int rc_ry_fused_rx_fused_outer = 0; rc_ry_fused_rx_fused_outer < 248; ++rc_ry_fused_rx_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (((((1 <= (((rc_ry_fused_rx_fused_outer & 7) >> 1) + (((int)blockIdx.x) & 1))) && ((((rc_ry_fused_rx_fused_outer & 7) >> 1) + (((int)blockIdx.x) & 1)) < 4)) && (1 <= (((rc_ry_fused_rx_fused_outer & 1) * 2) + ((int)threadIdx.x)))) ? data[((((((((((int)blockIdx.x) / 154) * 279) + ((rc_ry_fused_rx_fused_outer >> 3) * 9)) + (((rc_ry_fused_rx_fused_outer & 7) >> 1) * 3)) + ((((int)blockIdx.x) & 1) * 3)) + ((rc_ry_fused_rx_fused_outer & 1) * 2)) + ((int)threadIdx.x)) - 4)] : 0.000000e+00f) * kernel[(((((((int)blockIdx.x) % 154) >> 1) * 496) + (rc_ry_fused_rx_fused_outer * 2)) + ((int)threadIdx.x))]));\n  },\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    conv2d_nchw[((int)blockIdx.x)] = red_buf0[0];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 31, 3, 3), \"float32\"), kernel: T.Buffer((77, 31, 4, 4), \"float32\"), conv2d_nchw: T.Buffer((3, 77, 2, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused in T.parallel(77):\n            pad_temp = T.allocate([1860], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((1860,), data=pad_temp)\n            for i0, i1, i2, i3_s in T.grid(3, 31, 5, 4):\n                data_1 = T.Buffer((837,), data=data.data)\n                pad_temp_1[i0 * 620 + i1 * 20 + i2 * 4 + i3_s] = T.if_then_else(1 <= i2 and i2 < 4 and 1 <= i3_s, data_1[i0 * 279 + i1 * 9 + i2 * 3 + i3_s - 4], T.float32(0))\n            for nn_outer_outer_inner in range(3):\n                conv2d_nchw_1 = T.Buffer((462,), data=conv2d_nchw.data)\n                for yy_inner_init in range(2):\n                    conv2d_nchw_1[nn_outer_outer_inner * 154 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused * 2 + yy_inner_init] = T.float32(0)\n                for rc_outer, rx_outer, ry_inner, rx_inner, yy_inner in T.grid(31, 2, 4, 2, 2):\n                    cse_var_3: T.int32 = ry_inner * 4\n                    cse_var_2: T.int32 = rx_outer * 2\n                    cse_var_1: T.int32 = nn_outer_outer_inner * 154 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused * 2 + yy_inner\n                    kernel_1 = T.Buffer((38192,), data=kernel.data)\n                    conv2d_nchw_1[cse_var_1] = conv2d_nchw_1[cse_var_1] + pad_temp_1[nn_outer_outer_inner * 620 + rc_outer * 20 + yy_inner * 4 + cse_var_3 + cse_var_2 + rx_inner] * kernel_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused * 496 + rc_outer * 16 + cse_var_3 + cse_var_2 + rx_inner]",
        "data": "3_31_3_3",
        "kernel": "77_31_4_4"
    },
    {
        "op_name": "conv2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv2d_nchw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused < 99; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)7296, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t i1 = 0; i1 < 57; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n          ((float*)pad_temp)[(((i1 * 32) + (i2 * 8)) + i3_s)] = (((((1 <= i2) && (i2 < 3)) && (1 <= i3_s)) && (i3_s < 7)) ? ((float*)data_1)[((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 9) * 684) + (i1 * 12)) + (i2 * 6)) + i3_s) - 7)] : 0.000000e+00f);\n        },\n      },\n    },\n    for (int32_t xx_outer_outer_inner = 0; xx_outer_outer_inner < 3; ++xx_outer_outer_inner) {\n      for (int32_t ff_outer_inner_init = 0; ff_outer_inner_init < 5; ++ff_outer_inner_init) {\n        for (int32_t yy_outer_inner_init = 0; yy_outer_inner_init < 2; ++yy_outer_inner_init) {\n          *(float2*)(((float*)conv2d_nchw_1) + ((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 9) * 660) + ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused / 9) * 60)) + (ff_outer_inner_init * 12)) + (yy_outer_inner_init * 6)) + (xx_outer_outer_inner * 2))) = ((float2)(0.000000e+00f, 0.000000e+00f));\n        },\n      },\n      for (int32_t rc_outer = 0; rc_outer < 57; ++rc_outer) {\n        for (int32_t rx_outer = 0; rx_outer < 3; ++rx_outer) {\n          for (int32_t ff_outer_inner = 0; ff_outer_inner < 5; ++ff_outer_inner) {\n            for (int32_t yy_outer_inner = 0; yy_outer_inner < 2; ++yy_outer_inner) {\n              for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n                int32_t cse_var_3 = (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused / 9);\n                int32_t cse_var_2 = (xx_outer_outer_inner * 2);\n                int32_t cse_var_1 = ((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 9) * 660) + (cse_var_3 * 60)) + (ff_outer_inner * 12)) + (yy_outer_inner * 6)) + cse_var_2);\n                int32_t2 v_ = int32_t2((cse_var_1)+(1*0), (cse_var_1)+(1*1));\n                int32_t2 v__1 = int32_t2(((((((rc_outer * 32) + (yy_outer_inner * 8)) + (ry_inner * 8)) + cse_var_2) + rx_outer))+(1*0), ((((((rc_outer * 32) + (yy_outer_inner * 8)) + (ry_inner * 8)) + cse_var_2) + rx_outer))+(1*1));\n                *(float2*)(((float*)conv2d_nchw_1) + cse_var_1) = ((float2(((float*)conv2d_nchw_1)[v_.s0],((float*)conv2d_nchw_1)[v_.s1])) + ((float2(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1])) * ((float2)(((float*)kernel_1)[(((((cse_var_3 * 2565) + (ff_outer_inner * 513)) + (rc_outer * 9)) + (ry_inner * 3)) + rx_outer)], ((float*)kernel_1)[(((((cse_var_3 * 2565) + (ff_outer_inner * 513)) + (rc_outer * 9)) + (ry_inner * 3)) + rx_outer)]))));\n              },\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(33) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(33) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[12];\n  __shared__ float pad_temp_shared[5472];\n  __shared__ float kernel_shared[5643];\n  for (int xx_c_outer_inner_init = 0; xx_c_outer_inner_init < 6; ++xx_c_outer_inner_init) {\n    for (int yy_c_inner_init = 0; yy_c_inner_init < 2; ++yy_c_inner_init) {\n      conv2d_nchw_local[((yy_c_inner_init * 6) + xx_c_outer_inner_init)] = 0.000000e+00f;\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 166; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 11) + (((int)threadIdx.x) / 3)) < 1824) {\n      pad_temp_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 33) + ((int)threadIdx.x))] = (((((8 <= ((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer + ((int)threadIdx.x)) & 31)) && (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer + ((int)threadIdx.x)) & 31) < 24)) && (1 <= ((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer + ((int)threadIdx.x)) & 7))) && (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer + ((int)threadIdx.x)) & 7) < 7)) ? data[((((((((int)blockIdx.x) / 5) * 2052) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 33) + ((int)threadIdx.x)) >> 5) * 12)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer + ((int)threadIdx.x)) & 31) >> 3) * 6)) + ((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer + ((int)threadIdx.x)) & 7)) - 7)] : 0.000000e+00f);\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 < 171; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) {\n    kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 33) + ((int)threadIdx.x))] = kernel[((((((int)blockIdx.x) % 5) * 5643) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 33)) + ((int)threadIdx.x))];\n  },\n  __syncthreads();\n  for (int ry_outer_inner = 0; ry_outer_inner < 3; ++ry_outer_inner) {\n    for (int rx_outer_inner = 0; rx_outer_inner < 3; ++rx_outer_inner) {\n      for (int xx_c_outer_inner = 0; xx_c_outer_inner < 6; ++xx_c_outer_inner) {\n        for (int rc_inner = 0; rc_inner < 57; ++rc_inner) {\n          for (int yy_c_inner = 0; yy_c_inner < 2; ++yy_c_inner) {\n            conv2d_nchw_local[((yy_c_inner * 6) + xx_c_outer_inner)] = (conv2d_nchw_local[((yy_c_inner * 6) + xx_c_outer_inner)] + (pad_temp_shared[(((((((((int)threadIdx.x) / 11) * 1824) + (rc_inner * 32)) + (yy_c_inner * 8)) + (ry_outer_inner * 8)) + xx_c_outer_inner) + rx_outer_inner)] * kernel_shared[(((((((int)threadIdx.x) % 11) * 513) + (rc_inner * 9)) + (ry_outer_inner * 3)) + rx_outer_inner)]));\n          },\n        },\n      },\n    },\n  },\n  for (int yy_inner = 0; yy_inner < 2; ++yy_inner) {\n    for (int xx_inner = 0; xx_inner < 6; ++xx_inner) {\n      conv2d_nchw[(((((((((int)blockIdx.x) / 5) * 1980) + ((((int)threadIdx.x) / 11) * 660)) + ((((int)blockIdx.x) % 5) * 132)) + ((((int)threadIdx.x) % 11) * 12)) + (yy_inner * 6)) + xx_inner)] = conv2d_nchw_local[((yy_inner * 6) + xx_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 57, 2, 6), \"float32\"), kernel: T.Buffer((55, 57, 3, 3), \"float32\"), conv2d_nchw: T.Buffer((9, 55, 2, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused in T.parallel(99):\n            pad_temp = T.allocate([1824], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((1824,), data=pad_temp)\n            for i1, i2, i3_s in T.grid(57, 4, 8):\n                data_1 = T.Buffer((6156,), data=data.data)\n                pad_temp_1[i1 * 32 + i2 * 8 + i3_s] = T.if_then_else(1 <= i2 and i2 < 3 and 1 <= i3_s and i3_s < 7, data_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 9 * 684 + i1 * 12 + i2 * 6 + i3_s - 7], T.float32(0))\n            for xx_outer_outer_inner in range(3):\n                conv2d_nchw_1 = T.Buffer((5940,), data=conv2d_nchw.data)\n                for ff_outer_inner_init, yy_outer_inner_init in T.grid(5, 2):\n                    conv2d_nchw_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 9 * 660 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 9 * 60 + ff_outer_inner_init * 12 + yy_outer_inner_init * 6 + xx_outer_outer_inner * 2:nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 9 * 660 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 9 * 60 + ff_outer_inner_init * 12 + yy_outer_inner_init * 6 + xx_outer_outer_inner * 2 + 2] = T.Broadcast(T.float32(0), 2)\n                for rc_outer, rx_outer, ff_outer_inner, yy_outer_inner, ry_inner in T.grid(57, 3, 5, 2, 3):\n                    cse_var_3: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 9\n                    cse_var_2: T.int32 = xx_outer_outer_inner * 2\n                    cse_var_1: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 9 * 660 + cse_var_3 * 60 + ff_outer_inner * 12 + yy_outer_inner * 6 + cse_var_2\n                    kernel_1 = T.Buffer((28215,), data=kernel.data)\n                    conv2d_nchw_1[cse_var_1:cse_var_1 + 2] = conv2d_nchw_1[cse_var_1:cse_var_1 + 2] + pad_temp_1[rc_outer * 32 + yy_outer_inner * 8 + ry_inner * 8 + cse_var_2 + rx_outer:rc_outer * 32 + yy_outer_inner * 8 + ry_inner * 8 + cse_var_2 + rx_outer + 2] * T.Broadcast(kernel_1[cse_var_3 * 2565 + ff_outer_inner * 513 + rc_outer * 9 + ry_inner * 3 + rx_outer], 2)",
        "data": "9_57_2_6",
        "kernel": "55_57_3_3"
    },
    {
        "op_name": "conv2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv2d_nchw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_ff_outer_fused_yy_outer_fused = 0; nn_outer_ff_outer_fused_yy_outer_fused < 38; ++nn_outer_ff_outer_fused_yy_outer_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)216000, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    void* conv2d_nchw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1920, 2, 32);\n    if (conv2d_nchw_local == NULL) {\n      return -1;\n    },\n    for (int32_t i0 = 0; i0 < 10; ++i0) {\n      for (int32_t i1 = 0; i1 < 100; ++i1) {\n        for (int32_t i2 = 0; i2 < 6; ++i2) {\n          for (int32_t i3_s = 0; i3_s < 9; ++i3_s) {\n            int32_t cse_var_2 = (nn_outer_ff_outer_fused_yy_outer_fused & 1);\n            int32_t cse_var_1 = ((cse_var_2 * 3) + i2);\n            ((float*)pad_temp)[((((i0 * 5400) + (i1 * 54)) + (i2 * 9)) + i3_s)] = ((((1 <= cse_var_1) && (cse_var_1 < 8)) && (1 <= i3_s)) ? ((float*)data_1)[((((((i0 * 5600) + (i1 * 56)) + (cse_var_2 * 24)) + (i2 * 8)) + i3_s) - 9)] : 0.000000e+00f);\n          },\n        },\n      },\n    },\n    for (int32_t yy_c_outer_outer_inner = 0; yy_c_outer_outer_inner < 3; ++yy_c_outer_outer_inner) {\n      for (int32_t xx_c_outer_inner_init = 0; xx_c_outer_inner_init < 2; ++xx_c_outer_inner_init) {\n        for (int32_t nn_c_inner_init = 0; nn_c_inner_init < 10; ++nn_c_inner_init) {\n          for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 4; ++ff_c_inner_init) {\n            *(float2*)(((float*)conv2d_nchw_local) + ((((nn_c_inner_init * 48) + (ff_c_inner_init * 12)) + (yy_c_outer_outer_inner * 4)) + (xx_c_outer_inner_init * 2))) = ((float2)(0.000000e+00f, 0.000000e+00f));\n          },\n        },\n      },\n      for (int32_t rc_outer = 0; rc_outer < 25; ++rc_outer) {\n        for (int32_t xx_c_outer_inner = 0; xx_c_outer_inner < 2; ++xx_c_outer_inner) {\n          for (int32_t rc_inner = 0; rc_inner < 4; ++rc_inner) {\n            for (int32_t ry_inner = 0; ry_inner < 4; ++ry_inner) {\n              for (int32_t rx_inner = 0; rx_inner < 3; ++rx_inner) {\n                for (int32_t nn_c_inner = 0; nn_c_inner < 10; ++nn_c_inner) {\n                  for (int32_t ff_c_inner = 0; ff_c_inner < 4; ++ff_c_inner) {\n                    int32_t cse_var_3 = ((((nn_c_inner * 48) + (ff_c_inner * 12)) + (yy_c_outer_outer_inner * 4)) + (xx_c_outer_inner * 2));\n                    int32_t2 v_ = int32_t2((cse_var_3)+(1*0), (cse_var_3)+(1*1));\n                    int32_t2 v__1 = int32_t2(((((((((nn_c_inner * 5400) + (rc_outer * 216)) + (rc_inner * 54)) + (yy_c_outer_outer_inner * 9)) + (ry_inner * 9)) + (xx_c_outer_inner * 4)) + rx_inner))+(2*0), ((((((((nn_c_inner * 5400) + (rc_outer * 216)) + (rc_inner * 54)) + (yy_c_outer_outer_inner * 9)) + (ry_inner * 9)) + (xx_c_outer_inner * 4)) + rx_inner))+(2*1));\n                    *(float2*)(((float*)conv2d_nchw_local) + cse_var_3) = ((float2(((float*)conv2d_nchw_local)[v_.s0],((float*)conv2d_nchw_local)[v_.s1])) + ((float2(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1])) * ((float2)(((float*)kernel_1)[(((((((nn_outer_ff_outer_fused_yy_outer_fused >> 1) * 4800) + (ff_c_inner * 1200)) + (rc_outer * 48)) + (rc_inner * 12)) + (ry_inner * 3)) + rx_inner)], ((float*)kernel_1)[(((((((nn_outer_ff_outer_fused_yy_outer_fused >> 1) * 4800) + (ff_c_inner * 1200)) + (rc_outer * 48)) + (rc_inner * 12)) + (ry_inner * 3)) + rx_inner)]))));\n                  },\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 10; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 4; ++ff_inner) {\n        for (int32_t yy_inner = 0; yy_inner < 3; ++yy_inner) {\n          int32_t cse_var_4 = (yy_inner * 4);\n          int32_t4 v__2 = int32_t4(((((nn_inner * 48) + (ff_inner * 12)) + cse_var_4))+(1*0), ((((nn_inner * 48) + (ff_inner * 12)) + cse_var_4))+(1*1), ((((nn_inner * 48) + (ff_inner * 12)) + cse_var_4))+(1*2), ((((nn_inner * 48) + (ff_inner * 12)) + cse_var_4))+(1*3));\n          *(float4*)(((float*)conv2d_nchw_1) + (((((nn_inner * 1824) + ((nn_outer_ff_outer_fused_yy_outer_fused >> 1) * 96)) + (ff_inner * 24)) + ((nn_outer_ff_outer_fused_yy_outer_fused & 1) * 12)) + cse_var_4)) = (float4(((float*)conv2d_nchw_local)[v__2.s0],((float*)conv2d_nchw_local)[v__2.s1],((float*)conv2d_nchw_local)[v__2.s2],((float*)conv2d_nchw_local)[v__2.s3]));\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv2d_nchw_local) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(38) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(38) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[16];\n  __shared__ float pad_temp_shared[70];\n  __shared__ float kernel_shared[380];\n  for (int xx_c_outer_inner_init = 0; xx_c_outer_inner_init < 4; ++xx_c_outer_inner_init) {\n    conv2d_nchw_local[xx_c_outer_inner_init] = 0.000000e+00f;\n    conv2d_nchw_local[(xx_c_outer_inner_init + 4)] = 0.000000e+00f;\n    conv2d_nchw_local[(xx_c_outer_inner_init + 8)] = 0.000000e+00f;\n    conv2d_nchw_local[(xx_c_outer_inner_init + 12)] = 0.000000e+00f;\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 20; ++rc_outer_outer) {\n    for (int ry_outer_outer = 0; ry_outer_outer < 4; ++ry_outer_outer) {\n      for (int rx_outer_outer = 0; rx_outer_outer < 3; ++rx_outer_outer) {\n        __syncthreads();\n        for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n          if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 19) + (((int)threadIdx.x) >> 1)) < 35) {\n            pad_temp_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 38) + ((int)threadIdx.x))] = ((((1 <= ((((((int)blockIdx.x) % 3) * 2) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 10) + ((int)threadIdx.x)) % 14) / 7)) + ry_outer_outer)) && ((((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 10) + ((int)threadIdx.x)) % 14) / 7) + ry_outer_outer) >> 1) + (((int)blockIdx.x) % 3)) < 4)) && (1 <= (rx_outer_outer + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 3) + ((int)threadIdx.x)) % 7)))) ? data[((((((((((((int)blockIdx.x) / 3) * 5600) + (rc_outer_outer * 280)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 19) + (((int)threadIdx.x) >> 1)) / 7) * 56)) + ((((int)blockIdx.x) % 3) * 16)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 10) + ((int)threadIdx.x)) % 14) / 7) * 8)) + (ry_outer_outer * 8)) + rx_outer_outer) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 3) + ((int)threadIdx.x)) % 7)) - 9)] : 0.000000e+00f);\n          },\n        },\n        for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 < 10; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) {\n          kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 38) + ((int)threadIdx.x))] = kernel[((((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 38) + ((int)threadIdx.x)) / 5) * 1200) + (rc_outer_outer * 60)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 3) + ((int)threadIdx.x)) % 5) * 12)) + (ry_outer_outer * 3)) + rx_outer_outer)];\n        },\n        __syncthreads();\n        for (int rc_outer_inner = 0; rc_outer_inner < 5; ++rc_outer_inner) {\n          for (int xx_c_outer_inner = 0; xx_c_outer_inner < 4; ++xx_c_outer_inner) {\n            conv2d_nchw_local[xx_c_outer_inner] = (conv2d_nchw_local[xx_c_outer_inner] + (pad_temp_shared[((rc_outer_inner * 14) + (xx_c_outer_inner * 2))] * kernel_shared[((((int)threadIdx.x) * 5) + rc_outer_inner)]));\n            conv2d_nchw_local[(xx_c_outer_inner + 4)] = (conv2d_nchw_local[(xx_c_outer_inner + 4)] + (pad_temp_shared[(((rc_outer_inner * 14) + (xx_c_outer_inner * 2)) + 7)] * kernel_shared[((((int)threadIdx.x) * 5) + rc_outer_inner)]));\n            conv2d_nchw_local[(xx_c_outer_inner + 8)] = (conv2d_nchw_local[(xx_c_outer_inner + 8)] + (pad_temp_shared[((rc_outer_inner * 14) + (xx_c_outer_inner * 2))] * kernel_shared[(((((int)threadIdx.x) * 5) + rc_outer_inner) + 190)]));\n            conv2d_nchw_local[(xx_c_outer_inner + 12)] = (conv2d_nchw_local[(xx_c_outer_inner + 12)] + (pad_temp_shared[(((rc_outer_inner * 14) + (xx_c_outer_inner * 2)) + 7)] * kernel_shared[(((((int)threadIdx.x) * 5) + rc_outer_inner) + 190)]));\n          },\n        },\n      },\n    },\n  },\n  for (int xx_inner = 0; xx_inner < 4; ++xx_inner) {\n    conv2d_nchw[(((((((int)blockIdx.x) / 3) * 1824) + (((int)threadIdx.x) * 24)) + ((((int)blockIdx.x) % 3) * 8)) + xx_inner)] = conv2d_nchw_local[xx_inner];\n    conv2d_nchw[((((((((int)blockIdx.x) / 3) * 1824) + (((int)threadIdx.x) * 24)) + ((((int)blockIdx.x) % 3) * 8)) + xx_inner) + 4)] = conv2d_nchw_local[(xx_inner + 4)];\n    conv2d_nchw[((((((((int)blockIdx.x) / 3) * 1824) + (((int)threadIdx.x) * 24)) + ((((int)blockIdx.x) % 3) * 8)) + xx_inner) + 912)] = conv2d_nchw_local[(xx_inner + 8)];\n    conv2d_nchw[((((((((int)blockIdx.x) / 3) * 1824) + (((int)threadIdx.x) * 24)) + ((((int)blockIdx.x) % 3) * 8)) + xx_inner) + 916)] = conv2d_nchw_local[(xx_inner + 12)];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 100, 7, 8), \"float32\"), kernel: T.Buffer((76, 100, 4, 3), \"float32\"), conv2d_nchw: T.Buffer((10, 76, 6, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_ff_outer_fused_yy_outer_fused in T.parallel(38):\n            pad_temp = T.allocate([54000], \"float32\", \"global\")\n            conv2d_nchw_local = T.allocate([480], \"float32\", \"local\")\n            pad_temp_1 = T.Buffer((54000,), data=pad_temp)\n            for i0, i1, i2, i3_s in T.grid(10, 100, 6, 9):\n                cse_var_2: T.int32 = nn_outer_ff_outer_fused_yy_outer_fused % 2\n                cse_var_1: T.int32 = cse_var_2 * 3 + i2\n                data_1 = T.Buffer((56000,), data=data.data)\n                pad_temp_1[i0 * 5400 + i1 * 54 + i2 * 9 + i3_s] = T.if_then_else(1 <= cse_var_1 and cse_var_1 < 8 and 1 <= i3_s, data_1[i0 * 5600 + i1 * 56 + cse_var_2 * 24 + i2 * 8 + i3_s - 9], T.float32(0))\n            conv2d_nchw_local_1 = T.Buffer((480,), data=conv2d_nchw_local, scope=\"local\")\n            for yy_c_outer_outer_inner in range(3):\n                for xx_c_outer_inner_init, nn_c_inner_init, ff_c_inner_init in T.grid(2, 10, 4):\n                    conv2d_nchw_local_1[nn_c_inner_init * 48 + ff_c_inner_init * 12 + yy_c_outer_outer_inner * 4 + xx_c_outer_inner_init * 2:nn_c_inner_init * 48 + ff_c_inner_init * 12 + yy_c_outer_outer_inner * 4 + xx_c_outer_inner_init * 2 + 2] = T.Broadcast(T.float32(0), 2)\n                for rc_outer, xx_c_outer_inner, rc_inner, ry_inner, rx_inner, nn_c_inner, ff_c_inner in T.grid(25, 2, 4, 4, 3, 10, 4):\n                    cse_var_3: T.int32 = nn_c_inner * 48 + ff_c_inner * 12 + yy_c_outer_outer_inner * 4 + xx_c_outer_inner * 2\n                    kernel_1 = T.Buffer((91200,), data=kernel.data)\n                    conv2d_nchw_local_1[cse_var_3:cse_var_3 + 2] = conv2d_nchw_local_1[cse_var_3:cse_var_3 + 2] + pad_temp_1[nn_c_inner * 5400 + rc_outer * 216 + rc_inner * 54 + yy_c_outer_outer_inner * 9 + ry_inner * 9 + xx_c_outer_inner * 4 + rx_inner:nn_c_inner * 5400 + rc_outer * 216 + rc_inner * 54 + yy_c_outer_outer_inner * 9 + ry_inner * 9 + xx_c_outer_inner * 4 + rx_inner + 4:2] * T.Broadcast(kernel_1[nn_outer_ff_outer_fused_yy_outer_fused // 2 * 4800 + ff_c_inner * 1200 + rc_outer * 48 + rc_inner * 12 + ry_inner * 3 + rx_inner], 2)\n            for nn_inner, ff_inner, yy_inner in T.grid(10, 4, 3):\n                cse_var_4: T.int32 = yy_inner * 4\n                conv2d_nchw_1 = T.Buffer((18240,), data=conv2d_nchw.data)\n                conv2d_nchw_1[nn_inner * 1824 + nn_outer_ff_outer_fused_yy_outer_fused // 2 * 96 + ff_inner * 24 + nn_outer_ff_outer_fused_yy_outer_fused % 2 * 12 + cse_var_4:nn_inner * 1824 + nn_outer_ff_outer_fused_yy_outer_fused // 2 * 96 + ff_inner * 24 + nn_outer_ff_outer_fused_yy_outer_fused % 2 * 12 + cse_var_4 + 4] = conv2d_nchw_local_1[nn_inner * 48 + ff_inner * 12 + cse_var_4:nn_inner * 48 + ff_inner * 12 + cse_var_4 + 4]",
        "data": "10_100_7_8",
        "kernel": "76_100_4_3"
    },
    {
        "op_name": "conv2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv2d_nchw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused = 0; nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused < 10; ++nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused) {\n    void* conv2d_nchw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)408, 2, 32);\n    if (conv2d_nchw_local == NULL) {\n      return -1;\n    },\n    float pad_temp[14];\n    for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 34; ++ff_c_inner_init) {\n      for (int32_t yy_c_inner_init = 0; yy_c_inner_init < 3; ++yy_c_inner_init) {\n        ((float*)conv2d_nchw_local)[((ff_c_inner_init * 3) + yy_c_inner_init)] = 0.000000e+00f;\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 17; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 4; ++ry_outer) {\n        for (int32_t rx_outer = 0; rx_outer < 2; ++rx_outer) {\n          for (int32_t i2 = 0; i2 < 7; ++i2) {\n            for (int32_t i3_s = 0; i3_s < 2; ++i3_s) {\n              int32_t cse_var_1 = (rx_outer * 2);\n              pad_temp[((i2 * 2) + i3_s)] = (((1 <= (i2 + ry_outer)) && (1 <= (cse_var_1 + i3_s))) ? ((float*)data_1)[((((((((nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused >> 1) * 459) + (rc_outer * 27)) + (i2 * 3)) + (ry_outer * 3)) + cse_var_1) + i3_s) - 4)] : 0.000000e+00f);\n            },\n          },\n          for (int32_t rx_inner = 0; rx_inner < 2; ++rx_inner) {\n            for (int32_t ff_c_inner = 0; ff_c_inner < 34; ++ff_c_inner) {\n              for (int32_t yy_c_inner = 0; yy_c_inner < 3; ++yy_c_inner) {\n                int32_t cse_var_2 = ((ff_c_inner * 3) + yy_c_inner);\n                ((float*)conv2d_nchw_local)[cse_var_2] = (((float*)conv2d_nchw_local)[cse_var_2] + (pad_temp[((yy_c_inner * 6) + rx_inner)] * ((float*)kernel_1)[(((((((nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused & 1) * 9248) + (ff_c_inner * 272)) + (rc_outer * 16)) + (ry_outer * 4)) + (rx_outer * 2)) + rx_inner)]));\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t ff_inner = 0; ff_inner < 34; ++ff_inner) {\n      for (int32_t yy_inner = 0; yy_inner < 3; ++yy_inner) {\n        int32_t cse_var_3 = (ff_inner * 3);\n        ((float*)conv2d_nchw_1)[(((nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused * 102) + cse_var_3) + yy_inner)] = ((float*)conv2d_nchw_local)[(cse_var_3 + yy_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv2d_nchw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(255) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(255) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[2];\n  __shared__ float pad_temp_shared[200];\n  __shared__ float kernel_shared[544];\n  for (int ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 2; ++ff_c_outer_inner_init) {\n    conv2d_nchw_local[ff_c_outer_inner_init] = 0.000000e+00f;\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 17; ++rc_outer_outer) {\n    __syncthreads();\n    if (((int)threadIdx.x) < 200) {\n      pad_temp_shared[((int)threadIdx.x)] = (((4 <= (((int)threadIdx.x) % 40)) && (1 <= (((int)threadIdx.x) & 3))) ? data[((((((((int)threadIdx.x) / 40) * 459) + (rc_outer_outer * 27)) + (((((int)threadIdx.x) % 40) >> 2) * 3)) + (((int)threadIdx.x) & 3)) - 4)] : 0.000000e+00f);\n    },\n    if (((int)threadIdx.x) < 136) {\n      *(float4*)(kernel_shared + (((int)threadIdx.x) * 4)) = *(float4*)(kernel + ((((((int)blockIdx.x) * 9248) + ((((int)threadIdx.x) >> 2) * 272)) + (rc_outer_outer * 16)) + ((((int)threadIdx.x) & 3) * 4)));\n    },\n    __syncthreads();\n    for (int ry_outer_inner = 0; ry_outer_inner < 2; ++ry_outer_inner) {\n      for (int rx_outer_inner = 0; rx_outer_inner < 4; ++rx_outer_inner) {\n        for (int ff_c_outer_inner = 0; ff_c_outer_inner < 2; ++ff_c_outer_inner) {\n          for (int ry_inner = 0; ry_inner < 2; ++ry_inner) {\n            conv2d_nchw_local[ff_c_outer_inner] = (conv2d_nchw_local[ff_c_outer_inner] + (pad_temp_shared[((((((((int)threadIdx.x) / 51) * 40) + ((((int)threadIdx.x) % 3) * 12)) + (ry_outer_inner * 8)) + (ry_inner * 4)) + rx_outer_inner)] * kernel_shared[(((((((((int)threadIdx.x) % 51) / 3) * 32) + (ff_c_outer_inner * 16)) + (ry_outer_inner * 8)) + (ry_inner * 4)) + rx_outer_inner)]));\n          },\n        },\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 2; ++ff_inner) {\n    conv2d_nchw[((((((((int)threadIdx.x) / 51) * 204) + (((int)blockIdx.x) * 102)) + (((((int)threadIdx.x) % 51) / 3) * 6)) + (ff_inner * 3)) + (((int)threadIdx.x) % 3))] = conv2d_nchw_local[ff_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 17, 9, 3), \"float32\"), kernel: T.Buffer((68, 17, 4, 4), \"float32\"), conv2d_nchw: T.Buffer((5, 68, 3, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused in T.parallel(10):\n            conv2d_nchw_local = T.allocate([102], \"float32\", \"local\")\n            pad_temp = T.allocate([14], \"float32\", \"global\")\n            conv2d_nchw_local_1 = T.Buffer((102,), data=conv2d_nchw_local, scope=\"local\")\n            for ff_c_inner_init, yy_c_inner_init in T.grid(34, 3):\n                conv2d_nchw_local_1[ff_c_inner_init * 3 + yy_c_inner_init] = T.float32(0)\n            for rc_outer, ry_outer, rx_outer in T.grid(17, 4, 2):\n                pad_temp_1 = T.Buffer((14,), data=pad_temp, align=32)\n                for i2, i3_s in T.grid(7, 2):\n                    cse_var_1: T.int32 = rx_outer * 2\n                    data_1 = T.Buffer((2295,), data=data.data)\n                    pad_temp_1[i2 * 2 + i3_s] = T.if_then_else(1 <= i2 + ry_outer and 1 <= cse_var_1 + i3_s, data_1[nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused // 2 * 459 + rc_outer * 27 + i2 * 3 + ry_outer * 3 + cse_var_1 + i3_s - 4], T.float32(0))\n                for rx_inner, ff_c_inner, yy_c_inner in T.grid(2, 34, 3):\n                    cse_var_2: T.int32 = ff_c_inner * 3 + yy_c_inner\n                    kernel_1 = T.Buffer((18496,), data=kernel.data)\n                    conv2d_nchw_local_1[cse_var_2] = conv2d_nchw_local_1[cse_var_2] + pad_temp_1[yy_c_inner * 6 + rx_inner] * kernel_1[nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused % 2 * 9248 + ff_c_inner * 272 + rc_outer * 16 + ry_outer * 4 + rx_outer * 2 + rx_inner]\n            for ff_inner, yy_inner in T.grid(34, 3):\n                cse_var_3: T.int32 = ff_inner * 3\n                conv2d_nchw_1 = T.Buffer((1020,), data=conv2d_nchw.data)\n                conv2d_nchw_1[nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused * 102 + cse_var_3 + yy_inner] = conv2d_nchw_local_1[cse_var_3 + yy_inner]",
        "data": "5_17_9_3",
        "kernel": "68_17_4_4"
    },
    {
        "op_name": "conv2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv2d_nchw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused < 6; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)7920, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    void* conv2d_nchw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1040, 2, 32);\n    if (conv2d_nchw_local == NULL) {\n      return -1;\n    },\n    for (int32_t i0 = 0; i0 < 4; ++i0) {\n      for (int32_t i1 = 0; i1 < 55; ++i1) {\n        for (int32_t i2 = 0; i2 < 3; ++i2) {\n          for (int32_t i3_s = 0; i3_s < 3; ++i3_s) {\n            int32_t cse_var_1 = ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused >> 1) * 3);\n            ((float*)pad_temp)[((((i0 * 495) + (i1 * 9)) + (i2 * 3)) + i3_s)] = ((((1 <= i2) && (i2 < 2)) && (1 <= (cse_var_1 + i3_s))) ? ((float*)data_1)[((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused & 1) * 1760) + (i0 * 440)) + (i1 * 8)) + (i2 * 8)) + cse_var_1) + i3_s) - 9)] : 0.000000e+00f);\n          },\n        },\n      },\n    },\n    for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 65; ++ff_c_outer_inner_init) {\n      for (int32_t nn_c_inner_init = 0; nn_c_inner_init < 4; ++nn_c_inner_init) {\n        ((float*)conv2d_nchw_local)[((nn_c_inner_init * 65) + ff_c_outer_inner_init)] = 0.000000e+00f;\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 11; ++rc_outer) {\n      for (int32_t rx_outer = 0; rx_outer < 3; ++rx_outer) {\n        for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 65; ++ff_c_outer_inner) {\n          for (int32_t rc_inner = 0; rc_inner < 5; ++rc_inner) {\n            for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n              for (int32_t nn_c_inner = 0; nn_c_inner < 4; ++nn_c_inner) {\n                int32_t cse_var_5 = (rc_outer * 45);\n                int32_t cse_var_4 = (rc_inner * 9);\n                int32_t cse_var_3 = (ry_inner * 3);\n                int32_t cse_var_2 = ((nn_c_inner * 65) + ff_c_outer_inner);\n                ((float*)conv2d_nchw_local)[cse_var_2] = (((float*)conv2d_nchw_local)[cse_var_2] + (((float*)pad_temp)[(((((nn_c_inner * 495) + cse_var_5) + cse_var_4) + cse_var_3) + rx_outer)] * ((float*)kernel_1)[(((((ff_c_outer_inner * 495) + cse_var_5) + cse_var_4) + cse_var_3) + rx_outer)]));\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 4; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 65; ++ff_inner) {\n        ((float*)conv2d_nchw_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused & 1) * 780) + (nn_inner * 195)) + (ff_inner * 3)) + (nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused >> 1))] = ((float*)conv2d_nchw_local)[((nn_inner * 65) + ff_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv2d_nchw_local) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(39) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(39) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[2];\n  __shared__ float pad_temp_shared[2970];\n  __shared__ float kernel_shared[6435];\n  for (int nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 2; ++nn_c_outer_inner_init) {\n    conv2d_nchw_local[nn_c_outer_inner_init] = 0.000000e+00f;\n  },\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 16; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 13) + (((int)threadIdx.x) / 3)) < 198) {\n      for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_s = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_s < 5; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_s) {\n        pad_temp_shared[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 195) + (((int)threadIdx.x) * 5)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_s)] = ((((3 <= (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 2) + (((((int)threadIdx.x) * 5) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_s) / 3)) % 9)) && (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 195) + (((int)threadIdx.x) * 5)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_s) % 27) < 18)) && (1 <= ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 195) + (((int)threadIdx.x) * 5)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_s) % 9))) ? data[((((((((int)blockIdx.x) / 5) * 880) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 65) + (((((int)threadIdx.x) * 5) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_s) / 3)) / 9) * 8)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 2) + (((((int)threadIdx.x) * 5) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_s) / 3)) % 9) / 3) * 8)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 195) + (((int)threadIdx.x) * 5)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_s) % 9)) - 9)] : 0.000000e+00f);\n      },\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 < 165; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) {\n    kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 39) + ((int)threadIdx.x))] = kernel[((((((int)blockIdx.x) % 5) * 6435) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 39)) + ((int)threadIdx.x))];\n  },\n  __syncthreads();\n  for (int rc_outer_inner = 0; rc_outer_inner < 55; ++rc_outer_inner) {\n    for (int ry_outer_inner = 0; ry_outer_inner < 3; ++ry_outer_inner) {\n      for (int nn_c_outer_inner = 0; nn_c_outer_inner < 2; ++nn_c_outer_inner) {\n        for (int rx_inner = 0; rx_inner < 3; ++rx_inner) {\n          conv2d_nchw_local[nn_c_outer_inner] = (conv2d_nchw_local[nn_c_outer_inner] + (pad_temp_shared[(((((nn_c_outer_inner * 1485) + (rc_outer_inner * 27)) + (ry_outer_inner * 9)) + ((((int)threadIdx.x) % 3) * 3)) + rx_inner)] * kernel_shared[(((((((int)threadIdx.x) / 3) * 495) + (rc_outer_inner * 9)) + (ry_outer_inner * 3)) + rx_inner)]));\n        },\n      },\n    },\n  },\n  for (int nn_inner = 0; nn_inner < 2; ++nn_inner) {\n    conv2d_nchw[(((((((int)blockIdx.x) / 5) * 390) + (nn_inner * 195)) + ((((int)blockIdx.x) % 5) * 39)) + ((int)threadIdx.x))] = conv2d_nchw_local[nn_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 55, 1, 8), \"float32\"), kernel: T.Buffer((65, 55, 3, 3), \"float32\"), conv2d_nchw: T.Buffer((8, 65, 1, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused in T.parallel(6):\n            pad_temp = T.allocate([1980], \"float32\", \"global\")\n            conv2d_nchw_local = T.allocate([260], \"float32\", \"local\")\n            pad_temp_1 = T.Buffer((1980,), data=pad_temp)\n            for i0, i1, i2, i3_s in T.grid(4, 55, 3, 3):\n                cse_var_1: T.int32 = nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused // 2 * 3\n                data_1 = T.Buffer((3520,), data=data.data)\n                pad_temp_1[i0 * 495 + i1 * 9 + i2 * 3 + i3_s] = T.if_then_else(1 <= i2 and i2 < 2 and 1 <= cse_var_1 + i3_s, data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 2 * 1760 + i0 * 440 + i1 * 8 + i2 * 8 + cse_var_1 + i3_s - 9], T.float32(0))\n            conv2d_nchw_local_1 = T.Buffer((260,), data=conv2d_nchw_local, scope=\"local\")\n            for ff_c_outer_inner_init, nn_c_inner_init in T.grid(65, 4):\n                conv2d_nchw_local_1[nn_c_inner_init * 65 + ff_c_outer_inner_init] = T.float32(0)\n            for rc_outer, rx_outer, ff_c_outer_inner, rc_inner, ry_inner, nn_c_inner in T.grid(11, 3, 65, 5, 3, 4):\n                cse_var_5: T.int32 = rc_outer * 45\n                cse_var_4: T.int32 = rc_inner * 9\n                cse_var_3: T.int32 = ry_inner * 3\n                cse_var_2: T.int32 = nn_c_inner * 65 + ff_c_outer_inner\n                kernel_1 = T.Buffer((32175,), data=kernel.data)\n                conv2d_nchw_local_1[cse_var_2] = conv2d_nchw_local_1[cse_var_2] + pad_temp_1[nn_c_inner * 495 + cse_var_5 + cse_var_4 + cse_var_3 + rx_outer] * kernel_1[ff_c_outer_inner * 495 + cse_var_5 + cse_var_4 + cse_var_3 + rx_outer]\n            for nn_inner, ff_inner in T.grid(4, 65):\n                conv2d_nchw_1 = T.Buffer((1560,), data=conv2d_nchw.data)\n                conv2d_nchw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused % 2 * 780 + nn_inner * 195 + ff_inner * 3 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused // 2] = conv2d_nchw_local_1[nn_inner * 65 + ff_inner]",
        "data": "8_55_1_8",
        "kernel": "65_55_3_3"
    },
    {
        "op_name": "conv2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv2d_nchw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused = 0; nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused < 35; ++nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused) {\n    void* conv2d_nchw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1152, 2, 32);\n    if (conv2d_nchw_local == NULL) {\n      return -1;\n    },\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1512, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t ff_c_outer_outer_inner = 0; ff_c_outer_outer_inner < 16; ++ff_c_outer_outer_inner) {\n      for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 3; ++ff_c_outer_inner_init) {\n        for (int32_t nn_c_inner_init = 0; nn_c_inner_init < 3; ++nn_c_inner_init) {\n          for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 2; ++ff_c_inner_init) {\n            ((float*)conv2d_nchw_local)[((((nn_c_inner_init * 96) + (ff_c_outer_outer_inner * 6)) + (ff_c_outer_inner_init * 2)) + ff_c_inner_init)] = 0.000000e+00f;\n          },\n        },\n      },\n      for (int32_t rc_outer = 0; rc_outer < 5; ++rc_outer) {\n        for (int32_t i0 = 0; i0 < 3; ++i0) {\n          for (int32_t i1 = 0; i1 < 14; ++i1) {\n            for (int32_t i2 = 0; i2 < 3; ++i2) {\n              for (int32_t i3_s = 0; i3_s < 3; ++i3_s) {\n                int32_t cse_var_3 = (nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused % 7);\n                int32_t cse_var_2 = (nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused / 7);\n                int32_t cse_var_1 = (i3_s + cse_var_3);\n                ((float*)pad_temp)[((((i0 * 126) + (i1 * 9)) + (i2 * 3)) + i3_s)] = (((((1 <= ((cse_var_2 * 2) + i2)) && ((cse_var_2 + (i2 >> 1)) < 5)) && (1 <= cse_var_1)) && (cse_var_1 < 8)) ? ((float*)data_1)[((((((((i0 * 4410) + (rc_outer * 882)) + (i1 * 63)) + (cse_var_2 * 14)) + (i2 * 7)) + i3_s) + cse_var_3) - 8)] : 0.000000e+00f);\n              },\n            },\n          },\n        },\n        for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 3; ++ff_c_outer_inner) {\n          for (int32_t rc_inner = 0; rc_inner < 14; ++rc_inner) {\n            for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n              for (int32_t rx_inner = 0; rx_inner < 3; ++rx_inner) {\n                for (int32_t nn_c_inner = 0; nn_c_inner < 3; ++nn_c_inner) {\n                  for (int32_t ff_c_inner = 0; ff_c_inner < 2; ++ff_c_inner) {\n                    int32_t cse_var_6 = (rc_inner * 9);\n                    int32_t cse_var_5 = (ry_inner * 3);\n                    int32_t cse_var_4 = ((((nn_c_inner * 96) + (ff_c_outer_outer_inner * 6)) + (ff_c_outer_inner * 2)) + ff_c_inner);\n                    ((float*)conv2d_nchw_local)[cse_var_4] = (((float*)conv2d_nchw_local)[cse_var_4] + (((float*)pad_temp)[((((nn_c_inner * 126) + cse_var_6) + cse_var_5) + rx_inner)] * ((float*)kernel_1)[(((((((ff_c_outer_outer_inner * 3780) + (ff_c_outer_inner * 1260)) + (ff_c_inner * 630)) + (rc_outer * 126)) + cse_var_6) + cse_var_5) + rx_inner)]));\n                  },\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 3; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 96; ++ff_inner) {\n        ((float*)conv2d_nchw_1)[(((nn_inner * 3360) + (ff_inner * 35)) + nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused)] = ((float*)conv2d_nchw_local)[((nn_inner * 96) + ff_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv2d_nchw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(84) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(84) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[24];\n  __shared__ float pad_temp_shared[810];\n  __shared__ float kernel_shared[8640];\n  for (int ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 2; ++ff_c_outer_inner_init) {\n    for (int nn_c_inner_init = 0; nn_c_inner_init < 3; ++nn_c_inner_init) {\n      conv2d_nchw_local[((nn_c_inner_init * 2) + ff_c_outer_inner_init)] = 0.000000e+00f;\n      conv2d_nchw_local[(((nn_c_inner_init * 2) + ff_c_outer_inner_init) + 6)] = 0.000000e+00f;\n      conv2d_nchw_local[(((nn_c_inner_init * 2) + ff_c_outer_inner_init) + 12)] = 0.000000e+00f;\n      conv2d_nchw_local[(((nn_c_inner_init * 2) + ff_c_outer_inner_init) + 18)] = 0.000000e+00f;\n    },\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 7; ++rc_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 10; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 14) + (((int)threadIdx.x) / 6)) < 135) {\n        pad_temp_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 84) + ((int)threadIdx.x))] = (((((1 <= ((((int)blockIdx.x) * 2) + ((((((int)threadIdx.x) / 3) + ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) % 9) / 3))) && ((((((((int)threadIdx.x) / 3) + ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) % 9) / 6) + ((int)blockIdx.x)) < 5)) && (1 <= (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 3) + ((int)threadIdx.x)) % 9))) && ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 3) + ((int)threadIdx.x)) % 9) < 8)) ? data[((((((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 14) + (((int)threadIdx.x) / 6)) / 45) * 4410) + (rc_outer_outer * 630)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 28) + (((int)threadIdx.x) / 3)) % 90) / 9) * 63)) + (((int)blockIdx.x) * 14)) + (((((((int)threadIdx.x) / 3) + ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) % 9) / 3) * 7)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 3) + ((int)threadIdx.x)) % 9)) - 8)] : 0.000000e+00f);\n      },\n    },\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 < 35; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) {\n      if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 7) + (((int)threadIdx.x) / 12)) < 240) {\n        *(float3*)(kernel_shared + ((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 252) + (((int)threadIdx.x) * 3))) = *(float3*)(kernel + (((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 14) + (((int)threadIdx.x) / 6)) / 5) * 630) + (rc_outer_outer * 90)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 8) + (((int)threadIdx.x) / 3)) % 10) * 9)) + ((((int)threadIdx.x) % 3) * 3)));\n      },\n    },\n    __syncthreads();\n    for (int rc_outer_inner = 0; rc_outer_inner < 10; ++rc_outer_inner) {\n      for (int rx_outer_inner = 0; rx_outer_inner < 3; ++rx_outer_inner) {\n        for (int ff_c_outer_inner = 0; ff_c_outer_inner < 2; ++ff_c_outer_inner) {\n          for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n            for (int nn_c_inner = 0; nn_c_inner < 3; ++nn_c_inner) {\n              conv2d_nchw_local[((nn_c_inner * 2) + ff_c_outer_inner)] = (conv2d_nchw_local[((nn_c_inner * 2) + ff_c_outer_inner)] + (pad_temp_shared[(((((nn_c_inner * 270) + (rc_outer_inner * 27)) + (ry_inner * 9)) + rx_outer_inner) + (((int)threadIdx.x) % 7))] * kernel_shared[((((((((int)threadIdx.x) / 7) * 180) + (ff_c_outer_inner * 90)) + (rc_outer_inner * 9)) + (ry_inner * 3)) + rx_outer_inner)]));\n              conv2d_nchw_local[(((nn_c_inner * 2) + ff_c_outer_inner) + 6)] = (conv2d_nchw_local[(((nn_c_inner * 2) + ff_c_outer_inner) + 6)] + (pad_temp_shared[(((((nn_c_inner * 270) + (rc_outer_inner * 27)) + (ry_inner * 9)) + rx_outer_inner) + (((int)threadIdx.x) % 7))] * kernel_shared[(((((((((int)threadIdx.x) / 7) * 180) + (ff_c_outer_inner * 90)) + (rc_outer_inner * 9)) + (ry_inner * 3)) + rx_outer_inner) + 2160)]));\n              conv2d_nchw_local[(((nn_c_inner * 2) + ff_c_outer_inner) + 12)] = (conv2d_nchw_local[(((nn_c_inner * 2) + ff_c_outer_inner) + 12)] + (pad_temp_shared[(((((nn_c_inner * 270) + (rc_outer_inner * 27)) + (ry_inner * 9)) + rx_outer_inner) + (((int)threadIdx.x) % 7))] * kernel_shared[(((((((((int)threadIdx.x) / 7) * 180) + (ff_c_outer_inner * 90)) + (rc_outer_inner * 9)) + (ry_inner * 3)) + rx_outer_inner) + 4320)]));\n              conv2d_nchw_local[(((nn_c_inner * 2) + ff_c_outer_inner) + 18)] = (conv2d_nchw_local[(((nn_c_inner * 2) + ff_c_outer_inner) + 18)] + (pad_temp_shared[(((((nn_c_inner * 270) + (rc_outer_inner * 27)) + (ry_inner * 9)) + rx_outer_inner) + (((int)threadIdx.x) % 7))] * kernel_shared[(((((((((int)threadIdx.x) / 7) * 180) + (ff_c_outer_inner * 90)) + (rc_outer_inner * 9)) + (ry_inner * 3)) + rx_outer_inner) + 6480)]));\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int nn_inner = 0; nn_inner < 3; ++nn_inner) {\n    for (int ff_inner = 0; ff_inner < 2; ++ff_inner) {\n      conv2d_nchw[(((((nn_inner * 3360) + ((((int)threadIdx.x) / 7) * 70)) + (ff_inner * 35)) + (((int)blockIdx.x) * 7)) + (((int)threadIdx.x) % 7))] = conv2d_nchw_local[((nn_inner * 2) + ff_inner)];\n      conv2d_nchw[((((((nn_inner * 3360) + ((((int)threadIdx.x) / 7) * 70)) + (ff_inner * 35)) + (((int)blockIdx.x) * 7)) + (((int)threadIdx.x) % 7)) + 840)] = conv2d_nchw_local[(((nn_inner * 2) + ff_inner) + 6)];\n      conv2d_nchw[((((((nn_inner * 3360) + ((((int)threadIdx.x) / 7) * 70)) + (ff_inner * 35)) + (((int)blockIdx.x) * 7)) + (((int)threadIdx.x) % 7)) + 1680)] = conv2d_nchw_local[(((nn_inner * 2) + ff_inner) + 12)];\n      conv2d_nchw[((((((nn_inner * 3360) + ((((int)threadIdx.x) / 7) * 70)) + (ff_inner * 35)) + (((int)blockIdx.x) * 7)) + (((int)threadIdx.x) % 7)) + 2520)] = conv2d_nchw_local[(((nn_inner * 2) + ff_inner) + 18)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 70, 9, 7), \"float32\"), kernel: T.Buffer((96, 70, 3, 3), \"float32\"), conv2d_nchw: T.Buffer((3, 96, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused in T.parallel(35):\n            conv2d_nchw_local = T.allocate([288], \"float32\", \"local\")\n            pad_temp = T.allocate([378], \"float32\", \"global\")\n            conv2d_nchw_local_1 = T.Buffer((288,), data=conv2d_nchw_local, scope=\"local\")\n            for ff_c_outer_outer_inner in range(16):\n                for ff_c_outer_inner_init, nn_c_inner_init, ff_c_inner_init in T.grid(3, 3, 2):\n                    conv2d_nchw_local_1[nn_c_inner_init * 96 + ff_c_outer_outer_inner * 6 + ff_c_outer_inner_init * 2 + ff_c_inner_init] = T.float32(0)\n                for rc_outer in range(5):\n                    pad_temp_1 = T.Buffer((378,), data=pad_temp)\n                    for i0, i1, i2, i3_s in T.grid(3, 14, 3, 3):\n                        cse_var_3: T.int32 = nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused % 7\n                        cse_var_2: T.int32 = nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused // 7\n                        cse_var_1: T.int32 = i3_s + cse_var_3\n                        data_1 = T.Buffer((13230,), data=data.data)\n                        pad_temp_1[i0 * 126 + i1 * 9 + i2 * 3 + i3_s] = T.if_then_else(1 <= cse_var_2 * 2 + i2 and cse_var_2 + i2 // 2 < 5 and 1 <= cse_var_1 and cse_var_1 < 8, data_1[i0 * 4410 + rc_outer * 882 + i1 * 63 + cse_var_2 * 14 + i2 * 7 + i3_s + cse_var_3 - 8], T.float32(0))\n                    for ff_c_outer_inner, rc_inner, ry_inner, rx_inner, nn_c_inner, ff_c_inner in T.grid(3, 14, 3, 3, 3, 2):\n                        cse_var_6: T.int32 = rc_inner * 9\n                        cse_var_5: T.int32 = ry_inner * 3\n                        cse_var_4: T.int32 = nn_c_inner * 96 + ff_c_outer_outer_inner * 6 + ff_c_outer_inner * 2 + ff_c_inner\n                        kernel_1 = T.Buffer((60480,), data=kernel.data)\n                        conv2d_nchw_local_1[cse_var_4] = conv2d_nchw_local_1[cse_var_4] + pad_temp_1[nn_c_inner * 126 + cse_var_6 + cse_var_5 + rx_inner] * kernel_1[ff_c_outer_outer_inner * 3780 + ff_c_outer_inner * 1260 + ff_c_inner * 630 + rc_outer * 126 + cse_var_6 + cse_var_5 + rx_inner]\n            for nn_inner, ff_inner in T.grid(3, 96):\n                conv2d_nchw_1 = T.Buffer((10080,), data=conv2d_nchw.data)\n                conv2d_nchw_1[nn_inner * 3360 + ff_inner * 35 + nn_outer_ff_outer_fused_yy_outer_fused_xx_outer_fused] = conv2d_nchw_local_1[nn_inner * 96 + ff_inner]",
        "data": "3_70_9_7",
        "kernel": "96_70_3_3"
    },
    {
        "op_name": "conv2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv2d_nchw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused < 90; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)7200, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t i1 = 0; i1 < 100; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        for (int32_t i3_s = 0; i3_s < 3; ++i3_s) {\n          int32_t cse_var_3 = ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused % 10) / 5);\n          int32_t cse_var_2 = ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused % 5) * 2);\n          int32_t cse_var_1 = ((cse_var_3 * 4) + i2);\n          ((float*)pad_temp)[(((i1 * 18) + (i2 * 3)) + i3_s)] = ((((1 <= cse_var_1) && (cse_var_1 < 9)) && (1 <= (cse_var_2 + i3_s))) ? ((float*)data_1)[((((((i1 * 80) + (cse_var_3 * 40)) + (i2 * 10)) + cse_var_2) + i3_s) - 11)] : 0.000000e+00f);\n        },\n      },\n    },\n    for (int32_t ff_inner_init = 0; ff_inner_init < 4; ++ff_inner_init) {\n      for (int32_t yy_inner_init = 0; yy_inner_init < 4; ++yy_inner_init) {\n        ((float*)conv2d_nchw_1)[((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused / 10) * 160) + (ff_inner_init * 40)) + (((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused % 10) / 5) * 20)) + (yy_inner_init * 5)) + (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused % 5))] = 0.000000e+00f;\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 5; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n        for (int32_t rx_outer = 0; rx_outer < 3; ++rx_outer) {\n          for (int32_t rc_inner = 0; rc_inner < 20; ++rc_inner) {\n            for (int32_t ff_inner = 0; ff_inner < 4; ++ff_inner) {\n              for (int32_t yy_inner = 0; yy_inner < 4; ++yy_inner) {\n                int32_t cse_var_6 = (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused / 10);\n                int32_t cse_var_5 = (ry_outer * 3);\n                int32_t cse_var_4 = (((((cse_var_6 * 160) + (ff_inner * 40)) + (((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused % 10) / 5) * 20)) + (yy_inner * 5)) + (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused % 5));\n                ((float*)conv2d_nchw_1)[cse_var_4] = (((float*)conv2d_nchw_1)[cse_var_4] + (((float*)pad_temp)[(((((rc_outer * 360) + (rc_inner * 18)) + (yy_inner * 3)) + cse_var_5) + rx_outer)] * ((float*)kernel_1)[((((((cse_var_6 * 3600) + (ff_inner * 900)) + (rc_outer * 180)) + (rc_inner * 9)) + cse_var_5) + rx_outer)]));\n              },\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[12];\n  __shared__ float pad_temp_shared[880];\n  __shared__ float kernel_shared[2160];\n  for (int ff_c_inner_init = 0; ff_c_inner_init < 3; ++ff_c_inner_init) {\n    for (int yy_c_inner_init = 0; yy_c_inner_init < 4; ++yy_c_inner_init) {\n      conv2d_nchw_local[((ff_c_inner_init * 4) + yy_c_inner_init)] = 0.000000e+00f;\n    },\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 5; ++rc_outer_outer) {\n    for (int ry_outer_outer = 0; ry_outer_outer < 3; ++ry_outer_outer) {\n      __syncthreads();\n      for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 6) + (((int)threadIdx.x) / 10)) < 11) {\n          for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_s = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_s < 8; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_s) {\n            pad_temp_shared[(((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 480) + (((int)threadIdx.x) * 8)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_s)] = ((((1 <= (((((int)blockIdx.x) * 4) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 480) + (((int)threadIdx.x) * 8)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_s) % 44) / 11)) + ry_outer_outer)) && ((((((int)blockIdx.x) * 4) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 480) + (((int)threadIdx.x) * 8)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_s) % 44) / 11)) + ry_outer_outer) < 9)) && (1 <= ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 480) + (((int)threadIdx.x) * 8)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_s) % 11))) ? data[(((((((rc_outer_outer * 1600) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 120) + (((int)threadIdx.x) * 2)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_s >> 2)) % 220) / 11) * 80)) + (((int)blockIdx.x) * 40)) + ((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 480) + (((int)threadIdx.x) * 8)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_s) % 44) / 11) * 10)) + (ry_outer_outer * 10)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 480) + (((int)threadIdx.x) * 8)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_s) % 11)) - 11)] : 0.000000e+00f);\n          },\n        },\n      },\n      for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 < 36; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) {\n        kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 60) + ((int)threadIdx.x))] = kernel[(((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 900) + (rc_outer_outer * 180)) + ((((int)threadIdx.x) / 3) * 9)) + (ry_outer_outer * 3)) + (((int)threadIdx.x) % 3))];\n      },\n      __syncthreads();\n      for (int rc_outer_inner = 0; rc_outer_inner < 2; ++rc_outer_inner) {\n        for (int rc_inner = 0; rc_inner < 10; ++rc_inner) {\n          for (int rx_inner = 0; rx_inner < 3; ++rx_inner) {\n            for (int ff_c_inner = 0; ff_c_inner < 3; ++ff_c_inner) {\n              for (int yy_c_inner = 0; yy_c_inner < 4; ++yy_c_inner) {\n                conv2d_nchw_local[((ff_c_inner * 4) + yy_c_inner)] = (conv2d_nchw_local[((ff_c_inner * 4) + yy_c_inner)] + (pad_temp_shared[(((((rc_outer_inner * 440) + (rc_inner * 44)) + (yy_c_inner * 11)) + ((((int)threadIdx.x) % 5) * 2)) + rx_inner)] * kernel_shared[((((((((int)threadIdx.x) / 5) * 180) + (ff_c_inner * 60)) + (rc_outer_inner * 30)) + (rc_inner * 3)) + rx_inner)]));\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 3; ++ff_inner) {\n    for (int yy_inner = 0; yy_inner < 4; ++yy_inner) {\n      conv2d_nchw[((((((((int)threadIdx.x) / 5) * 120) + (ff_inner * 40)) + (((int)blockIdx.x) * 20)) + (yy_inner * 5)) + (((int)threadIdx.x) % 5))] = conv2d_nchw_local[((ff_inner * 4) + yy_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 100, 8, 10), \"float32\"), kernel: T.Buffer((36, 100, 3, 3), \"float32\"), conv2d_nchw: T.Buffer((1, 36, 8, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused in T.parallel(90):\n            pad_temp = T.allocate([1800], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((1800,), data=pad_temp)\n            for i1, i2, i3_s in T.grid(100, 6, 3):\n                cse_var_3: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused % 10 // 5\n                cse_var_2: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused % 5 * 2\n                cse_var_1: T.int32 = cse_var_3 * 4 + i2\n                data_1 = T.Buffer((8000,), data=data.data)\n                pad_temp_1[i1 * 18 + i2 * 3 + i3_s] = T.if_then_else(1 <= cse_var_1 and cse_var_1 < 9 and 1 <= cse_var_2 + i3_s, data_1[i1 * 80 + cse_var_3 * 40 + i2 * 10 + cse_var_2 + i3_s - 11], T.float32(0))\n            conv2d_nchw_1 = T.Buffer((1440,), data=conv2d_nchw.data)\n            for ff_inner_init, yy_inner_init in T.grid(4, 4):\n                conv2d_nchw_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused // 10 * 160 + ff_inner_init * 40 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused % 10 // 5 * 20 + yy_inner_init * 5 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused % 5] = T.float32(0)\n            for rc_outer, ry_outer, rx_outer, rc_inner, ff_inner, yy_inner in T.grid(5, 3, 3, 20, 4, 4):\n                cse_var_6: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused // 10\n                cse_var_5: T.int32 = ry_outer * 3\n                cse_var_4: T.int32 = cse_var_6 * 160 + ff_inner * 40 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused % 10 // 5 * 20 + yy_inner * 5 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused_xx_outer_outer_inner_fused % 5\n                kernel_1 = T.Buffer((32400,), data=kernel.data)\n                conv2d_nchw_1[cse_var_4] = conv2d_nchw_1[cse_var_4] + pad_temp_1[rc_outer * 360 + rc_inner * 18 + yy_inner * 3 + cse_var_5 + rx_outer] * kernel_1[cse_var_6 * 3600 + ff_inner * 900 + rc_outer * 180 + rc_inner * 9 + cse_var_5 + rx_outer]",
        "data": "1_100_8_10",
        "kernel": "36_100_3_3"
    },
    {
        "op_name": "conv2d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv2d_nchw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv2d_nchw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv2d_nchw_1 = (((DLTensor*)conv2d_nchw)[0].data);\n  void* default_function_conv2d_nchw_shape = (((DLTensor*)conv2d_nchw)[0].shape);\n  void* default_function_conv2d_nchw_strides = (((DLTensor*)conv2d_nchw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv2d_nchw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused < 26; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)17280, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    void* conv2d_nchw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)96, 2, 32);\n    if (conv2d_nchw_local == NULL) {\n      return -1;\n    },\n    for (int32_t i1 = 0; i1 < 48; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        for (int32_t i3_s = 0; i3_s < 10; ++i3_s) {\n          ((float*)pad_temp)[(((i1 * 90) + (i2 * 10)) + i3_s)] = (((1 <= i2) && (1 <= i3_s)) ? ((float*)data_1)[((((i1 * 72) + (i2 * 9)) + i3_s) - 10)] : 0.000000e+00f);\n        },\n      },\n    },\n    for (int32_t xx_outer_outer = 0; xx_outer_outer < 2; ++xx_outer_outer) {\n      for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 3; ++ff_c_inner_init) {\n        for (int32_t yy_c_inner_init = 0; yy_c_inner_init < 4; ++yy_c_inner_init) {\n          ((float2*)conv2d_nchw_local)[((ff_c_inner_init * 4) + yy_c_inner_init)] = ((float2)(0.000000e+00f, 0.000000e+00f));\n        },\n      },\n      for (int32_t rc_outer = 0; rc_outer < 16; ++rc_outer) {\n        for (int32_t rc_inner = 0; rc_inner < 3; ++rc_inner) {\n          for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n            for (int32_t rx_inner = 0; rx_inner < 4; ++rx_inner) {\n              for (int32_t ff_c_inner = 0; ff_c_inner < 3; ++ff_c_inner) {\n                for (int32_t yy_c_inner = 0; yy_c_inner < 4; ++yy_c_inner) {\n                  int32_t cse_var_1 = ((ff_c_inner * 4) + yy_c_inner);\n                  int32_t2 v_ = int32_t2((((((((rc_outer * 270) + (rc_inner * 90)) + (yy_c_inner * 20)) + (ry_inner * 10)) + (xx_outer_outer * 4)) + rx_inner))+(2*0), (((((((rc_outer * 270) + (rc_inner * 90)) + (yy_c_inner * 20)) + (ry_inner * 10)) + (xx_outer_outer * 4)) + rx_inner))+(2*1));\n                  ((float2*)conv2d_nchw_local)[cse_var_1] = (((float2*)conv2d_nchw_local)[cse_var_1] + ((float2(((float*)pad_temp)[v_.s0],((float*)pad_temp)[v_.s1])) * ((float2)(((float*)kernel_1)[((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 1728) + (ff_c_inner * 576)) + (rc_outer * 36)) + (rc_inner * 12)) + (ry_inner * 4)) + rx_inner)], ((float*)kernel_1)[((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 1728) + (ff_c_inner * 576)) + (rc_outer * 36)) + (rc_inner * 12)) + (ry_inner * 4)) + rx_inner)]))));\n                },\n              },\n            },\n          },\n        },\n      },\n      for (int32_t ff_inner = 0; ff_inner < 3; ++ff_inner) {\n        for (int32_t yy_inner = 0; yy_inner < 4; ++yy_inner) {\n          *(float2*)(((float*)conv2d_nchw_1) + ((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 48) + (ff_inner * 16)) + (yy_inner * 4)) + (xx_outer_outer * 2))) = ((float2*)conv2d_nchw_local)[((ff_inner * 4) + yy_inner)];\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv2d_nchw_local) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(156) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(156) default_function_kernel(float* __restrict__ conv2d_nchw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv2d_nchw_local[4];\n  __shared__ float pad_temp_shared[540];\n  __shared__ float kernel_shared[2808];\n  for (int xx_c_inner_init = 0; xx_c_inner_init < 2; ++xx_c_inner_init) {\n    conv2d_nchw_local[xx_c_inner_init] = 0.000000e+00f;\n    conv2d_nchw_local[(xx_c_inner_init + 2)] = 0.000000e+00f;\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 8; ++rc_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 13) + (((int)threadIdx.x) / 12)) < 45) {\n        pad_temp_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 156) + ((int)threadIdx.x))] = (((5 <= (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 33) + (((int)threadIdx.x) >> 1)) % 45)) && (1 <= (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 6) + ((int)threadIdx.x)) % 10))) ? data[(((((rc_outer_outer * 432) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 26) + (((int)threadIdx.x) / 6)) / 15) * 72)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 33) + (((int)threadIdx.x) >> 1)) % 45) / 5) * 9)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 6) + ((int)threadIdx.x)) % 10)) - 10)] : 0.000000e+00f);\n      },\n    },\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 < 18; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 156) + ((int)threadIdx.x))] = kernel[(((((((int)blockIdx.x) * 22464) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 13) + (((int)threadIdx.x) / 12)) / 6) * 576)) + (rc_outer_outer * 72)) + ((((((int)threadIdx.x) / 12) + ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) % 6) * 12)) + (((int)threadIdx.x) % 12))];\n    },\n    __syncthreads();\n    for (int rc_outer_inner = 0; rc_outer_inner < 2; ++rc_outer_inner) {\n      for (int rx_outer_inner = 0; rx_outer_inner < 4; ++rx_outer_inner) {\n        for (int rc_inner = 0; rc_inner < 3; ++rc_inner) {\n          for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n            for (int xx_c_inner = 0; xx_c_inner < 2; ++xx_c_inner) {\n              conv2d_nchw_local[xx_c_inner] = (conv2d_nchw_local[xx_c_inner] + (pad_temp_shared[(((((((rc_outer_inner * 270) + (rc_inner * 90)) + (((((int)threadIdx.x) & 3) >> 1) * 20)) + (ry_inner * 10)) + ((((int)threadIdx.x) & 1) * 4)) + (xx_c_inner * 2)) + rx_outer_inner)] * kernel_shared[((((((((int)threadIdx.x) >> 2) * 72) + (rc_outer_inner * 36)) + (rc_inner * 12)) + (ry_inner * 4)) + rx_outer_inner)]));\n              conv2d_nchw_local[(xx_c_inner + 2)] = (conv2d_nchw_local[(xx_c_inner + 2)] + (pad_temp_shared[((((((((rc_outer_inner * 270) + (rc_inner * 90)) + (((((int)threadIdx.x) & 3) >> 1) * 20)) + (ry_inner * 10)) + ((((int)threadIdx.x) & 1) * 4)) + (xx_c_inner * 2)) + rx_outer_inner) + 40)] * kernel_shared[((((((((int)threadIdx.x) >> 2) * 72) + (rc_outer_inner * 36)) + (rc_inner * 12)) + (ry_inner * 4)) + rx_outer_inner)]));\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int xx_inner = 0; xx_inner < 2; ++xx_inner) {\n    conv2d_nchw[((((((int)blockIdx.x) * 624) + ((((int)threadIdx.x) >> 2) * 16)) + ((((int)threadIdx.x) & 3) * 2)) + xx_inner)] = conv2d_nchw_local[xx_inner];\n    conv2d_nchw[(((((((int)blockIdx.x) * 624) + ((((int)threadIdx.x) >> 2) * 16)) + ((((int)threadIdx.x) & 3) * 2)) + xx_inner) + 8)] = conv2d_nchw_local[(xx_inner + 2)];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 48, 8, 9), \"float32\"), kernel: T.Buffer((78, 48, 3, 4), \"float32\"), conv2d_nchw: T.Buffer((1, 78, 4, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused in T.parallel(26):\n            pad_temp = T.allocate([4320], \"float32\", \"global\")\n            conv2d_nchw_local = T.allocate([12], \"float32x2\", \"local\")\n            pad_temp_1 = T.Buffer((4320,), data=pad_temp)\n            for i1, i2, i3_s in T.grid(48, 9, 10):\n                data_1 = T.Buffer((3456,), data=data.data)\n                pad_temp_1[i1 * 90 + i2 * 10 + i3_s] = T.if_then_else(1 <= i2 and 1 <= i3_s, data_1[i1 * 72 + i2 * 9 + i3_s - 10], T.float32(0))\n            for xx_outer_outer in range(2):\n                conv2d_nchw_local_1 = T.Buffer((12,), \"float32x2\", data=conv2d_nchw_local, scope=\"local\")\n                for ff_c_inner_init, yy_c_inner_init in T.grid(3, 4):\n                    conv2d_nchw_local_1[ff_c_inner_init * 4 + yy_c_inner_init] = T.Broadcast(T.float32(0), 2)\n                for rc_outer, rc_inner, ry_inner, rx_inner, ff_c_inner, yy_c_inner in T.grid(16, 3, 3, 4, 3, 4):\n                    cse_var_1: T.int32 = ff_c_inner * 4 + yy_c_inner\n                    kernel_1 = T.Buffer((44928,), data=kernel.data)\n                    conv2d_nchw_local_1[cse_var_1] = conv2d_nchw_local_1[cse_var_1] + pad_temp_1[rc_outer * 270 + rc_inner * 90 + yy_c_inner * 20 + ry_inner * 10 + xx_outer_outer * 4 + rx_inner:rc_outer * 270 + rc_inner * 90 + yy_c_inner * 20 + ry_inner * 10 + xx_outer_outer * 4 + rx_inner + 4:2] * T.Broadcast(kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 1728 + ff_c_inner * 576 + rc_outer * 36 + rc_inner * 12 + ry_inner * 4 + rx_inner], 2)\n                for ff_inner, yy_inner in T.grid(3, 4):\n                    conv2d_nchw_1 = T.Buffer((1248,), data=conv2d_nchw.data)\n                    conv2d_nchw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 48 + ff_inner * 16 + yy_inner * 4 + xx_outer_outer * 2:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 48 + ff_inner * 16 + yy_inner * 4 + xx_outer_outer * 2 + 2] = conv2d_nchw_local_1[ff_inner * 4 + yy_inner]",
        "data": "1_48_8_9",
        "kernel": "78_48_3_4"
    },
    {
        "op_name": "conv1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 14; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)112, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t yy_c_outer_inner_init = 0; yy_c_outer_inner_init < 2; ++yy_c_outer_inner_init) {\n      for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 14; ++ff_c_inner_init) {\n        ((float*)conv1d_ncw_local)[((ff_c_inner_init * 2) + yy_c_outer_inner_init)] = 0.000000e+00f;\n      },\n    },\n    for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n      for (int32_t yy_c_outer_inner = 0; yy_c_outer_inner < 2; ++yy_c_outer_inner) {\n        for (int32_t rc_inner = 0; rc_inner < 28; ++rc_inner) {\n          for (int32_t ff_c_inner = 0; ff_c_inner < 14; ++ff_c_inner) {\n            int32_t cse_var_1 = ((ff_c_inner * 2) + yy_c_outer_inner);\n            ((float*)conv1d_ncw_local)[cse_var_1] = (((float*)conv1d_ncw_local)[cse_var_1] + (((float*)data_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 7) * 168) + (rc_inner * 6)) + (yy_c_outer_inner * 2)) + ry_outer)] * ((float*)kernel_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 7) * 126) + (ff_c_inner * 9)) + (rc_inner * 3)) + ry_outer)]));\n          },\n        },\n      },\n    },\n    for (int32_t ff_inner = 0; ff_inner < 14; ++ff_inner) {\n      int32_t cse_var_2 = (ff_inner * 2);\n      int32_t2 v_ = int32_t2((cse_var_2)+(1*0), (cse_var_2)+(1*1));\n      *(float2*)(((float*)conv1d_ncw_1) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 28) + cse_var_2)) = (float2(((float*)conv1d_ncw_local)[v_.s0],((float*)conv1d_ncw_local)[v_.s1]));\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(98) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(98) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[2];\n  __shared__ float pad_temp_shared[168];\n  __shared__ float kernel_shared[882];\n  for (int ff_c_inner_init = 0; ff_c_inner_init < 2; ++ff_c_inner_init) {\n    conv1d_ncw_local[ff_c_inner_init] = 0.000000e+00f;\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 7) + (((int)threadIdx.x) / 14)) < 12) {\n      pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 98) + ((int)threadIdx.x))] = data[((((((ax0_ax1_fused_ax2_fused_outer_outer * 98) + ((int)threadIdx.x)) / 3) * 6) + (((int)blockIdx.x) * 2)) + (((ax0_ax1_fused_ax2_fused_outer_outer * 2) + ((int)threadIdx.x)) % 3))];\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 9; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 98) + ((int)threadIdx.x))] = kernel[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 98) + ((int)threadIdx.x))];\n  },\n  __syncthreads();\n  for (int rc_outer_inner = 0; rc_outer_inner < 28; ++rc_outer_inner) {\n    for (int ry_outer_inner = 0; ry_outer_inner < 3; ++ry_outer_inner) {\n      for (int ff_c_inner = 0; ff_c_inner < 2; ++ff_c_inner) {\n        conv1d_ncw_local[ff_c_inner] = (conv1d_ncw_local[ff_c_inner] + (pad_temp_shared[((((((int)threadIdx.x) / 49) * 84) + (rc_outer_inner * 3)) + ry_outer_inner)] * kernel_shared[(((((((int)threadIdx.x) % 49) * 18) + (ff_c_inner * 9)) + (rc_outer_inner * 3)) + ry_outer_inner)]));\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 2; ++ff_inner) {\n    conv1d_ncw[(((((int)threadIdx.x) * 4) + (ff_inner * 2)) + ((int)blockIdx.x))] = conv1d_ncw_local[ff_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 28, 6), \"float32\"), kernel: T.Buffer((98, 3, 3), \"float32\"), conv1d_ncw: T.Buffer((2, 98, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(14):\n            conv1d_ncw_local = T.allocate([28], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((28,), data=conv1d_ncw_local, scope=\"local\")\n            for yy_c_outer_inner_init, ff_c_inner_init in T.grid(2, 14):\n                conv1d_ncw_local_1[ff_c_inner_init * 2 + yy_c_outer_inner_init] = T.float32(0)\n            for ry_outer, yy_c_outer_inner, rc_inner, ff_c_inner in T.grid(3, 2, 28, 14):\n                cse_var_1: T.int32 = ff_c_inner * 2 + yy_c_outer_inner\n                data_1 = T.Buffer((336,), data=data.data)\n                kernel_1 = T.Buffer((882,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 7 * 168 + rc_inner * 6 + yy_c_outer_inner * 2 + ry_outer] * kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 7 * 126 + ff_c_inner * 9 + rc_inner * 3 + ry_outer]\n            for ff_inner in range(14):\n                cse_var_2: T.int32 = ff_inner * 2\n                conv1d_ncw_1 = T.Buffer((392,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 28 + cse_var_2:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 28 + cse_var_2 + 2] = conv1d_ncw_local_1[cse_var_2:cse_var_2 + 2]",
        "data": "2_28_6",
        "kernel": "98_3_3"
    },
    {
        "op_name": "conv1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 40; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)96, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t nn_c_inner_init = 0; nn_c_inner_init < 2; ++nn_c_inner_init) {\n      for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 12; ++ff_c_inner_init) {\n        ((float*)conv1d_ncw_local)[((nn_c_inner_init * 12) + ff_c_inner_init)] = 0.000000e+00f;\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 98; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n        for (int32_t nn_c_inner = 0; nn_c_inner < 2; ++nn_c_inner) {\n          for (int32_t ff_c_inner = 0; ff_c_inner < 12; ++ff_c_inner) {\n            int32_t cse_var_2 = (rc_outer * 3);\n            int32_t cse_var_1 = ((nn_c_inner * 12) + ff_c_inner);\n            ((float*)conv1d_ncw_local)[cse_var_1] = (((float*)conv1d_ncw_local)[cse_var_1] + (((float*)data_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused >> 3) * 588) + (nn_c_inner * 294)) + cse_var_2) + ry_outer)] * ((float*)kernel_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 7) * 108) + (ff_c_inner * 9)) + cse_var_2) + ry_outer)]));\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 2; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 12; ++ff_inner) {\n        ((float*)conv1d_ncw_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused >> 3) * 192) + (nn_inner * 96)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 7) * 12)) + ff_inner)] = ((float*)conv1d_ncw_local)[((nn_inner * 12) + ff_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(80) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(80) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[6];\n  __shared__ float pad_temp_shared[20];\n  __shared__ float kernel_shared[96];\n  for (int ff_c_inner_init = 0; ff_c_inner_init < 3; ++ff_c_inner_init) {\n    conv1d_ncw_local[ff_c_inner_init] = 0.000000e+00f;\n    conv1d_ncw_local[(ff_c_inner_init + 3)] = 0.000000e+00f;\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 49; ++rc_outer_outer) {\n    for (int ry_outer_outer = 0; ry_outer_outer < 3; ++ry_outer_outer) {\n      __syncthreads();\n      if (((int)threadIdx.x) < 20) {\n        pad_temp_shared[((int)threadIdx.x)] = data[(((((((int)threadIdx.x) >> 1) * 294) + (rc_outer_outer * 6)) + ((((int)threadIdx.x) & 1) * 3)) + ry_outer_outer)];\n      },\n      for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n        if (((ax0_ax1_fused_ax2_fused_outer_outer * 5) + (((int)threadIdx.x) >> 4)) < 6) {\n          if (((rc_outer_outer * 2) + (((int)threadIdx.x) & 1)) < 3) {\n            kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 80) + ((int)threadIdx.x))] = kernel[((((((((int)blockIdx.x) * 432) + (ax0_ax1_fused_ax2_fused_outer_outer * 360)) + ((((int)threadIdx.x) >> 1) * 9)) + (rc_outer_outer * 6)) + ((((int)threadIdx.x) & 1) * 3)) + ry_outer_outer)];\n          },\n        },\n      },\n      __syncthreads();\n      for (int rc_inner = 0; rc_inner < 2; ++rc_inner) {\n        for (int ff_c_inner = 0; ff_c_inner < 3; ++ff_c_inner) {\n          conv1d_ncw_local[ff_c_inner] = (conv1d_ncw_local[ff_c_inner] + (pad_temp_shared[(((((int)threadIdx.x) >> 3) * 2) + rc_inner)] * kernel_shared[((((((int)threadIdx.x) & 7) * 6) + (ff_c_inner * 2)) + rc_inner)]));\n          conv1d_ncw_local[(ff_c_inner + 3)] = (conv1d_ncw_local[(ff_c_inner + 3)] + (pad_temp_shared[(((((int)threadIdx.x) >> 3) * 2) + rc_inner)] * kernel_shared[(((((((int)threadIdx.x) & 7) * 6) + (ff_c_inner * 2)) + rc_inner) + 48)]));\n        },\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 3; ++ff_inner) {\n    conv1d_ncw[(((((((int)threadIdx.x) >> 3) * 96) + (((int)blockIdx.x) * 48)) + ((((int)threadIdx.x) & 7) * 3)) + ff_inner)] = conv1d_ncw_local[ff_inner];\n    conv1d_ncw[((((((((int)threadIdx.x) >> 3) * 96) + (((int)blockIdx.x) * 48)) + ((((int)threadIdx.x) & 7) * 3)) + ff_inner) + 24)] = conv1d_ncw_local[(ff_inner + 3)];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 98, 3), \"float32\"), kernel: T.Buffer((96, 3, 3), \"float32\"), conv1d_ncw: T.Buffer((10, 96, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(40):\n            conv1d_ncw_local = T.allocate([24], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((24,), data=conv1d_ncw_local, scope=\"local\")\n            for nn_c_inner_init, ff_c_inner_init in T.grid(2, 12):\n                conv1d_ncw_local_1[nn_c_inner_init * 12 + ff_c_inner_init] = T.float32(0)\n            for rc_outer, ry_outer, nn_c_inner, ff_c_inner in T.grid(98, 3, 2, 12):\n                cse_var_2: T.int32 = rc_outer * 3\n                cse_var_1: T.int32 = nn_c_inner * 12 + ff_c_inner\n                data_1 = T.Buffer((2940,), data=data.data)\n                kernel_1 = T.Buffer((864,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 8 * 588 + nn_c_inner * 294 + cse_var_2 + ry_outer] * kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 8 * 108 + ff_c_inner * 9 + cse_var_2 + ry_outer]\n            for nn_inner, ff_inner in T.grid(2, 12):\n                conv1d_ncw_1 = T.Buffer((960,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 8 * 192 + nn_inner * 96 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 8 * 12 + ff_inner] = conv1d_ncw_local_1[nn_inner * 12 + ff_inner]",
        "data": "10_98_3",
        "kernel": "96_3_3"
    },
    {
        "op_name": "conv1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused < 26; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused) {\n    for (int32_t nn_outer_inner_init = 0; nn_outer_inner_init < 4; ++nn_outer_inner_init) {\n      *(float4*)(((float*)conv1d_ncw_1) + ((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused / 13) * 208) + (nn_outer_inner_init * 52)) + ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 13) * 4))) = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n    },\n    for (int32_t rc_outer = 0; rc_outer < 3; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n        for (int32_t nn_outer_inner = 0; nn_outer_inner < 4; ++nn_outer_inner) {\n          for (int32_t rc_inner = 0; rc_inner < 14; ++rc_inner) {\n            int32_t cse_var_3 = (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused / 13);\n            int32_t cse_var_2 = (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 13);\n            int32_t cse_var_1 = (((cse_var_3 * 208) + (nn_outer_inner * 52)) + (cse_var_2 * 4));\n            int32_t4 v_ = int32_t4((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3));\n            int32_t4 v__1 = int32_t4(((((((cse_var_3 * 1680) + (nn_outer_inner * 420)) + (rc_outer * 140)) + (rc_inner * 10)) + ry_outer))+(2*0), ((((((cse_var_3 * 1680) + (nn_outer_inner * 420)) + (rc_outer * 140)) + (rc_inner * 10)) + ry_outer))+(2*1), ((((((cse_var_3 * 1680) + (nn_outer_inner * 420)) + (rc_outer * 140)) + (rc_inner * 10)) + ry_outer))+(2*2), ((((((cse_var_3 * 1680) + (nn_outer_inner * 420)) + (rc_outer * 140)) + (rc_inner * 10)) + ry_outer))+(2*3));\n            *(float4*)(((float*)conv1d_ncw_1) + cse_var_1) = ((float4(((float*)conv1d_ncw_1)[v_.s0],((float*)conv1d_ncw_1)[v_.s1],((float*)conv1d_ncw_1)[v_.s2],((float*)conv1d_ncw_1)[v_.s3])) + ((float4(((float*)data_1)[v__1.s0],((float*)data_1)[v__1.s1],((float*)data_1)[v__1.s2],((float*)data_1)[v__1.s3])) * ((float4)(((float*)kernel_1)[((((rc_outer * 42) + (cse_var_2 * 9)) + (rc_inner * 3)) + ry_outer)], ((float*)kernel_1)[((((rc_outer * 42) + (cse_var_2 * 9)) + (rc_inner * 3)) + ry_outer)], ((float*)kernel_1)[((((rc_outer * 42) + (cse_var_2 * 9)) + (rc_inner * 3)) + ry_outer)], ((float*)kernel_1)[((((rc_outer * 42) + (cse_var_2 * 9)) + (rc_inner * 3)) + ry_outer)]))));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(52) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(52) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[2];\n  __shared__ float pad_temp_shared[840];\n  __shared__ float kernel_shared[117];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  conv1d_ncw_local[1] = 0.000000e+00f;\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 17; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 13) + (((int)threadIdx.x) >> 2)) < 210) {\n      pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 52) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) >> 1) * 1680) + ((((ax0_ax1_fused_ax2_fused_outer_outer * 52) + ((int)threadIdx.x)) / 5) * 10)) + ((((int)blockIdx.x) & 1) * 4)) + (((ax0_ax1_fused_ax2_fused_outer_outer * 2) + ((int)threadIdx.x)) % 5))];\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 3; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer_1 * 4) + (((int)threadIdx.x) / 13)) < 9) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 52) + ((int)threadIdx.x))] = kernel[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 52) + ((int)threadIdx.x))];\n    },\n  },\n  __syncthreads();\n  for (int rc_outer_inner = 0; rc_outer_inner < 14; ++rc_outer_inner) {\n    for (int ry_outer_inner = 0; ry_outer_inner < 3; ++ry_outer_inner) {\n      for (int rc_inner = 0; rc_inner < 3; ++rc_inner) {\n        conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[((((((((int)threadIdx.x) / 26) * 210) + (rc_outer_inner * 15)) + (rc_inner * 5)) + ((((int)threadIdx.x) & 1) * 2)) + ry_outer_inner)] * kernel_shared[((((((((int)threadIdx.x) % 26) >> 1) * 9) + (rc_outer_inner * 9)) + (rc_inner * 3)) + ry_outer_inner)]));\n        conv1d_ncw_local[1] = (conv1d_ncw_local[1] + (pad_temp_shared[(((((((((int)threadIdx.x) / 26) * 210) + (rc_outer_inner * 15)) + (rc_inner * 5)) + ((((int)threadIdx.x) & 1) * 2)) + ry_outer_inner) + 420)] * kernel_shared[((((((((int)threadIdx.x) % 26) >> 1) * 9) + (rc_outer_inner * 9)) + (rc_inner * 3)) + ry_outer_inner)]));\n      },\n    },\n  },\n  conv1d_ncw[(((((((int)blockIdx.x) >> 1) * 208) + ((((int)threadIdx.x) >> 1) * 4)) + ((((int)blockIdx.x) & 1) * 2)) + (((int)threadIdx.x) & 1))] = conv1d_ncw_local[0];\n  conv1d_ncw[((((((((int)blockIdx.x) >> 1) * 208) + ((((int)threadIdx.x) >> 1) * 4)) + ((((int)blockIdx.x) & 1) * 2)) + (((int)threadIdx.x) & 1)) + 104)] = conv1d_ncw_local[1];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 42, 10), \"float32\"), kernel: T.Buffer((13, 3, 3), \"float32\"), conv1d_ncw: T.Buffer((8, 13, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused in T.parallel(26):\n            conv1d_ncw_1 = T.Buffer((416,), data=conv1d_ncw.data)\n            for nn_outer_inner_init in range(4):\n                conv1d_ncw_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 13 * 208 + nn_outer_inner_init * 52 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 13 * 4:nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 13 * 208 + nn_outer_inner_init * 52 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 13 * 4 + 4] = T.Broadcast(T.float32(0), 4)\n            for rc_outer, ry_outer, nn_outer_inner, rc_inner in T.grid(3, 3, 4, 14):\n                cse_var_3: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 13\n                cse_var_2: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 13\n                cse_var_1: T.int32 = cse_var_3 * 208 + nn_outer_inner * 52 + cse_var_2 * 4\n                data_1 = T.Buffer((3360,), data=data.data)\n                kernel_1 = T.Buffer((117,), data=kernel.data)\n                conv1d_ncw_1[cse_var_1:cse_var_1 + 4] = conv1d_ncw_1[cse_var_1:cse_var_1 + 4] + data_1[cse_var_3 * 1680 + nn_outer_inner * 420 + rc_outer * 140 + rc_inner * 10 + ry_outer:cse_var_3 * 1680 + nn_outer_inner * 420 + rc_outer * 140 + rc_inner * 10 + ry_outer + 8:2] * T.Broadcast(kernel_1[rc_outer * 42 + cse_var_2 * 9 + rc_inner * 3 + ry_outer], 4)",
        "data": "8_42_10",
        "kernel": "13_3_3"
    },
    {
        "op_name": "conv1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 94; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)96, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t yy_c_outer_inner_init = 0; yy_c_outer_inner_init < 3; ++yy_c_outer_inner_init) {\n      for (int32_t nn_c_inner_init = 0; nn_c_inner_init < 4; ++nn_c_inner_init) {\n        for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 2; ++ff_c_inner_init) {\n          ((float*)conv1d_ncw_local)[(((nn_c_inner_init * 6) + (ff_c_inner_init * 3)) + yy_c_outer_inner_init)] = 0.000000e+00f;\n        },\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 67; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 2; ++ry_outer) {\n        for (int32_t yy_c_outer_inner = 0; yy_c_outer_inner < 3; ++yy_c_outer_inner) {\n          for (int32_t ry_inner = 0; ry_inner < 2; ++ry_inner) {\n            for (int32_t nn_c_inner = 0; nn_c_inner < 4; ++nn_c_inner) {\n              for (int32_t ff_c_inner = 0; ff_c_inner < 2; ++ff_c_inner) {\n                int32_t cse_var_2 = (ry_outer * 2);\n                int32_t cse_var_1 = (((nn_c_inner * 6) + (ff_c_inner * 3)) + yy_c_outer_inner);\n                ((float*)conv1d_ncw_local)[cse_var_1] = (((float*)conv1d_ncw_local)[cse_var_1] + (((float*)data_1)[(((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 47) * 2412) + (nn_c_inner * 603)) + (rc_outer * 9)) + (yy_c_outer_inner * 2)) + cse_var_2) + ry_inner)] * ((float*)kernel_1)[((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 47) * 32) + (ff_c_inner * 16)) + (rc_outer * 4)) + cse_var_2) + ry_inner)]));\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 4; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 2; ++ff_inner) {\n        int32_t cse_var_3 = (ff_inner * 3);\n        int32_t3 v_ = int32_t3((((nn_inner * 6) + cse_var_3))+(1*0), (((nn_inner * 6) + cse_var_3))+(1*1), (((nn_inner * 6) + cse_var_3))+(1*2));\n        *(float3*)(((float*)conv1d_ncw_1) + (((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 47) * 1128) + (nn_inner * 282)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 47) * 6)) + cse_var_3)) = (float3(((float*)conv1d_ncw_local)[v_.s0],((float*)conv1d_ncw_local)[v_.s1],((float*)conv1d_ncw_local)[v_.s2]));\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(47) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(47) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[4];\n  __shared__ float pad_temp_shared[134];\n  __shared__ float kernel_shared[376];\n  for (int ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 2; ++ff_c_outer_inner_init) {\n    for (int nn_c_inner_init = 0; nn_c_inner_init < 2; ++nn_c_inner_init) {\n      conv1d_ncw_local[((nn_c_inner_init * 2) + ff_c_outer_inner_init)] = 0.000000e+00f;\n    },\n  },\n  for (int ry_outer_outer = 0; ry_outer_outer < 4; ++ry_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 3; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer * 47) + ((int)threadIdx.x)) < 134) {\n        pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 47) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) / 3) * 1206) + (ax0_ax1_fused_ax2_fused_outer_outer * 423)) + (((int)threadIdx.x) * 9)) + ((((int)blockIdx.x) % 3) * 2)) + ry_outer_outer)];\n      },\n    },\n    for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 8; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 47) + ((int)threadIdx.x))] = kernel[(((ax0_ax1_fused_ax2_fused_outer_outer_1 * 188) + (((int)threadIdx.x) * 4)) + ry_outer_outer)];\n    },\n    __syncthreads();\n    for (int rc_outer_inner = 0; rc_outer_inner < 67; ++rc_outer_inner) {\n      for (int ff_c_outer_inner = 0; ff_c_outer_inner < 2; ++ff_c_outer_inner) {\n        for (int nn_c_inner = 0; nn_c_inner < 2; ++nn_c_inner) {\n          conv1d_ncw_local[((nn_c_inner * 2) + ff_c_outer_inner)] = (conv1d_ncw_local[((nn_c_inner * 2) + ff_c_outer_inner)] + (pad_temp_shared[((nn_c_inner * 67) + rc_outer_inner)] * kernel_shared[(((((int)threadIdx.x) * 8) + (ff_c_outer_inner * 4)) + rc_outer_inner)]));\n        },\n      },\n    },\n  },\n  for (int nn_inner = 0; nn_inner < 2; ++nn_inner) {\n    for (int ff_inner = 0; ff_inner < 2; ++ff_inner) {\n      conv1d_ncw[((((((((int)blockIdx.x) / 3) * 564) + (nn_inner * 282)) + (((int)threadIdx.x) * 6)) + (ff_inner * 3)) + (((int)blockIdx.x) % 3))] = conv1d_ncw_local[((nn_inner * 2) + ff_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 67, 9), \"float32\"), kernel: T.Buffer((94, 4, 4), \"float32\"), conv1d_ncw: T.Buffer((8, 94, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(94):\n            conv1d_ncw_local = T.allocate([24], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((24,), data=conv1d_ncw_local, scope=\"local\")\n            for yy_c_outer_inner_init, nn_c_inner_init, ff_c_inner_init in T.grid(3, 4, 2):\n                conv1d_ncw_local_1[nn_c_inner_init * 6 + ff_c_inner_init * 3 + yy_c_outer_inner_init] = T.float32(0)\n            for rc_outer, ry_outer, yy_c_outer_inner, ry_inner, nn_c_inner, ff_c_inner in T.grid(67, 2, 3, 2, 4, 2):\n                cse_var_2: T.int32 = ry_outer * 2\n                cse_var_1: T.int32 = nn_c_inner * 6 + ff_c_inner * 3 + yy_c_outer_inner\n                data_1 = T.Buffer((4824,), data=data.data)\n                kernel_1 = T.Buffer((1504,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 47 * 2412 + nn_c_inner * 603 + rc_outer * 9 + yy_c_outer_inner * 2 + cse_var_2 + ry_inner] * kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 47 * 32 + ff_c_inner * 16 + rc_outer * 4 + cse_var_2 + ry_inner]\n            for nn_inner, ff_inner in T.grid(4, 2):\n                cse_var_3: T.int32 = ff_inner * 3\n                conv1d_ncw_1 = T.Buffer((2256,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 47 * 1128 + nn_inner * 282 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 47 * 6 + cse_var_3:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 47 * 1128 + nn_inner * 282 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 47 * 6 + cse_var_3 + 3] = conv1d_ncw_local_1[nn_inner * 6 + cse_var_3:nn_inner * 6 + cse_var_3 + 3]",
        "data": "8_67_9",
        "kernel": "94_4_4"
    },
    {
        "op_name": "conv1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 154; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)8, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t yy_c_outer_inner_init = 0; yy_c_outer_inner_init < 2; ++yy_c_outer_inner_init) {\n      ((float*)conv1d_ncw_local)[yy_c_outer_inner_init] = 0.000000e+00f;\n    },\n    for (int32_t rc_outer = 0; rc_outer < 5; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 4; ++ry_outer) {\n        for (int32_t yy_c_outer_inner = 0; yy_c_outer_inner < 2; ++yy_c_outer_inner) {\n          for (int32_t rc_inner = 0; rc_inner < 19; ++rc_inner) {\n            ((float*)conv1d_ncw_local)[yy_c_outer_inner] = (((float*)conv1d_ncw_local)[yy_c_outer_inner] + (((float*)data_1)[((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 77) / 11) * 950) + (rc_outer * 190)) + (rc_inner * 10)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 77) * 4)) + (yy_c_outer_inner * 2)) + ry_outer)] * ((float*)kernel_1)[((((rc_outer * 76) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 11) * 16)) + (rc_inner * 4)) + ry_outer)]));\n          },\n        },\n      },\n    },\n    *(float2*)(((float*)conv1d_ncw_1) + (((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 77) * 4) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 77) * 2))) = *(float2*)(((float*)conv1d_ncw_local) + 0);\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int rc_ry_fused_outer = 0; rc_ry_fused_outer < 24; ++rc_ry_fused_outer) {\n    if (((rc_ry_fused_outer * 4) + (((int)threadIdx.x) >> 2)) < 95) {\n      normal_reduce_temp0[0] = (normal_reduce_temp0[0] + (data[((((((((int)blockIdx.x) / 44) * 950) + (rc_ry_fused_outer * 40)) + ((((int)threadIdx.x) >> 2) * 10)) + ((((int)blockIdx.x) & 3) * 2)) + (((int)threadIdx.x) & 3))] * kernel[(((((((int)blockIdx.x) % 44) >> 2) * 16) + (rc_ry_fused_outer * 16)) + ((int)threadIdx.x))]));\n    },\n  },\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    conv1d_ncw[((int)blockIdx.x)] = red_buf0[0];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 95, 10), \"float32\"), kernel: T.Buffer((11, 4, 4), \"float32\"), conv1d_ncw: T.Buffer((7, 11, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(154):\n            conv1d_ncw_local = T.allocate([2], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((2,), data=conv1d_ncw_local, scope=\"local\", align=8)\n            for yy_c_outer_inner_init in range(2):\n                conv1d_ncw_local_1[yy_c_outer_inner_init] = T.float32(0)\n            for rc_outer, ry_outer, yy_c_outer_inner, rc_inner in T.grid(5, 4, 2, 19):\n                data_1 = T.Buffer((6650,), data=data.data)\n                kernel_1 = T.Buffer((176,), data=kernel.data)\n                conv1d_ncw_local_1[yy_c_outer_inner] = conv1d_ncw_local_1[yy_c_outer_inner] + data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 77 // 11 * 950 + rc_outer * 190 + rc_inner * 10 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 77 * 4 + yy_c_outer_inner * 2 + ry_outer] * kernel_1[rc_outer * 76 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 11 * 16 + rc_inner * 4 + ry_outer]\n            conv1d_ncw_1 = T.Buffer((308,), data=conv1d_ncw.data)\n            conv1d_ncw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 77 * 4 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 77 * 2:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 77 * 4 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 77 * 2 + 2] = conv1d_ncw_local_1[0:2]",
        "data": "7_95_10",
        "kernel": "11_4_4"
    },
    {
        "op_name": "conv1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 81; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)40, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 5; ++nn_c_outer_inner_init) {\n      for (int32_t nn_c_inner_init = 0; nn_c_inner_init < 2; ++nn_c_inner_init) {\n        ((float*)conv1d_ncw_local)[((nn_c_outer_inner_init * 2) + nn_c_inner_init)] = 0.000000e+00f;\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 50; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n        for (int32_t nn_c_outer_inner = 0; nn_c_outer_inner < 5; ++nn_c_outer_inner) {\n          for (int32_t nn_c_inner = 0; nn_c_inner < 2; ++nn_c_inner) {\n            int32_t cse_var_1 = ((nn_c_outer_inner * 2) + nn_c_inner);\n            ((float*)conv1d_ncw_local)[cse_var_1] = (((float*)conv1d_ncw_local)[cse_var_1] + (((float*)data_1)[(((((nn_c_outer_inner * 700) + (nn_c_inner * 350)) + (rc_outer * 7)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 3) * 2)) + ry_outer)] * ((float*)kernel_1)[((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 3) * 9) + (rc_outer * 3)) + ry_outer)]));\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 10; ++nn_inner) {\n      ((float*)conv1d_ncw_1)[((nn_inner * 81) + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused)] = ((float*)conv1d_ncw_local)[nn_inner];\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(135) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(135) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[3];\n  __shared__ float pad_temp_shared[175];\n  __shared__ float kernel_shared[405];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  conv1d_ncw_local[1] = 0.000000e+00f;\n  conv1d_ncw_local[2] = 0.000000e+00f;\n  for (int rc_outer_outer = 0; rc_outer_outer < 10; ++rc_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer * 27) + (((int)threadIdx.x) / 5)) < 35) {\n        pad_temp_shared[((((((ax0_ax1_fused_ax2_fused_outer_outer * 27) + (((int)threadIdx.x) / 5)) / 7) * 35) + (((((ax0_ax1_fused_ax2_fused_outer_outer * 30) + ((int)threadIdx.x)) % 35) / 7) * 7)) + (((ax0_ax1_fused_ax2_fused_outer_outer * 2) + ((int)threadIdx.x)) % 7))] = data[(((((((int)blockIdx.x) * 1750) + ((((ax0_ax1_fused_ax2_fused_outer_outer * 27) + (((int)threadIdx.x) / 5)) / 7) * 350)) + (rc_outer_outer * 35)) + (((((ax0_ax1_fused_ax2_fused_outer_outer * 30) + ((int)threadIdx.x)) % 35) / 7) * 7)) + (((ax0_ax1_fused_ax2_fused_outer_outer * 2) + ((int)threadIdx.x)) % 7))];\n      },\n    },\n    if (((rc_outer_outer * 5) + (((int)threadIdx.x) % 5)) < 3) {\n      *(float3*)(kernel_shared + (((int)threadIdx.x) * 3)) = *(float3*)(kernel + (((rc_outer_outer * 15) + ((((int)threadIdx.x) / 5) * 9)) + ((((int)threadIdx.x) % 5) * 3)));\n    },\n    __syncthreads();\n    for (int rc_inner = 0; rc_inner < 5; ++rc_inner) {\n      for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n        conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[(((((((int)threadIdx.x) / 27) * 35) + (rc_inner * 7)) + ((((int)threadIdx.x) % 3) * 2)) + ry_inner)] * kernel_shared[(((((((int)threadIdx.x) % 27) / 3) * 15) + (rc_inner * 3)) + ry_inner)]));\n        conv1d_ncw_local[1] = (conv1d_ncw_local[1] + (pad_temp_shared[(((((((int)threadIdx.x) / 27) * 35) + (rc_inner * 7)) + ((((int)threadIdx.x) % 3) * 2)) + ry_inner)] * kernel_shared[((((((((int)threadIdx.x) % 27) / 3) * 15) + (rc_inner * 3)) + ry_inner) + 135)]));\n        conv1d_ncw_local[2] = (conv1d_ncw_local[2] + (pad_temp_shared[(((((((int)threadIdx.x) / 27) * 35) + (rc_inner * 7)) + ((((int)threadIdx.x) % 3) * 2)) + ry_inner)] * kernel_shared[((((((((int)threadIdx.x) % 27) / 3) * 15) + (rc_inner * 3)) + ry_inner) + 270)]));\n      },\n    },\n  },\n  conv1d_ncw[(((((int)blockIdx.x) * 405) + ((((int)threadIdx.x) / 27) * 81)) + (((int)threadIdx.x) % 27))] = conv1d_ncw_local[0];\n  conv1d_ncw[((((((int)blockIdx.x) * 405) + ((((int)threadIdx.x) / 27) * 81)) + (((int)threadIdx.x) % 27)) + 27)] = conv1d_ncw_local[1];\n  conv1d_ncw[((((((int)blockIdx.x) * 405) + ((((int)threadIdx.x) / 27) * 81)) + (((int)threadIdx.x) % 27)) + 54)] = conv1d_ncw_local[2];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 50, 7), \"float32\"), kernel: T.Buffer((27, 3, 3), \"float32\"), conv1d_ncw: T.Buffer((10, 27, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(81):\n            conv1d_ncw_local = T.allocate([10], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((10,), data=conv1d_ncw_local, scope=\"local\", align=32)\n            for nn_c_outer_inner_init, nn_c_inner_init in T.grid(5, 2):\n                conv1d_ncw_local_1[nn_c_outer_inner_init * 2 + nn_c_inner_init] = T.float32(0)\n            for rc_outer, ry_outer, nn_c_outer_inner, nn_c_inner in T.grid(50, 3, 5, 2):\n                cse_var_1: T.int32 = nn_c_outer_inner * 2 + nn_c_inner\n                data_1 = T.Buffer((3500,), data=data.data)\n                kernel_1 = T.Buffer((243,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_c_outer_inner * 700 + nn_c_inner * 350 + rc_outer * 7 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 3 * 2 + ry_outer] * kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 3 * 9 + rc_outer * 3 + ry_outer]\n            for nn_inner in range(10):\n                conv1d_ncw_1 = T.Buffer((810,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_inner * 81 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused] = conv1d_ncw_local_1[nn_inner]",
        "data": "10_50_7",
        "kernel": "27_3_3"
    },
    {
        "op_name": "conv1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 290; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)12, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 3; ++ff_c_inner_init) {\n      ((float*)conv1d_ncw_local)[ff_c_inner_init] = 0.000000e+00f;\n    },\n    for (int32_t rc_outer = 0; rc_outer < 11; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n        for (int32_t ff_c_inner = 0; ff_c_inner < 3; ++ff_c_inner) {\n          ((float*)conv1d_ncw_local)[ff_c_inner] = (((float*)conv1d_ncw_local)[ff_c_inner] + (((float*)data_1)[((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 29) * 44) + (rc_outer * 4)) + ry_outer)] * ((float*)kernel_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 29) * 27) + (ff_c_inner * 9)) + (rc_outer * 3)) + ry_outer)]));\n        },\n      },\n    },\n    for (int32_t ff_inner = 0; ff_inner < 3; ++ff_inner) {\n      ((float*)conv1d_ncw_1)[((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 3) + ff_inner)] = ((float*)conv1d_ncw_local)[ff_inner];\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(87) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(87) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[2];\n  __shared__ float pad_temp_shared[66];\n  __shared__ float kernel_shared[783];\n  for (int nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 2; ++nn_c_outer_inner_init) {\n    conv1d_ncw_local[nn_c_outer_inner_init] = 0.000000e+00f;\n  },\n  if (((int)threadIdx.x) < 66) {\n    pad_temp_shared[((int)threadIdx.x)] = data[(((((int)blockIdx.x) * 88) + ((((int)threadIdx.x) / 3) * 4)) + (((int)threadIdx.x) % 3))];\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 9; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 87) + ((int)threadIdx.x))] = kernel[((ax0_ax1_fused_ax2_fused_outer_outer * 87) + ((int)threadIdx.x))];\n  },\n  __syncthreads();\n  for (int nn_c_outer_inner = 0; nn_c_outer_inner < 2; ++nn_c_outer_inner) {\n    for (int rc_inner = 0; rc_inner < 11; ++rc_inner) {\n      for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n        conv1d_ncw_local[nn_c_outer_inner] = (conv1d_ncw_local[nn_c_outer_inner] + (pad_temp_shared[(((nn_c_outer_inner * 33) + (rc_inner * 3)) + ry_inner)] * kernel_shared[(((((int)threadIdx.x) * 9) + (rc_inner * 3)) + ry_inner)]));\n      },\n    },\n  },\n  for (int nn_inner = 0; nn_inner < 2; ++nn_inner) {\n    conv1d_ncw[(((((int)blockIdx.x) * 174) + (nn_inner * 87)) + ((int)threadIdx.x))] = conv1d_ncw_local[nn_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 11, 4), \"float32\"), kernel: T.Buffer((87, 3, 3), \"float32\"), conv1d_ncw: T.Buffer((10, 87, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(290):\n            conv1d_ncw_local = T.allocate([3], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((3,), data=conv1d_ncw_local, scope=\"local\", align=8)\n            for ff_c_inner_init in range(3):\n                conv1d_ncw_local_1[ff_c_inner_init] = T.float32(0)\n            for rc_outer, ry_outer, ff_c_inner in T.grid(11, 3, 3):\n                data_1 = T.Buffer((440,), data=data.data)\n                kernel_1 = T.Buffer((783,), data=kernel.data)\n                conv1d_ncw_local_1[ff_c_inner] = conv1d_ncw_local_1[ff_c_inner] + data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 29 * 44 + rc_outer * 4 + ry_outer] * kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 29 * 27 + ff_c_inner * 9 + rc_outer * 3 + ry_outer]\n            for ff_inner in range(3):\n                conv1d_ncw_1 = T.Buffer((870,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 3 + ff_inner] = conv1d_ncw_local_1[ff_inner]",
        "data": "10_11_4",
        "kernel": "87_3_3"
    },
    {
        "op_name": "conv1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 111; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)20, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 5; ++nn_c_outer_inner_init) {\n      ((float*)conv1d_ncw_local)[nn_c_outer_inner_init] = 0.000000e+00f;\n    },\n    for (int32_t rc_outer = 0; rc_outer < 6; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 4; ++ry_outer) {\n        for (int32_t nn_c_outer_inner = 0; nn_c_outer_inner < 5; ++nn_c_outer_inner) {\n          for (int32_t rc_inner = 0; rc_inner < 11; ++rc_inner) {\n            ((float*)conv1d_ncw_local)[nn_c_outer_inner] = (((float*)conv1d_ncw_local)[nn_c_outer_inner] + (((float*)data_1)[(((((nn_c_outer_inner * 594) + (rc_outer * 99)) + (rc_inner * 9)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 3) * 2)) + ry_outer)] * ((float*)kernel_1)[((((rc_outer * 44) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 3) * 16)) + (rc_inner * 4)) + ry_outer)]));\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 5; ++nn_inner) {\n      ((float*)conv1d_ncw_1)[((nn_inner * 111) + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused)] = ((float*)conv1d_ncw_local)[nn_inner];\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(185) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(185) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[1];\n  __shared__ float pad_temp_shared[1320];\n  __shared__ float kernel_shared[592];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 8; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 37) + (((int)threadIdx.x) / 5)) < 264) {\n      pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 185) + ((int)threadIdx.x))] = data[((((((ax0_ax1_fused_ax2_fused_outer_outer * 185) + ((int)threadIdx.x)) >> 2) * 9) + (((int)blockIdx.x) * 2)) + ((ax0_ax1_fused_ax2_fused_outer_outer + ((int)threadIdx.x)) & 3))];\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 4; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer_1 * 5) + (((int)threadIdx.x) / 37)) < 16) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 185) + ((int)threadIdx.x))] = kernel[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 185) + ((int)threadIdx.x))];\n    },\n  },\n  __syncthreads();\n  for (int rc_outer_inner = 0; rc_outer_inner < 11; ++rc_outer_inner) {\n    for (int rc_inner = 0; rc_inner < 6; ++rc_inner) {\n      for (int ry_inner = 0; ry_inner < 4; ++ry_inner) {\n        conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[(((((((int)threadIdx.x) / 37) * 264) + (rc_outer_inner * 24)) + (rc_inner * 4)) + ry_inner)] * kernel_shared[((((rc_outer_inner * 24) + ((((int)threadIdx.x) % 37) * 16)) + (rc_inner * 4)) + ry_inner)]));\n      },\n    },\n  },\n  conv1d_ncw[((((int)threadIdx.x) * 3) + ((int)blockIdx.x))] = conv1d_ncw_local[0];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 66, 9), \"float32\"), kernel: T.Buffer((37, 4, 4), \"float32\"), conv1d_ncw: T.Buffer((5, 37, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(111):\n            conv1d_ncw_local = T.allocate([5], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((5,), data=conv1d_ncw_local, scope=\"local\", align=16)\n            for nn_c_outer_inner_init in range(5):\n                conv1d_ncw_local_1[nn_c_outer_inner_init] = T.float32(0)\n            for rc_outer, ry_outer, nn_c_outer_inner, rc_inner in T.grid(6, 4, 5, 11):\n                data_1 = T.Buffer((2970,), data=data.data)\n                kernel_1 = T.Buffer((592,), data=kernel.data)\n                conv1d_ncw_local_1[nn_c_outer_inner] = conv1d_ncw_local_1[nn_c_outer_inner] + data_1[nn_c_outer_inner * 594 + rc_outer * 99 + rc_inner * 9 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 3 * 2 + ry_outer] * kernel_1[rc_outer * 44 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 3 * 16 + rc_inner * 4 + ry_outer]\n            for nn_inner in range(5):\n                conv1d_ncw_1 = T.Buffer((555,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_inner * 111 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused] = conv1d_ncw_local_1[nn_inner]",
        "data": "5_66_9",
        "kernel": "37_4_4"
    },
    {
        "op_name": "conv1d",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 68; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)48, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 2; ++nn_c_outer_inner_init) {\n      for (int32_t nn_c_inner_init = 0; nn_c_inner_init < 2; ++nn_c_inner_init) {\n        ((float3*)conv1d_ncw_local)[((nn_c_outer_inner_init * 2) + nn_c_inner_init)] = ((float3)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n      },\n    },\n    for (int32_t nn_c_outer_inner = 0; nn_c_outer_inner < 2; ++nn_c_outer_inner) {\n      for (int32_t rc_inner = 0; rc_inner < 31; ++rc_inner) {\n        for (int32_t ry_inner = 0; ry_inner < 4; ++ry_inner) {\n          for (int32_t nn_c_inner = 0; nn_c_inner < 2; ++nn_c_inner) {\n            int32_t cse_var_1 = ((nn_c_outer_inner * 2) + nn_c_inner);\n            int32_t3 v_ = int32_t3((((((nn_c_outer_inner * 496) + (nn_c_inner * 248)) + (rc_inner * 8)) + ry_inner))+(2*0), (((((nn_c_outer_inner * 496) + (nn_c_inner * 248)) + (rc_inner * 8)) + ry_inner))+(2*1), (((((nn_c_outer_inner * 496) + (nn_c_inner * 248)) + (rc_inner * 8)) + ry_inner))+(2*2));\n            ((float3*)conv1d_ncw_local)[cse_var_1] = (((float3*)conv1d_ncw_local)[cse_var_1] + ((float3(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2])) * ((float3)(((float*)kernel_1)[(((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 16) + (rc_inner * 4)) + ry_inner)], ((float*)kernel_1)[(((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 16) + (rc_inner * 4)) + ry_inner)], ((float*)kernel_1)[(((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 16) + (rc_inner * 4)) + ry_inner)]))));\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 4; ++nn_inner) {\n      *(float3*)(((float*)conv1d_ncw_1) + ((nn_inner * 204) + (nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 3))) = ((float3*)conv1d_ncw_local)[nn_inner];\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(34) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(34) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[2];\n  __shared__ float pad_temp_shared[8];\n  __shared__ float kernel_shared[136];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  conv1d_ncw_local[1] = 0.000000e+00f;\n  for (int rc_outer_outer = 0; rc_outer_outer < 31; ++rc_outer_outer) {\n    __syncthreads();\n    if (((int)threadIdx.x) < 8) {\n      pad_temp_shared[((int)threadIdx.x)] = data[((((((((int)blockIdx.x) / 6) * 496) + ((((int)threadIdx.x) >> 2) * 248)) + (rc_outer_outer * 8)) + ((((int)blockIdx.x) % 3) * 2)) + (((int)threadIdx.x) & 3))];\n    },\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 4; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      if (rc_outer_outer < 4) {\n        kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 34) + ((int)threadIdx.x))] = kernel[((((((((int)blockIdx.x) % 6) / 3) * 544) + ((((ax0_ax1_fused_ax2_fused_outer_outer * 17) + (((int)threadIdx.x) >> 1)) >> 1) * 16)) + (rc_outer_outer * 4)) + (((ax0_ax1_fused_ax2_fused_outer_outer * 2) + ((int)threadIdx.x)) & 3))];\n      },\n    },\n    __syncthreads();\n    for (int ry_outer_inner = 0; ry_outer_inner < 2; ++ry_outer_inner) {\n      for (int ry_inner = 0; ry_inner < 2; ++ry_inner) {\n        conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[((ry_outer_inner * 2) + ry_inner)] * kernel_shared[(((((int)threadIdx.x) * 4) + (ry_outer_inner * 2)) + ry_inner)]));\n        conv1d_ncw_local[1] = (conv1d_ncw_local[1] + (pad_temp_shared[(((ry_outer_inner * 2) + ry_inner) + 4)] * kernel_shared[(((((int)threadIdx.x) * 4) + (ry_outer_inner * 2)) + ry_inner)]));\n      },\n    },\n  },\n  conv1d_ncw[(((((((int)blockIdx.x) / 6) * 408) + (((((int)blockIdx.x) % 6) / 3) * 102)) + (((int)threadIdx.x) * 3)) + (((int)blockIdx.x) % 3))] = conv1d_ncw_local[0];\n  conv1d_ncw[((((((((int)blockIdx.x) / 6) * 408) + (((((int)blockIdx.x) % 6) / 3) * 102)) + (((int)threadIdx.x) * 3)) + (((int)blockIdx.x) % 3)) + 204)] = conv1d_ncw_local[1];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 31, 8), \"float32\"), kernel: T.Buffer((68, 4, 4), \"float32\"), conv1d_ncw: T.Buffer((4, 68, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(68):\n            conv1d_ncw_local = T.allocate([4], \"float32x3\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((4,), \"float32x3\", data=conv1d_ncw_local, scope=\"local\", align=32)\n            for nn_c_outer_inner_init, nn_c_inner_init in T.grid(2, 2):\n                conv1d_ncw_local_1[nn_c_outer_inner_init * 2 + nn_c_inner_init] = T.Broadcast(T.float32(0), 3)\n            for nn_c_outer_inner, rc_inner, ry_inner, nn_c_inner in T.grid(2, 31, 4, 2):\n                cse_var_1: T.int32 = nn_c_outer_inner * 2 + nn_c_inner\n                data_1 = T.Buffer((992,), data=data.data)\n                kernel_1 = T.Buffer((1088,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_c_outer_inner * 496 + nn_c_inner * 248 + rc_inner * 8 + ry_inner:nn_c_outer_inner * 496 + nn_c_inner * 248 + rc_inner * 8 + ry_inner + 6:2] * T.Broadcast(kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 16 + rc_inner * 4 + ry_inner], 3)\n            for nn_inner in range(4):\n                conv1d_ncw_1 = T.Buffer((816,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_inner * 204 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 3:nn_inner * 204 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 3 + 3] = conv1d_ncw_local_1[nn_inner]",
        "data": "4_31_8",
        "kernel": "68_4_4"
    },
    {
        "op_name": "conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused < 18; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused) {\n    for (int32_t ff_outer_inner_init = 0; ff_outer_inner_init < 9; ++ff_outer_inner_init) {\n      for (int32_t ff_inner_init = 0; ff_inner_init < 3; ++ff_inner_init) {\n        ((float*)conv1d_ncw_1)[(((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused >> 1) * 54) + (ff_outer_inner_init * 6)) + (ff_inner_init * 2)) + (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused & 1))] = 0.000000e+00f;\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 5; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n        for (int32_t ff_outer_inner = 0; ff_outer_inner < 9; ++ff_outer_inner) {\n          for (int32_t rc_inner = 0; rc_inner < 7; ++rc_inner) {\n            for (int32_t ff_inner = 0; ff_inner < 3; ++ff_inner) {\n              int32_t cse_var_2 = (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused & 1);\n              int32_t cse_var_1 = (((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused >> 1) * 54) + (ff_outer_inner * 6)) + (ff_inner * 2)) + cse_var_2);\n              ((float*)conv1d_ncw_1)[cse_var_1] = (((float*)conv1d_ncw_1)[cse_var_1] + (((float*)data_1)[((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused / 6) * 210) + (rc_outer * 42)) + (rc_inner * 6)) + (cse_var_2 * 2)) + ry_outer)] * ((float*)kernel_1)[((((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 6) >> 1) * 243) + (ff_outer_inner * 27)) + (rc_outer * 21)) + (ff_inner * 9)) + (rc_inner * 3)) + ry_outer)]));\n            },\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(81) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(81) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[3];\n  __shared__ float pad_temp_shared[315];\n  __shared__ float kernel_shared[729];\n  for (int ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 3; ++ff_c_outer_inner_init) {\n    conv1d_ncw_local[ff_c_outer_inner_init] = 0.000000e+00f;\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 4; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 9) + (((int)threadIdx.x) / 9)) < 35) {\n      pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 81) + ((int)threadIdx.x))] = data[((((ax0_ax1_fused_ax2_fused_outer_outer * 162) + ((((int)threadIdx.x) / 3) * 6)) + (((int)blockIdx.x) * 2)) + (((int)threadIdx.x) % 3))];\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 9; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 81) + ((int)threadIdx.x))] = kernel[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 81) + ((int)threadIdx.x))];\n  },\n  __syncthreads();\n  for (int rc_outer_inner = 0; rc_outer_inner < 5; ++rc_outer_inner) {\n    for (int ff_c_outer_inner = 0; ff_c_outer_inner < 3; ++ff_c_outer_inner) {\n      for (int rc_inner = 0; rc_inner < 7; ++rc_inner) {\n        for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n          conv1d_ncw_local[ff_c_outer_inner] = (conv1d_ncw_local[ff_c_outer_inner] + (pad_temp_shared[(((((((int)threadIdx.x) / 27) * 105) + (rc_outer_inner * 21)) + (rc_inner * 3)) + ry_inner)] * kernel_shared[((((((((int)threadIdx.x) % 27) * 27) + (rc_outer_inner * 21)) + (ff_c_outer_inner * 9)) + (rc_inner * 3)) + ry_inner)]));\n        },\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 3; ++ff_inner) {\n    conv1d_ncw[(((((int)threadIdx.x) * 6) + (ff_inner * 2)) + ((int)blockIdx.x))] = conv1d_ncw_local[ff_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 35, 6), \"float32\"), kernel: T.Buffer((81, 3, 3), \"float32\"), conv1d_ncw: T.Buffer((3, 81, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused in T.parallel(18):\n            conv1d_ncw_1 = T.Buffer((486,), data=conv1d_ncw.data)\n            for ff_outer_inner_init, ff_inner_init in T.grid(9, 3):\n                conv1d_ncw_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 2 * 54 + ff_outer_inner_init * 6 + ff_inner_init * 2 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 2] = T.float32(0)\n            for rc_outer, ry_outer, ff_outer_inner, rc_inner, ff_inner in T.grid(5, 3, 9, 7, 3):\n                cse_var_2: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 2\n                cse_var_1: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 2 * 54 + ff_outer_inner * 6 + ff_inner * 2 + cse_var_2\n                data_1 = T.Buffer((630,), data=data.data)\n                kernel_1 = T.Buffer((729,), data=kernel.data)\n                conv1d_ncw_1[cse_var_1] = conv1d_ncw_1[cse_var_1] + data_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 6 * 210 + rc_outer * 42 + rc_inner * 6 + cse_var_2 * 2 + ry_outer] * kernel_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 6 // 2 * 243 + ff_outer_inner * 27 + rc_outer * 21 + ff_inner * 9 + rc_inner * 3 + ry_outer]",
        "data": "3_35_6",
        "kernel": "81_3_3"
    },
    {
        "op_name": "conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 19; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)180, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 3; ++nn_c_outer_inner_init) {\n      for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 5; ++ff_c_inner_init) {\n        ((float3*)conv1d_ncw_local)[((nn_c_outer_inner_init * 5) + ff_c_inner_init)] = ((float3)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 4; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 4; ++ry_outer) {\n        for (int32_t nn_c_outer_inner = 0; nn_c_outer_inner < 3; ++nn_c_outer_inner) {\n          for (int32_t rc_inner = 0; rc_inner < 9; ++rc_inner) {\n            for (int32_t ff_c_inner = 0; ff_c_inner < 5; ++ff_c_inner) {\n              int32_t cse_var_1 = ((nn_c_outer_inner * 5) + ff_c_inner);\n              int32_t3 v_ = int32_t3((((((nn_c_outer_inner * 288) + (rc_outer * 72)) + (rc_inner * 8)) + ry_outer))+(2*0), (((((nn_c_outer_inner * 288) + (rc_outer * 72)) + (rc_inner * 8)) + ry_outer))+(2*1), (((((nn_c_outer_inner * 288) + (rc_outer * 72)) + (rc_inner * 8)) + ry_outer))+(2*2));\n              ((float3*)conv1d_ncw_local)[cse_var_1] = (((float3*)conv1d_ncw_local)[cse_var_1] + ((float3(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2])) * ((float3)(((float*)kernel_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 80) + (rc_outer * 36)) + (ff_c_inner * 16)) + (rc_inner * 4)) + ry_outer)], ((float*)kernel_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 80) + (rc_outer * 36)) + (ff_c_inner * 16)) + (rc_inner * 4)) + ry_outer)], ((float*)kernel_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 80) + (rc_outer * 36)) + (ff_c_inner * 16)) + (rc_inner * 4)) + ry_outer)]))));\n            },\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 3; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 5; ++ff_inner) {\n        *(float3*)(((float*)conv1d_ncw_1) + (((nn_inner * 285) + (nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 15)) + (ff_inner * 3))) = ((float3*)conv1d_ncw_local)[((nn_inner * 5) + ff_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(95) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(95) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[3];\n  __shared__ float pad_temp_shared[216];\n  __shared__ float kernel_shared[760];\n  for (int nn_c_inner_init = 0; nn_c_inner_init < 3; ++nn_c_inner_init) {\n    conv1d_ncw_local[nn_c_inner_init] = 0.000000e+00f;\n  },\n  for (int ry_outer_outer = 0; ry_outer_outer < 2; ++ry_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 3; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer * 95) + ((int)threadIdx.x)) < 216) {\n        pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 95) + ((int)threadIdx.x))] = data[(((((((ax0_ax1_fused_ax2_fused_outer_outer * 95) + ((int)threadIdx.x)) >> 1) * 8) + (ry_outer_outer * 2)) + (((int)blockIdx.x) * 2)) + ((ax0_ax1_fused_ax2_fused_outer_outer + ((int)threadIdx.x)) & 1))];\n      },\n    },\n    for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 8; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 95) + ((int)threadIdx.x))] = kernel[((((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 95) + ((int)threadIdx.x)) >> 1) * 4) + (ry_outer_outer * 2)) + ((ax0_ax1_fused_ax2_fused_outer_outer_1 + ((int)threadIdx.x)) & 1))];\n    },\n    __syncthreads();\n    for (int rc_outer_inner = 0; rc_outer_inner < 2; ++rc_outer_inner) {\n      for (int rc_inner = 0; rc_inner < 18; ++rc_inner) {\n        for (int ry_inner = 0; ry_inner < 2; ++ry_inner) {\n          for (int nn_c_inner = 0; nn_c_inner < 3; ++nn_c_inner) {\n            conv1d_ncw_local[nn_c_inner] = (conv1d_ncw_local[nn_c_inner] + (pad_temp_shared[((((nn_c_inner * 72) + (rc_outer_inner * 36)) + (rc_inner * 2)) + ry_inner)] * kernel_shared[((((rc_outer_inner * 36) + (((int)threadIdx.x) * 8)) + (rc_inner * 2)) + ry_inner)]));\n          },\n        },\n      },\n    },\n  },\n  for (int nn_inner = 0; nn_inner < 3; ++nn_inner) {\n    conv1d_ncw[(((nn_inner * 285) + (((int)threadIdx.x) * 3)) + ((int)blockIdx.x))] = conv1d_ncw_local[nn_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 36, 8), \"float32\"), kernel: T.Buffer((95, 4, 4), \"float32\"), conv1d_ncw: T.Buffer((3, 95, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(19):\n            conv1d_ncw_local = T.allocate([15], \"float32x3\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((15,), \"float32x3\", data=conv1d_ncw_local, scope=\"local\")\n            for nn_c_outer_inner_init, ff_c_inner_init in T.grid(3, 5):\n                conv1d_ncw_local_1[nn_c_outer_inner_init * 5 + ff_c_inner_init] = T.Broadcast(T.float32(0), 3)\n            for rc_outer, ry_outer, nn_c_outer_inner, rc_inner, ff_c_inner in T.grid(4, 4, 3, 9, 5):\n                cse_var_1: T.int32 = nn_c_outer_inner * 5 + ff_c_inner\n                data_1 = T.Buffer((864,), data=data.data)\n                kernel_1 = T.Buffer((1520,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_c_outer_inner * 288 + rc_outer * 72 + rc_inner * 8 + ry_outer:nn_c_outer_inner * 288 + rc_outer * 72 + rc_inner * 8 + ry_outer + 6:2] * T.Broadcast(kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 80 + rc_outer * 36 + ff_c_inner * 16 + rc_inner * 4 + ry_outer], 3)\n            for nn_inner, ff_inner in T.grid(3, 5):\n                conv1d_ncw_1 = T.Buffer((855,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_inner * 285 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 15 + ff_inner * 3:nn_inner * 285 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 15 + ff_inner * 3 + 3] = conv1d_ncw_local_1[nn_inner * 5 + ff_inner]",
        "data": "3_36_8",
        "kernel": "95_4_4"
    },
    {
        "op_name": "conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused < 24; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused) {\n    for (int32_t ff_outer_inner_init = 0; ff_outer_inner_init < 4; ++ff_outer_inner_init) {\n      ((float*)conv1d_ncw_1)[((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 4) + ff_outer_inner_init)] = 0.000000e+00f;\n    },\n    for (int32_t ry_outer = 0; ry_outer < 4; ++ry_outer) {\n      for (int32_t ff_outer_inner = 0; ff_outer_inner < 4; ++ff_outer_inner) {\n        for (int32_t rc_inner = 0; rc_inner < 59; ++rc_inner) {\n          int32_t cse_var_2 = (rc_inner * 4);\n          int32_t cse_var_1 = ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 4) + ff_outer_inner);\n          ((float*)conv1d_ncw_1)[cse_var_1] = (((float*)conv1d_ncw_1)[cse_var_1] + (((float*)data_1)[((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused >> 2) * 236) + cse_var_2) + ry_outer)] * ((float*)kernel_1)[(((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused & 3) * 64) + (ff_outer_inner * 16)) + cse_var_2) + ry_outer)]));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[1];\n  __shared__ float pad_temp_shared[354];\n  __shared__ float kernel_shared[32];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  for (int ry_outer_outer = 0; ry_outer_outer < 4; ++ry_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 8; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer * 8) + (((int)threadIdx.x) / 6)) < 59) {\n        pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 48) + ((int)threadIdx.x))] = data[(((ax0_ax1_fused_ax2_fused_outer_outer * 192) + (((int)threadIdx.x) * 4)) + ry_outer_outer)];\n      },\n    },\n    if (((int)threadIdx.x) < 32) {\n      kernel_shared[((int)threadIdx.x)] = kernel[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + ry_outer_outer)];\n    },\n    __syncthreads();\n    for (int rc_inner = 0; rc_inner < 59; ++rc_inner) {\n      conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[(((((int)threadIdx.x) >> 3) * 59) + rc_inner)] * kernel_shared[(((((int)threadIdx.x) & 7) * 4) + rc_inner)]));\n    },\n  },\n  conv1d_ncw[((((((int)threadIdx.x) >> 3) * 16) + (((int)blockIdx.x) * 8)) + (((int)threadIdx.x) & 7))] = conv1d_ncw_local[0];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 59, 4), \"float32\"), kernel: T.Buffer((16, 4, 4), \"float32\"), conv1d_ncw: T.Buffer((6, 16, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused in T.parallel(24):\n            conv1d_ncw_1 = T.Buffer((96,), data=conv1d_ncw.data)\n            for ff_outer_inner_init in range(4):\n                conv1d_ncw_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 4 + ff_outer_inner_init] = T.float32(0)\n            for ry_outer, ff_outer_inner, rc_inner in T.grid(4, 4, 59):\n                cse_var_2: T.int32 = rc_inner * 4\n                cse_var_1: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 4 + ff_outer_inner\n                data_1 = T.Buffer((1416,), data=data.data)\n                kernel_1 = T.Buffer((256,), data=kernel.data)\n                conv1d_ncw_1[cse_var_1] = conv1d_ncw_1[cse_var_1] + data_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 4 * 236 + cse_var_2 + ry_outer] * kernel_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 4 * 64 + ff_outer_inner * 16 + cse_var_2 + ry_outer]",
        "data": "6_59_4",
        "kernel": "16_4_4"
    },
    {
        "op_name": "conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_inner_init = 0; nn_outer_inner_init < 2; ++nn_outer_inner_init) {\n    for (int32_t ff_outer_inner_init = 0; ff_outer_inner_init < 2; ++ff_outer_inner_init) {\n      for (int32_t ff_inner_init = 0; ff_inner_init < 28; ++ff_inner_init) {\n        ((float*)conv1d_ncw_1)[(((nn_outer_inner_init * 56) + (ff_outer_inner_init * 28)) + ff_inner_init)] = 0.000000e+00f;\n      },\n    },\n  },\n  for (int32_t rc_outer = 0; rc_outer < 23; ++rc_outer) {\n    for (int32_t nn_outer_inner = 0; nn_outer_inner < 2; ++nn_outer_inner) {\n      for (int32_t ff_outer_inner = 0; ff_outer_inner < 2; ++ff_outer_inner) {\n        for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n          for (int32_t ff_inner = 0; ff_inner < 28; ++ff_inner) {\n            int32_t cse_var_2 = (rc_outer * 3);\n            int32_t cse_var_1 = (((nn_outer_inner * 56) + (ff_outer_inner * 28)) + ff_inner);\n            ((float*)conv1d_ncw_1)[cse_var_1] = (((float*)conv1d_ncw_1)[cse_var_1] + (((float*)data_1)[(((nn_outer_inner * 69) + cse_var_2) + ry_inner)] * ((float*)kernel_1)[((((ff_outer_inner * 252) + (ff_inner * 9)) + cse_var_2) + ry_inner)]));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[1];\n  __shared__ float pad_temp_shared[46];\n  __shared__ float kernel_shared[84];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  for (int ry_outer_outer = 0; ry_outer_outer < 3; ++ry_outer_outer) {\n    __syncthreads();\n    if (((int)threadIdx.x) < 46) {\n      pad_temp_shared[((int)threadIdx.x)] = data[((((int)threadIdx.x) * 3) + ry_outer_outer)];\n    },\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer * 2) + (((int)threadIdx.x) / 28)) < 3) {\n        kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 56) + ((int)threadIdx.x))] = kernel[((((((int)blockIdx.x) * 252) + (ax0_ax1_fused_ax2_fused_outer_outer * 168)) + (((int)threadIdx.x) * 3)) + ry_outer_outer)];\n      },\n    },\n    __syncthreads();\n    for (int rc_outer_inner = 0; rc_outer_inner < 23; ++rc_outer_inner) {\n      conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[(((((int)threadIdx.x) / 28) * 23) + rc_outer_inner)] * kernel_shared[(((((int)threadIdx.x) % 28) * 3) + rc_outer_inner)]));\n    },\n  },\n  conv1d_ncw[((((((int)threadIdx.x) / 28) * 56) + (((int)blockIdx.x) * 28)) + (((int)threadIdx.x) % 28))] = conv1d_ncw_local[0];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 23, 3), \"float32\"), kernel: T.Buffer((56, 3, 3), \"float32\"), conv1d_ncw: T.Buffer((2, 56, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        conv1d_ncw_1 = T.Buffer((112,), data=conv1d_ncw.data)\n        for nn_outer_inner_init, ff_outer_inner_init, ff_inner_init in T.grid(2, 2, 28):\n            conv1d_ncw_1[nn_outer_inner_init * 56 + ff_outer_inner_init * 28 + ff_inner_init] = T.float32(0)\n        for rc_outer, nn_outer_inner, ff_outer_inner, ry_inner, ff_inner in T.grid(23, 2, 2, 3, 28):\n            cse_var_2: T.int32 = rc_outer * 3\n            cse_var_1: T.int32 = nn_outer_inner * 56 + ff_outer_inner * 28 + ff_inner\n            data_1 = T.Buffer((138,), data=data.data)\n            kernel_1 = T.Buffer((504,), data=kernel.data)\n            conv1d_ncw_1[cse_var_1] = conv1d_ncw_1[cse_var_1] + data_1[nn_outer_inner * 69 + cse_var_2 + ry_inner] * kernel_1[ff_outer_inner * 252 + ff_inner * 9 + cse_var_2 + ry_inner]",
        "data": "2_23_3",
        "kernel": "56_3_3"
    },
    {
        "op_name": "conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused < 13; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused) {\n    for (int32_t nn_inner_init = 0; nn_inner_init < 7; ++nn_inner_init) {\n      for (int32_t ff_inner_init = 0; ff_inner_init < 5; ++ff_inner_init) {\n        ((float*)conv1d_ncw_1)[(((nn_inner_init * 65) + (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 5)) + ff_inner_init)] = 0.000000e+00f;\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 37; ++rc_outer) {\n      for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n        for (int32_t nn_inner = 0; nn_inner < 7; ++nn_inner) {\n          for (int32_t ff_inner = 0; ff_inner < 5; ++ff_inner) {\n            int32_t cse_var_2 = (rc_outer * 3);\n            int32_t cse_var_1 = (((nn_inner * 65) + (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 5)) + ff_inner);\n            ((float*)conv1d_ncw_1)[cse_var_1] = (((float*)conv1d_ncw_1)[cse_var_1] + (((float*)data_1)[(((nn_inner * 111) + cse_var_2) + ry_inner)] * ((float*)kernel_1)[((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 45) + (ff_inner * 9)) + cse_var_2) + ry_inner)]));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(65) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(65) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[1];\n  __shared__ float pad_temp_shared[37];\n  __shared__ float kernel_shared[195];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  for (int ry_outer_outer = 0; ry_outer_outer < 3; ++ry_outer_outer) {\n    __syncthreads();\n    if (((int)threadIdx.x) < 37) {\n      pad_temp_shared[((int)threadIdx.x)] = data[(((((int)blockIdx.x) * 111) + (((int)threadIdx.x) * 3)) + ry_outer_outer)];\n    },\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 3; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 65) + ((int)threadIdx.x))] = kernel[(((ax0_ax1_fused_ax2_fused_outer_outer * 195) + (((int)threadIdx.x) * 3)) + ry_outer_outer)];\n    },\n    __syncthreads();\n    for (int rc_inner = 0; rc_inner < 37; ++rc_inner) {\n      conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[rc_inner] * kernel_shared[((((int)threadIdx.x) * 3) + rc_inner)]));\n    },\n  },\n  conv1d_ncw[((((int)blockIdx.x) * 65) + ((int)threadIdx.x))] = conv1d_ncw_local[0];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 37, 3), \"float32\"), kernel: T.Buffer((65, 3, 3), \"float32\"), conv1d_ncw: T.Buffer((7, 65, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused in T.parallel(13):\n            conv1d_ncw_1 = T.Buffer((455,), data=conv1d_ncw.data)\n            for nn_inner_init, ff_inner_init in T.grid(7, 5):\n                conv1d_ncw_1[nn_inner_init * 65 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 5 + ff_inner_init] = T.float32(0)\n            for rc_outer, ry_inner, nn_inner, ff_inner in T.grid(37, 3, 7, 5):\n                cse_var_2: T.int32 = rc_outer * 3\n                cse_var_1: T.int32 = nn_inner * 65 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 5 + ff_inner\n                data_1 = T.Buffer((777,), data=data.data)\n                kernel_1 = T.Buffer((585,), data=kernel.data)\n                conv1d_ncw_1[cse_var_1] = conv1d_ncw_1[cse_var_1] + data_1[nn_inner * 111 + cse_var_2 + ry_inner] * kernel_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 45 + ff_inner * 9 + cse_var_2 + ry_inner]",
        "data": "7_37_3",
        "kernel": "65_3_3"
    },
    {
        "op_name": "conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 7; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)288, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 4; ++ff_c_outer_inner_init) {\n      for (int32_t yy_c_outer_inner_init = 0; yy_c_outer_inner_init < 2; ++yy_c_outer_inner_init) {\n        for (int32_t nn_c_inner_init = 0; nn_c_inner_init < 3; ++nn_c_inner_init) {\n          for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 3; ++ff_c_inner_init) {\n            ((float*)conv1d_ncw_local)[((((nn_c_inner_init * 24) + (ff_c_outer_inner_init * 6)) + (ff_c_inner_init * 2)) + yy_c_outer_inner_init)] = 0.000000e+00f;\n          },\n        },\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 19; ++rc_outer) {\n      for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 4; ++ff_c_outer_inner) {\n        for (int32_t yy_c_outer_inner = 0; yy_c_outer_inner < 2; ++yy_c_outer_inner) {\n          for (int32_t ry_inner = 0; ry_inner < 4; ++ry_inner) {\n            for (int32_t nn_c_inner = 0; nn_c_inner < 3; ++nn_c_inner) {\n              for (int32_t ff_c_inner = 0; ff_c_inner < 3; ++ff_c_inner) {\n                int32_t cse_var_1 = ((((nn_c_inner * 24) + (ff_c_outer_inner * 6)) + (ff_c_inner * 2)) + yy_c_outer_inner);\n                ((float*)conv1d_ncw_local)[cse_var_1] = (((float*)conv1d_ncw_local)[cse_var_1] + (((float*)data_1)[((((nn_c_inner * 114) + (rc_outer * 6)) + (yy_c_outer_inner * 2)) + ry_inner)] * ((float*)kernel_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 192) + (ff_c_outer_inner * 48)) + (ff_c_inner * 16)) + (rc_outer * 4)) + ry_inner)]));\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 3; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 12; ++ff_inner) {\n        int32_t cse_var_2 = (ff_inner * 2);\n        int32_t2 v_ = int32_t2((((nn_inner * 24) + cse_var_2))+(1*0), (((nn_inner * 24) + cse_var_2))+(1*1));\n        *(float2*)(((float*)conv1d_ncw_1) + (((nn_inner * 168) + (nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 24)) + cse_var_2)) = (float2(((float*)conv1d_ncw_local)[v_.s0],((float*)conv1d_ncw_local)[v_.s1]));\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[3];\n  __shared__ float pad_temp_shared[114];\n  __shared__ float kernel_shared[1344];\n  for (int ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 3; ++ff_c_outer_inner_init) {\n    conv1d_ncw_local[ff_c_outer_inner_init] = 0.000000e+00f;\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 3; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 28) + (((int)threadIdx.x) >> 1)) < 57) {\n      pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 56) + ((int)threadIdx.x))] = data[(((((int)blockIdx.x) * 114) + (ax0_ax1_fused_ax2_fused_outer_outer * 56)) + ((int)threadIdx.x))];\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 24; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 56) + ((int)threadIdx.x))] = kernel[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 56) + ((int)threadIdx.x))];\n  },\n  __syncthreads();\n  for (int rc_outer_inner = 0; rc_outer_inner < 19; ++rc_outer_inner) {\n    for (int ry_outer_inner = 0; ry_outer_inner < 2; ++ry_outer_inner) {\n      for (int ff_c_outer_inner = 0; ff_c_outer_inner < 3; ++ff_c_outer_inner) {\n        for (int ry_inner = 0; ry_inner < 2; ++ry_inner) {\n          conv1d_ncw_local[ff_c_outer_inner] = (conv1d_ncw_local[ff_c_outer_inner] + (pad_temp_shared[((((rc_outer_inner * 6) + (ry_outer_inner * 2)) + ((((int)threadIdx.x) & 1) * 2)) + ry_inner)] * kernel_shared[((((((((int)threadIdx.x) >> 1) * 48) + (ff_c_outer_inner * 16)) + (rc_outer_inner * 4)) + (ry_outer_inner * 2)) + ry_inner)]));\n        },\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 3; ++ff_inner) {\n    conv1d_ncw[((((((int)blockIdx.x) * 168) + ((((int)threadIdx.x) >> 1) * 6)) + (ff_inner * 2)) + (((int)threadIdx.x) & 1))] = conv1d_ncw_local[ff_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 19, 6), \"float32\"), kernel: T.Buffer((84, 4, 4), \"float32\"), conv1d_ncw: T.Buffer((3, 84, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(7):\n            conv1d_ncw_local = T.allocate([72], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((72,), data=conv1d_ncw_local, scope=\"local\")\n            for ff_c_outer_inner_init, yy_c_outer_inner_init, nn_c_inner_init, ff_c_inner_init in T.grid(4, 2, 3, 3):\n                conv1d_ncw_local_1[nn_c_inner_init * 24 + ff_c_outer_inner_init * 6 + ff_c_inner_init * 2 + yy_c_outer_inner_init] = T.float32(0)\n            for rc_outer, ff_c_outer_inner, yy_c_outer_inner, ry_inner, nn_c_inner, ff_c_inner in T.grid(19, 4, 2, 4, 3, 3):\n                cse_var_1: T.int32 = nn_c_inner * 24 + ff_c_outer_inner * 6 + ff_c_inner * 2 + yy_c_outer_inner\n                data_1 = T.Buffer((342,), data=data.data)\n                kernel_1 = T.Buffer((1344,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_c_inner * 114 + rc_outer * 6 + yy_c_outer_inner * 2 + ry_inner] * kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 192 + ff_c_outer_inner * 48 + ff_c_inner * 16 + rc_outer * 4 + ry_inner]\n            for nn_inner, ff_inner in T.grid(3, 12):\n                cse_var_2: T.int32 = ff_inner * 2\n                conv1d_ncw_1 = T.Buffer((504,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_inner * 168 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 24 + cse_var_2:nn_inner * 168 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 24 + cse_var_2 + 2] = conv1d_ncw_local_1[nn_inner * 24 + cse_var_2:nn_inner * 24 + cse_var_2 + 2]",
        "data": "3_19_6",
        "kernel": "84_4_4"
    },
    {
        "op_name": "conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 14; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)432, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 4; ++nn_c_outer_inner_init) {\n      for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 3; ++ff_c_outer_inner_init) {\n        for (int32_t yy_c_outer_inner_init = 0; yy_c_outer_inner_init < 3; ++yy_c_outer_inner_init) {\n          for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 3; ++ff_c_inner_init) {\n            ((float*)conv1d_ncw_local)[((((nn_c_outer_inner_init * 27) + (ff_c_outer_inner_init * 9)) + (ff_c_inner_init * 3)) + yy_c_outer_inner_init)] = 0.000000e+00f;\n          },\n        },\n      },\n    },\n    for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n      for (int32_t nn_c_outer_inner = 0; nn_c_outer_inner < 4; ++nn_c_outer_inner) {\n        for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 3; ++ff_c_outer_inner) {\n          for (int32_t yy_c_outer_inner = 0; yy_c_outer_inner < 3; ++yy_c_outer_inner) {\n            for (int32_t rc_inner = 0; rc_inner < 23; ++rc_inner) {\n              for (int32_t ff_c_inner = 0; ff_c_inner < 3; ++ff_c_inner) {\n                int32_t cse_var_1 = ((((nn_c_outer_inner * 27) + (ff_c_outer_inner * 9)) + (ff_c_inner * 3)) + yy_c_outer_inner);\n                ((float*)conv1d_ncw_local)[cse_var_1] = (((float*)conv1d_ncw_local)[cse_var_1] + (((float*)data_1)[((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 7) * 736) + (nn_c_outer_inner * 184)) + (rc_inner * 8)) + (yy_c_outer_inner * 2)) + ry_outer)] * ((float*)kernel_1)[((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 7) * 81) + (ff_c_outer_inner * 27)) + (ff_c_inner * 9)) + (rc_inner * 3)) + ry_outer)]));\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 4; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 9; ++ff_inner) {\n        int32_t cse_var_2 = (ff_inner * 3);\n        int32_t3 v_ = int32_t3((((nn_inner * 27) + cse_var_2))+(1*0), (((nn_inner * 27) + cse_var_2))+(1*1), (((nn_inner * 27) + cse_var_2))+(1*2));\n        *(float3*)(((float*)conv1d_ncw_1) + (((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 7) * 756) + (nn_inner * 189)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 7) * 27)) + cse_var_2)) = (float3(((float*)conv1d_ncw_local)[v_.s0],((float*)conv1d_ncw_local)[v_.s1],((float*)conv1d_ncw_local)[v_.s2]));\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[6];\n  __shared__ float pad_temp_shared[644];\n  __shared__ float kernel_shared[189];\n  for (int ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 3; ++ff_c_outer_inner_init) {\n    for (int nn_c_inner_init = 0; nn_c_inner_init < 2; ++nn_c_inner_init) {\n      conv1d_ncw_local[((nn_c_inner_init * 3) + ff_c_outer_inner_init)] = 0.000000e+00f;\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 16; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 3) + (((int)threadIdx.x) / 14)) < 46) {\n      pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 42) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) / 3) * 736) + (ax0_ax1_fused_ax2_fused_outer_outer * 48)) + ((((int)threadIdx.x) / 7) * 8)) + (((int)threadIdx.x) % 7))];\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 5; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer_1 * 2) + (((int)threadIdx.x) / 21)) < 9) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 42) + ((int)threadIdx.x))] = kernel[((((((int)blockIdx.x) % 3) * 189) + (ax0_ax1_fused_ax2_fused_outer_outer_1 * 42)) + ((int)threadIdx.x))];\n    },\n  },\n  __syncthreads();\n  for (int ry_outer_inner = 0; ry_outer_inner < 3; ++ry_outer_inner) {\n    for (int ff_c_outer_inner = 0; ff_c_outer_inner < 3; ++ff_c_outer_inner) {\n      for (int rc_inner = 0; rc_inner < 23; ++rc_inner) {\n        for (int nn_c_inner = 0; nn_c_inner < 2; ++nn_c_inner) {\n          conv1d_ncw_local[((nn_c_inner * 3) + ff_c_outer_inner)] = (conv1d_ncw_local[((nn_c_inner * 3) + ff_c_outer_inner)] + (pad_temp_shared[((((((((int)threadIdx.x) / 21) * 322) + (nn_c_inner * 161)) + (rc_inner * 7)) + ((((int)threadIdx.x) % 3) * 2)) + ry_outer_inner)] * kernel_shared[((((((((int)threadIdx.x) % 21) / 3) * 27) + (ff_c_outer_inner * 9)) + (rc_inner * 3)) + ry_outer_inner)]));\n        },\n      },\n    },\n  },\n  for (int nn_inner = 0; nn_inner < 2; ++nn_inner) {\n    for (int ff_inner = 0; ff_inner < 3; ++ff_inner) {\n      conv1d_ncw[((((((((((int)blockIdx.x) / 3) * 756) + ((((int)threadIdx.x) / 21) * 378)) + (nn_inner * 189)) + ((((int)blockIdx.x) % 3) * 63)) + (((((int)threadIdx.x) % 21) / 3) * 9)) + (ff_inner * 3)) + (((int)threadIdx.x) % 3))] = conv1d_ncw_local[((nn_inner * 3) + ff_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 23, 8), \"float32\"), kernel: T.Buffer((63, 3, 3), \"float32\"), conv1d_ncw: T.Buffer((8, 63, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(14):\n            conv1d_ncw_local = T.allocate([108], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((108,), data=conv1d_ncw_local, scope=\"local\")\n            for nn_c_outer_inner_init, ff_c_outer_inner_init, yy_c_outer_inner_init, ff_c_inner_init in T.grid(4, 3, 3, 3):\n                conv1d_ncw_local_1[nn_c_outer_inner_init * 27 + ff_c_outer_inner_init * 9 + ff_c_inner_init * 3 + yy_c_outer_inner_init] = T.float32(0)\n            for ry_outer, nn_c_outer_inner, ff_c_outer_inner, yy_c_outer_inner, rc_inner, ff_c_inner in T.grid(3, 4, 3, 3, 23, 3):\n                cse_var_1: T.int32 = nn_c_outer_inner * 27 + ff_c_outer_inner * 9 + ff_c_inner * 3 + yy_c_outer_inner\n                data_1 = T.Buffer((1472,), data=data.data)\n                kernel_1 = T.Buffer((567,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 7 * 736 + nn_c_outer_inner * 184 + rc_inner * 8 + yy_c_outer_inner * 2 + ry_outer] * kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 7 * 81 + ff_c_outer_inner * 27 + ff_c_inner * 9 + rc_inner * 3 + ry_outer]\n            for nn_inner, ff_inner in T.grid(4, 9):\n                cse_var_2: T.int32 = ff_inner * 3\n                conv1d_ncw_1 = T.Buffer((1512,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 7 * 756 + nn_inner * 189 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 7 * 27 + cse_var_2:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 7 * 756 + nn_inner * 189 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 7 * 27 + cse_var_2 + 3] = conv1d_ncw_local_1[nn_inner * 27 + cse_var_2:nn_inner * 27 + cse_var_2 + 3]",
        "data": "8_23_8",
        "kernel": "63_3_3"
    },
    {
        "op_name": "conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_ff_outer_fused_yy_outer_fused = 0; nn_outer_ff_outer_fused_yy_outer_fused < 15; ++nn_outer_ff_outer_fused_yy_outer_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)304, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t yy_c_outer_outer_inner = 0; yy_c_outer_outer_inner < 2; ++yy_c_outer_outer_inner) {\n      for (int32_t nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 2; ++nn_c_outer_inner_init) {\n        for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 19; ++ff_c_inner_init) {\n          ((float*)conv1d_ncw_local)[(((nn_c_outer_inner_init * 38) + (ff_c_inner_init * 2)) + yy_c_outer_outer_inner)] = 0.000000e+00f;\n        },\n      },\n      for (int32_t nn_c_outer_inner = 0; nn_c_outer_inner < 2; ++nn_c_outer_inner) {\n        for (int32_t rc_inner = 0; rc_inner < 26; ++rc_inner) {\n          for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n            for (int32_t ff_c_inner = 0; ff_c_inner < 19; ++ff_c_inner) {\n              int32_t cse_var_1 = (((nn_c_outer_inner * 38) + (ff_c_inner * 2)) + yy_c_outer_outer_inner);\n              ((float*)conv1d_ncw_local)[cse_var_1] = (((float*)conv1d_ncw_local)[cse_var_1] + (((float*)data_1)[((((((nn_outer_ff_outer_fused_yy_outer_fused / 5) * 312) + (nn_c_outer_inner * 156)) + (rc_inner * 6)) + (yy_c_outer_outer_inner * 2)) + ry_inner)] * ((float*)kernel_1)[(((((nn_outer_ff_outer_fused_yy_outer_fused % 5) * 171) + (ff_c_inner * 9)) + (rc_inner * 3)) + ry_inner)]));\n            },\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 2; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 19; ++ff_inner) {\n        int32_t cse_var_2 = (ff_inner * 2);\n        int32_t2 v_ = int32_t2((((nn_inner * 38) + cse_var_2))+(1*0), (((nn_inner * 38) + cse_var_2))+(1*1));\n        *(float2*)(((float*)conv1d_ncw_1) + (((((nn_outer_ff_outer_fused_yy_outer_fused / 5) * 380) + (nn_inner * 190)) + ((nn_outer_ff_outer_fused_yy_outer_fused % 5) * 38)) + cse_var_2)) = (float2(((float*)conv1d_ncw_local)[v_.s0],((float*)conv1d_ncw_local)[v_.s1]));\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(57) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(57) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[10];\n  __shared__ float pad_temp_shared[234];\n  __shared__ float kernel_shared[285];\n  for (int ff_c_inner_init = 0; ff_c_inner_init < 5; ++ff_c_inner_init) {\n    conv1d_ncw_local[ff_c_inner_init] = 0.000000e+00f;\n    conv1d_ncw_local[(ff_c_inner_init + 5)] = 0.000000e+00f;\n  },\n  for (int ry_outer_outer = 0; ry_outer_outer < 3; ++ry_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 5; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer * 19) + (((int)threadIdx.x) / 3)) < 78) {\n        pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 57) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) * 468) + (ax0_ax1_fused_ax2_fused_outer_outer * 114)) + ((((int)threadIdx.x) / 3) * 6)) + ry_outer_outer) + (((int)threadIdx.x) % 3))];\n      },\n    },\n    for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 5; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 57) + ((int)threadIdx.x))] = kernel[(((ax0_ax1_fused_ax2_fused_outer_outer_1 * 171) + (((int)threadIdx.x) * 3)) + ry_outer_outer)];\n    },\n    __syncthreads();\n    for (int rc_outer_inner = 0; rc_outer_inner < 13; ++rc_outer_inner) {\n      for (int rc_inner = 0; rc_inner < 2; ++rc_inner) {\n        for (int ff_c_inner = 0; ff_c_inner < 5; ++ff_c_inner) {\n          conv1d_ncw_local[ff_c_inner] = (conv1d_ncw_local[ff_c_inner] + (pad_temp_shared[((((((int)threadIdx.x) / 19) * 78) + (rc_outer_inner * 6)) + (rc_inner * 3))] * kernel_shared[(((((((int)threadIdx.x) % 19) * 15) + (ff_c_inner * 3)) + (rc_outer_inner * 2)) + rc_inner)]));\n          conv1d_ncw_local[(ff_c_inner + 5)] = (conv1d_ncw_local[(ff_c_inner + 5)] + (pad_temp_shared[(((((((int)threadIdx.x) / 19) * 78) + (rc_outer_inner * 6)) + (rc_inner * 3)) + 2)] * kernel_shared[(((((((int)threadIdx.x) % 19) * 15) + (ff_c_inner * 3)) + (rc_outer_inner * 2)) + rc_inner)]));\n        },\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 5; ++ff_inner) {\n    conv1d_ncw[(((((int)blockIdx.x) * 570) + (((int)threadIdx.x) * 10)) + (ff_inner * 2))] = conv1d_ncw_local[ff_inner];\n    conv1d_ncw[((((((int)blockIdx.x) * 570) + (((int)threadIdx.x) * 10)) + (ff_inner * 2)) + 1)] = conv1d_ncw_local[(ff_inner + 5)];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 26, 6), \"float32\"), kernel: T.Buffer((95, 3, 3), \"float32\"), conv1d_ncw: T.Buffer((6, 95, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_ff_outer_fused_yy_outer_fused in T.parallel(15):\n            conv1d_ncw_local = T.allocate([76], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((76,), data=conv1d_ncw_local, scope=\"local\")\n            for yy_c_outer_outer_inner in range(2):\n                for nn_c_outer_inner_init, ff_c_inner_init in T.grid(2, 19):\n                    conv1d_ncw_local_1[nn_c_outer_inner_init * 38 + ff_c_inner_init * 2 + yy_c_outer_outer_inner] = T.float32(0)\n                for nn_c_outer_inner, rc_inner, ry_inner, ff_c_inner in T.grid(2, 26, 3, 19):\n                    cse_var_1: T.int32 = nn_c_outer_inner * 38 + ff_c_inner * 2 + yy_c_outer_outer_inner\n                    data_1 = T.Buffer((936,), data=data.data)\n                    kernel_1 = T.Buffer((855,), data=kernel.data)\n                    conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_outer_ff_outer_fused_yy_outer_fused // 5 * 312 + nn_c_outer_inner * 156 + rc_inner * 6 + yy_c_outer_outer_inner * 2 + ry_inner] * kernel_1[nn_outer_ff_outer_fused_yy_outer_fused % 5 * 171 + ff_c_inner * 9 + rc_inner * 3 + ry_inner]\n            for nn_inner, ff_inner in T.grid(2, 19):\n                cse_var_2: T.int32 = ff_inner * 2\n                conv1d_ncw_1 = T.Buffer((1140,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_outer_ff_outer_fused_yy_outer_fused // 5 * 380 + nn_inner * 190 + nn_outer_ff_outer_fused_yy_outer_fused % 5 * 38 + cse_var_2:nn_outer_ff_outer_fused_yy_outer_fused // 5 * 380 + nn_inner * 190 + nn_outer_ff_outer_fused_yy_outer_fused % 5 * 38 + cse_var_2 + 2] = conv1d_ncw_local_1[nn_inner * 38 + cse_var_2:nn_inner * 38 + cse_var_2 + 2]",
        "data": "6_26_6",
        "kernel": "95_3_3"
    },
    {
        "op_name": "conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_ff_outer_fused_yy_outer_fused = 0; nn_outer_ff_outer_fused_yy_outer_fused < 129; ++nn_outer_ff_outer_fused_yy_outer_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)48, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t nn_c_outer_outer_inner = 0; nn_c_outer_outer_inner < 2; ++nn_c_outer_outer_inner) {\n      for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 2; ++ff_c_outer_inner_init) {\n        ((float3*)conv1d_ncw_local)[((nn_c_outer_outer_inner * 2) + ff_c_outer_inner_init)] = ((float3)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n      },\n      for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 2; ++ff_c_outer_inner) {\n        for (int32_t rc_inner = 0; rc_inner < 10; ++rc_inner) {\n          for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n            int32_t cse_var_1 = ((nn_c_outer_outer_inner * 2) + ff_c_outer_inner);\n            int32_t3 v_ = int32_t3(((((((nn_outer_ff_outer_fused_yy_outer_fused / 43) * 140) + (nn_c_outer_outer_inner * 70)) + (rc_inner * 7)) + ry_inner))+(2*0), ((((((nn_outer_ff_outer_fused_yy_outer_fused / 43) * 140) + (nn_c_outer_outer_inner * 70)) + (rc_inner * 7)) + ry_inner))+(2*1), ((((((nn_outer_ff_outer_fused_yy_outer_fused / 43) * 140) + (nn_c_outer_outer_inner * 70)) + (rc_inner * 7)) + ry_inner))+(2*2));\n            ((float3*)conv1d_ncw_local)[cse_var_1] = (((float3*)conv1d_ncw_local)[cse_var_1] + ((float3(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2])) * ((float3)(((float*)kernel_1)[(((((nn_outer_ff_outer_fused_yy_outer_fused % 43) * 18) + (ff_c_outer_inner * 9)) + (rc_inner * 3)) + ry_inner)], ((float*)kernel_1)[(((((nn_outer_ff_outer_fused_yy_outer_fused % 43) * 18) + (ff_c_outer_inner * 9)) + (rc_inner * 3)) + ry_inner)], ((float*)kernel_1)[(((((nn_outer_ff_outer_fused_yy_outer_fused % 43) * 18) + (ff_c_outer_inner * 9)) + (rc_inner * 3)) + ry_inner)]))));\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 2; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 2; ++ff_inner) {\n        *(float3*)(((float*)conv1d_ncw_1) + (((((nn_outer_ff_outer_fused_yy_outer_fused / 43) * 516) + (nn_inner * 258)) + ((nn_outer_ff_outer_fused_yy_outer_fused % 43) * 6)) + (ff_inner * 3))) = ((float3*)conv1d_ncw_local)[((nn_inner * 2) + ff_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(43) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(43) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[4];\n  __shared__ float pad_temp_shared[60];\n  __shared__ float kernel_shared[774];\n  for (int nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 2; ++nn_c_outer_inner_init) {\n    conv1d_ncw_local[nn_c_outer_inner_init] = 0.000000e+00f;\n    conv1d_ncw_local[(nn_c_outer_inner_init + 2)] = 0.000000e+00f;\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 43) + ((int)threadIdx.x)) < 60) {\n      pad_temp_shared[((((((ax0_ax1_fused_ax2_fused_outer_outer * 43) + ((int)threadIdx.x)) / 30) * 30) + (((((ax0_ax1_fused_ax2_fused_outer_outer * 13) + ((int)threadIdx.x)) % 30) / 3) * 3)) + ((ax0_ax1_fused_ax2_fused_outer_outer + ((int)threadIdx.x)) % 3))] = data[((((((((int)blockIdx.x) / 3) * 140) + ((((ax0_ax1_fused_ax2_fused_outer_outer * 43) + ((int)threadIdx.x)) / 30) * 70)) + (((((ax0_ax1_fused_ax2_fused_outer_outer * 13) + ((int)threadIdx.x)) % 30) / 3) * 7)) + ((((int)blockIdx.x) % 3) * 2)) + ((ax0_ax1_fused_ax2_fused_outer_outer + ((int)threadIdx.x)) % 3))];\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 18; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 43) + ((int)threadIdx.x))] = kernel[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 43) + ((int)threadIdx.x))];\n  },\n  __syncthreads();\n  for (int ry_outer_inner = 0; ry_outer_inner < 3; ++ry_outer_inner) {\n    for (int nn_c_outer_inner = 0; nn_c_outer_inner < 2; ++nn_c_outer_inner) {\n      for (int rc_inner = 0; rc_inner < 10; ++rc_inner) {\n        conv1d_ncw_local[nn_c_outer_inner] = (conv1d_ncw_local[nn_c_outer_inner] + (pad_temp_shared[(((nn_c_outer_inner * 30) + (rc_inner * 3)) + ry_outer_inner)] * kernel_shared[(((((int)threadIdx.x) * 9) + (rc_inner * 3)) + ry_outer_inner)]));\n        conv1d_ncw_local[(nn_c_outer_inner + 2)] = (conv1d_ncw_local[(nn_c_outer_inner + 2)] + (pad_temp_shared[(((nn_c_outer_inner * 30) + (rc_inner * 3)) + ry_outer_inner)] * kernel_shared[((((((int)threadIdx.x) * 9) + (rc_inner * 3)) + ry_outer_inner) + 387)]));\n      },\n    },\n  },\n  for (int nn_inner = 0; nn_inner < 2; ++nn_inner) {\n    conv1d_ncw[(((((((int)blockIdx.x) / 3) * 516) + (nn_inner * 258)) + (((int)threadIdx.x) * 3)) + (((int)blockIdx.x) % 3))] = conv1d_ncw_local[nn_inner];\n    conv1d_ncw[((((((((int)blockIdx.x) / 3) * 516) + (nn_inner * 258)) + (((int)threadIdx.x) * 3)) + (((int)blockIdx.x) % 3)) + 129)] = conv1d_ncw_local[(nn_inner + 2)];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 10, 7), \"float32\"), kernel: T.Buffer((86, 3, 3), \"float32\"), conv1d_ncw: T.Buffer((6, 86, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_ff_outer_fused_yy_outer_fused in T.parallel(129):\n            conv1d_ncw_local = T.allocate([4], \"float32x3\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((4,), \"float32x3\", data=conv1d_ncw_local, scope=\"local\", align=32)\n            for nn_c_outer_outer_inner in range(2):\n                for ff_c_outer_inner_init in range(2):\n                    conv1d_ncw_local_1[nn_c_outer_outer_inner * 2 + ff_c_outer_inner_init] = T.Broadcast(T.float32(0), 3)\n                for ff_c_outer_inner, rc_inner, ry_inner in T.grid(2, 10, 3):\n                    cse_var_1: T.int32 = nn_c_outer_outer_inner * 2 + ff_c_outer_inner\n                    data_1 = T.Buffer((420,), data=data.data)\n                    kernel_1 = T.Buffer((774,), data=kernel.data)\n                    conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_outer_ff_outer_fused_yy_outer_fused // 43 * 140 + nn_c_outer_outer_inner * 70 + rc_inner * 7 + ry_inner:nn_outer_ff_outer_fused_yy_outer_fused // 43 * 140 + nn_c_outer_outer_inner * 70 + rc_inner * 7 + ry_inner + 6:2] * T.Broadcast(kernel_1[nn_outer_ff_outer_fused_yy_outer_fused % 43 * 18 + ff_c_outer_inner * 9 + rc_inner * 3 + ry_inner], 3)\n            for nn_inner, ff_inner in T.grid(2, 2):\n                conv1d_ncw_1 = T.Buffer((1548,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_outer_ff_outer_fused_yy_outer_fused // 43 * 516 + nn_inner * 258 + nn_outer_ff_outer_fused_yy_outer_fused % 43 * 6 + ff_inner * 3:nn_outer_ff_outer_fused_yy_outer_fused // 43 * 516 + nn_inner * 258 + nn_outer_ff_outer_fused_yy_outer_fused % 43 * 6 + ff_inner * 3 + 3] = conv1d_ncw_local_1[nn_inner * 2 + ff_inner]",
        "data": "6_10_7",
        "kernel": "86_3_3"
    },
    {
        "op_name": "conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_ff_outer_fused_yy_outer_fused = 0; nn_outer_ff_outer_fused_yy_outer_fused < 16; ++nn_outer_ff_outer_fused_yy_outer_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)80, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t ff_c_outer_outer_inner = 0; ff_c_outer_outer_inner < 2; ++ff_c_outer_outer_inner) {\n      for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 2; ++ff_c_outer_inner_init) {\n        for (int32_t nn_c_inner_init = 0; nn_c_inner_init < 5; ++nn_c_inner_init) {\n          ((float*)conv1d_ncw_local)[(((nn_c_inner_init * 4) + (ff_c_outer_outer_inner * 2)) + ff_c_outer_inner_init)] = 0.000000e+00f;\n        },\n      },\n      for (int32_t rc_outer = 0; rc_outer < 15; ++rc_outer) {\n        for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n          for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 2; ++ff_c_outer_inner) {\n            for (int32_t rc_inner = 0; rc_inner < 3; ++rc_inner) {\n              for (int32_t nn_c_inner = 0; nn_c_inner < 5; ++nn_c_inner) {\n                int32_t cse_var_3 = (rc_outer * 9);\n                int32_t cse_var_2 = (rc_inner * 3);\n                int32_t cse_var_1 = (((nn_c_inner * 4) + (ff_c_outer_outer_inner * 2)) + ff_c_outer_inner);\n                ((float*)conv1d_ncw_local)[cse_var_1] = (((float*)conv1d_ncw_local)[cse_var_1] + (((float*)data_1)[((((nn_c_inner * 135) + cse_var_3) + cse_var_2) + ry_outer)] * ((float*)kernel_1)[((((((nn_outer_ff_outer_fused_yy_outer_fused * 36) + (ff_c_outer_outer_inner * 18)) + (ff_c_outer_inner * 9)) + cse_var_3) + cse_var_2) + ry_outer)]));\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 5; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 4; ++ff_inner) {\n        ((float*)conv1d_ncw_1)[(((nn_inner * 64) + (nn_outer_ff_outer_fused_yy_outer_fused * 4)) + ff_inner)] = ((float*)conv1d_ncw_local)[((nn_inner * 4) + ff_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[10];\n  __shared__ float pad_temp_shared[225];\n  __shared__ float kernel_shared[192];\n  for (int nn_c_inner_init = 0; nn_c_inner_init < 5; ++nn_c_inner_init) {\n    for (int ff_c_inner_init = 0; ff_c_inner_init < 2; ++ff_c_inner_init) {\n      conv1d_ncw_local[((nn_c_inner_init * 2) + ff_c_inner_init)] = 0.000000e+00f;\n    },\n  },\n  for (int ry_outer_outer = 0; ry_outer_outer < 3; ++ry_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 8; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer * 32) + ((int)threadIdx.x)) < 225) {\n        pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 32) + ((int)threadIdx.x))] = data[(((ax0_ax1_fused_ax2_fused_outer_outer * 96) + (((int)threadIdx.x) * 3)) + ry_outer_outer)];\n      },\n    },\n    for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 6; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 32) + ((int)threadIdx.x))] = kernel[(((ax0_ax1_fused_ax2_fused_outer_outer_1 * 96) + (((int)threadIdx.x) * 3)) + ry_outer_outer)];\n    },\n    __syncthreads();\n    for (int rc_outer_inner = 0; rc_outer_inner < 45; ++rc_outer_inner) {\n      for (int nn_c_inner = 0; nn_c_inner < 5; ++nn_c_inner) {\n        for (int ff_c_inner = 0; ff_c_inner < 2; ++ff_c_inner) {\n          conv1d_ncw_local[((nn_c_inner * 2) + ff_c_inner)] = (conv1d_ncw_local[((nn_c_inner * 2) + ff_c_inner)] + (pad_temp_shared[((nn_c_inner * 45) + rc_outer_inner)] * kernel_shared[(((((int)threadIdx.x) * 6) + (ff_c_inner * 3)) + rc_outer_inner)]));\n        },\n      },\n    },\n  },\n  for (int nn_inner = 0; nn_inner < 5; ++nn_inner) {\n    for (int ff_inner = 0; ff_inner < 2; ++ff_inner) {\n      conv1d_ncw[(((nn_inner * 64) + (((int)threadIdx.x) * 2)) + ff_inner)] = conv1d_ncw_local[((nn_inner * 2) + ff_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 45, 3), \"float32\"), kernel: T.Buffer((64, 3, 3), \"float32\"), conv1d_ncw: T.Buffer((5, 64, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_ff_outer_fused_yy_outer_fused in T.parallel(16):\n            conv1d_ncw_local = T.allocate([20], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((20,), data=conv1d_ncw_local, scope=\"local\")\n            for ff_c_outer_outer_inner in range(2):\n                for ff_c_outer_inner_init, nn_c_inner_init in T.grid(2, 5):\n                    conv1d_ncw_local_1[nn_c_inner_init * 4 + ff_c_outer_outer_inner * 2 + ff_c_outer_inner_init] = T.float32(0)\n                for rc_outer, ry_outer, ff_c_outer_inner, rc_inner, nn_c_inner in T.grid(15, 3, 2, 3, 5):\n                    cse_var_3: T.int32 = rc_outer * 9\n                    cse_var_2: T.int32 = rc_inner * 3\n                    cse_var_1: T.int32 = nn_c_inner * 4 + ff_c_outer_outer_inner * 2 + ff_c_outer_inner\n                    data_1 = T.Buffer((675,), data=data.data)\n                    kernel_1 = T.Buffer((576,), data=kernel.data)\n                    conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_c_inner * 135 + cse_var_3 + cse_var_2 + ry_outer] * kernel_1[nn_outer_ff_outer_fused_yy_outer_fused * 36 + ff_c_outer_outer_inner * 18 + ff_c_outer_inner * 9 + cse_var_3 + cse_var_2 + ry_outer]\n            for nn_inner, ff_inner in T.grid(5, 4):\n                conv1d_ncw_1 = T.Buffer((320,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_inner * 64 + nn_outer_ff_outer_fused_yy_outer_fused * 4 + ff_inner] = conv1d_ncw_local_1[nn_inner * 4 + ff_inner]",
        "data": "5_45_3",
        "kernel": "64_3_3"
    },
    {
        "op_name": "conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_ff_outer_fused_yy_outer_fused = 0; nn_outer_ff_outer_fused_yy_outer_fused < 15; ++nn_outer_ff_outer_fused_yy_outer_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)72, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t nn_c_outer_outer_inner = 0; nn_c_outer_outer_inner < 2; ++nn_c_outer_outer_inner) {\n      for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 9; ++ff_c_outer_inner_init) {\n        ((float*)conv1d_ncw_local)[((nn_c_outer_outer_inner * 9) + ff_c_outer_inner_init)] = 0.000000e+00f;\n      },\n      for (int32_t rc_outer = 0; rc_outer < 22; ++rc_outer) {\n        for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 9; ++ff_c_outer_inner) {\n          for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n            int32_t cse_var_1 = ((nn_c_outer_outer_inner * 9) + ff_c_outer_inner);\n            ((float*)conv1d_ncw_local)[cse_var_1] = (((float*)conv1d_ncw_local)[cse_var_1] + (((float*)data_1)[(((((nn_outer_ff_outer_fused_yy_outer_fused / 5) * 176) + (nn_c_outer_outer_inner * 88)) + (rc_outer * 4)) + ry_inner)] * ((float*)kernel_1)[(((((nn_outer_ff_outer_fused_yy_outer_fused % 5) * 81) + (ff_c_outer_inner * 9)) + (rc_outer * 3)) + ry_inner)]));\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 2; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 9; ++ff_inner) {\n        ((float*)conv1d_ncw_1)[(((((nn_outer_ff_outer_fused_yy_outer_fused / 5) * 90) + (nn_inner * 45)) + ((nn_outer_ff_outer_fused_yy_outer_fused % 5) * 9)) + ff_inner)] = ((float*)conv1d_ncw_local)[((nn_inner * 9) + ff_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[3];\n  __shared__ float pad_temp_shared[198];\n  __shared__ float kernel_shared[81];\n  for (int ff_c_inner_init = 0; ff_c_inner_init < 3; ++ff_c_inner_init) {\n    conv1d_ncw_local[ff_c_inner_init] = 0.000000e+00f;\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 22; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 9) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) / 5) * 264) + (ax0_ax1_fused_ax2_fused_outer_outer * 12)) + ((((int)threadIdx.x) / 3) * 4)) + (((int)threadIdx.x) % 3))];\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 9; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 9) + ((int)threadIdx.x))] = kernel[((((((int)blockIdx.x) % 5) * 81) + (ax0_ax1_fused_ax2_fused_outer_outer_1 * 9)) + ((int)threadIdx.x))];\n  },\n  __syncthreads();\n  for (int rc_inner = 0; rc_inner < 22; ++rc_inner) {\n    for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n      for (int ff_c_inner = 0; ff_c_inner < 3; ++ff_c_inner) {\n        conv1d_ncw_local[ff_c_inner] = (conv1d_ncw_local[ff_c_inner] + (pad_temp_shared[((((((int)threadIdx.x) / 3) * 66) + (rc_inner * 3)) + ry_inner)] * kernel_shared[(((((((int)threadIdx.x) % 3) * 27) + (ff_c_inner * 9)) + (rc_inner * 3)) + ry_inner)]));\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 3; ++ff_inner) {\n    conv1d_ncw[((((((((int)blockIdx.x) / 5) * 135) + ((((int)threadIdx.x) / 3) * 45)) + ((((int)blockIdx.x) % 5) * 9)) + ((((int)threadIdx.x) % 3) * 3)) + ff_inner)] = conv1d_ncw_local[ff_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 22, 4), \"float32\"), kernel: T.Buffer((45, 3, 3), \"float32\"), conv1d_ncw: T.Buffer((6, 45, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_ff_outer_fused_yy_outer_fused in T.parallel(15):\n            conv1d_ncw_local = T.allocate([18], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((18,), data=conv1d_ncw_local, scope=\"local\")\n            for nn_c_outer_outer_inner in range(2):\n                for ff_c_outer_inner_init in range(9):\n                    conv1d_ncw_local_1[nn_c_outer_outer_inner * 9 + ff_c_outer_inner_init] = T.float32(0)\n                for rc_outer, ff_c_outer_inner, ry_inner in T.grid(22, 9, 3):\n                    cse_var_1: T.int32 = nn_c_outer_outer_inner * 9 + ff_c_outer_inner\n                    data_1 = T.Buffer((528,), data=data.data)\n                    kernel_1 = T.Buffer((405,), data=kernel.data)\n                    conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_outer_ff_outer_fused_yy_outer_fused // 5 * 176 + nn_c_outer_outer_inner * 88 + rc_outer * 4 + ry_inner] * kernel_1[nn_outer_ff_outer_fused_yy_outer_fused % 5 * 81 + ff_c_outer_inner * 9 + rc_outer * 3 + ry_inner]\n            for nn_inner, ff_inner in T.grid(2, 9):\n                conv1d_ncw_1 = T.Buffer((270,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_outer_ff_outer_fused_yy_outer_fused // 5 * 90 + nn_inner * 45 + nn_outer_ff_outer_fused_yy_outer_fused % 5 * 9 + ff_inner] = conv1d_ncw_local_1[nn_inner * 9 + ff_inner]",
        "data": "6_22_4",
        "kernel": "45_3_3"
    },
    {
        "op_name": "conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 37; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)8, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 2; ++ff_c_outer_inner_init) {\n      ((float*)conv1d_ncw_local)[ff_c_outer_inner_init] = 0.000000e+00f;\n    },\n    for (int32_t rc_outer = 0; rc_outer < 81; ++rc_outer) {\n      for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 2; ++ff_c_outer_inner) {\n        for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n          int32_t cse_var_1 = (rc_outer * 3);\n          ((float*)conv1d_ncw_local)[ff_c_outer_inner] = (((float*)conv1d_ncw_local)[ff_c_outer_inner] + (((float*)data_1)[(cse_var_1 + ry_inner)] * ((float*)kernel_1)[((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 18) + (ff_c_outer_inner * 9)) + cse_var_1) + ry_inner)]));\n        },\n      },\n    },\n    for (int32_t ff_inner = 0; ff_inner < 2; ++ff_inner) {\n      ((float*)conv1d_ncw_1)[((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 2) + ff_inner)] = ((float*)conv1d_ncw_local)[ff_inner];\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(37) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(37) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[1];\n  __shared__ float pad_temp_shared[243];\n  __shared__ float kernel_shared[333];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 3; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 37) + ((int)threadIdx.x)) < 81) {\n      *(float3*)(pad_temp_shared + ((ax0_ax1_fused_ax2_fused_outer_outer * 111) + (((int)threadIdx.x) * 3))) = *(float3*)(data + ((ax0_ax1_fused_ax2_fused_outer_outer * 111) + (((int)threadIdx.x) * 3)));\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 9; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 37) + ((int)threadIdx.x))] = kernel[(((((int)blockIdx.x) * 333) + (ax0_ax1_fused_ax2_fused_outer_outer_1 * 37)) + ((int)threadIdx.x))];\n  },\n  __syncthreads();\n  for (int rc_outer_inner = 0; rc_outer_inner < 81; ++rc_outer_inner) {\n    for (int ry_outer_inner = 0; ry_outer_inner < 3; ++ry_outer_inner) {\n      conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[((rc_outer_inner * 3) + ry_outer_inner)] * kernel_shared[(((((int)threadIdx.x) * 9) + (rc_outer_inner * 3)) + ry_outer_inner)]));\n    },\n  },\n  conv1d_ncw[((((int)blockIdx.x) * 37) + ((int)threadIdx.x))] = conv1d_ncw_local[0];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 81, 3), \"float32\"), kernel: T.Buffer((74, 3, 3), \"float32\"), conv1d_ncw: T.Buffer((1, 74, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(37):\n            conv1d_ncw_local = T.allocate([2], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((2,), data=conv1d_ncw_local, scope=\"local\", align=8)\n            for ff_c_outer_inner_init in range(2):\n                conv1d_ncw_local_1[ff_c_outer_inner_init] = T.float32(0)\n            for rc_outer, ff_c_outer_inner, ry_inner in T.grid(81, 2, 3):\n                cse_var_1: T.int32 = rc_outer * 3\n                data_1 = T.Buffer((243,), data=data.data)\n                kernel_1 = T.Buffer((666,), data=kernel.data)\n                conv1d_ncw_local_1[ff_c_outer_inner] = conv1d_ncw_local_1[ff_c_outer_inner] + data_1[cse_var_1 + ry_inner] * kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 18 + ff_c_outer_inner * 9 + cse_var_1 + ry_inner]\n            for ff_inner in range(2):\n                conv1d_ncw_1 = T.Buffer((74,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 2 + ff_inner] = conv1d_ncw_local_1[ff_inner]",
        "data": "1_81_3",
        "kernel": "74_3_3"
    },
    {
        "op_name": "conv1d_transpose_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t b_outer_c_outer_fused_w_outer_fused = 0; b_outer_c_outer_fused_w_outer_fused < 6806; ++b_outer_c_outer_fused_w_outer_fused) {\n    float data_dilate[72];\n    void* compute_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)24, 2, 32);\n    if (compute_local == NULL) {\n      return -1;\n    },\n    float kernel_2[16];\n    for (int32_t i0 = 0; i0 < 3; ++i0) {\n      for (int32_t i1 = 0; i1 < 6; ++i1) {\n        for (int32_t i2_s = 0; i2_s < 4; ++i2_s) {\n          if (3 <= (i2_s + (b_outer_c_outer_fused_w_outer_fused % 166))) {\n            if ((i2_s + (b_outer_c_outer_fused_w_outer_fused % 166)) < 166) {\n              int32_t cse_var_1 = (i2_s + (b_outer_c_outer_fused_w_outer_fused % 166));\n              data_dilate[(((i0 * 24) + (i1 * 4)) + i2_s)] = ((((cse_var_1 + 1) % 2) == 0) ? ((float*)data_1)[(((i0 * 492) + (i1 * 82)) + ((cse_var_1 - 3) >> 1))] : 0.000000e+00f);\n            },\n          },\n        },\n      },\n    },\n    for (int32_t i0_1 = 0; i0_1 < 3; ++i0_1) {\n      for (int32_t i1_1 = 0; i1_1 < 6; ++i1_1) {\n        for (int32_t i2_s_1 = 0; i2_s_1 < 4; ++i2_s_1) {\n          int32_t cse_var_3 = (i2_s_1 + (b_outer_c_outer_fused_w_outer_fused % 166));\n          int32_t cse_var_2 = (((i0_1 * 24) + (i1_1 * 4)) + i2_s_1);\n          data_dilate[cse_var_2] = (((3 <= cse_var_3) && (cse_var_3 < 166)) ? data_dilate[cse_var_2] : 0.000000e+00f);\n        },\n      },\n    },\n    for (int32_t b_c_outer_inner_init = 0; b_c_outer_inner_init < 3; ++b_c_outer_inner_init) {\n      for (int32_t c_c_inner_init = 0; c_c_inner_init < 2; ++c_c_inner_init) {\n        ((float*)compute_local)[((b_c_outer_inner_init * 2) + c_c_inner_init)] = 0.000000e+00f;\n      },\n    },\n    for (int32_t dc_outer = 0; dc_outer < 3; ++dc_outer) {\n      for (int32_t o = 0; o < 2; ++o) {\n        for (int32_t i = 0; i < 2; ++i) {\n          int32_t4 v_ = int32_t4(((((((dc_outer * 656) + (i * 328)) + ((b_outer_c_outer_fused_w_outer_fused / 166) * 8)) + (o * 4)) + 3))+(-1*0), ((((((dc_outer * 656) + (i * 328)) + ((b_outer_c_outer_fused_w_outer_fused / 166) * 8)) + (o * 4)) + 3))+(-1*1), ((((((dc_outer * 656) + (i * 328)) + ((b_outer_c_outer_fused_w_outer_fused / 166) * 8)) + (o * 4)) + 3))+(-1*2), ((((((dc_outer * 656) + (i * 328)) + ((b_outer_c_outer_fused_w_outer_fused / 166) * 8)) + (o * 4)) + 3))+(-1*3));\n          *(float4*)(kernel_2 + ((o * 8) + (i * 4))) = (float4(((float*)kernel_1)[v_.s0],((float*)kernel_1)[v_.s1],((float*)kernel_1)[v_.s2],((float*)kernel_1)[v_.s3]));\n        },\n      },\n      for (int32_t dw_outer = 0; dw_outer < 4; ++dw_outer) {\n        for (int32_t b_c_outer_inner = 0; b_c_outer_inner < 3; ++b_c_outer_inner) {\n          for (int32_t dc_inner = 0; dc_inner < 2; ++dc_inner) {\n            for (int32_t c_c_inner = 0; c_c_inner < 2; ++c_c_inner) {\n              int32_t cse_var_5 = (dc_inner * 4);\n              int32_t cse_var_4 = ((b_c_outer_inner * 2) + c_c_inner);\n              ((float*)compute_local)[cse_var_4] = (((float*)compute_local)[cse_var_4] + (data_dilate[((((b_c_outer_inner * 24) + (dc_outer * 8)) + cse_var_5) + dw_outer)] * kernel_2[(((c_c_inner * 8) + cse_var_5) + dw_outer)]));\n            },\n          },\n        },\n      },\n    },\n    for (int32_t b_inner = 0; b_inner < 3; ++b_inner) {\n      for (int32_t c_inner = 0; c_inner < 2; ++c_inner) {\n        ((float*)compute_1)[((((b_inner * 13612) + ((b_outer_c_outer_fused_w_outer_fused / 166) * 332)) + (c_inner * 166)) + (b_outer_c_outer_fused_w_outer_fused % 166))] = ((float*)compute_local)[((b_inner * 2) + c_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, compute_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(166) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(166) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel) {\n  float compute_local[6];\n  __shared__ float data_pad_shared[2988];\n  __shared__ float kernel_shared[12];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 3; ++b_c_outer_inner_init) {\n    for (int w_c_inner_init = 0; w_c_inner_init < 2; ++w_c_inner_init) {\n      compute_local[((b_c_outer_inner_init * 2) + w_c_inner_init)] = 0.000000e+00f;\n    },\n  },\n  for (int dw_outer_outer = 0; dw_outer_outer < 4; ++dw_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 18; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      data_pad_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 166) + ((int)threadIdx.x))] = ((((3 <= (((int)threadIdx.x) + dw_outer_outer)) && ((((int)threadIdx.x) + dw_outer_outer) < 166)) && ((((((int)threadIdx.x) + dw_outer_outer) + 1) % 2) == 0)) ? data[((ax0_ax1_fused_ax2_fused_outer_outer * 82) + (((dw_outer_outer + ((int)threadIdx.x)) - 3) >> 1))] : 0.000000e+00f);\n    },\n    if (((int)threadIdx.x) < 12) {\n      kernel_shared[((int)threadIdx.x)] = kernel[((((((((int)threadIdx.x) % 6) * 328) + (((int)blockIdx.x) * 8)) + ((((int)threadIdx.x) / 6) * 4)) + 3) - dw_outer_outer)];\n    },\n    __syncthreads();\n    for (int dc_outer_inner = 0; dc_outer_inner < 3; ++dc_outer_inner) {\n      for (int b_c_outer_inner = 0; b_c_outer_inner < 3; ++b_c_outer_inner) {\n        for (int dc_inner = 0; dc_inner < 2; ++dc_inner) {\n          for (int w_c_inner = 0; w_c_inner < 2; ++w_c_inner) {\n            compute_local[((b_c_outer_inner * 2) + w_c_inner)] = (compute_local[((b_c_outer_inner * 2) + w_c_inner)] + (data_pad_shared[(((((b_c_outer_inner * 996) + (dc_outer_inner * 332)) + (dc_inner * 166)) + ((((int)threadIdx.x) % 83) * 2)) + w_c_inner)] * kernel_shared[((((((int)threadIdx.x) / 83) * 6) + (dc_outer_inner * 2)) + dc_inner)]));\n          },\n        },\n      },\n    },\n  },\n  for (int b_inner = 0; b_inner < 3; ++b_inner) {\n    for (int w_inner = 0; w_inner < 2; ++w_inner) {\n      compute[((((b_inner * 13612) + (((int)blockIdx.x) * 332)) + (((int)threadIdx.x) * 2)) + w_inner)] = compute_local[((b_inner * 2) + w_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 6, 82), \"float32\"), kernel: T.Buffer((18, 82, 4), \"float32\"), compute: T.Buffer((3, 82, 166), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_c_outer_fused_w_outer_fused in T.parallel(6806):\n            data_dilate = T.allocate([72], \"float32\", \"global\")\n            compute_local = T.allocate([6], \"float32\", \"local\")\n            kernel_1 = T.allocate([16], \"float32\", \"global\")\n            data_dilate_1 = T.Buffer((72,), data=data_dilate)\n            for i0, i1, i2_s in T.grid(3, 6, 4):\n                if T.likely(3 <= i2_s + b_outer_c_outer_fused_w_outer_fused % 166):\n                    if T.likely(i2_s + b_outer_c_outer_fused_w_outer_fused % 166 < 166):\n                        data_1 = T.Buffer((1476,), data=data.data)\n                        cse_var_1: T.int32 = i2_s + b_outer_c_outer_fused_w_outer_fused % 166\n                        data_dilate_1[i0 * 24 + i1 * 4 + i2_s] = T.if_then_else((cse_var_1 + 1) % 2 == 0, data_1[i0 * 492 + i1 * 82 + (cse_var_1 - 3) // 2], T.float32(0))\n            data_dilate_2 = T.Buffer((72,), data=data_dilate)\n            for i0, i1, i2_s in T.grid(3, 6, 4):\n                cse_var_3: T.int32 = i2_s + b_outer_c_outer_fused_w_outer_fused % 166\n                cse_var_2: T.int32 = i0 * 24 + i1 * 4 + i2_s\n                data_dilate_2[cse_var_2] = T.if_then_else(3 <= cse_var_3 and cse_var_3 < 166, data_dilate_1[cse_var_2], T.float32(0))\n            compute_local_1 = T.Buffer((6,), data=compute_local, scope=\"local\", align=16)\n            for b_c_outer_inner_init, c_c_inner_init in T.grid(3, 2):\n                compute_local_1[b_c_outer_inner_init * 2 + c_c_inner_init] = T.float32(0)\n            for dc_outer in range(3):\n                kernel_2 = T.Buffer((16,), data=kernel_1)\n                for o, i in T.grid(2, 2):\n                    kernel_3 = T.Buffer((5904,), data=kernel.data)\n                    kernel_2[o * 8 + i * 4:o * 8 + i * 4 + 4] = kernel_3[dc_outer * 656 + i * 328 + b_outer_c_outer_fused_w_outer_fused // 166 * 8 + o * 4 + 3:dc_outer * 656 + i * 328 + b_outer_c_outer_fused_w_outer_fused // 166 * 8 + o * 4 + 3 + -4:-1]\n                for dw_outer, b_c_outer_inner, dc_inner, c_c_inner in T.grid(4, 3, 2, 2):\n                    cse_var_5: T.int32 = dc_inner * 4\n                    cse_var_4: T.int32 = b_c_outer_inner * 2 + c_c_inner\n                    compute_local_1[cse_var_4] = compute_local_1[cse_var_4] + data_dilate_2[b_c_outer_inner * 24 + dc_outer * 8 + cse_var_5 + dw_outer] * kernel_2[c_c_inner * 8 + cse_var_5 + dw_outer]\n            for b_inner, c_inner in T.grid(3, 2):\n                compute_1 = T.Buffer((40836,), data=compute.data)\n                compute_1[b_inner * 13612 + b_outer_c_outer_fused_w_outer_fused // 166 * 332 + c_inner * 166 + b_outer_c_outer_fused_w_outer_fused % 166] = compute_local_1[b_inner * 2 + c_inner]",
        "data": "3_6_82",
        "kernel": "18_82_4"
    },
    {
        "op_name": "conv1d_transpose_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t b_outer_c_outer_fused_w_outer_fused = 0; b_outer_c_outer_fused_w_outer_fused < 90; ++b_outer_c_outer_fused_w_outer_fused) {\n    void* data_pad = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1288, 2, 32);\n    if (data_pad == NULL) {\n      return -1;\n    },\n    float data_dilate[1];\n    void* compute_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1760, 2, 32);\n    if (compute_local == NULL) {\n      return -1;\n    },\n    void* kernel_2 = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1232, 2, 32);\n    if (kernel_2 == NULL) {\n      return -1;\n    },\n    for (int32_t i0 = 0; i0 < 2; ++i0) {\n      for (int32_t i1 = 0; i1 < 7; ++i1) {\n        for (int32_t i2 = 0; i2 < 23; ++i2) {\n          int32_t cse_var_1 = (b_outer_c_outer_fused_w_outer_fused % 10);\n          if (3 <= ((cse_var_1 * 20) + i2)) {\n            if (((i2 / 20) + cse_var_1) < 10) {\n              data_dilate[0] = ((((i2 + 1) % 2) == 0) ? ((float*)data_1)[((((i0 * 693) + (i1 * 99)) + (cse_var_1 * 10)) + ((i2 - 3) >> 1))] : 0.000000e+00f);\n            },\n          },\n          ((float*)data_pad)[(((i0 * 161) + (i1 * 23)) + i2)] = (((3 <= ((cse_var_1 * 20) + i2)) && (((i2 / 20) + cse_var_1) < 10)) ? data_dilate[0] : 0.000000e+00f);\n        },\n      },\n    },\n    for (int32_t w_c_outer_outer_inner = 0; w_c_outer_outer_inner < 10; ++w_c_outer_outer_inner) {\n      for (int32_t b_c_outer_inner_init = 0; b_c_outer_inner_init < 2; ++b_c_outer_inner_init) {\n        for (int32_t c_c_inner_init = 0; c_c_inner_init < 11; ++c_c_inner_init) {\n          *(float2*)(((float*)compute_local) + (((b_c_outer_inner_init * 220) + (c_c_inner_init * 20)) + (w_c_outer_outer_inner * 2))) = ((float2)(0.000000e+00f, 0.000000e+00f));\n        },\n      },\n      for (int32_t o = 0; o < 11; ++o) {\n        for (int32_t i = 0; i < 7; ++i) {\n          int32_t4 v_ = int32_t4((((((i * 396) + ((b_outer_c_outer_fused_w_outer_fused / 10) * 44)) + (o * 4)) + 3))+(-1*0), (((((i * 396) + ((b_outer_c_outer_fused_w_outer_fused / 10) * 44)) + (o * 4)) + 3))+(-1*1), (((((i * 396) + ((b_outer_c_outer_fused_w_outer_fused / 10) * 44)) + (o * 4)) + 3))+(-1*2), (((((i * 396) + ((b_outer_c_outer_fused_w_outer_fused / 10) * 44)) + (o * 4)) + 3))+(-1*3));\n          *(float4*)(((float*)kernel_2) + ((o * 28) + (i * 4))) = (float4(((float*)kernel_1)[v_.s0],((float*)kernel_1)[v_.s1],((float*)kernel_1)[v_.s2],((float*)kernel_1)[v_.s3]));\n        },\n      },\n      for (int32_t b_c_outer_inner = 0; b_c_outer_inner < 2; ++b_c_outer_inner) {\n        for (int32_t dc_inner = 0; dc_inner < 7; ++dc_inner) {\n          for (int32_t dw_inner = 0; dw_inner < 4; ++dw_inner) {\n            for (int32_t c_c_inner = 0; c_c_inner < 11; ++c_c_inner) {\n              int32_t cse_var_3 = (w_c_outer_outer_inner * 2);\n              int32_t cse_var_2 = (((b_c_outer_inner * 220) + (c_c_inner * 20)) + cse_var_3);\n              int32_t2 v__1 = int32_t2((cse_var_2)+(1*0), (cse_var_2)+(1*1));\n              int32_t2 v__2 = int32_t2((((((b_c_outer_inner * 161) + (dc_inner * 23)) + cse_var_3) + dw_inner))+(1*0), (((((b_c_outer_inner * 161) + (dc_inner * 23)) + cse_var_3) + dw_inner))+(1*1));\n              *(float2*)(((float*)compute_local) + cse_var_2) = ((float2(((float*)compute_local)[v__1.s0],((float*)compute_local)[v__1.s1])) + ((float2(((float*)data_pad)[v__2.s0],((float*)data_pad)[v__2.s1])) * ((float2)(((float*)kernel_2)[(((c_c_inner * 28) + (dc_inner * 4)) + dw_inner)], ((float*)kernel_2)[(((c_c_inner * 28) + (dc_inner * 4)) + dw_inner)]))));\n            },\n          },\n        },\n      },\n    },\n    for (int32_t b_inner = 0; b_inner < 2; ++b_inner) {\n      for (int32_t c_inner = 0; c_inner < 11; ++c_inner) {\n        for (int32_t w_inner = 0; w_inner < 20; ++w_inner) {\n          ((float*)compute_1)[(((((b_inner * 19800) + ((b_outer_c_outer_fused_w_outer_fused / 10) * 2200)) + (c_inner * 200)) + ((b_outer_c_outer_fused_w_outer_fused % 10) * 20)) + w_inner)] = ((float*)compute_local)[(((b_inner * 220) + (c_inner * 20)) + w_inner)];\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, kernel_2) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, compute_local) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, data_pad) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(225) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(225) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel) {\n  float compute_local[1];\n  __shared__ float data_pad_shared[196];\n  __shared__ float kernel_shared[252];\n  compute_local[0] = 0.000000e+00f;\n  if (((int)threadIdx.x) < 196) {\n    data_pad_shared[((int)threadIdx.x)] = ((((3 <= (((((int)blockIdx.x) & 7) * 25) + (((int)threadIdx.x) % 28))) && ((((((int)threadIdx.x) % 28) / 25) + (((int)blockIdx.x) & 7)) < 8)) && ((((((((int)blockIdx.x) & 7) * 25) + (((int)threadIdx.x) % 28)) + 1) % 2) == 0)) ? data[((((((int)blockIdx.x) / 88) * 693) + ((((int)threadIdx.x) / 28) * 99)) + (((((((int)blockIdx.x) & 7) * 25) + (((int)threadIdx.x) % 28)) - 3) >> 1))] : 0.000000e+00f);\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 25) + (((int)threadIdx.x) / 9)) < 28) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 225) + ((int)threadIdx.x))] = kernel[((((((((ax0_ax1_fused_ax2_fused_outer_outer + ((int)threadIdx.x)) % 28) >> 2) * 396) + (((((int)blockIdx.x) % 88) >> 3) * 36)) + ((((ax0_ax1_fused_ax2_fused_outer_outer * 225) + ((int)threadIdx.x)) / 28) * 4)) + 3) - ((ax0_ax1_fused_ax2_fused_outer_outer + ((int)threadIdx.x)) & 3))];\n    },\n  },\n  __syncthreads();\n  for (int dc_inner = 0; dc_inner < 7; ++dc_inner) {\n    for (int dw_inner = 0; dw_inner < 4; ++dw_inner) {\n      compute_local[0] = (compute_local[0] + (data_pad_shared[(((dc_inner * 28) + dw_inner) + (((int)threadIdx.x) % 25))] * kernel_shared[((((((int)threadIdx.x) / 25) * 28) + (dc_inner * 4)) + dw_inner)]));\n    },\n  },\n  compute[(((((((int)blockIdx.x) >> 3) * 1800) + ((((int)threadIdx.x) / 25) * 200)) + ((((int)blockIdx.x) & 7) * 25)) + (((int)threadIdx.x) % 25))] = compute_local[0];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 7, 99), \"float32\"), kernel: T.Buffer((48, 99, 4), \"float32\"), compute: T.Buffer((2, 99, 200), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_c_outer_fused_w_outer_fused in T.parallel(90):\n            data_pad = T.allocate([322], \"float32\", \"global\")\n            data_dilate = T.allocate([1], \"float32\", \"global\")\n            compute_local = T.allocate([440], \"float32\", \"local\")\n            kernel_1 = T.allocate([308], \"float32\", \"global\")\n            data_pad_1 = T.Buffer((322,), data=data_pad)\n            for i0, i1, i2 in T.grid(2, 7, 23):\n                cse_var_1: T.int32 = b_outer_c_outer_fused_w_outer_fused % 10\n                data_dilate_1 = T.Buffer((1,), data=data_dilate, align=4)\n                if T.likely(3 <= cse_var_1 * 20 + i2):\n                    if T.likely(i2 // 20 + cse_var_1 < 10):\n                        data_1 = T.Buffer((1386,), data=data.data)\n                        data_dilate_1[0] = T.if_then_else((i2 + 1) % 2 == 0, data_1[i0 * 693 + i1 * 99 + cse_var_1 * 10 + (i2 - 3) // 2], T.float32(0))\n                data_pad_1[i0 * 161 + i1 * 23 + i2] = T.if_then_else(3 <= cse_var_1 * 20 + i2 and i2 // 20 + cse_var_1 < 10, data_dilate_1[0], T.float32(0))\n            compute_local_1 = T.Buffer((440,), data=compute_local, scope=\"local\")\n            for w_c_outer_outer_inner in range(10):\n                for b_c_outer_inner_init, c_c_inner_init in T.grid(2, 11):\n                    compute_local_1[b_c_outer_inner_init * 220 + c_c_inner_init * 20 + w_c_outer_outer_inner * 2:b_c_outer_inner_init * 220 + c_c_inner_init * 20 + w_c_outer_outer_inner * 2 + 2] = T.Broadcast(T.float32(0), 2)\n                kernel_2 = T.Buffer((308,), data=kernel_1)\n                for o, i in T.grid(11, 7):\n                    kernel_3 = T.Buffer((19008,), data=kernel.data)\n                    kernel_2[o * 28 + i * 4:o * 28 + i * 4 + 4] = kernel_3[i * 396 + b_outer_c_outer_fused_w_outer_fused // 10 * 44 + o * 4 + 3:i * 396 + b_outer_c_outer_fused_w_outer_fused // 10 * 44 + o * 4 + 3 + -4:-1]\n                for b_c_outer_inner, dc_inner, dw_inner, c_c_inner in T.grid(2, 7, 4, 11):\n                    cse_var_3: T.int32 = w_c_outer_outer_inner * 2\n                    cse_var_2: T.int32 = b_c_outer_inner * 220 + c_c_inner * 20 + cse_var_3\n                    compute_local_1[cse_var_2:cse_var_2 + 2] = compute_local_1[cse_var_2:cse_var_2 + 2] + data_pad_1[b_c_outer_inner * 161 + dc_inner * 23 + cse_var_3 + dw_inner:b_c_outer_inner * 161 + dc_inner * 23 + cse_var_3 + dw_inner + 2] * T.Broadcast(kernel_2[c_c_inner * 28 + dc_inner * 4 + dw_inner], 2)\n            for b_inner, c_inner, w_inner in T.grid(2, 11, 20):\n                compute_1 = T.Buffer((39600,), data=compute.data)\n                compute_1[b_inner * 19800 + b_outer_c_outer_fused_w_outer_fused // 10 * 2200 + c_inner * 200 + b_outer_c_outer_fused_w_outer_fused % 10 * 20 + w_inner] = compute_local_1[b_inner * 220 + c_inner * 20 + w_inner]",
        "data": "2_7_99",
        "kernel": "48_99_4"
    },
    {
        "op_name": "conv1d_transpose_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused < 168; ++b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused) {\n    void* data_dilate = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)8880, 2, 32);\n    if (data_dilate == NULL) {\n      return -1;\n    },\n    void* data_pad = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)3280, 2, 32);\n    if (data_pad == NULL) {\n      return -1;\n    },\n    float kernel_2[40];\n    for (int32_t i0 = 0; i0 < 2; ++i0) {\n      for (int32_t i1 = 0; i1 < 10; ++i1) {\n        for (int32_t i2 = 0; i2 < 111; ++i2) {\n          ((float*)data_dilate)[(((i0 * 1110) + (i1 * 111)) + i2)] = (((i2 % 2) == 0) ? ((float*)data_1)[(((((b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused / 56) * 1120) + (i0 * 560)) + (i1 * 56)) + (i2 >> 1))] : 0.000000e+00f);\n        },\n      },\n    },\n    for (int32_t w_outer_outer_inner = 0; w_outer_outer_inner < 3; ++w_outer_outer_inner) {\n      for (int32_t i0_1 = 0; i0_1 < 2; ++i0_1) {\n        for (int32_t i1_1 = 0; i1_1 < 10; ++i1_1) {\n          for (int32_t i2_1 = 0; i2_1 < 41; ++i2_1) {\n            int32_t cse_var_1 = (w_outer_outer_inner * 38);\n            ((float*)data_pad)[(((i0_1 * 410) + (i1_1 * 41)) + i2_1)] = (((3 <= (cse_var_1 + i2_1)) && (((i2_1 / 38) + w_outer_outer_inner) < 3)) ? ((float*)data_dilate)[(((((i0_1 * 1110) + (i1_1 * 111)) + cse_var_1) + i2_1) - 3)] : 0.000000e+00f);\n          },\n        },\n      },\n      for (int32_t i = 0; i < 10; ++i) {\n        int32_t4 v_ = int32_t4(((((i * 224) + ((b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 56) * 4)) + 3))+(-1*0), ((((i * 224) + ((b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 56) * 4)) + 3))+(-1*1), ((((i * 224) + ((b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 56) * 4)) + 3))+(-1*2), ((((i * 224) + ((b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 56) * 4)) + 3))+(-1*3));\n        *(float4*)(kernel_2 + (i * 4)) = (float4(((float*)kernel_1)[v_.s0],((float*)kernel_1)[v_.s1],((float*)kernel_1)[v_.s2],((float*)kernel_1)[v_.s3]));\n      },\n      for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 2; ++b_outer_inner_init) {\n        for (int32_t w_outer_inner_init = 0; w_outer_inner_init < 19; ++w_outer_inner_init) {\n          *(float2*)(((float*)compute_1) + ((((((b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused / 56) * 12768) + (b_outer_inner_init * 6384)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 56) * 114)) + (w_outer_outer_inner * 38)) + (w_outer_inner_init * 2))) = ((float2)(0.000000e+00f, 0.000000e+00f));\n        },\n      },\n      for (int32_t dc_outer = 0; dc_outer < 10; ++dc_outer) {\n        for (int32_t dw_outer = 0; dw_outer < 4; ++dw_outer) {\n          for (int32_t b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n            for (int32_t w_outer_inner = 0; w_outer_inner < 19; ++w_outer_inner) {\n              int32_t cse_var_3 = (w_outer_inner * 2);\n              int32_t cse_var_2 = ((((((b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused / 56) * 12768) + (b_outer_inner * 6384)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 56) * 114)) + (w_outer_outer_inner * 38)) + cse_var_3);\n              int32_t2 v__1 = int32_t2((cse_var_2)+(1*0), (cse_var_2)+(1*1));\n              int32_t2 v__2 = int32_t2((((((b_outer_inner * 410) + (dc_outer * 41)) + cse_var_3) + dw_outer))+(1*0), (((((b_outer_inner * 410) + (dc_outer * 41)) + cse_var_3) + dw_outer))+(1*1));\n              *(float2*)(((float*)compute_1) + cse_var_2) = ((float2(((float*)compute_1)[v__1.s0],((float*)compute_1)[v__1.s1])) + ((float2(((float*)data_pad)[v__2.s0],((float*)data_pad)[v__2.s1])) * ((float2)(kernel_2[((dc_outer * 4) + dw_outer)], kernel_2[((dc_outer * 4) + dw_outer)]))));\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, data_pad) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, data_dilate) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(76) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(76) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel) {\n  float compute_local[4];\n  __shared__ float data_pad_shared[440];\n  __shared__ float kernel_shared[320];\n  for (int c_c_outer_inner_init = 0; c_c_outer_inner_init < 2; ++c_c_outer_inner_init) {\n    for (int b_c_inner_init = 0; b_c_inner_init < 2; ++b_c_inner_init) {\n      compute_local[((b_c_inner_init * 2) + c_c_outer_inner_init)] = 0.000000e+00f;\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 6; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 19) + (((int)threadIdx.x) >> 2)) < 110) {\n      data_pad_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 76) + ((int)threadIdx.x))] = ((((3 <= (((((int)blockIdx.x) % 6) * 19) + (((ax0_ax1_fused_ax2_fused_outer_outer * 10) + ((int)threadIdx.x)) % 22))) && ((((((ax0_ax1_fused_ax2_fused_outer_outer * 10) + ((int)threadIdx.x)) % 22) / 19) + (((int)blockIdx.x) % 6)) < 6)) && ((((((((int)blockIdx.x) % 6) * 19) + (((ax0_ax1_fused_ax2_fused_outer_outer * 10) + ((int)threadIdx.x)) % 22)) + 1) % 2) == 0)) ? data[((((((int)blockIdx.x) / 42) * 1120) + ((((ax0_ax1_fused_ax2_fused_outer_outer * 38) + (((int)threadIdx.x) >> 1)) / 11) * 56)) + (((((((int)blockIdx.x) % 6) * 19) + (((ax0_ax1_fused_ax2_fused_outer_outer * 10) + ((int)threadIdx.x)) % 22)) - 3) >> 1))] : 0.000000e+00f);\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 2; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer_1 * 19) + (((int)threadIdx.x) >> 2)) < 20) {\n      int4 v_ = make_int4(((((((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 6) + ((int)threadIdx.x)) % 10) * 224) + (((((int)blockIdx.x) % 42) / 6) * 32)) + ((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 38) + (((int)threadIdx.x) >> 1)) / 5) * 4)) + 3))+(-1*0), ((((((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 6) + ((int)threadIdx.x)) % 10) * 224) + (((((int)blockIdx.x) % 42) / 6) * 32)) + ((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 38) + (((int)threadIdx.x) >> 1)) / 5) * 4)) + 3))+(-1*1), ((((((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 6) + ((int)threadIdx.x)) % 10) * 224) + (((((int)blockIdx.x) % 42) / 6) * 32)) + ((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 38) + (((int)threadIdx.x) >> 1)) / 5) * 4)) + 3))+(-1*2), ((((((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 6) + ((int)threadIdx.x)) % 10) * 224) + (((((int)blockIdx.x) % 42) / 6) * 32)) + ((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 38) + (((int)threadIdx.x) >> 1)) / 5) * 4)) + 3))+(-1*3));\n      *(float4*)(kernel_shared + ((ax0_ax1_fused_ax2_fused_outer_outer_1 * 304) + (((int)threadIdx.x) * 4))) = make_float4(kernel[v_.x],kernel[v_.y],kernel[v_.z],kernel[v_.w]);\n    },\n  },\n  __syncthreads();\n  for (int dw_outer_inner = 0; dw_outer_inner < 2; ++dw_outer_inner) {\n    for (int c_c_outer_inner = 0; c_c_outer_inner < 2; ++c_c_outer_inner) {\n      for (int dc_inner = 0; dc_inner < 10; ++dc_inner) {\n        for (int dw_inner = 0; dw_inner < 2; ++dw_inner) {\n          for (int b_c_inner = 0; b_c_inner < 2; ++b_c_inner) {\n            compute_local[((b_c_inner * 2) + c_c_outer_inner)] = (compute_local[((b_c_inner * 2) + c_c_outer_inner)] + (data_pad_shared[(((((b_c_inner * 220) + (dc_inner * 22)) + (dw_outer_inner * 2)) + dw_inner) + (((int)threadIdx.x) % 19))] * kernel_shared[((((((((int)threadIdx.x) / 19) * 80) + (c_c_outer_inner * 40)) + (dc_inner * 4)) + (dw_outer_inner * 2)) + dw_inner)]));\n          },\n        },\n      },\n    },\n  },\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    for (int c_inner = 0; c_inner < 2; ++c_inner) {\n      compute[((((((((((int)blockIdx.x) / 42) * 12768) + (b_inner * 6384)) + (((((int)blockIdx.x) % 42) / 6) * 912)) + ((((int)threadIdx.x) / 19) * 228)) + (c_inner * 114)) + ((((int)blockIdx.x) % 6) * 19)) + (((int)threadIdx.x) % 19))] = compute_local[((b_inner * 2) + c_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 10, 56), \"float32\"), kernel: T.Buffer((68, 56, 4), \"float32\"), compute: T.Buffer((6, 56, 114), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused in T.parallel(168):\n            data_dilate = T.allocate([2220], \"float32\", \"global\")\n            data_pad = T.allocate([820], \"float32\", \"global\")\n            kernel_1 = T.allocate([40], \"float32\", \"global\")\n            data_dilate_1 = T.Buffer((2220,), data=data_dilate)\n            for i0, i1, i2 in T.grid(2, 10, 111):\n                data_1 = T.Buffer((3360,), data=data.data)\n                data_dilate_1[i0 * 1110 + i1 * 111 + i2] = T.if_then_else(i2 % 2 == 0, data_1[b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused // 56 * 1120 + i0 * 560 + i1 * 56 + i2 // 2], T.float32(0))\n            for w_outer_outer_inner in range(3):\n                data_pad_1 = T.Buffer((820,), data=data_pad)\n                for i0, i1, i2 in T.grid(2, 10, 41):\n                    cse_var_1: T.int32 = w_outer_outer_inner * 38\n                    data_pad_1[i0 * 410 + i1 * 41 + i2] = T.if_then_else(3 <= cse_var_1 + i2 and i2 // 38 + w_outer_outer_inner < 3, data_dilate_1[i0 * 1110 + i1 * 111 + cse_var_1 + i2 - 3], T.float32(0))\n                kernel_2 = T.Buffer((40,), data=kernel_1)\n                for i in range(10):\n                    kernel_3 = T.Buffer((15232,), data=kernel.data)\n                    kernel_2[i * 4:i * 4 + 4] = kernel_3[i * 224 + b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 56 * 4 + 3:i * 224 + b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 56 * 4 + 3 + -4:-1]\n                compute_1 = T.Buffer((38304,), data=compute.data)\n                for b_outer_inner_init, w_outer_inner_init in T.grid(2, 19):\n                    compute_1[b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused // 56 * 12768 + b_outer_inner_init * 6384 + b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 56 * 114 + w_outer_outer_inner * 38 + w_outer_inner_init * 2:b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused // 56 * 12768 + b_outer_inner_init * 6384 + b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 56 * 114 + w_outer_outer_inner * 38 + w_outer_inner_init * 2 + 2] = T.Broadcast(T.float32(0), 2)\n                for dc_outer, dw_outer, b_outer_inner, w_outer_inner in T.grid(10, 4, 2, 19):\n                    cse_var_3: T.int32 = w_outer_inner * 2\n                    cse_var_2: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused // 56 * 12768 + b_outer_inner * 6384 + b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 56 * 114 + w_outer_outer_inner * 38 + cse_var_3\n                    compute_1[cse_var_2:cse_var_2 + 2] = compute_1[cse_var_2:cse_var_2 + 2] + data_pad_1[b_outer_inner * 410 + dc_outer * 41 + cse_var_3 + dw_outer:b_outer_inner * 410 + dc_outer * 41 + cse_var_3 + dw_outer + 2] * T.Broadcast(kernel_2[dc_outer * 4 + dw_outer], 2)",
        "data": "6_10_56",
        "kernel": "68_56_4"
    },
    {
        "op_name": "conv1d_transpose_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused < 17; ++b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused) {\n    void* data_pad = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)7560, 2, 32);\n    if (data_pad == NULL) {\n      return -1;\n    },\n    float data_dilate[1];\n    float kernel_2[27];\n    for (int32_t i0 = 0; i0 < 2; ++i0) {\n      for (int32_t i1 = 0; i1 < 9; ++i1) {\n        for (int32_t i2 = 0; i2 < 105; ++i2) {\n          if (2 <= i2) {\n            if (i2 < 103) {\n              data_dilate[0] = (((i2 % 2) == 0) ? ((float*)data_1)[((((i0 * 459) + (i1 * 51)) + (i2 >> 1)) - 1)] : 0.000000e+00f);\n            },\n          },\n          ((float*)data_pad)[(((i0 * 945) + (i1 * 105)) + i2)] = (((2 <= i2) && (i2 < 103)) ? data_dilate[0] : 0.000000e+00f);\n        },\n      },\n    },\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 2; ++b_outer_inner_init) {\n      for (int32_t w_outer_inner_init = 0; w_outer_inner_init < 103; ++w_outer_inner_init) {\n        for (int32_t c_inner_init = 0; c_inner_init < 3; ++c_inner_init) {\n          ((float*)compute_1)[((((b_outer_inner_init * 5253) + (b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused * 309)) + (c_inner_init * 103)) + w_outer_inner_init)] = 0.000000e+00f;\n        },\n      },\n    },\n    for (int32_t dw_outer = 0; dw_outer < 3; ++dw_outer) {\n      for (int32_t o = 0; o < 3; ++o) {\n        for (int32_t i = 0; i < 9; ++i) {\n          kernel_2[((o * 9) + i)] = ((float*)kernel_1)[(((((i * 153) + (b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused * 9)) + (o * 3)) + 2) - dw_outer)];\n        },\n      },\n      for (int32_t b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n        for (int32_t w_outer_inner = 0; w_outer_inner < 103; ++w_outer_inner) {\n          for (int32_t dc_inner = 0; dc_inner < 9; ++dc_inner) {\n            for (int32_t c_inner = 0; c_inner < 3; ++c_inner) {\n              int32_t cse_var_1 = ((((b_outer_inner * 5253) + (b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused * 309)) + (c_inner * 103)) + w_outer_inner);\n              ((float*)compute_1)[cse_var_1] = (((float*)compute_1)[cse_var_1] + (((float*)data_pad)[((((b_outer_inner * 945) + (dc_inner * 105)) + w_outer_inner) + dw_outer)] * kernel_2[((c_inner * 9) + dc_inner)]));\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, data_pad) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(206) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(206) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel) {\n  float compute_local[3];\n  __shared__ float data_pad_shared[1890];\n  __shared__ float kernel_shared[81];\n  compute_local[0] = 0.000000e+00f;\n  compute_local[1] = 0.000000e+00f;\n  compute_local[2] = 0.000000e+00f;\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 10; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 103) + (((int)threadIdx.x) >> 1)) < 945) {\n      data_pad_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 206) + ((int)threadIdx.x))] = ((((2 <= (((ax0_ax1_fused_ax2_fused_outer_outer * 101) + ((int)threadIdx.x)) % 105)) && ((((ax0_ax1_fused_ax2_fused_outer_outer * 101) + ((int)threadIdx.x)) % 105) < 103)) && (((((ax0_ax1_fused_ax2_fused_outer_outer * 101) + ((int)threadIdx.x)) % 105) % 2) == 0)) ? data[((((((ax0_ax1_fused_ax2_fused_outer_outer * 206) + ((int)threadIdx.x)) / 105) * 51) + ((((ax0_ax1_fused_ax2_fused_outer_outer * 101) + ((int)threadIdx.x)) % 105) >> 1)) - 1)] : 0.000000e+00f);\n    },\n  },\n  if (((int)threadIdx.x) < 27) {\n    int3 v_ = make_int3(((((((((int)threadIdx.x) % 9) * 153) + (((int)blockIdx.x) * 9)) + ((((int)threadIdx.x) / 9) * 3)) + 2))+(-1*0), ((((((((int)threadIdx.x) % 9) * 153) + (((int)blockIdx.x) * 9)) + ((((int)threadIdx.x) / 9) * 3)) + 2))+(-1*1), ((((((((int)threadIdx.x) % 9) * 153) + (((int)blockIdx.x) * 9)) + ((((int)threadIdx.x) / 9) * 3)) + 2))+(-1*2));\n    *(float3*)(kernel_shared + (((int)threadIdx.x) * 3)) = make_float3(kernel[v_.x],kernel[v_.y],kernel[v_.z]);\n  },\n  __syncthreads();\n  for (int dc_outer_inner = 0; dc_outer_inner < 3; ++dc_outer_inner) {\n    for (int dw_outer_inner = 0; dw_outer_inner < 3; ++dw_outer_inner) {\n      for (int dc_inner = 0; dc_inner < 3; ++dc_inner) {\n        compute_local[0] = (compute_local[0] + (data_pad_shared[((((((((int)threadIdx.x) / 103) * 945) + (dc_outer_inner * 315)) + (dc_inner * 105)) + dw_outer_inner) + (((int)threadIdx.x) % 103))] * kernel_shared[(((dc_outer_inner * 9) + (dc_inner * 3)) + dw_outer_inner)]));\n        compute_local[1] = (compute_local[1] + (data_pad_shared[((((((((int)threadIdx.x) / 103) * 945) + (dc_outer_inner * 315)) + (dc_inner * 105)) + dw_outer_inner) + (((int)threadIdx.x) % 103))] * kernel_shared[((((dc_outer_inner * 9) + (dc_inner * 3)) + dw_outer_inner) + 27)]));\n        compute_local[2] = (compute_local[2] + (data_pad_shared[((((((((int)threadIdx.x) / 103) * 945) + (dc_outer_inner * 315)) + (dc_inner * 105)) + dw_outer_inner) + (((int)threadIdx.x) % 103))] * kernel_shared[((((dc_outer_inner * 9) + (dc_inner * 3)) + dw_outer_inner) + 54)]));\n      },\n    },\n  },\n  compute[((((((int)threadIdx.x) / 103) * 5253) + (((int)blockIdx.x) * 309)) + (((int)threadIdx.x) % 103))] = compute_local[0];\n  compute[(((((((int)threadIdx.x) / 103) * 5253) + (((int)blockIdx.x) * 309)) + (((int)threadIdx.x) % 103)) + 103)] = compute_local[1];\n  compute[(((((((int)threadIdx.x) / 103) * 5253) + (((int)blockIdx.x) * 309)) + (((int)threadIdx.x) % 103)) + 206)] = compute_local[2];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 9, 51), \"float32\"), kernel: T.Buffer((40, 51, 3), \"float32\"), compute: T.Buffer((2, 51, 103), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused in T.parallel(17):\n            data_pad = T.allocate([1890], \"float32\", \"global\")\n            data_dilate = T.allocate([1], \"float32\", \"global\")\n            kernel_1 = T.allocate([27], \"float32\", \"global\")\n            data_pad_1 = T.Buffer((1890,), data=data_pad)\n            for i0, i1, i2 in T.grid(2, 9, 105):\n                data_dilate_1 = T.Buffer((1,), data=data_dilate, align=4)\n                if T.likely(2 <= i2):\n                    if T.likely(i2 < 103):\n                        data_1 = T.Buffer((918,), data=data.data)\n                        data_dilate_1[0] = T.if_then_else(i2 % 2 == 0, data_1[i0 * 459 + i1 * 51 + i2 // 2 - 1], T.float32(0))\n                data_pad_1[i0 * 945 + i1 * 105 + i2] = T.if_then_else(2 <= i2 and i2 < 103, data_dilate_1[0], T.float32(0))\n            compute_1 = T.Buffer((10506,), data=compute.data)\n            for b_outer_inner_init, w_outer_inner_init, c_inner_init in T.grid(2, 103, 3):\n                compute_1[b_outer_inner_init * 5253 + b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused * 309 + c_inner_init * 103 + w_outer_inner_init] = T.float32(0)\n            for dw_outer in range(3):\n                kernel_2 = T.Buffer((27,), data=kernel_1)\n                for o, i in T.grid(3, 9):\n                    kernel_3 = T.Buffer((6120,), data=kernel.data)\n                    kernel_2[o * 9 + i] = kernel_3[i * 153 + b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused * 9 + o * 3 + 2 - dw_outer]\n                for b_outer_inner, w_outer_inner, dc_inner, c_inner in T.grid(2, 103, 9, 3):\n                    cse_var_1: T.int32 = b_outer_inner * 5253 + b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused * 309 + c_inner * 103 + w_outer_inner\n                    compute_1[cse_var_1] = compute_1[cse_var_1] + data_pad_1[b_outer_inner * 945 + dc_inner * 105 + w_outer_inner + dw_outer] * kernel_2[c_inner * 9 + dc_inner]",
        "data": "2_9_51",
        "kernel": "40_51_3"
    },
    {
        "op_name": "conv1d_transpose_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_c_outer_outer_fused_w_outer_outer_fused = 0; b_outer_outer_c_outer_outer_fused_w_outer_outer_fused < 28; ++b_outer_outer_c_outer_outer_fused_w_outer_outer_fused) {\n    void* kernel_2 = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1296, 2, 32);\n    if (kernel_2 == NULL) {\n      return -1;\n    },\n    void* data_pad = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1260, 2, 32);\n    if (data_pad == NULL) {\n      return -1;\n    },\n    float data_dilate[35];\n    void* compute_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)384, 2, 32);\n    if (compute_local == NULL) {\n      return -1;\n    },\n    for (int32_t o = 0; o < 9; ++o) {\n      for (int32_t i = 0; i < 9; ++i) {\n        int32_t4 v_ = int32_t4((((((i * 252) + ((b_outer_outer_c_outer_outer_fused_w_outer_outer_fused >> 2) * 36)) + (o * 4)) + 3))+(-1*0), (((((i * 252) + ((b_outer_outer_c_outer_outer_fused_w_outer_outer_fused >> 2) * 36)) + (o * 4)) + 3))+(-1*1), (((((i * 252) + ((b_outer_outer_c_outer_outer_fused_w_outer_outer_fused >> 2) * 36)) + (o * 4)) + 3))+(-1*2), (((((i * 252) + ((b_outer_outer_c_outer_outer_fused_w_outer_outer_fused >> 2) * 36)) + (o * 4)) + 3))+(-1*3));\n        *(float4*)(((float*)kernel_2) + ((o * 36) + (i * 4))) = (float4(((float*)kernel_1)[v_.s0],((float*)kernel_1)[v_.s1],((float*)kernel_1)[v_.s2],((float*)kernel_1)[v_.s3]));\n      },\n    },\n    for (int32_t c_outer_inner = 0; c_outer_inner < 3; ++c_outer_inner) {\n      for (int32_t i1 = 0; i1 < 9; ++i1) {\n        for (int32_t i2 = 0; i2 < 35; ++i2) {\n          if (3 <= (((b_outer_outer_c_outer_outer_fused_w_outer_outer_fused & 3) * 32) + i2)) {\n            if (((i2 >> 5) + (b_outer_outer_c_outer_outer_fused_w_outer_outer_fused & 3)) < 4) {\n              data_dilate[i2] = ((((i2 + 1) % 2) == 0) ? ((float*)data_1)[(((i1 * 63) + ((b_outer_outer_c_outer_outer_fused_w_outer_outer_fused & 3) * 16)) + ((i2 - 3) >> 1))] : 0.000000e+00f);\n            },\n          },\n        },\n        for (int32_t i2_1 = 0; i2_1 < 35; ++i2_1) {\n          int32_t cse_var_1 = (b_outer_outer_c_outer_outer_fused_w_outer_outer_fused & 3);\n          ((float*)data_pad)[((i1 * 35) + i2_1)] = (((3 <= ((cse_var_1 * 32) + i2_1)) && (((i2_1 >> 5) + cse_var_1) < 4)) ? data_dilate[i2_1] : 0.000000e+00f);\n        },\n      },\n      for (int32_t w_c_outer_inner_init = 0; w_c_outer_inner_init < 32; ++w_c_outer_inner_init) {\n        for (int32_t c_c_inner_init = 0; c_c_inner_init < 3; ++c_c_inner_init) {\n          ((float*)compute_local)[((c_c_inner_init * 32) + w_c_outer_inner_init)] = 0.000000e+00f;\n        },\n      },\n      for (int32_t dc_outer = 0; dc_outer < 3; ++dc_outer) {\n        for (int32_t dw_outer = 0; dw_outer < 2; ++dw_outer) {\n          for (int32_t w_c_outer_inner = 0; w_c_outer_inner < 32; ++w_c_outer_inner) {\n            for (int32_t dc_inner = 0; dc_inner < 3; ++dc_inner) {\n              for (int32_t dw_inner = 0; dw_inner < 2; ++dw_inner) {\n                for (int32_t c_c_inner = 0; c_c_inner < 3; ++c_c_inner) {\n                  int32_t cse_var_3 = (dw_outer * 2);\n                  int32_t cse_var_2 = ((c_c_inner * 32) + w_c_outer_inner);\n                  ((float*)compute_local)[cse_var_2] = (((float*)compute_local)[cse_var_2] + (((float*)data_pad)[(((((dc_outer * 105) + (dc_inner * 35)) + cse_var_3) + w_c_outer_inner) + dw_inner)] * ((float*)kernel_2)[((((((c_outer_inner * 108) + (c_c_inner * 36)) + (dc_outer * 12)) + (dc_inner * 4)) + cse_var_3) + dw_inner)]));\n                },\n              },\n            },\n          },\n        },\n      },\n      for (int32_t c_inner = 0; c_inner < 3; ++c_inner) {\n        for (int32_t w_inner = 0; w_inner < 32; ++w_inner) {\n          ((float*)compute_1)[((((((b_outer_outer_c_outer_outer_fused_w_outer_outer_fused >> 2) * 1152) + (c_outer_inner * 384)) + (c_inner * 128)) + ((b_outer_outer_c_outer_outer_fused_w_outer_outer_fused & 3) * 32)) + w_inner)] = ((float*)compute_local)[((c_inner * 32) + w_inner)];\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, compute_local) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, data_pad) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, kernel_2) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel) {\n  float compute_local[2];\n  __shared__ float data_pad_shared[48];\n  __shared__ float kernel_shared[21];\n  for (int w_c_inner_init = 0; w_c_inner_init < 2; ++w_c_inner_init) {\n    compute_local[w_c_inner_init] = 0.000000e+00f;\n  },\n  for (int dc_outer_outer = 0; dc_outer_outer < 3; ++dc_outer_outer) {\n    for (int dw_outer_outer = 0; dw_outer_outer < 4; ++dw_outer_outer) {\n      __syncthreads();\n      if (((int)threadIdx.x) < 24) {\n        for (int ax0_ax1_fused_ax2_fused_inner_s = 0; ax0_ax1_fused_ax2_fused_inner_s < 2; ++ax0_ax1_fused_ax2_fused_inner_s) {\n          data_pad_shared[((((int)threadIdx.x) * 2) + ax0_ax1_fused_ax2_fused_inner_s)] = ((((3 <= (((((((int)blockIdx.x) & 7) * 16) + ((((int)threadIdx.x) & 7) * 2)) + dw_outer_outer) + ax0_ax1_fused_ax2_fused_inner_s)) && ((((((((int)blockIdx.x) & 7) * 16) + ((((int)threadIdx.x) & 7) * 2)) + dw_outer_outer) + ax0_ax1_fused_ax2_fused_inner_s) < 128)) && ((((dw_outer_outer + ax0_ax1_fused_ax2_fused_inner_s) + 1) % 2) == 0)) ? data[(((((dc_outer_outer * 189) + ((((int)threadIdx.x) >> 3) * 63)) + ((((int)blockIdx.x) & 7) * 8)) + (((dw_outer_outer + ax0_ax1_fused_ax2_fused_inner_s) - 3) >> 1)) + (((int)threadIdx.x) & 7))] : 0.000000e+00f);\n        },\n      },\n      if (((int)threadIdx.x) < 21) {\n        kernel_shared[((int)threadIdx.x)] = kernel[((((((dc_outer_outer * 756) + ((((int)threadIdx.x) % 3) * 252)) + ((((int)blockIdx.x) >> 3) * 28)) + ((((int)threadIdx.x) / 3) * 4)) + 3) - dw_outer_outer)];\n      },\n      __syncthreads();\n      for (int dc_outer_inner = 0; dc_outer_inner < 3; ++dc_outer_inner) {\n        for (int w_c_inner = 0; w_c_inner < 2; ++w_c_inner) {\n          compute_local[w_c_inner] = (compute_local[w_c_inner] + (data_pad_shared[(((dc_outer_inner * 16) + ((((int)threadIdx.x) & 7) * 2)) + w_c_inner)] * kernel_shared[(((((int)threadIdx.x) >> 3) * 3) + dc_outer_inner)]));\n        },\n      },\n    },\n  },\n  for (int w_inner = 0; w_inner < 2; ++w_inner) {\n    compute[((((((((int)blockIdx.x) >> 3) * 896) + ((((int)threadIdx.x) >> 3) * 128)) + ((((int)blockIdx.x) & 7) * 16)) + ((((int)threadIdx.x) & 7) * 2)) + w_inner)] = compute_local[w_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 9, 63), \"float32\"), kernel: T.Buffer((63, 63, 4), \"float32\"), compute: T.Buffer((1, 63, 128), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_c_outer_outer_fused_w_outer_outer_fused in T.parallel(28):\n            kernel_1 = T.allocate([324], \"float32\", \"global\")\n            data_pad = T.allocate([315], \"float32\", \"global\")\n            data_dilate = T.allocate([35], \"float32\", \"global\")\n            compute_local = T.allocate([96], \"float32\", \"local\")\n            kernel_2 = T.Buffer((324,), data=kernel_1)\n            for o, i in T.grid(9, 9):\n                kernel_3 = T.Buffer((15876,), data=kernel.data)\n                kernel_2[o * 36 + i * 4:o * 36 + i * 4 + 4] = kernel_3[i * 252 + b_outer_outer_c_outer_outer_fused_w_outer_outer_fused // 4 * 36 + o * 4 + 3:i * 252 + b_outer_outer_c_outer_outer_fused_w_outer_outer_fused // 4 * 36 + o * 4 + 3 + -4:-1]\n            for c_outer_inner in range(3):\n                data_pad_1 = T.Buffer((315,), data=data_pad)\n                for i1 in range(9):\n                    data_dilate_1 = T.Buffer((35,), data=data_dilate)\n                    for i2 in range(35):\n                        if T.likely(3 <= b_outer_outer_c_outer_outer_fused_w_outer_outer_fused % 4 * 32 + i2):\n                            if T.likely(i2 // 32 + b_outer_outer_c_outer_outer_fused_w_outer_outer_fused % 4 < 4):\n                                data_1 = T.Buffer((567,), data=data.data)\n                                data_dilate_1[i2] = T.if_then_else((i2 + 1) % 2 == 0, data_1[i1 * 63 + b_outer_outer_c_outer_outer_fused_w_outer_outer_fused % 4 * 16 + (i2 - 3) // 2], T.float32(0))\n                    for i2 in range(35):\n                        cse_var_1: T.int32 = b_outer_outer_c_outer_outer_fused_w_outer_outer_fused % 4\n                        data_pad_1[i1 * 35 + i2] = T.if_then_else(3 <= cse_var_1 * 32 + i2 and i2 // 32 + cse_var_1 < 4, data_dilate_1[i2], T.float32(0))\n                compute_local_1 = T.Buffer((96,), data=compute_local, scope=\"local\")\n                for w_c_outer_inner_init, c_c_inner_init in T.grid(32, 3):\n                    compute_local_1[c_c_inner_init * 32 + w_c_outer_inner_init] = T.float32(0)\n                for dc_outer, dw_outer, w_c_outer_inner, dc_inner, dw_inner, c_c_inner in T.grid(3, 2, 32, 3, 2, 3):\n                    cse_var_3: T.int32 = dw_outer * 2\n                    cse_var_2: T.int32 = c_c_inner * 32 + w_c_outer_inner\n                    compute_local_1[cse_var_2] = compute_local_1[cse_var_2] + data_pad_1[dc_outer * 105 + dc_inner * 35 + cse_var_3 + w_c_outer_inner + dw_inner] * kernel_2[c_outer_inner * 108 + c_c_inner * 36 + dc_outer * 12 + dc_inner * 4 + cse_var_3 + dw_inner]\n                for c_inner, w_inner in T.grid(3, 32):\n                    compute_1 = T.Buffer((8064,), data=compute.data)\n                    compute_1[b_outer_outer_c_outer_outer_fused_w_outer_outer_fused // 4 * 1152 + c_outer_inner * 384 + c_inner * 128 + b_outer_outer_c_outer_outer_fused_w_outer_outer_fused % 4 * 32 + w_inner] = compute_local_1[c_inner * 32 + w_inner]",
        "data": "1_9_63",
        "kernel": "63_63_4"
    },
    {
        "op_name": "conv1d_transpose_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  void* kernel_2 = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)9632, 2, 32);\n  if (kernel_2 == NULL) {\n    return -1;\n  },\n  for (int32_t o_i_fused_w_fused = 0; o_i_fused_w_fused < 2408; ++o_i_fused_w_fused) {\n    ((float*)kernel_2)[o_i_fused_w_fused] = ((float*)kernel_1)[((((((o_i_fused_w_fused % 28) >> 2) * 344) + ((o_i_fused_w_fused / 28) * 4)) + 3) - (o_i_fused_w_fused & 3))];\n  },\n  for (int32_t b_outer_c_outer_fused_w_outer_fused = 0; b_outer_c_outer_fused_w_outer_fused < 58; ++b_outer_c_outer_fused_w_outer_fused) {\n    float data_pad[126];\n    float data_dilate[63];\n    void* compute_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)2064, 2, 32);\n    if (compute_local == NULL) {\n      return -1;\n    },\n    for (int32_t i0 = 0; i0 < 2; ++i0) {\n      for (int32_t i1 = 0; i1 < 7; ++i1) {\n        for (int32_t i2_s = 0; i2_s < 9; ++i2_s) {\n          if (1 <= (((b_outer_c_outer_fused_w_outer_fused % 29) * 2) + (i2_s / 3))) {\n            if (((i2_s / 6) + (b_outer_c_outer_fused_w_outer_fused % 29)) < 29) {\n              data_dilate[((i1 * 9) + i2_s)] = ((((i2_s + 1) % 2) == 0) ? ((float*)data_1)[((((i0 * 602) + (i1 * 86)) + ((b_outer_c_outer_fused_w_outer_fused % 29) * 3)) + ((i2_s - 3) >> 1))] : 0.000000e+00f);\n            },\n          },\n        },\n      },\n      for (int32_t i1_1 = 0; i1_1 < 7; ++i1_1) {\n        for (int32_t i2_s_1 = 0; i2_s_1 < 9; ++i2_s_1) {\n          int32_t cse_var_2 = (b_outer_c_outer_fused_w_outer_fused % 29);\n          int32_t cse_var_1 = (i1_1 * 9);\n          data_pad[(((i0 * 63) + cse_var_1) + i2_s_1)] = (((1 <= ((cse_var_2 * 2) + (i2_s_1 / 3))) && (((i2_s_1 / 6) + cse_var_2) < 29)) ? data_dilate[(cse_var_1 + i2_s_1)] : 0.000000e+00f);\n        },\n      },\n    },\n    for (int32_t c_c_outer_inner_init = 0; c_c_outer_inner_init < 43; ++c_c_outer_inner_init) {\n      for (int32_t b_c_inner_init = 0; b_c_inner_init < 2; ++b_c_inner_init) {\n        ((float6*)compute_local)[((b_c_inner_init * 43) + c_c_outer_inner_init)] = ((float6)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n      },\n    },\n    for (int32_t dc_outer = 0; dc_outer < 7; ++dc_outer) {\n      for (int32_t dw_outer = 0; dw_outer < 4; ++dw_outer) {\n        for (int32_t c_c_outer_inner = 0; c_c_outer_inner < 43; ++c_c_outer_inner) {\n          for (int32_t b_c_inner = 0; b_c_inner < 2; ++b_c_inner) {\n            int32_t cse_var_3 = ((b_c_inner * 43) + c_c_outer_inner);\n            int32_t6 v_ = int32_t6(((((b_c_inner * 63) + (dc_outer * 9)) + dw_outer))+(1*0), ((((b_c_inner * 63) + (dc_outer * 9)) + dw_outer))+(1*1), ((((b_c_inner * 63) + (dc_outer * 9)) + dw_outer))+(1*2), ((((b_c_inner * 63) + (dc_outer * 9)) + dw_outer))+(1*3), ((((b_c_inner * 63) + (dc_outer * 9)) + dw_outer))+(1*4), ((((b_c_inner * 63) + (dc_outer * 9)) + dw_outer))+(1*5));\n            ((float6*)compute_local)[cse_var_3] = (((float6*)compute_local)[cse_var_3] + ((float6(data_pad[v_.s0],data_pad[v_.s1],data_pad[v_.s2],data_pad[v_.s3],data_pad[v_.s4],data_pad[v_.s5])) * ((float6)(((float*)kernel_2)[(((((b_outer_c_outer_fused_w_outer_fused / 29) * 1204) + (c_c_outer_inner * 28)) + (dc_outer * 4)) + dw_outer)], ((float*)kernel_2)[(((((b_outer_c_outer_fused_w_outer_fused / 29) * 1204) + (c_c_outer_inner * 28)) + (dc_outer * 4)) + dw_outer)], ((float*)kernel_2)[(((((b_outer_c_outer_fused_w_outer_fused / 29) * 1204) + (c_c_outer_inner * 28)) + (dc_outer * 4)) + dw_outer)], ((float*)kernel_2)[(((((b_outer_c_outer_fused_w_outer_fused / 29) * 1204) + (c_c_outer_inner * 28)) + (dc_outer * 4)) + dw_outer)], ((float*)kernel_2)[(((((b_outer_c_outer_fused_w_outer_fused / 29) * 1204) + (c_c_outer_inner * 28)) + (dc_outer * 4)) + dw_outer)], ((float*)kernel_2)[(((((b_outer_c_outer_fused_w_outer_fused / 29) * 1204) + (c_c_outer_inner * 28)) + (dc_outer * 4)) + dw_outer)]))));\n          },\n        },\n      },\n    },\n    for (int32_t b_inner = 0; b_inner < 2; ++b_inner) {\n      for (int32_t c_inner = 0; c_inner < 43; ++c_inner) {\n        *(float6*)(((float*)compute_1) + ((((b_inner * 14964) + ((b_outer_c_outer_fused_w_outer_fused / 29) * 7482)) + (c_inner * 174)) + ((b_outer_c_outer_fused_w_outer_fused % 29) * 6))) = ((float6*)compute_local)[((b_inner * 43) + c_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, compute_local) != 0) {\n      return -1;\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, kernel_2) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(86) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(86) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel) {\n  float compute_local[3];\n  __shared__ float data_pad_shared[84];\n  __shared__ float kernel_shared[1204];\n  for (int w_c_inner_init = 0; w_c_inner_init < 3; ++w_c_inner_init) {\n    compute_local[w_c_inner_init] = 0.000000e+00f;\n  },\n  if (((int)threadIdx.x) < 84) {\n    data_pad_shared[((int)threadIdx.x)] = ((((1 <= (((((int)threadIdx.x) % 6) / 3) + (((int)blockIdx.x) % 58))) && ((((((int)threadIdx.x) % 6) / 3) + (((int)blockIdx.x) % 58)) < 58)) && ((((((((int)blockIdx.x) % 58) * 3) + (((int)threadIdx.x) % 6)) + 1) % 2) == 0)) ? data[(((((int)threadIdx.x) / 6) * 86) + (((((((int)blockIdx.x) % 58) * 3) + (((int)threadIdx.x) % 6)) - 3) >> 1))] : 0.000000e+00f);\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 7; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    int2 v_ = make_int2((((((((((((int)threadIdx.x) >> 1) + ax0_ax1_fused_ax2_fused_outer_outer) % 7) * 344) + ((((int)blockIdx.x) / 58) * 172)) + ((((ax0_ax1_fused_ax2_fused_outer_outer * 43) + (((int)threadIdx.x) >> 1)) / 7) * 4)) + 3) - ((((int)threadIdx.x) & 1) * 2)))+(-1*0), (((((((((((int)threadIdx.x) >> 1) + ax0_ax1_fused_ax2_fused_outer_outer) % 7) * 344) + ((((int)blockIdx.x) / 58) * 172)) + ((((ax0_ax1_fused_ax2_fused_outer_outer * 43) + (((int)threadIdx.x) >> 1)) / 7) * 4)) + 3) - ((((int)threadIdx.x) & 1) * 2)))+(-1*1));\n    *(float2*)(kernel_shared + ((ax0_ax1_fused_ax2_fused_outer_outer * 172) + (((int)threadIdx.x) * 2))) = make_float2(kernel[v_.x],kernel[v_.y]);\n  },\n  __syncthreads();\n  for (int dw_outer_inner = 0; dw_outer_inner < 2; ++dw_outer_inner) {\n    for (int dc_inner = 0; dc_inner < 7; ++dc_inner) {\n      for (int dw_inner = 0; dw_inner < 2; ++dw_inner) {\n        for (int w_c_inner = 0; w_c_inner < 3; ++w_c_inner) {\n          compute_local[w_c_inner] = (compute_local[w_c_inner] + (data_pad_shared[((((((((int)threadIdx.x) / 43) * 42) + (dc_inner * 6)) + (dw_outer_inner * 2)) + w_c_inner) + dw_inner)] * kernel_shared[(((((((int)threadIdx.x) % 43) * 28) + (dc_inner * 4)) + (dw_outer_inner * 2)) + dw_inner)]));\n        },\n      },\n    },\n  },\n  for (int w_inner = 0; w_inner < 3; ++w_inner) {\n    compute[((((((((int)threadIdx.x) / 43) * 14964) + ((((int)blockIdx.x) / 58) * 7482)) + ((((int)threadIdx.x) % 43) * 174)) + ((((int)blockIdx.x) % 58) * 3)) + w_inner)] = compute_local[w_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 7, 86), \"float32\"), kernel: T.Buffer((40, 86, 4), \"float32\"), compute: T.Buffer((2, 86, 174), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        kernel_1 = T.allocate([2408], \"float32\", \"global\")\n        kernel_2 = T.Buffer((2408,), data=kernel_1)\n        for o_i_fused_w_fused in T.parallel(2408):\n            kernel_3 = T.Buffer((13760,), data=kernel.data)\n            kernel_2[o_i_fused_w_fused] = kernel_3[o_i_fused_w_fused % 28 // 4 * 344 + o_i_fused_w_fused // 28 * 4 + 3 - o_i_fused_w_fused % 4]\n        for b_outer_c_outer_fused_w_outer_fused in T.parallel(58):\n            data_pad = T.allocate([126], \"float32\", \"global\")\n            data_dilate = T.allocate([63], \"float32\", \"global\")\n            compute_local = T.allocate([86], \"float32x6\", \"local\")\n            data_pad_1 = T.Buffer((126,), data=data_pad)\n            for i0 in range(2):\n                data_dilate_1 = T.Buffer((63,), data=data_dilate)\n                for i1, i2_s in T.grid(7, 9):\n                    if T.likely(1 <= b_outer_c_outer_fused_w_outer_fused % 29 * 2 + i2_s // 3):\n                        if T.likely(i2_s // 6 + b_outer_c_outer_fused_w_outer_fused % 29 < 29):\n                            data_1 = T.Buffer((1204,), data=data.data)\n                            data_dilate_1[i1 * 9 + i2_s] = T.if_then_else((i2_s + 1) % 2 == 0, data_1[i0 * 602 + i1 * 86 + b_outer_c_outer_fused_w_outer_fused % 29 * 3 + (i2_s - 3) // 2], T.float32(0))\n                for i1, i2_s in T.grid(7, 9):\n                    cse_var_2: T.int32 = b_outer_c_outer_fused_w_outer_fused % 29\n                    cse_var_1: T.int32 = i1 * 9\n                    data_pad_1[i0 * 63 + cse_var_1 + i2_s] = T.if_then_else(1 <= cse_var_2 * 2 + i2_s // 3 and i2_s // 6 + cse_var_2 < 29, data_dilate_1[cse_var_1 + i2_s], T.float32(0))\n            compute_local_1 = T.Buffer((86,), \"float32x6\", data=compute_local, scope=\"local\")\n            for c_c_outer_inner_init, b_c_inner_init in T.grid(43, 2):\n                compute_local_1[b_c_inner_init * 43 + c_c_outer_inner_init] = T.Broadcast(T.float32(0), 6)\n            for dc_outer, dw_outer, c_c_outer_inner, b_c_inner in T.grid(7, 4, 43, 2):\n                cse_var_3: T.int32 = b_c_inner * 43 + c_c_outer_inner\n                compute_local_1[cse_var_3] = compute_local_1[cse_var_3] + data_pad_1[b_c_inner * 63 + dc_outer * 9 + dw_outer:b_c_inner * 63 + dc_outer * 9 + dw_outer + 6] * T.Broadcast(kernel_2[b_outer_c_outer_fused_w_outer_fused // 29 * 1204 + c_c_outer_inner * 28 + dc_outer * 4 + dw_outer], 6)\n            for b_inner, c_inner in T.grid(2, 43):\n                compute_1 = T.Buffer((29928,), data=compute.data)\n                compute_1[b_inner * 14964 + b_outer_c_outer_fused_w_outer_fused // 29 * 7482 + c_inner * 174 + b_outer_c_outer_fused_w_outer_fused % 29 * 6:b_inner * 14964 + b_outer_c_outer_fused_w_outer_fused // 29 * 7482 + c_inner * 174 + b_outer_c_outer_fused_w_outer_fused % 29 * 6 + 6] = compute_local_1[b_inner * 43 + c_inner]",
        "data": "2_7_86",
        "kernel": "40_86_4"
    },
    {
        "op_name": "conv1d_transpose_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t b_outer_c_outer_fused_w_outer_fused = 0; b_outer_c_outer_fused_w_outer_fused < 88; ++b_outer_c_outer_fused_w_outer_fused) {\n    void* data_pad = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)2700, 2, 32);\n    if (data_pad == NULL) {\n      return -1;\n    },\n    float data_dilate[131];\n    void* compute_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)3192, 2, 32);\n    if (compute_local == NULL) {\n      return -1;\n    },\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 131; ++i2) {\n        data_dilate[i2] = (((i2 % 2) == 0) ? ((float*)data_1)[((((b_outer_c_outer_fused_w_outer_fused / 11) * 330) + (i1 * 66)) + (i2 >> 1))] : 0.000000e+00f);\n      },\n      for (int32_t i2_1 = 0; i2_1 < 135; ++i2_1) {\n        ((float*)data_pad)[((i1 * 135) + i2_1)] = (((2 <= i2_1) && (i2_1 < 133)) ? data_dilate[(i2_1 - 2)] : 0.000000e+00f);\n      },\n    },\n    for (int32_t o = 0; o < 6; ++o) {\n      for (int32_t i = 0; i < 5; ++i) {\n        int32_t3 v_ = int32_t3((((((i * 198) + ((b_outer_c_outer_fused_w_outer_fused % 11) * 18)) + (o * 3)) + 2))+(-1*0), (((((i * 198) + ((b_outer_c_outer_fused_w_outer_fused % 11) * 18)) + (o * 3)) + 2))+(-1*1), (((((i * 198) + ((b_outer_c_outer_fused_w_outer_fused % 11) * 18)) + (o * 3)) + 2))+(-1*2));\n        *(float3*)(data_dilate + ((o * 15) + (i * 3))) = (float3(((float*)kernel_1)[v_.s0],((float*)kernel_1)[v_.s1],((float*)kernel_1)[v_.s2]));\n      },\n    },\n    for (int32_t c_c_outer_inner_init = 0; c_c_outer_inner_init < 6; ++c_c_outer_inner_init) {\n      for (int32_t w_c_outer_inner_init = 0; w_c_outer_inner_init < 133; ++w_c_outer_inner_init) {\n        ((float*)compute_local)[((c_c_outer_inner_init * 133) + w_c_outer_inner_init)] = 0.000000e+00f;\n      },\n    },\n    for (int32_t dc_outer = 0; dc_outer < 5; ++dc_outer) {\n      for (int32_t c_c_outer_inner = 0; c_c_outer_inner < 6; ++c_c_outer_inner) {\n        for (int32_t w_c_outer_inner = 0; w_c_outer_inner < 133; ++w_c_outer_inner) {\n          for (int32_t dw_inner = 0; dw_inner < 3; ++dw_inner) {\n            int32_t cse_var_1 = ((c_c_outer_inner * 133) + w_c_outer_inner);\n            ((float*)compute_local)[cse_var_1] = (((float*)compute_local)[cse_var_1] + (((float*)data_pad)[(((dc_outer * 135) + w_c_outer_inner) + dw_inner)] * data_dilate[(((c_c_outer_inner * 15) + (dc_outer * 3)) + dw_inner)]));\n          },\n        },\n      },\n    },\n    for (int32_t c_inner = 0; c_inner < 6; ++c_inner) {\n      for (int32_t w_inner = 0; w_inner < 133; ++w_inner) {\n        int32_t cse_var_2 = (c_inner * 133);\n        ((float*)compute_1)[(((b_outer_c_outer_fused_w_outer_fused * 798) + cse_var_2) + w_inner)] = ((float*)compute_local)[(cse_var_2 + w_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, compute_local) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, data_pad) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(11) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(11) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel) {\n  float compute_local[24];\n  __shared__ float data_pad_shared[24];\n  __shared__ float kernel_shared[99];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 4; ++b_c_outer_inner_init) {\n    for (int c_c_outer_inner_init = 0; c_c_outer_inner_init < 3; ++c_c_outer_inner_init) {\n      for (int b_c_inner_init = 0; b_c_inner_init < 2; ++b_c_inner_init) {\n        compute_local[(((b_c_outer_inner_init * 6) + (b_c_inner_init * 3)) + c_c_outer_inner_init)] = 0.000000e+00f;\n      },\n    },\n  },\n  for (int dc_outer_outer = 0; dc_outer_outer < 5; ++dc_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 3; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer * 11) + ((int)threadIdx.x)) < 24) {\n        data_pad_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 11) + ((int)threadIdx.x))] = ((((2 <= ((((int)blockIdx.x) % 133) + (((ax0_ax1_fused_ax2_fused_outer_outer * 2) + ((int)threadIdx.x)) % 3))) && (((((int)blockIdx.x) % 133) + (((ax0_ax1_fused_ax2_fused_outer_outer * 2) + ((int)threadIdx.x)) % 3)) < 133)) && ((((((int)blockIdx.x) % 133) + (((ax0_ax1_fused_ax2_fused_outer_outer * 2) + ((int)threadIdx.x)) % 3)) % 2) == 0)) ? data[(((((((ax0_ax1_fused_ax2_fused_outer_outer * 11) + ((int)threadIdx.x)) / 3) * 330) + (dc_outer_outer * 66)) + (((((int)blockIdx.x) % 133) + (((ax0_ax1_fused_ax2_fused_outer_outer * 2) + ((int)threadIdx.x)) % 3)) >> 1)) - 1)] : 0.000000e+00f);\n      },\n    },\n    for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 9; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 11) + ((int)threadIdx.x))] = kernel[(((((dc_outer_outer * 198) + ((((int)blockIdx.x) / 133) * 99)) + ((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 11) + ((int)threadIdx.x)) / 3) * 3)) + 2) - (((ax0_ax1_fused_ax2_fused_outer_outer_1 * 2) + ((int)threadIdx.x)) % 3))];\n    },\n    __syncthreads();\n    for (int dw_outer_inner = 0; dw_outer_inner < 3; ++dw_outer_inner) {\n      for (int b_c_outer_inner = 0; b_c_outer_inner < 4; ++b_c_outer_inner) {\n        for (int c_c_outer_inner = 0; c_c_outer_inner < 3; ++c_c_outer_inner) {\n          for (int b_c_inner = 0; b_c_inner < 2; ++b_c_inner) {\n            compute_local[(((b_c_outer_inner * 6) + (b_c_inner * 3)) + c_c_outer_inner)] = (compute_local[(((b_c_outer_inner * 6) + (b_c_inner * 3)) + c_c_outer_inner)] + (data_pad_shared[(((b_c_outer_inner * 6) + (b_c_inner * 3)) + dw_outer_inner)] * kernel_shared[(((((int)threadIdx.x) * 9) + (c_c_outer_inner * 3)) + dw_outer_inner)]));\n          },\n        },\n      },\n    },\n  },\n  for (int b_inner = 0; b_inner < 8; ++b_inner) {\n    for (int c_inner = 0; c_inner < 3; ++c_inner) {\n      compute[(((((b_inner * 8778) + ((((int)blockIdx.x) / 133) * 4389)) + (((int)threadIdx.x) * 399)) + (c_inner * 133)) + (((int)blockIdx.x) % 133))] = compute_local[((b_inner * 3) + c_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 5, 66), \"float32\"), kernel: T.Buffer((32, 66, 3), \"float32\"), compute: T.Buffer((8, 66, 133), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_c_outer_fused_w_outer_fused in T.parallel(88):\n            data_pad = T.allocate([675], \"float32\", \"global\")\n            data_dilate = T.allocate([131], \"float32\", \"global\")\n            compute_local = T.allocate([798], \"float32\", \"local\")\n            data_pad_1 = T.Buffer((675,), data=data_pad)\n            for i1 in range(5):\n                data_dilate_1 = T.Buffer((131,), data=data_dilate)\n                for i2 in range(131):\n                    data_1 = T.Buffer((2640,), data=data.data)\n                    data_dilate_1[i2] = T.if_then_else(i2 % 2 == 0, data_1[b_outer_c_outer_fused_w_outer_fused // 11 * 330 + i1 * 66 + i2 // 2], T.float32(0))\n                for i2 in range(135):\n                    data_pad_1[i1 * 135 + i2] = T.if_then_else(2 <= i2 and i2 < 133, data_dilate_1[i2 - 2], T.float32(0))\n            data_dilate_1 = T.Buffer((90,), data=data_dilate)\n            for o, i in T.grid(6, 5):\n                kernel_1 = T.Buffer((6336,), data=kernel.data)\n                data_dilate_1[o * 15 + i * 3:o * 15 + i * 3 + 3] = kernel_1[i * 198 + b_outer_c_outer_fused_w_outer_fused % 11 * 18 + o * 3 + 2:i * 198 + b_outer_c_outer_fused_w_outer_fused % 11 * 18 + o * 3 + 2 + -3:-1]\n            compute_local_1 = T.Buffer((798,), data=compute_local, scope=\"local\")\n            for c_c_outer_inner_init, w_c_outer_inner_init in T.grid(6, 133):\n                compute_local_1[c_c_outer_inner_init * 133 + w_c_outer_inner_init] = T.float32(0)\n            for dc_outer, c_c_outer_inner, w_c_outer_inner, dw_inner in T.grid(5, 6, 133, 3):\n                cse_var_1: T.int32 = c_c_outer_inner * 133 + w_c_outer_inner\n                compute_local_1[cse_var_1] = compute_local_1[cse_var_1] + data_pad_1[dc_outer * 135 + w_c_outer_inner + dw_inner] * data_dilate_1[c_c_outer_inner * 15 + dc_outer * 3 + dw_inner]\n            for c_inner, w_inner in T.grid(6, 133):\n                cse_var_2: T.int32 = c_inner * 133\n                compute_1 = T.Buffer((70224,), data=compute.data)\n                compute_1[b_outer_c_outer_fused_w_outer_fused * 798 + cse_var_2 + w_inner] = compute_local_1[cse_var_2 + w_inner]",
        "data": "8_5_66",
        "kernel": "32_66_3"
    },
    {
        "op_name": "conv1d_transpose_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  void* data_pad = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1104, 2, 32);\n  if (data_pad == NULL) {\n    return -1;\n  },\n  float data_dilate[38];\n  void* compute_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)720, 2, 32);\n  if (compute_local == NULL) {\n    return -1;\n  },\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        data_dilate[((i1 * 19) + i2)] = (((i2 % 2) == 0) ? ((float*)data_1)[(((i0 * 20) + (i1 * 10)) + (i2 >> 1))] : 0.000000e+00f);\n      },\n    },\n    for (int32_t i1_1 = 0; i1_1 < 2; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 23; ++i2_1) {\n        ((float*)data_pad)[(((i0 * 46) + (i1_1 * 23)) + i2_1)] = (((2 <= i2_1) && (i2_1 < 21)) ? data_dilate[(((i1_1 * 19) + i2_1) - 2)] : 0.000000e+00f);\n      },\n    },\n  },\n  for (int32_t w_outer = 0; w_outer < 7; ++w_outer) {\n    for (int32_t c_c_outer_outer_inner = 0; c_c_outer_outer_inner < 5; ++c_c_outer_outer_inner) {\n      for (int32_t o = 0; o < 2; ++o) {\n        int32_t6 v_ = (((((int32_t6((0)+(1*0), (0)+(1*1), (0)+(1*2), (0)+(1*3), (0)+(1*4), (0)+(1*5)) / ((int32_t6)(3, 3, 3, 3, 3, 3))) * ((int32_t6)(30, 30, 30, 30, 30, 30))) + ((int32_t6)((c_c_outer_outer_inner * 6), (c_c_outer_outer_inner * 6), (c_c_outer_outer_inner * 6), (c_c_outer_outer_inner * 6), (c_c_outer_outer_inner * 6), (c_c_outer_outer_inner * 6)))) + ((int32_t6)((o * 3), (o * 3), (o * 3), (o * 3), (o * 3), (o * 3)))) + ((int32_t6)(2, 2, 2, 2, 2, 2))) - (int32_t6((0)+(1*0), (0)+(1*1), (0)+(1*2), (0)+(1*3), (0)+(1*4), (0)+(1*5)) % ((int32_t6)(3, 3, 3, 3, 3, 3)));\n        *(float6*)(data_dilate + (o * 6)) = (float6(((float*)kernel_1)[v_.s0],((float*)kernel_1)[v_.s1],((float*)kernel_1)[v_.s2],((float*)kernel_1)[v_.s3],((float*)kernel_1)[v_.s4],((float*)kernel_1)[v_.s5]));\n      },\n      for (int32_t w_c_outer_outer_inner = 0; w_c_outer_outer_inner < 3; ++w_c_outer_outer_inner) {\n        for (int32_t b_c_outer_inner_init = 0; b_c_outer_inner_init < 6; ++b_c_outer_inner_init) {\n          for (int32_t c_c_outer_inner_init = 0; c_c_outer_inner_init < 2; ++c_c_outer_inner_init) {\n            ((float*)compute_local)[((((b_c_outer_inner_init * 30) + (c_c_outer_outer_inner * 6)) + (c_c_outer_inner_init * 3)) + w_c_outer_outer_inner)] = 0.000000e+00f;\n          },\n        },\n        for (int32_t dw_outer = 0; dw_outer < 3; ++dw_outer) {\n          for (int32_t b_c_outer_inner = 0; b_c_outer_inner < 6; ++b_c_outer_inner) {\n            for (int32_t c_c_outer_inner = 0; c_c_outer_inner < 2; ++c_c_outer_inner) {\n              for (int32_t dc_inner = 0; dc_inner < 2; ++dc_inner) {\n                int32_t cse_var_1 = ((((b_c_outer_inner * 30) + (c_c_outer_outer_inner * 6)) + (c_c_outer_inner * 3)) + w_c_outer_outer_inner);\n                ((float*)compute_local)[cse_var_1] = (((float*)compute_local)[cse_var_1] + (((float*)data_pad)[(((((b_c_outer_inner * 46) + (dc_inner * 23)) + (w_outer * 3)) + w_c_outer_outer_inner) + dw_outer)] * data_dilate[(((c_c_outer_inner * 6) + (dc_inner * 3)) + dw_outer)]));\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t b_inner = 0; b_inner < 6; ++b_inner) {\n      for (int32_t c_inner = 0; c_inner < 10; ++c_inner) {\n        *(float3*)(((float*)compute_1) + (((b_inner * 210) + (c_inner * 21)) + (w_outer * 3))) = *(float3*)(((float*)compute_local) + ((b_inner * 30) + (c_inner * 3)));\n      },\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, compute_local) != 0) {\n    return -1;\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, data_pad) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(315) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(315) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel) {\n  float compute_local[2];\n  __shared__ float data_pad_shared[138];\n  __shared__ float kernel_shared[60];\n  for (int c_c_inner_init = 0; c_c_inner_init < 2; ++c_c_inner_init) {\n    compute_local[c_c_inner_init] = 0.000000e+00f;\n  },\n  if (((int)threadIdx.x) < 138) {\n    data_pad_shared[((int)threadIdx.x)] = ((((2 <= (((int)threadIdx.x) % 23)) && ((((int)threadIdx.x) % 23) < 21)) && (((((int)threadIdx.x) % 23) % 2) == 0)) ? data[((((((int)blockIdx.x) * 60) + ((((int)threadIdx.x) / 23) * 10)) + ((((int)threadIdx.x) % 23) >> 1)) - 1)] : 0.000000e+00f);\n  },\n  if (((int)threadIdx.x) < 60) {\n    kernel_shared[((int)threadIdx.x)] = kernel[((((((((int)threadIdx.x) % 6) / 3) * 30) + ((((int)threadIdx.x) / 6) * 3)) + 2) - (((int)threadIdx.x) % 3))];\n  },\n  __syncthreads();\n  for (int dc_outer_inner = 0; dc_outer_inner < 2; ++dc_outer_inner) {\n    for (int dw_inner = 0; dw_inner < 3; ++dw_inner) {\n      for (int c_c_inner = 0; c_c_inner < 2; ++c_c_inner) {\n        compute_local[c_c_inner] = (compute_local[c_c_inner] + (data_pad_shared[(((((((int)threadIdx.x) / 105) * 46) + (dc_outer_inner * 23)) + dw_inner) + (((int)threadIdx.x) % 21))] * kernel_shared[((((((((int)threadIdx.x) % 105) / 21) * 12) + (c_c_inner * 6)) + (dc_outer_inner * 3)) + dw_inner)]));\n      },\n    },\n  },\n  for (int c_inner = 0; c_inner < 2; ++c_inner) {\n    compute[((((((int)blockIdx.x) * 630) + ((((int)threadIdx.x) / 21) * 42)) + (c_inner * 21)) + (((int)threadIdx.x) % 21))] = compute_local[c_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 2, 10), \"float32\"), kernel: T.Buffer((26, 10, 3), \"float32\"), compute: T.Buffer((6, 10, 21), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        data_pad = T.allocate([276], \"float32\", \"global\")\n        data_dilate = T.allocate([38], \"float32\", \"global\")\n        compute_local = T.allocate([180], \"float32\", \"local\")\n        data_pad_1 = T.Buffer((276,), data=data_pad)\n        for i0 in range(6):\n            data_dilate_1 = T.Buffer((38,), data=data_dilate)\n            for i1, i2 in T.grid(2, 19):\n                data_1 = T.Buffer((120,), data=data.data)\n                data_dilate_1[i1 * 19 + i2] = T.if_then_else(i2 % 2 == 0, data_1[i0 * 20 + i1 * 10 + i2 // 2], T.float32(0))\n            for i1, i2 in T.grid(2, 23):\n                data_pad_1[i0 * 46 + i1 * 23 + i2] = T.if_then_else(2 <= i2 and i2 < 21, data_dilate_1[i1 * 19 + i2 - 2], T.float32(0))\n        for w_outer in range(7):\n            compute_local_1 = T.Buffer((180,), data=compute_local, scope=\"local\")\n            for c_c_outer_outer_inner in range(5):\n                data_dilate_1 = T.Buffer((12,), data=data_dilate, align=32)\n                for o in range(2):\n                    kernel_1 = T.Buffer((780,), data=kernel.data)\n                    data_dilate_1[o * 6:o * 6 + 6] = kernel_1[T.Ramp(0, 1, 6) // T.Broadcast(3, 6) * T.Broadcast(30, 6) + T.Broadcast(c_c_outer_outer_inner * 6, 6) + T.Broadcast(o * 3, 6) + T.Broadcast(2, 6) - T.Ramp(0, 1, 6) % T.Broadcast(3, 6)]\n                for w_c_outer_outer_inner in range(3):\n                    for b_c_outer_inner_init, c_c_outer_inner_init in T.grid(6, 2):\n                        compute_local_1[b_c_outer_inner_init * 30 + c_c_outer_outer_inner * 6 + c_c_outer_inner_init * 3 + w_c_outer_outer_inner] = T.float32(0)\n                    for dw_outer, b_c_outer_inner, c_c_outer_inner, dc_inner in T.grid(3, 6, 2, 2):\n                        cse_var_1: T.int32 = b_c_outer_inner * 30 + c_c_outer_outer_inner * 6 + c_c_outer_inner * 3 + w_c_outer_outer_inner\n                        compute_local_1[cse_var_1] = compute_local_1[cse_var_1] + data_pad_1[b_c_outer_inner * 46 + dc_inner * 23 + w_outer * 3 + w_c_outer_outer_inner + dw_outer] * data_dilate_1[c_c_outer_inner * 6 + dc_inner * 3 + dw_outer]\n            for b_inner, c_inner in T.grid(6, 10):\n                compute_1 = T.Buffer((1260,), data=compute.data)\n                compute_1[b_inner * 210 + c_inner * 21 + w_outer * 3:b_inner * 210 + c_inner * 21 + w_outer * 3 + 3] = compute_local_1[b_inner * 30 + c_inner * 3:b_inner * 30 + c_inner * 3 + 3]",
        "data": "6_2_10",
        "kernel": "26_10_3"
    },
    {
        "op_name": "conv1d_transpose_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  void* data_pad = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)25760, 2, 32);\n  if (data_pad == NULL) {\n    return -1;\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 6440; ++i0_i1_fused_i2_fused) {\n    int32_t cse_var_1 = (i0_i1_fused_i2_fused % 161);\n    float data_dilate[1];\n    if (2 <= cse_var_1) {\n      if (cse_var_1 < 159) {\n        data_dilate[0] = (((cse_var_1 % 2) == 0) ? ((float*)data_1)[((((i0_i1_fused_i2_fused / 161) * 79) + (cse_var_1 >> 1)) - 1)] : 0.000000e+00f);\n      },\n    },\n    ((float*)data_pad)[i0_i1_fused_i2_fused] = (((2 <= cse_var_1) && (cse_var_1 < 159)) ? data_dilate[0] : 0.000000e+00f);\n  },\n  for (int32_t b_outer_c_outer_fused_w_outer_fused = 0; b_outer_c_outer_fused_w_outer_fused < 158; ++b_outer_c_outer_fused_w_outer_fused) {\n    float kernel_2[15];\n    void* compute_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)2544, 2, 32);\n    if (compute_local == NULL) {\n      return -1;\n    },\n    for (int32_t i = 0; i < 5; ++i) {\n      int32_t3 v_ = int32_t3(((((i * 237) + ((b_outer_c_outer_fused_w_outer_fused % 79) * 3)) + 2))+(-1*0), ((((i * 237) + ((b_outer_c_outer_fused_w_outer_fused % 79) * 3)) + 2))+(-1*1), ((((i * 237) + ((b_outer_c_outer_fused_w_outer_fused % 79) * 3)) + 2))+(-1*2));\n      *(float3*)(kernel_2 + (i * 3)) = (float3(((float*)kernel_1)[v_.s0],((float*)kernel_1)[v_.s1],((float*)kernel_1)[v_.s2]));\n    },\n    for (int32_t w_c_outer_outer_inner = 0; w_c_outer_outer_inner < 3; ++w_c_outer_outer_inner) {\n      for (int32_t w_c_outer_inner_init = 0; w_c_outer_inner_init < 53; ++w_c_outer_inner_init) {\n        for (int32_t b_c_inner_init = 0; b_c_inner_init < 4; ++b_c_inner_init) {\n          ((float*)compute_local)[(((b_c_inner_init * 159) + (w_c_outer_outer_inner * 53)) + w_c_outer_inner_init)] = 0.000000e+00f;\n        },\n      },\n      for (int32_t dc_outer = 0; dc_outer < 5; ++dc_outer) {\n        for (int32_t w_c_outer_inner = 0; w_c_outer_inner < 53; ++w_c_outer_inner) {\n          for (int32_t dw_inner = 0; dw_inner < 3; ++dw_inner) {\n            for (int32_t b_c_inner = 0; b_c_inner < 4; ++b_c_inner) {\n              int32_t cse_var_3 = (w_c_outer_outer_inner * 53);\n              int32_t cse_var_2 = (((b_c_inner * 159) + cse_var_3) + w_c_outer_inner);\n              ((float*)compute_local)[cse_var_2] = (((float*)compute_local)[cse_var_2] + (((float*)data_pad)[(((((((b_outer_c_outer_fused_w_outer_fused / 79) * 3220) + (b_c_inner * 805)) + (dc_outer * 161)) + cse_var_3) + w_c_outer_inner) + dw_inner)] * kernel_2[((dc_outer * 3) + dw_inner)]));\n            },\n          },\n        },\n      },\n    },\n    for (int32_t b_inner = 0; b_inner < 4; ++b_inner) {\n      for (int32_t w_inner = 0; w_inner < 159; ++w_inner) {\n        ((float*)compute_1)[(((((b_outer_c_outer_fused_w_outer_fused / 79) * 50244) + (b_inner * 12561)) + ((b_outer_c_outer_fused_w_outer_fused % 79) * 159)) + w_inner)] = ((float*)compute_local)[((b_inner * 159) + w_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, compute_local) != 0) {\n      return -1;\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, data_pad) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(316) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(316) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel) {\n  float compute_local[3];\n  __shared__ float data_pad_shared[100];\n  __shared__ float kernel_shared[1185];\n  for (int w_c_outer_inner_init = 0; w_c_outer_inner_init < 3; ++w_c_outer_inner_init) {\n    compute_local[w_c_outer_inner_init] = 0.000000e+00f;\n  },\n  if (((int)threadIdx.x) < 20) {\n    for (int ax0_ax1_fused_ax2_fused_inner_s = 0; ax0_ax1_fused_ax2_fused_inner_s < 5; ++ax0_ax1_fused_ax2_fused_inner_s) {\n      data_pad_shared[((((int)threadIdx.x) * 5) + ax0_ax1_fused_ax2_fused_inner_s)] = ((((2 <= (((((int)blockIdx.x) % 53) * 3) + ax0_ax1_fused_ax2_fused_inner_s)) && (((ax0_ax1_fused_ax2_fused_inner_s / 3) + (((int)blockIdx.x) % 53)) < 53)) && (((ax0_ax1_fused_ax2_fused_inner_s + (((int)blockIdx.x) % 53)) % 2) == 0)) ? data[(((((((int)blockIdx.x) / 53) * 1580) + (((int)threadIdx.x) * 79)) + ((((((int)blockIdx.x) % 53) * 3) + ax0_ax1_fused_ax2_fused_inner_s) >> 1)) - 1)] : 0.000000e+00f);\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 4; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 4) + (((int)threadIdx.x) / 79)) < 15) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 316) + ((int)threadIdx.x))] = kernel[(((((((ax0_ax1_fused_ax2_fused_outer_outer + ((int)threadIdx.x)) % 15) / 3) * 237) + ((((ax0_ax1_fused_ax2_fused_outer_outer * 316) + ((int)threadIdx.x)) / 15) * 3)) + 2) - ((ax0_ax1_fused_ax2_fused_outer_outer + ((int)threadIdx.x)) % 3))];\n    },\n  },\n  __syncthreads();\n  for (int w_c_outer_inner = 0; w_c_outer_inner < 3; ++w_c_outer_inner) {\n    for (int dc_inner = 0; dc_inner < 5; ++dc_inner) {\n      for (int dw_inner = 0; dw_inner < 3; ++dw_inner) {\n        compute_local[w_c_outer_inner] = (compute_local[w_c_outer_inner] + (data_pad_shared[(((((((int)threadIdx.x) / 79) * 25) + (dc_inner * 5)) + w_c_outer_inner) + dw_inner)] * kernel_shared[((((((int)threadIdx.x) % 79) * 15) + (dc_inner * 3)) + dw_inner)]));\n      },\n    },\n  },\n  for (int w_inner = 0; w_inner < 3; ++w_inner) {\n    compute[(((((((int)blockIdx.x) / 53) * 50244) + (((int)threadIdx.x) * 159)) + ((((int)blockIdx.x) % 53) * 3)) + w_inner)] = compute_local[w_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 5, 79), \"float32\"), kernel: T.Buffer((85, 79, 3), \"float32\"), compute: T.Buffer((8, 79, 159), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        data_pad = T.allocate([6440], \"float32\", \"global\")\n        data_pad_1 = T.Buffer((6440,), data=data_pad)\n        for i0_i1_fused_i2_fused in T.parallel(6440):\n            cse_var_1: T.int32 = i0_i1_fused_i2_fused % 161\n            data_dilate = T.allocate([1], \"float32\", \"global\")\n            data_dilate_1 = T.Buffer((1,), data=data_dilate, align=4)\n            if T.likely(2 <= cse_var_1):\n                if T.likely(cse_var_1 < 159):\n                    data_1 = T.Buffer((3160,), data=data.data)\n                    data_dilate_1[0] = T.if_then_else(cse_var_1 % 2 == 0, data_1[i0_i1_fused_i2_fused // 161 * 79 + cse_var_1 // 2 - 1], T.float32(0))\n            data_pad_1[i0_i1_fused_i2_fused] = T.if_then_else(2 <= cse_var_1 and cse_var_1 < 159, data_dilate_1[0], T.float32(0))\n        for b_outer_c_outer_fused_w_outer_fused in T.parallel(158):\n            kernel_1 = T.allocate([15], \"float32\", \"global\")\n            compute_local = T.allocate([636], \"float32\", \"local\")\n            kernel_2 = T.Buffer((15,), data=kernel_1, align=32)\n            for i in range(5):\n                kernel_3 = T.Buffer((20145,), data=kernel.data)\n                kernel_2[i * 3:i * 3 + 3] = kernel_3[i * 237 + b_outer_c_outer_fused_w_outer_fused % 79 * 3 + 2:i * 237 + b_outer_c_outer_fused_w_outer_fused % 79 * 3 + 2 + -3:-1]\n            compute_local_1 = T.Buffer((636,), data=compute_local, scope=\"local\")\n            for w_c_outer_outer_inner in range(3):\n                for w_c_outer_inner_init, b_c_inner_init in T.grid(53, 4):\n                    compute_local_1[b_c_inner_init * 159 + w_c_outer_outer_inner * 53 + w_c_outer_inner_init] = T.float32(0)\n                for dc_outer, w_c_outer_inner, dw_inner, b_c_inner in T.grid(5, 53, 3, 4):\n                    cse_var_3: T.int32 = w_c_outer_outer_inner * 53\n                    cse_var_2: T.int32 = b_c_inner * 159 + cse_var_3 + w_c_outer_inner\n                    compute_local_1[cse_var_2] = compute_local_1[cse_var_2] + data_pad_1[b_outer_c_outer_fused_w_outer_fused // 79 * 3220 + b_c_inner * 805 + dc_outer * 161 + cse_var_3 + w_c_outer_inner + dw_inner] * kernel_2[dc_outer * 3 + dw_inner]\n            for b_inner, w_inner in T.grid(4, 159):\n                compute_1 = T.Buffer((100488,), data=compute.data)\n                compute_1[b_outer_c_outer_fused_w_outer_fused // 79 * 50244 + b_inner * 12561 + b_outer_c_outer_fused_w_outer_fused % 79 * 159 + w_inner] = compute_local_1[b_inner * 159 + w_inner]",
        "data": "8_5_79",
        "kernel": "85_79_3"
    },
    {
        "op_name": "conv1d_transpose_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_compute_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused < 93; ++b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused) {\n    void* data_pad = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)5460, 2, 32);\n    if (data_pad == NULL) {\n      return -1;\n    },\n    void* data_dilate = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1708, 2, 32);\n    if (data_dilate == NULL) {\n      return -1;\n    },\n    float kernel_2[21];\n    for (int32_t i0 = 0; i0 < 3; ++i0) {\n      for (int32_t i1 = 0; i1 < 7; ++i1) {\n        for (int32_t i2 = 0; i2 < 61; ++i2) {\n          ((float*)data_dilate)[((i1 * 61) + i2)] = (((i2 % 2) == 0) ? ((float*)data_1)[(((((b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused / 31) * 651) + (i0 * 217)) + (i1 * 31)) + (i2 >> 1))] : 0.000000e+00f);\n        },\n      },\n      for (int32_t i1_1 = 0; i1_1 < 7; ++i1_1) {\n        for (int32_t i2_1 = 0; i2_1 < 65; ++i2_1) {\n          ((float*)data_pad)[(((i0 * 455) + (i1_1 * 65)) + i2_1)] = (((2 <= i2_1) && (i2_1 < 63)) ? ((float*)data_dilate)[(((i1_1 * 61) + i2_1) - 2)] : 0.000000e+00f);\n        },\n      },\n    },\n    for (int32_t w_outer_outer_inner = 0; w_outer_outer_inner < 9; ++w_outer_outer_inner) {\n      for (int32_t i = 0; i < 7; ++i) {\n        int32_t3 v_ = int32_t3(((((i * 93) + ((b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 31) * 3)) + 2))+(-1*0), ((((i * 93) + ((b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 31) * 3)) + 2))+(-1*1), ((((i * 93) + ((b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 31) * 3)) + 2))+(-1*2));\n        *(float3*)(kernel_2 + (i * 3)) = (float3(((float*)kernel_1)[v_.s0],((float*)kernel_1)[v_.s1],((float*)kernel_1)[v_.s2]));\n      },\n      for (int32_t b_inner_init = 0; b_inner_init < 3; ++b_inner_init) {\n        *(float7*)(((float*)compute_1) + (((((b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused / 31) * 5859) + (b_inner_init * 1953)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 31) * 63)) + (w_outer_outer_inner * 7))) = ((float7)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n      },\n      for (int32_t dc_outer = 0; dc_outer < 7; ++dc_outer) {\n        for (int32_t dw_inner = 0; dw_inner < 3; ++dw_inner) {\n          for (int32_t b_inner = 0; b_inner < 3; ++b_inner) {\n            int32_t cse_var_2 = (w_outer_outer_inner * 7);\n            int32_t cse_var_1 = (((((b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused / 31) * 5859) + (b_inner * 1953)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 31) * 63)) + cse_var_2);\n            int32_t7 v__1 = int32_t7((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6));\n            int32_t7 v__2 = int32_t7((((((b_inner * 455) + (dc_outer * 65)) + cse_var_2) + dw_inner))+(1*0), (((((b_inner * 455) + (dc_outer * 65)) + cse_var_2) + dw_inner))+(1*1), (((((b_inner * 455) + (dc_outer * 65)) + cse_var_2) + dw_inner))+(1*2), (((((b_inner * 455) + (dc_outer * 65)) + cse_var_2) + dw_inner))+(1*3), (((((b_inner * 455) + (dc_outer * 65)) + cse_var_2) + dw_inner))+(1*4), (((((b_inner * 455) + (dc_outer * 65)) + cse_var_2) + dw_inner))+(1*5), (((((b_inner * 455) + (dc_outer * 65)) + cse_var_2) + dw_inner))+(1*6));\n            *(float7*)(((float*)compute_1) + cse_var_1) = ((float7(((float*)compute_1)[v__1.s0],((float*)compute_1)[v__1.s1],((float*)compute_1)[v__1.s2],((float*)compute_1)[v__1.s3],((float*)compute_1)[v__1.s4],((float*)compute_1)[v__1.s5],((float*)compute_1)[v__1.s6])) + ((float7(((float*)data_pad)[v__2.s0],((float*)data_pad)[v__2.s1],((float*)data_pad)[v__2.s2],((float*)data_pad)[v__2.s3],((float*)data_pad)[v__2.s4],((float*)data_pad)[v__2.s5],((float*)data_pad)[v__2.s6])) * ((float7)(kernel_2[((dc_outer * 3) + dw_inner)], kernel_2[((dc_outer * 3) + dw_inner)], kernel_2[((dc_outer * 3) + dw_inner)], kernel_2[((dc_outer * 3) + dw_inner)], kernel_2[((dc_outer * 3) + dw_inner)], kernel_2[((dc_outer * 3) + dw_inner)], kernel_2[((dc_outer * 3) + dw_inner)]))));\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, data_dilate) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, data_pad) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(217) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(217) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ kernel) {\n  float compute_local[9];\n  __shared__ float data_pad_shared[483];\n  __shared__ float kernel_shared[651];\n  for (int b_c_inner_init = 0; b_c_inner_init < 3; ++b_c_inner_init) {\n    for (int w_c_inner_init = 0; w_c_inner_init < 3; ++w_c_inner_init) {\n      compute_local[((b_c_inner_init * 3) + w_c_inner_init)] = 0.000000e+00f;\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 3; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 31) + (((int)threadIdx.x) / 7)) < 69) {\n      data_pad_shared[((((((ax0_ax1_fused_ax2_fused_outer_outer * 31) + (((int)threadIdx.x) / 7)) / 23) * 161) + (((((ax0_ax1_fused_ax2_fused_outer_outer * 56) + ((int)threadIdx.x)) % 161) / 23) * 23)) + (((ax0_ax1_fused_ax2_fused_outer_outer * 10) + ((int)threadIdx.x)) % 23))] = ((((2 <= (((((int)blockIdx.x) % 3) * 21) + (((ax0_ax1_fused_ax2_fused_outer_outer * 10) + ((int)threadIdx.x)) % 23))) && ((((((ax0_ax1_fused_ax2_fused_outer_outer * 10) + ((int)threadIdx.x)) % 23) / 21) + (((int)blockIdx.x) % 3)) < 3)) && ((((((ax0_ax1_fused_ax2_fused_outer_outer * 10) + ((int)threadIdx.x)) % 23) + (((int)blockIdx.x) % 3)) % 2) == 0)) ? data[((((((((int)blockIdx.x) / 3) * 651) + ((((ax0_ax1_fused_ax2_fused_outer_outer * 31) + (((int)threadIdx.x) / 7)) / 23) * 217)) + (((((ax0_ax1_fused_ax2_fused_outer_outer * 56) + ((int)threadIdx.x)) % 161) / 23) * 31)) + ((((((int)blockIdx.x) % 3) * 21) + (((ax0_ax1_fused_ax2_fused_outer_outer * 10) + ((int)threadIdx.x)) % 23)) >> 1)) - 1)] : 0.000000e+00f);\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 3; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 217) + ((int)threadIdx.x))] = kernel[((((((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 7) + ((int)threadIdx.x)) % 21) / 3) * 93) + ((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 31) + (((int)threadIdx.x) / 7)) / 3) * 3)) + 2) - ((ax0_ax1_fused_ax2_fused_outer_outer_1 + ((int)threadIdx.x)) % 3))];\n  },\n  __syncthreads();\n  for (int dc_outer_inner = 0; dc_outer_inner < 7; ++dc_outer_inner) {\n    for (int dw_inner = 0; dw_inner < 3; ++dw_inner) {\n      for (int b_c_inner = 0; b_c_inner < 3; ++b_c_inner) {\n        for (int w_c_inner = 0; w_c_inner < 3; ++w_c_inner) {\n          compute_local[((b_c_inner * 3) + w_c_inner)] = (compute_local[((b_c_inner * 3) + w_c_inner)] + (data_pad_shared[(((((b_c_inner * 161) + (dc_outer_inner * 23)) + ((((int)threadIdx.x) % 7) * 3)) + w_c_inner) + dw_inner)] * kernel_shared[((((((int)threadIdx.x) / 7) * 21) + (dc_outer_inner * 3)) + dw_inner)]));\n        },\n      },\n    },\n  },\n  for (int b_inner = 0; b_inner < 3; ++b_inner) {\n    for (int w_inner = 0; w_inner < 3; ++w_inner) {\n      compute[(((((((((int)blockIdx.x) / 3) * 5859) + (b_inner * 1953)) + ((((int)threadIdx.x) / 7) * 63)) + ((((int)blockIdx.x) % 3) * 21)) + ((((int)threadIdx.x) % 7) * 3)) + w_inner)] = compute_local[((b_inner * 3) + w_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 7, 31), \"float32\"), kernel: T.Buffer((13, 31, 3), \"float32\"), compute: T.Buffer((9, 31, 63), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused in T.parallel(93):\n            data_pad = T.allocate([1365], \"float32\", \"global\")\n            data_dilate = T.allocate([427], \"float32\", \"global\")\n            kernel_1 = T.allocate([21], \"float32\", \"global\")\n            data_pad_1 = T.Buffer((1365,), data=data_pad)\n            for i0 in range(3):\n                data_dilate_1 = T.Buffer((427,), data=data_dilate)\n                for i1, i2 in T.grid(7, 61):\n                    data_1 = T.Buffer((1953,), data=data.data)\n                    data_dilate_1[i1 * 61 + i2] = T.if_then_else(i2 % 2 == 0, data_1[b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused // 31 * 651 + i0 * 217 + i1 * 31 + i2 // 2], T.float32(0))\n                for i1, i2 in T.grid(7, 65):\n                    data_pad_1[i0 * 455 + i1 * 65 + i2] = T.if_then_else(2 <= i2 and i2 < 63, data_dilate_1[i1 * 61 + i2 - 2], T.float32(0))\n            for w_outer_outer_inner in range(9):\n                kernel_2 = T.Buffer((21,), data=kernel_1)\n                for i in range(7):\n                    kernel_3 = T.Buffer((1209,), data=kernel.data)\n                    kernel_2[i * 3:i * 3 + 3] = kernel_3[i * 93 + b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 31 * 3 + 2:i * 93 + b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 31 * 3 + 2 + -3:-1]\n                compute_1 = T.Buffer((17577,), data=compute.data)\n                for b_inner_init in range(3):\n                    compute_1[b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused // 31 * 5859 + b_inner_init * 1953 + b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 31 * 63 + w_outer_outer_inner * 7:b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused // 31 * 5859 + b_inner_init * 1953 + b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 31 * 63 + w_outer_outer_inner * 7 + 7] = T.Broadcast(T.float32(0), 7)\n                for dc_outer, dw_inner, b_inner in T.grid(7, 3, 3):\n                    cse_var_2: T.int32 = w_outer_outer_inner * 7\n                    cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused // 31 * 5859 + b_inner * 1953 + b_outer_outer_outer_c_outer_outer_outer_fused_w_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 31 * 63 + cse_var_2\n                    compute_1[cse_var_1:cse_var_1 + 7] = compute_1[cse_var_1:cse_var_1 + 7] + data_pad_1[b_inner * 455 + dc_outer * 65 + cse_var_2 + dw_inner:b_inner * 455 + dc_outer * 65 + cse_var_2 + dw_inner + 7] * T.Broadcast(kernel_2[dc_outer * 3 + dw_inner], 7)",
        "data": "9_7_31",
        "kernel": "13_31_3"
    },
    {
        "op_name": "conv3d_ncdhw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv3d_ncdhw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv3d_ncdhw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv3d_ncdhw_1 = (((DLTensor*)conv3d_ncdhw)[0].data);\n  void* default_function_conv3d_ncdhw_shape = (((DLTensor*)conv3d_ncdhw)[0].shape);\n  void* default_function_conv3d_ncdhw_strides = (((DLTensor*)conv3d_ncdhw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv3d_ncdhw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused < 946; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)124160, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    void* conv3d_ncdhw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)64, 2, 32);\n    if (conv3d_ncdhw_local == NULL) {\n      return -1;\n    },\n    for (int32_t i1 = 0; i1 < 97; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        for (int32_t i3 = 0; i3 < 4; ++i3) {\n          for (int32_t i4_s = 0; i4_s < 10; ++i4_s) {\n            int32_t cse_var_2 = (nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused % 11);\n            int32_t cse_var_1 = ((cse_var_2 * 8) + i2);\n            ((float*)pad_temp)[((((i1 * 320) + (i2 * 40)) + (i3 * 10)) + i4_s)] = (((((((1 <= cse_var_1) && (cse_var_1 < 87)) && (1 <= i3)) && (i3 < 3)) && (1 <= i4_s)) && (i4_s < 9)) ? ((float*)data_1)[((((((i1 * 1376) + (cse_var_2 * 128)) + (i2 * 16)) + (i3 * 8)) + i4_s) - 25)] : 0.000000e+00f);\n          },\n        },\n      },\n    },\n    for (int32_t yy_outer_inner = 0; yy_outer_inner < 2; ++yy_outer_inner) {\n      for (int32_t zz_outer_inner = 0; zz_outer_inner < 2; ++zz_outer_inner) {\n        for (int32_t yy_c_outer_inner_init = 0; yy_c_outer_inner_init < 4; ++yy_c_outer_inner_init) {\n          ((float4*)conv3d_ncdhw_local)[yy_c_outer_inner_init] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n        },\n        for (int32_t rc_outer = 0; rc_outer < 97; ++rc_outer) {\n          for (int32_t rx_outer = 0; rx_outer < 4; ++rx_outer) {\n            for (int32_t yy_c_outer_inner = 0; yy_c_outer_inner < 4; ++yy_c_outer_inner) {\n              for (int32_t rz_inner = 0; rz_inner < 3; ++rz_inner) {\n                int32_t4 v_ = int32_t4((((((((rc_outer * 320) + (yy_outer_inner * 160)) + (yy_c_outer_inner * 40)) + (rx_outer * 10)) + (zz_outer_inner * 4)) + rz_inner))+(1*0), (((((((rc_outer * 320) + (yy_outer_inner * 160)) + (yy_c_outer_inner * 40)) + (rx_outer * 10)) + (zz_outer_inner * 4)) + rz_inner))+(1*1), (((((((rc_outer * 320) + (yy_outer_inner * 160)) + (yy_c_outer_inner * 40)) + (rx_outer * 10)) + (zz_outer_inner * 4)) + rz_inner))+(1*2), (((((((rc_outer * 320) + (yy_outer_inner * 160)) + (yy_c_outer_inner * 40)) + (rx_outer * 10)) + (zz_outer_inner * 4)) + rz_inner))+(1*3));\n                ((float4*)conv3d_ncdhw_local)[yy_c_outer_inner] = (((float4*)conv3d_ncdhw_local)[yy_c_outer_inner] + ((float4(((float*)pad_temp)[v_.s0],((float*)pad_temp)[v_.s1],((float*)pad_temp)[v_.s2],((float*)pad_temp)[v_.s3])) * ((float4)(((float*)kernel_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused / 11) * 1164) + (rc_outer * 12)) + (rx_outer * 3)) + rz_inner)], ((float*)kernel_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused / 11) * 1164) + (rc_outer * 12)) + (rx_outer * 3)) + rz_inner)], ((float*)kernel_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused / 11) * 1164) + (rc_outer * 12)) + (rx_outer * 3)) + rz_inner)], ((float*)kernel_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused / 11) * 1164) + (rc_outer * 12)) + (rx_outer * 3)) + rz_inner)]))));\n              },\n            },\n          },\n        },\n        for (int32_t yy_inner = 0; yy_inner < 4; ++yy_inner) {\n          *(float4*)(((float*)conv3d_ncdhw_1) + ((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 64) + (yy_outer_inner * 32)) + (yy_inner * 8)) + (zz_outer_inner * 4))) = ((float4*)conv3d_ncdhw_local)[yy_inner];\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv3d_ncdhw_local) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(344) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(344) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv3d_ncdhw_local[8];\n  __shared__ float pad_temp_shared[192];\n  __shared__ float kernel_shared[1032];\n  for (int zz_c_outer_inner_init = 0; zz_c_outer_inner_init < 2; ++zz_c_outer_inner_init) {\n    for (int yy_c_inner_init = 0; yy_c_inner_init < 2; ++yy_c_inner_init) {\n      for (int zz_c_inner_init = 0; zz_c_inner_init < 2; ++zz_c_inner_init) {\n        conv3d_ncdhw_local[(((yy_c_inner_init * 4) + (zz_c_outer_inner_init * 2)) + zz_c_inner_init)] = 0.000000e+00f;\n      },\n    },\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 97; ++rc_outer_outer) {\n    __syncthreads();\n    if (((int)threadIdx.x) < 8) {\n      for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s < 24; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s) {\n        pad_temp_shared[((((int)threadIdx.x) * 24) + ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s)] = (((((((1 <= (((((int)blockIdx.x) >> 1) * 8) + ((int)threadIdx.x))) && ((((((int)blockIdx.x) >> 1) * 8) + ((int)threadIdx.x)) < 87)) && (6 <= ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s)) && (ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s < 18)) && (1 <= (((((int)blockIdx.x) & 1) * 4) + (ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s % 6)))) && ((((((int)blockIdx.x) & 1) * 4) + (ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s % 6)) < 9)) ? data[(((((((rc_outer_outer * 1376) + ((((int)blockIdx.x) >> 1) * 128)) + (((int)threadIdx.x) * 16)) + ((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s / 6) * 8)) + ((((int)blockIdx.x) & 1) * 4)) + (ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s % 6)) - 25)] : 0.000000e+00f);\n      },\n    },\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer < 3; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 344) + ((int)threadIdx.x))] = kernel[(((((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 86) + (((int)threadIdx.x) >> 2)) / 3) * 1164) + (rc_outer_outer * 12)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 8) + ((int)threadIdx.x)) % 12) / 3) * 3)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 2) + ((int)threadIdx.x)) % 3))];\n    },\n    __syncthreads();\n    for (int rx_outer_inner = 0; rx_outer_inner < 2; ++rx_outer_inner) {\n      for (int zz_c_outer_inner = 0; zz_c_outer_inner < 2; ++zz_c_outer_inner) {\n        for (int rx_inner = 0; rx_inner < 2; ++rx_inner) {\n          for (int rz_inner = 0; rz_inner < 3; ++rz_inner) {\n            for (int yy_c_inner = 0; yy_c_inner < 2; ++yy_c_inner) {\n              for (int zz_c_inner = 0; zz_c_inner < 2; ++zz_c_inner) {\n                conv3d_ncdhw_local[(((yy_c_inner * 4) + (zz_c_outer_inner * 2)) + zz_c_inner)] = (conv3d_ncdhw_local[(((yy_c_inner * 4) + (zz_c_outer_inner * 2)) + zz_c_inner)] + (pad_temp_shared[((((((((((int)threadIdx.x) & 3) * 48) + (yy_c_inner * 24)) + (rx_outer_inner * 12)) + (rx_inner * 6)) + (zz_c_outer_inner * 2)) + zz_c_inner) + rz_inner)] * kernel_shared[(((((((int)threadIdx.x) >> 2) * 12) + (rx_outer_inner * 6)) + (rx_inner * 3)) + rz_inner)]));\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int yy_inner = 0; yy_inner < 2; ++yy_inner) {\n    for (int zz_inner = 0; zz_inner < 4; ++zz_inner) {\n      conv3d_ncdhw[(((((((((int)threadIdx.x) >> 2) * 704) + ((((int)blockIdx.x) >> 1) * 64)) + ((((int)threadIdx.x) & 3) * 16)) + (yy_inner * 8)) + ((((int)blockIdx.x) & 1) * 4)) + zz_inner)] = conv3d_ncdhw_local[((yy_inner * 4) + zz_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 97, 86, 2, 8), \"float32\"), kernel: T.Buffer((86, 97, 1, 4, 3), \"float32\"), conv3d_ncdhw: T.Buffer((1, 86, 88, 1, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused in T.parallel(946):\n            pad_temp = T.allocate([31040], \"float32\", \"global\")\n            conv3d_ncdhw_local = T.allocate([4], \"float32x4\", \"local\")\n            pad_temp_1 = T.Buffer((31040,), data=pad_temp)\n            for i1, i2, i3, i4_s in T.grid(97, 8, 4, 10):\n                cse_var_2: T.int32 = nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused % 11\n                cse_var_1: T.int32 = cse_var_2 * 8 + i2\n                data_1 = T.Buffer((133472,), data=data.data)\n                pad_temp_1[i1 * 320 + i2 * 40 + i3 * 10 + i4_s] = T.if_then_else(1 <= cse_var_1 and cse_var_1 < 87 and 1 <= i3 and i3 < 3 and 1 <= i4_s and i4_s < 9, data_1[i1 * 1376 + cse_var_2 * 128 + i2 * 16 + i3 * 8 + i4_s - 25], T.float32(0))\n            for yy_outer_inner, zz_outer_inner in T.grid(2, 2):\n                conv3d_ncdhw_local_1 = T.Buffer((4,), \"float32x4\", data=conv3d_ncdhw_local, scope=\"local\")\n                for yy_c_outer_inner_init in range(4):\n                    conv3d_ncdhw_local_1[yy_c_outer_inner_init] = T.Broadcast(T.float32(0), 4)\n                for rc_outer, rx_outer, yy_c_outer_inner, rz_inner in T.grid(97, 4, 4, 3):\n                    kernel_1 = T.Buffer((100104,), data=kernel.data)\n                    conv3d_ncdhw_local_1[yy_c_outer_inner] = conv3d_ncdhw_local_1[yy_c_outer_inner] + pad_temp_1[rc_outer * 320 + yy_outer_inner * 160 + yy_c_outer_inner * 40 + rx_outer * 10 + zz_outer_inner * 4 + rz_inner:rc_outer * 320 + yy_outer_inner * 160 + yy_c_outer_inner * 40 + rx_outer * 10 + zz_outer_inner * 4 + rz_inner + 4] * T.Broadcast(kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused // 11 * 1164 + rc_outer * 12 + rx_outer * 3 + rz_inner], 4)\n                for yy_inner in range(4):\n                    conv3d_ncdhw_1 = T.Buffer((60544,), data=conv3d_ncdhw.data)\n                    conv3d_ncdhw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 64 + yy_outer_inner * 32 + yy_inner * 8 + zz_outer_inner * 4:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused * 64 + yy_outer_inner * 32 + yy_inner * 8 + zz_outer_inner * 4 + 4] = conv3d_ncdhw_local_1[yy_inner]",
        "data": "1_97_86_2_8",
        "kernel": "86_97_1_4_3"
    },
    {
        "op_name": "conv3d_ncdhw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv3d_ncdhw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv3d_ncdhw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv3d_ncdhw_1 = (((DLTensor*)conv3d_ncdhw)[0].data);\n  void* default_function_conv3d_ncdhw_shape = (((DLTensor*)conv3d_ncdhw)[0].shape);\n  void* default_function_conv3d_ncdhw_strides = (((DLTensor*)conv3d_ncdhw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv3d_ncdhw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused < 406; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)191520, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 60; ++i2) {\n        for (int32_t i3 = 0; i3 < 7; ++i3) {\n          for (int32_t i4_s = 0; i4_s < 6; ++i4_s) {\n            ((float*)pad_temp)[((((i1 * 2520) + (i2 * 42)) + (i3 * 6)) + i4_s)] = (((((((1 <= i2) && (i2 < 59)) && (1 <= i3)) && (i3 < 6)) && (1 <= i4_s)) && (i4_s < 5)) ? ((float*)data_1)[(((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused / 58) * 22040) + (i1 * 1160)) + (i2 * 20)) + (i3 * 4)) + i4_s) - 25)] : 0.000000e+00f);\n          },\n        },\n      },\n    },\n    for (int32_t yy_outer_outer_inner = 0; yy_outer_outer_inner < 9; ++yy_outer_outer_inner) {\n      for (int32_t xx_outer_outer_inner = 0; xx_outer_outer_inner < 5; ++xx_outer_outer_inner) {\n        for (int32_t yy_outer_inner_init = 0; yy_outer_inner_init < 2; ++yy_outer_inner_init) {\n          for (int32_t yy_inner_init = 0; yy_inner_init < 3; ++yy_inner_init) {\n            *(float3*)(((float*)conv3d_ncdhw_1) + (((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused * 810) + (yy_outer_outer_inner * 90)) + (yy_outer_inner_init * 45)) + (yy_inner_init * 15)) + (xx_outer_outer_inner * 3))) = ((float3)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n          },\n        },\n        for (int32_t yy_outer_inner = 0; yy_outer_inner < 2; ++yy_outer_inner) {\n          for (int32_t rc_inner = 0; rc_inner < 19; ++rc_inner) {\n            for (int32_t ry_inner = 0; ry_inner < 7; ++ry_inner) {\n              for (int32_t rx_inner = 0; rx_inner < 3; ++rx_inner) {\n                for (int32_t rz_inner = 0; rz_inner < 4; ++rz_inner) {\n                  for (int32_t yy_inner = 0; yy_inner < 3; ++yy_inner) {\n                    int32_t cse_var_1 = (((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused * 810) + (yy_outer_outer_inner * 90)) + (yy_outer_inner * 45)) + (yy_inner * 15)) + (xx_outer_outer_inner * 3));\n                    int32_t3 v_ = int32_t3((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2));\n                    int32_t3 v__1 = int32_t3((((((((((rc_inner * 2520) + (yy_outer_outer_inner * 252)) + (yy_outer_inner * 126)) + (yy_inner * 42)) + (ry_inner * 42)) + (xx_outer_outer_inner * 6)) + (rx_inner * 6)) + rz_inner))+(1*0), (((((((((rc_inner * 2520) + (yy_outer_outer_inner * 252)) + (yy_outer_inner * 126)) + (yy_inner * 42)) + (ry_inner * 42)) + (xx_outer_outer_inner * 6)) + (rx_inner * 6)) + rz_inner))+(1*1), (((((((((rc_inner * 2520) + (yy_outer_outer_inner * 252)) + (yy_outer_inner * 126)) + (yy_inner * 42)) + (ry_inner * 42)) + (xx_outer_outer_inner * 6)) + (rx_inner * 6)) + rz_inner))+(1*2));\n                    *(float3*)(((float*)conv3d_ncdhw_1) + cse_var_1) = ((float3(((float*)conv3d_ncdhw_1)[v_.s0],((float*)conv3d_ncdhw_1)[v_.s1],((float*)conv3d_ncdhw_1)[v_.s2])) + ((float3(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1],((float*)pad_temp)[v__1.s2])) * ((float3)(((float*)kernel_1)[((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 58) * 1596) + (rc_inner * 84)) + (ry_inner * 12)) + (rx_inner * 4)) + rz_inner)], ((float*)kernel_1)[((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 58) * 1596) + (rc_inner * 84)) + (ry_inner * 12)) + (rx_inner * 4)) + rz_inner)], ((float*)kernel_1)[((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 58) * 1596) + (rc_inner * 84)) + (ry_inner * 12)) + (rx_inner * 4)) + rz_inner)]))));\n                  },\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(21) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(21) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv3d_ncdhw_local[116];\n  __shared__ float pad_temp_shared[1064];\n  __shared__ float kernel_shared[2204];\n  for (int ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 29; ++ff_c_outer_inner_init) {\n    for (int ff_c_inner_init = 0; ff_c_inner_init < 2; ++ff_c_inner_init) {\n      for (int yy_c_inner_init = 0; yy_c_inner_init < 2; ++yy_c_inner_init) {\n        conv3d_ncdhw_local[(((ff_c_outer_inner_init * 4) + (ff_c_inner_init * 2)) + yy_c_inner_init)] = 0.000000e+00f;\n      },\n    },\n  },\n  for (int ry_outer_outer = 0; ry_outer_outer < 7; ++ry_outer_outer) {\n    for (int rx_outer_outer = 0; rx_outer_outer < 3; ++rx_outer_outer) {\n      for (int rz_outer_outer = 0; rz_outer_outer < 2; ++rz_outer_outer) {\n        __syncthreads();\n        for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer < 51; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer) {\n          if (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 3) + (((int)threadIdx.x) / 7)) < 152) {\n            pad_temp_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 21) + ((int)threadIdx.x))] = (((((((1 <= ((((((int)blockIdx.x) / 5) * 2) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 5) + ((int)threadIdx.x)) & 7) >> 2)) + ry_outer_outer)) && (((((((int)blockIdx.x) / 5) * 2) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 5) + ((int)threadIdx.x)) & 7) >> 2)) + ry_outer_outer) < 59)) && (1 <= (rx_outer_outer + (((int)blockIdx.x) % 5)))) && ((rx_outer_outer + (((int)blockIdx.x) % 5)) < 6)) && (1 <= ((rz_outer_outer * 2) + ((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer + ((int)threadIdx.x)) & 3)))) && (((rz_outer_outer * 2) + ((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer + ((int)threadIdx.x)) & 3)) < 5)) ? data[((((((((((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 21) + ((int)threadIdx.x)) >> 3) * 1160) + ((((int)blockIdx.x) / 5) * 40)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 5) + ((int)threadIdx.x)) & 7) >> 2) * 20)) + (ry_outer_outer * 20)) + (rx_outer_outer * 4)) + ((((int)blockIdx.x) % 5) * 4)) + (rz_outer_outer * 2)) + ((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer + ((int)threadIdx.x)) & 3)) - 25)] : 0.000000e+00f);\n          },\n        },\n        for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 < 105; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1) {\n          if (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 * 21) + ((int)threadIdx.x)) < 2204) {\n            kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 * 21) + ((int)threadIdx.x))] = kernel[((((((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 * 21) + ((int)threadIdx.x)) >> 1) * 84) + (ry_outer_outer * 12)) + (rx_outer_outer * 4)) + (rz_outer_outer * 2)) + ((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 + ((int)threadIdx.x)) & 1))];\n          },\n        },\n        __syncthreads();\n        for (int rc_outer_inner = 0; rc_outer_inner < 19; ++rc_outer_inner) {\n          for (int ff_c_outer_inner = 0; ff_c_outer_inner < 29; ++ff_c_outer_inner) {\n            for (int rz_inner = 0; rz_inner < 2; ++rz_inner) {\n              for (int ff_c_inner = 0; ff_c_inner < 2; ++ff_c_inner) {\n                for (int yy_c_inner = 0; yy_c_inner < 2; ++yy_c_inner) {\n                  conv3d_ncdhw_local[(((ff_c_outer_inner * 4) + (ff_c_inner * 2)) + yy_c_inner)] = (conv3d_ncdhw_local[(((ff_c_outer_inner * 4) + (ff_c_inner * 2)) + yy_c_inner)] + (pad_temp_shared[((((((((int)threadIdx.x) / 3) * 152) + (rc_outer_inner * 8)) + (yy_c_inner * 4)) + rz_inner) + (((int)threadIdx.x) % 3))] * kernel_shared[((((ff_c_outer_inner * 76) + (ff_c_inner * 38)) + (rc_outer_inner * 2)) + rz_inner)]));\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 58; ++ff_inner) {\n    for (int yy_inner = 0; yy_inner < 2; ++yy_inner) {\n      conv3d_ncdhw[(((((((((int)threadIdx.x) / 3) * 46980) + (ff_inner * 810)) + ((((int)blockIdx.x) / 5) * 30)) + (yy_inner * 15)) + ((((int)blockIdx.x) % 5) * 3)) + (((int)threadIdx.x) % 3))] = conv3d_ncdhw_local[((ff_inner * 2) + yy_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 19, 58, 5, 4), \"float32\"), kernel: T.Buffer((58, 19, 7, 3, 4), \"float32\"), conv3d_ncdhw: T.Buffer((7, 58, 54, 5, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused in T.parallel(406):\n            pad_temp = T.allocate([47880], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((47880,), data=pad_temp)\n            for i1, i2, i3, i4_s in T.grid(19, 60, 7, 6):\n                data_1 = T.Buffer((154280,), data=data.data)\n                pad_temp_1[i1 * 2520 + i2 * 42 + i3 * 6 + i4_s] = T.if_then_else(1 <= i2 and i2 < 59 and 1 <= i3 and i3 < 6 and 1 <= i4_s and i4_s < 5, data_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused // 58 * 22040 + i1 * 1160 + i2 * 20 + i3 * 4 + i4_s - 25], T.float32(0))\n            for yy_outer_outer_inner, xx_outer_outer_inner in T.grid(9, 5):\n                conv3d_ncdhw_1 = T.Buffer((328860,), data=conv3d_ncdhw.data)\n                for yy_outer_inner_init, yy_inner_init in T.grid(2, 3):\n                    conv3d_ncdhw_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused * 810 + yy_outer_outer_inner * 90 + yy_outer_inner_init * 45 + yy_inner_init * 15 + xx_outer_outer_inner * 3:nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused * 810 + yy_outer_outer_inner * 90 + yy_outer_inner_init * 45 + yy_inner_init * 15 + xx_outer_outer_inner * 3 + 3] = T.Broadcast(T.float32(0), 3)\n                for yy_outer_inner, rc_inner, ry_inner, rx_inner, rz_inner, yy_inner in T.grid(2, 19, 7, 3, 4, 3):\n                    cse_var_1: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused * 810 + yy_outer_outer_inner * 90 + yy_outer_inner * 45 + yy_inner * 15 + xx_outer_outer_inner * 3\n                    kernel_1 = T.Buffer((92568,), data=kernel.data)\n                    conv3d_ncdhw_1[cse_var_1:cse_var_1 + 3] = conv3d_ncdhw_1[cse_var_1:cse_var_1 + 3] + pad_temp_1[rc_inner * 2520 + yy_outer_outer_inner * 252 + yy_outer_inner * 126 + yy_inner * 42 + ry_inner * 42 + xx_outer_outer_inner * 6 + rx_inner * 6 + rz_inner:rc_inner * 2520 + yy_outer_outer_inner * 252 + yy_outer_inner * 126 + yy_inner * 42 + ry_inner * 42 + xx_outer_outer_inner * 6 + rx_inner * 6 + rz_inner + 3] * T.Broadcast(kernel_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused % 58 * 1596 + rc_inner * 84 + ry_inner * 12 + rx_inner * 4 + rz_inner], 3)",
        "data": "7_19_58_5_4",
        "kernel": "58_19_7_3_4"
    },
    {
        "op_name": "conv3d_ncdhw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv3d_ncdhw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv3d_ncdhw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv3d_ncdhw_1 = (((DLTensor*)conv3d_ncdhw)[0].data);\n  void* default_function_conv3d_ncdhw_shape = (((DLTensor*)conv3d_ncdhw)[0].shape);\n  void* default_function_conv3d_ncdhw_strides = (((DLTensor*)conv3d_ncdhw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv3d_ncdhw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused < 711; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused) {\n    void* conv3d_ncdhw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1264, 2, 32);\n    if (conv3d_ncdhw_local == NULL) {\n      return -1;\n    },\n    float pad_temp[108];\n    for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 79; ++ff_c_outer_inner_init) {\n      for (int32_t zz_c_outer_inner_init = 0; zz_c_outer_inner_init < 2; ++zz_c_outer_inner_init) {\n        *(float2*)(((float*)conv3d_ncdhw_local) + ((ff_c_outer_inner_init * 4) + (zz_c_outer_inner_init * 2))) = ((float2)(0.000000e+00f, 0.000000e+00f));\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 24; ++rc_outer) {\n      for (int32_t i1 = 0; i1 < 2; ++i1) {\n        for (int32_t i2 = 0; i2 < 3; ++i2) {\n          for (int32_t i3 = 0; i3 < 3; ++i3) {\n            for (int32_t i4_s = 0; i4_s < 6; ++i4_s) {\n              int32_t cse_var_3 = (nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 237);\n              int32_t cse_var_2 = (i3 + (nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 3));\n              int32_t cse_var_1 = ((cse_var_3 / 3) + i2);\n              pad_temp[((((i1 * 54) + (i2 * 18)) + (i3 * 6)) + i4_s)] = (((((((1 <= cse_var_1) && (cse_var_1 < 80)) && (1 <= cse_var_2)) && (cse_var_2 < 4)) && (1 <= i4_s)) && (i4_s < 5)) ? ((float*)data_1)[(((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused / 237) * 45504) + (rc_outer * 1896)) + (i1 * 948)) + (i2 * 12)) + (i3 * 4)) + (cse_var_3 * 4)) + i4_s) - 17)] : 0.000000e+00f);\n            },\n          },\n        },\n      },\n      for (int32_t rz_outer = 0; rz_outer < 3; ++rz_outer) {\n        for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 79; ++ff_c_outer_inner) {\n          for (int32_t zz_c_outer_inner = 0; zz_c_outer_inner < 2; ++zz_c_outer_inner) {\n            for (int32_t rc_inner = 0; rc_inner < 2; ++rc_inner) {\n              for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n                for (int32_t rx_inner = 0; rx_inner < 3; ++rx_inner) {\n                  int32_t cse_var_5 = (zz_c_outer_inner * 2);\n                  int32_t cse_var_4 = ((ff_c_outer_inner * 4) + cse_var_5);\n                  int32_t2 v_ = int32_t2((cse_var_4)+(1*0), (cse_var_4)+(1*1));\n                  int32_t2 v__1 = int32_t2(((((((rc_inner * 54) + (ry_inner * 18)) + (rx_inner * 6)) + cse_var_5) + rz_outer))+(1*0), ((((((rc_inner * 54) + (ry_inner * 18)) + (rx_inner * 6)) + cse_var_5) + rz_outer))+(1*1));\n                  *(float2*)(((float*)conv3d_ncdhw_local) + cse_var_4) = ((float2(((float*)conv3d_ncdhw_local)[v_.s0],((float*)conv3d_ncdhw_local)[v_.s1])) + ((float2(pad_temp[v__1.s0],pad_temp[v__1.s1])) * ((float2)(((float*)kernel_1)[((((((ff_c_outer_inner * 1296) + (rc_outer * 54)) + (rc_inner * 27)) + (ry_inner * 9)) + (rx_inner * 3)) + rz_outer)], ((float*)kernel_1)[((((((ff_c_outer_inner * 1296) + (rc_outer * 54)) + (rc_inner * 27)) + (ry_inner * 9)) + (rx_inner * 3)) + rz_outer)]))));\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t ff_inner = 0; ff_inner < 79; ++ff_inner) {\n      *(float4*)(((float*)conv3d_ncdhw_1) + ((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused / 237) * 74892) + (ff_inner * 948)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 237) * 4))) = *(float4*)(((float*)conv3d_ncdhw_local) + (ff_inner * 4));\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv3d_ncdhw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(237) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(237) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv3d_ncdhw_local[12];\n  __shared__ float pad_temp_shared[180];\n  __shared__ float kernel_shared[1422];\n  for (int nn_c_inner_init = 0; nn_c_inner_init < 3; ++nn_c_inner_init) {\n    conv3d_ncdhw_local[nn_c_inner_init] = 0.000000e+00f;\n    conv3d_ncdhw_local[(nn_c_inner_init + 3)] = 0.000000e+00f;\n    conv3d_ncdhw_local[(nn_c_inner_init + 6)] = 0.000000e+00f;\n    conv3d_ncdhw_local[(nn_c_inner_init + 9)] = 0.000000e+00f;\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 24; ++rc_outer_outer) {\n    for (int ry_outer_outer = 0; ry_outer_outer < 3; ++ry_outer_outer) {\n      __syncthreads();\n      if (((int)threadIdx.x) < 180) {\n        pad_temp_shared[((int)threadIdx.x)] = (((((((1 <= (ry_outer_outer + ((int)blockIdx.x))) && ((ry_outer_outer + ((int)blockIdx.x)) < 80)) && (6 <= (((int)threadIdx.x) % 30))) && ((((int)threadIdx.x) % 30) < 24)) && (1 <= (((int)threadIdx.x) % 6))) && ((((int)threadIdx.x) % 6) < 5)) ? data[(((((((((((int)threadIdx.x) / 60) * 45504) + (rc_outer_outer * 1896)) + (((((int)threadIdx.x) % 60) / 30) * 948)) + (ry_outer_outer * 12)) + (((int)blockIdx.x) * 12)) + (((((int)threadIdx.x) % 30) / 6) * 4)) + (((int)threadIdx.x) % 6)) - 17)] : 0.000000e+00f);\n      },\n      for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer < 6; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer) {\n        kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 237) + ((int)threadIdx.x))] = kernel[(((((((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 79) + (((int)threadIdx.x) / 3)) / 6) * 1296) + (rc_outer_outer * 54)) + (((((((int)threadIdx.x) / 3) + ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer) % 6) / 3) * 27)) + (ry_outer_outer * 9)) + ((((((int)threadIdx.x) / 3) + ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer) % 3) * 3)) + (((int)threadIdx.x) % 3))];\n      },\n      __syncthreads();\n      for (int rc_outer_inner = 0; rc_outer_inner < 2; ++rc_outer_inner) {\n        for (int rx_inner = 0; rx_inner < 3; ++rx_inner) {\n          for (int rz_inner = 0; rz_inner < 3; ++rz_inner) {\n            for (int nn_c_inner = 0; nn_c_inner < 3; ++nn_c_inner) {\n              conv3d_ncdhw_local[nn_c_inner] = (conv3d_ncdhw_local[nn_c_inner] + (pad_temp_shared[(((((nn_c_inner * 60) + (rc_outer_inner * 30)) + (rx_inner * 6)) + ((((int)threadIdx.x) % 3) * 6)) + rz_inner)] * kernel_shared[(((((((int)threadIdx.x) / 3) * 18) + (rc_outer_inner * 9)) + (rx_inner * 3)) + rz_inner)]));\n              conv3d_ncdhw_local[(nn_c_inner + 3)] = (conv3d_ncdhw_local[(nn_c_inner + 3)] + (pad_temp_shared[((((((nn_c_inner * 60) + (rc_outer_inner * 30)) + (rx_inner * 6)) + ((((int)threadIdx.x) % 3) * 6)) + rz_inner) + 1)] * kernel_shared[(((((((int)threadIdx.x) / 3) * 18) + (rc_outer_inner * 9)) + (rx_inner * 3)) + rz_inner)]));\n              conv3d_ncdhw_local[(nn_c_inner + 6)] = (conv3d_ncdhw_local[(nn_c_inner + 6)] + (pad_temp_shared[((((((nn_c_inner * 60) + (rc_outer_inner * 30)) + (rx_inner * 6)) + ((((int)threadIdx.x) % 3) * 6)) + rz_inner) + 2)] * kernel_shared[(((((((int)threadIdx.x) / 3) * 18) + (rc_outer_inner * 9)) + (rx_inner * 3)) + rz_inner)]));\n              conv3d_ncdhw_local[(nn_c_inner + 9)] = (conv3d_ncdhw_local[(nn_c_inner + 9)] + (pad_temp_shared[((((((nn_c_inner * 60) + (rc_outer_inner * 30)) + (rx_inner * 6)) + ((((int)threadIdx.x) % 3) * 6)) + rz_inner) + 3)] * kernel_shared[(((((((int)threadIdx.x) / 3) * 18) + (rc_outer_inner * 9)) + (rx_inner * 3)) + rz_inner)]));\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int nn_inner = 0; nn_inner < 3; ++nn_inner) {\n    conv3d_ncdhw[((((nn_inner * 74892) + ((((int)threadIdx.x) / 3) * 948)) + (((int)blockIdx.x) * 12)) + ((((int)threadIdx.x) % 3) * 4))] = conv3d_ncdhw_local[nn_inner];\n    conv3d_ncdhw[(((((nn_inner * 74892) + ((((int)threadIdx.x) / 3) * 948)) + (((int)blockIdx.x) * 12)) + ((((int)threadIdx.x) % 3) * 4)) + 1)] = conv3d_ncdhw_local[(nn_inner + 3)];\n    conv3d_ncdhw[(((((nn_inner * 74892) + ((((int)threadIdx.x) / 3) * 948)) + (((int)blockIdx.x) * 12)) + ((((int)threadIdx.x) % 3) * 4)) + 2)] = conv3d_ncdhw_local[(nn_inner + 6)];\n    conv3d_ncdhw[(((((nn_inner * 74892) + ((((int)threadIdx.x) / 3) * 948)) + (((int)blockIdx.x) * 12)) + ((((int)threadIdx.x) % 3) * 4)) + 3)] = conv3d_ncdhw_local[(nn_inner + 9)];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 48, 79, 3, 4), \"float32\"), kernel: T.Buffer((79, 48, 3, 3, 3), \"float32\"), conv3d_ncdhw: T.Buffer((3, 79, 79, 3, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused in T.parallel(711):\n            conv3d_ncdhw_local = T.allocate([316], \"float32\", \"local\")\n            pad_temp = T.allocate([108], \"float32\", \"global\")\n            conv3d_ncdhw_local_1 = T.Buffer((316,), data=conv3d_ncdhw_local, scope=\"local\")\n            for ff_c_outer_inner_init, zz_c_outer_inner_init in T.grid(79, 2):\n                conv3d_ncdhw_local_1[ff_c_outer_inner_init * 4 + zz_c_outer_inner_init * 2:ff_c_outer_inner_init * 4 + zz_c_outer_inner_init * 2 + 2] = T.Broadcast(T.float32(0), 2)\n            for rc_outer in range(24):\n                pad_temp_1 = T.Buffer((108,), data=pad_temp)\n                for i1, i2, i3, i4_s in T.grid(2, 3, 3, 6):\n                    cse_var_3: T.int32 = nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 237\n                    cse_var_2: T.int32 = i3 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 3\n                    cse_var_1: T.int32 = cse_var_3 // 3 + i2\n                    data_1 = T.Buffer((136512,), data=data.data)\n                    pad_temp_1[i1 * 54 + i2 * 18 + i3 * 6 + i4_s] = T.if_then_else(1 <= cse_var_1 and cse_var_1 < 80 and 1 <= cse_var_2 and cse_var_2 < 4 and 1 <= i4_s and i4_s < 5, data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused // 237 * 45504 + rc_outer * 1896 + i1 * 948 + i2 * 12 + i3 * 4 + cse_var_3 * 4 + i4_s - 17], T.float32(0))\n                for rz_outer, ff_c_outer_inner, zz_c_outer_inner, rc_inner, ry_inner, rx_inner in T.grid(3, 79, 2, 2, 3, 3):\n                    cse_var_5: T.int32 = zz_c_outer_inner * 2\n                    cse_var_4: T.int32 = ff_c_outer_inner * 4 + cse_var_5\n                    kernel_1 = T.Buffer((102384,), data=kernel.data)\n                    conv3d_ncdhw_local_1[cse_var_4:cse_var_4 + 2] = conv3d_ncdhw_local_1[cse_var_4:cse_var_4 + 2] + pad_temp_1[rc_inner * 54 + ry_inner * 18 + rx_inner * 6 + cse_var_5 + rz_outer:rc_inner * 54 + ry_inner * 18 + rx_inner * 6 + cse_var_5 + rz_outer + 2] * T.Broadcast(kernel_1[ff_c_outer_inner * 1296 + rc_outer * 54 + rc_inner * 27 + ry_inner * 9 + rx_inner * 3 + rz_outer], 2)\n            for ff_inner in range(79):\n                conv3d_ncdhw_1 = T.Buffer((224676,), data=conv3d_ncdhw.data)\n                conv3d_ncdhw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused // 237 * 74892 + ff_inner * 948 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 237 * 4:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused // 237 * 74892 + ff_inner * 948 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 237 * 4 + 4] = conv3d_ncdhw_local_1[ff_inner * 4:ff_inner * 4 + 4]",
        "data": "3_48_79_3_4",
        "kernel": "79_48_3_3_3"
    },
    {
        "op_name": "conv3d_ncdhw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv3d_ncdhw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv3d_ncdhw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv3d_ncdhw_1 = (((DLTensor*)conv3d_ncdhw)[0].data);\n  void* default_function_conv3d_ncdhw_shape = (((DLTensor*)conv3d_ncdhw)[0].shape);\n  void* default_function_conv3d_ncdhw_strides = (((DLTensor*)conv3d_ncdhw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv3d_ncdhw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused < 43; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)147312, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t i0 = 0; i0 < 3; ++i0) {\n      for (int32_t i1 = 0; i1 < 62; ++i1) {\n        for (int32_t i2 = 0; i2 < 3; ++i2) {\n          for (int32_t i3 = 0; i3 < 6; ++i3) {\n            for (int32_t i4_s = 0; i4_s < 11; ++i4_s) {\n              int32_t cse_var_1 = (i2 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused);\n              ((float*)pad_temp)[(((((i0 * 12276) + (i1 * 198)) + (i2 * 66)) + (i3 * 11)) + i4_s)] = (((((((1 <= cse_var_1) && (cse_var_1 < 44)) && (1 <= i3)) && (i3 < 5)) && (1 <= i4_s)) && (i4_s < 10)) ? ((float*)data_1)[(((((((i0 * 95976) + (i1 * 1548)) + (i2 * 36)) + (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused * 36)) + (i3 * 9)) + i4_s) - 46)] : 0.000000e+00f);\n            },\n          },\n        },\n      },\n    },\n    for (int32_t xx_outer_outer_outer = 0; xx_outer_outer_outer < 2; ++xx_outer_outer_outer) {\n      for (int32_t zz_outer_outer_inner = 0; zz_outer_outer_inner < 2; ++zz_outer_outer_inner) {\n        for (int32_t nn_outer_inner_init = 0; nn_outer_inner_init < 3; ++nn_outer_inner_init) {\n          for (int32_t xx_outer_inner_init = 0; xx_outer_inner_init < 2; ++xx_outer_inner_init) {\n            for (int32_t ff_inner_init = 0; ff_inner_init < 43; ++ff_inner_init) {\n              *(float4*)(((float*)conv3d_ncdhw_1) + ((((((nn_outer_inner_init * 59168) + (ff_inner_init * 1376)) + (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused * 32)) + (xx_outer_outer_outer * 16)) + (xx_outer_inner_init * 8)) + (zz_outer_outer_inner * 4))) = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n            },\n          },\n        },\n        for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n          for (int32_t nn_outer_inner = 0; nn_outer_inner < 3; ++nn_outer_inner) {\n            for (int32_t xx_outer_inner = 0; xx_outer_inner < 2; ++xx_outer_inner) {\n              for (int32_t rc_inner = 0; rc_inner < 62; ++rc_inner) {\n                for (int32_t rx_inner = 0; rx_inner < 3; ++rx_inner) {\n                  for (int32_t rz_inner = 0; rz_inner < 4; ++rz_inner) {\n                    for (int32_t ff_inner = 0; ff_inner < 43; ++ff_inner) {\n                      int32_t cse_var_3 = (zz_outer_outer_inner * 4);\n                      int32_t cse_var_2 = ((((((nn_outer_inner * 59168) + (ff_inner * 1376)) + (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused * 32)) + (xx_outer_outer_outer * 16)) + (xx_outer_inner * 8)) + cse_var_3);\n                      int32_t4 v_ = int32_t4((cse_var_2)+(1*0), (cse_var_2)+(1*1), (cse_var_2)+(1*2), (cse_var_2)+(1*3));\n                      int32_t4 v__1 = int32_t4((((((((((nn_outer_inner * 12276) + (rc_inner * 198)) + (ry_outer * 66)) + (xx_outer_outer_outer * 22)) + (xx_outer_inner * 11)) + (rx_inner * 11)) + cse_var_3) + rz_inner))+(1*0), (((((((((nn_outer_inner * 12276) + (rc_inner * 198)) + (ry_outer * 66)) + (xx_outer_outer_outer * 22)) + (xx_outer_inner * 11)) + (rx_inner * 11)) + cse_var_3) + rz_inner))+(1*1), (((((((((nn_outer_inner * 12276) + (rc_inner * 198)) + (ry_outer * 66)) + (xx_outer_outer_outer * 22)) + (xx_outer_inner * 11)) + (rx_inner * 11)) + cse_var_3) + rz_inner))+(1*2), (((((((((nn_outer_inner * 12276) + (rc_inner * 198)) + (ry_outer * 66)) + (xx_outer_outer_outer * 22)) + (xx_outer_inner * 11)) + (rx_inner * 11)) + cse_var_3) + rz_inner))+(1*3));\n                      *(float4*)(((float*)conv3d_ncdhw_1) + cse_var_2) = ((float4(((float*)conv3d_ncdhw_1)[v_.s0],((float*)conv3d_ncdhw_1)[v_.s1],((float*)conv3d_ncdhw_1)[v_.s2],((float*)conv3d_ncdhw_1)[v_.s3])) + ((float4(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1],((float*)pad_temp)[v__1.s2],((float*)pad_temp)[v__1.s3])) * ((float4)(((float*)kernel_1)[(((((ff_inner * 2232) + (rc_inner * 36)) + (ry_outer * 12)) + (rx_inner * 4)) + rz_inner)], ((float*)kernel_1)[(((((ff_inner * 2232) + (rc_inner * 36)) + (ry_outer * 12)) + (rx_inner * 4)) + rz_inner)], ((float*)kernel_1)[(((((ff_inner * 2232) + (rc_inner * 36)) + (ry_outer * 12)) + (rx_inner * 4)) + rz_inner)], ((float*)kernel_1)[(((((ff_inner * 2232) + (rc_inner * 36)) + (ry_outer * 12)) + (rx_inner * 4)) + rz_inner)]))));\n                    },\n                  },\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(43) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(43) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv3d_ncdhw_local[16];\n  __shared__ float pad_temp_shared[64];\n  __shared__ float kernel_shared[258];\n  for (int xx_c_outer_inner_init = 0; xx_c_outer_inner_init < 2; ++xx_c_outer_inner_init) {\n    for (int zz_c_inner_init = 0; zz_c_inner_init < 2; ++zz_c_inner_init) {\n      conv3d_ncdhw_local[((xx_c_outer_inner_init * 2) + zz_c_inner_init)] = 0.000000e+00f;\n      conv3d_ncdhw_local[(((xx_c_outer_inner_init * 2) + zz_c_inner_init) + 4)] = 0.000000e+00f;\n      conv3d_ncdhw_local[(((xx_c_outer_inner_init * 2) + zz_c_inner_init) + 8)] = 0.000000e+00f;\n      conv3d_ncdhw_local[(((xx_c_outer_inner_init * 2) + zz_c_inner_init) + 12)] = 0.000000e+00f;\n    },\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 31; ++rc_outer_outer) {\n    for (int ry_outer_outer = 0; ry_outer_outer < 3; ++ry_outer_outer) {\n      for (int rz_outer_outer = 0; rz_outer_outer < 4; ++rz_outer_outer) {\n        __syncthreads();\n        for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer) {\n          if (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 43) + ((int)threadIdx.x)) < 64) {\n            pad_temp_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 43) + ((int)threadIdx.x))] = (((((((1 <= (((((int)blockIdx.x) % 86) >> 1) + ry_outer_outer)) && ((((((int)blockIdx.x) % 86) >> 1) + ry_outer_outer) < 44)) && (1 <= (((((int)blockIdx.x) & 1) * 2) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 11) + ((int)threadIdx.x)) & 31) >> 3)))) && ((((((int)blockIdx.x) & 1) * 2) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 11) + ((int)threadIdx.x)) & 31) >> 3)) < 5)) && (1 <= (rz_outer_outer + (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 3) + ((int)threadIdx.x)) & 7)))) && ((rz_outer_outer + (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 3) + ((int)threadIdx.x)) & 7)) < 10)) ? data[((((((((((((int)blockIdx.x) / 86) * 95976) + (rc_outer_outer * 3096)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 43) + ((int)threadIdx.x)) >> 5) * 1548)) + (ry_outer_outer * 36)) + ((((int)blockIdx.x) % 86) * 18)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 11) + ((int)threadIdx.x)) & 31) >> 3) * 9)) + rz_outer_outer) + (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 3) + ((int)threadIdx.x)) & 7)) - 46)] : 0.000000e+00f);\n          },\n        },\n        for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 < 6; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1) {\n          kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 * 43) + ((int)threadIdx.x))] = kernel[(((((((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 * 43) + ((int)threadIdx.x)) / 6) * 2232) + (rc_outer_outer * 72)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 + ((int)threadIdx.x)) % 6) / 3) * 36)) + (ry_outer_outer * 12)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 + ((int)threadIdx.x)) % 3) * 4)) + rz_outer_outer)];\n        },\n        __syncthreads();\n        for (int xx_c_outer_inner = 0; xx_c_outer_inner < 2; ++xx_c_outer_inner) {\n          for (int rc_inner = 0; rc_inner < 2; ++rc_inner) {\n            for (int rx_inner = 0; rx_inner < 3; ++rx_inner) {\n              for (int zz_c_inner = 0; zz_c_inner < 2; ++zz_c_inner) {\n                conv3d_ncdhw_local[((xx_c_outer_inner * 2) + zz_c_inner)] = (conv3d_ncdhw_local[((xx_c_outer_inner * 2) + zz_c_inner)] + (pad_temp_shared[((((rc_inner * 32) + (xx_c_outer_inner * 8)) + (rx_inner * 8)) + zz_c_inner)] * kernel_shared[(((((int)threadIdx.x) * 6) + (rc_inner * 3)) + rx_inner)]));\n                conv3d_ncdhw_local[(((xx_c_outer_inner * 2) + zz_c_inner) + 4)] = (conv3d_ncdhw_local[(((xx_c_outer_inner * 2) + zz_c_inner) + 4)] + (pad_temp_shared[(((((rc_inner * 32) + (xx_c_outer_inner * 8)) + (rx_inner * 8)) + zz_c_inner) + 2)] * kernel_shared[(((((int)threadIdx.x) * 6) + (rc_inner * 3)) + rx_inner)]));\n                conv3d_ncdhw_local[(((xx_c_outer_inner * 2) + zz_c_inner) + 8)] = (conv3d_ncdhw_local[(((xx_c_outer_inner * 2) + zz_c_inner) + 8)] + (pad_temp_shared[(((((rc_inner * 32) + (xx_c_outer_inner * 8)) + (rx_inner * 8)) + zz_c_inner) + 4)] * kernel_shared[(((((int)threadIdx.x) * 6) + (rc_inner * 3)) + rx_inner)]));\n                conv3d_ncdhw_local[(((xx_c_outer_inner * 2) + zz_c_inner) + 12)] = (conv3d_ncdhw_local[(((xx_c_outer_inner * 2) + zz_c_inner) + 12)] + (pad_temp_shared[(((((rc_inner * 32) + (xx_c_outer_inner * 8)) + (rx_inner * 8)) + zz_c_inner) + 6)] * kernel_shared[(((((int)threadIdx.x) * 6) + (rc_inner * 3)) + rx_inner)]));\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int xx_inner = 0; xx_inner < 2; ++xx_inner) {\n    for (int zz_inner = 0; zz_inner < 2; ++zz_inner) {\n      conv3d_ncdhw[((((((((int)blockIdx.x) / 86) * 59168) + (((int)threadIdx.x) * 1376)) + ((((int)blockIdx.x) % 86) * 16)) + (xx_inner * 8)) + zz_inner)] = conv3d_ncdhw_local[((xx_inner * 2) + zz_inner)];\n      conv3d_ncdhw[(((((((((int)blockIdx.x) / 86) * 59168) + (((int)threadIdx.x) * 1376)) + ((((int)blockIdx.x) % 86) * 16)) + (xx_inner * 8)) + zz_inner) + 2)] = conv3d_ncdhw_local[(((xx_inner * 2) + zz_inner) + 4)];\n      conv3d_ncdhw[(((((((((int)blockIdx.x) / 86) * 59168) + (((int)threadIdx.x) * 1376)) + ((((int)blockIdx.x) % 86) * 16)) + (xx_inner * 8)) + zz_inner) + 4)] = conv3d_ncdhw_local[(((xx_inner * 2) + zz_inner) + 8)];\n      conv3d_ncdhw[(((((((((int)blockIdx.x) / 86) * 59168) + (((int)threadIdx.x) * 1376)) + ((((int)blockIdx.x) % 86) * 16)) + (xx_inner * 8)) + zz_inner) + 6)] = conv3d_ncdhw_local[(((xx_inner * 2) + zz_inner) + 12)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 62, 43, 4, 9), \"float32\"), kernel: T.Buffer((43, 62, 3, 3, 4), \"float32\"), conv3d_ncdhw: T.Buffer((3, 43, 43, 4, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused in T.parallel(43):\n            pad_temp = T.allocate([36828], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((36828,), data=pad_temp)\n            for i0, i1, i2, i3, i4_s in T.grid(3, 62, 3, 6, 11):\n                cse_var_1: T.int32 = i2 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused\n                data_1 = T.Buffer((287928,), data=data.data)\n                pad_temp_1[i0 * 12276 + i1 * 198 + i2 * 66 + i3 * 11 + i4_s] = T.if_then_else(1 <= cse_var_1 and cse_var_1 < 44 and 1 <= i3 and i3 < 5 and 1 <= i4_s and i4_s < 10, data_1[i0 * 95976 + i1 * 1548 + i2 * 36 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused * 36 + i3 * 9 + i4_s - 46], T.float32(0))\n            for xx_outer_outer_outer, zz_outer_outer_inner in T.grid(2, 2):\n                conv3d_ncdhw_1 = T.Buffer((177504,), data=conv3d_ncdhw.data)\n                for nn_outer_inner_init, xx_outer_inner_init, ff_inner_init in T.grid(3, 2, 43):\n                    conv3d_ncdhw_1[nn_outer_inner_init * 59168 + ff_inner_init * 1376 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused * 32 + xx_outer_outer_outer * 16 + xx_outer_inner_init * 8 + zz_outer_outer_inner * 4:nn_outer_inner_init * 59168 + ff_inner_init * 1376 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused * 32 + xx_outer_outer_outer * 16 + xx_outer_inner_init * 8 + zz_outer_outer_inner * 4 + 4] = T.Broadcast(T.float32(0), 4)\n                for ry_outer, nn_outer_inner, xx_outer_inner, rc_inner, rx_inner, rz_inner, ff_inner in T.grid(3, 3, 2, 62, 3, 4, 43):\n                    cse_var_3: T.int32 = zz_outer_outer_inner * 4\n                    cse_var_2: T.int32 = nn_outer_inner * 59168 + ff_inner * 1376 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused * 32 + xx_outer_outer_outer * 16 + xx_outer_inner * 8 + cse_var_3\n                    kernel_1 = T.Buffer((95976,), data=kernel.data)\n                    conv3d_ncdhw_1[cse_var_2:cse_var_2 + 4] = conv3d_ncdhw_1[cse_var_2:cse_var_2 + 4] + pad_temp_1[nn_outer_inner * 12276 + rc_inner * 198 + ry_outer * 66 + xx_outer_outer_outer * 22 + xx_outer_inner * 11 + rx_inner * 11 + cse_var_3 + rz_inner:nn_outer_inner * 12276 + rc_inner * 198 + ry_outer * 66 + xx_outer_outer_outer * 22 + xx_outer_inner * 11 + rx_inner * 11 + cse_var_3 + rz_inner + 4] * T.Broadcast(kernel_1[ff_inner * 2232 + rc_inner * 36 + ry_outer * 12 + rx_inner * 4 + rz_inner], 4)",
        "data": "3_62_43_4_9",
        "kernel": "43_62_3_3_4"
    },
    {
        "op_name": "conv3d_ncdhw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv3d_ncdhw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv3d_ncdhw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv3d_ncdhw_1 = (((DLTensor*)conv3d_ncdhw)[0].data);\n  void* default_function_conv3d_ncdhw_shape = (((DLTensor*)conv3d_ncdhw)[0].shape);\n  void* default_function_conv3d_ncdhw_strides = (((DLTensor*)conv3d_ncdhw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv3d_ncdhw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused < 216; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)168960, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    void* conv3d_ncdhw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)232, 2, 32);\n    if (conv3d_ncdhw_local == NULL) {\n      return -1;\n    },\n    for (int32_t i1 = 0; i1 < 44; ++i1) {\n      for (int32_t i2 = 0; i2 < 60; ++i2) {\n        for (int32_t i3 = 0; i3 < 4; ++i3) {\n          for (int32_t i4_s = 0; i4_s < 4; ++i4_s) {\n            int32_t cse_var_4 = ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused % 108) / 12);\n            int32_t cse_var_3 = ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused % 12) / 3);\n            int32_t cse_var_2 = (cse_var_4 + i3);\n            int32_t cse_var_1 = (cse_var_3 + i4_s);\n            ((float*)pad_temp)[((((i1 * 960) + (i2 * 16)) + (i3 * 4)) + i4_s)] = (((((((1 <= i2) && (i2 < 59)) && (1 <= cse_var_2)) && (cse_var_2 < 11)) && (1 <= cse_var_1)) && (cse_var_1 < 6)) ? ((float*)data_1)[((((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused / 108) * 382800) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused % 3) * 127600)) + (i1 * 2900)) + (i2 * 50)) + (cse_var_4 * 5)) + (i3 * 5)) + cse_var_3) + i4_s) - 56)] : 0.000000e+00f);\n          },\n        },\n      },\n    },\n    for (int32_t yy_outer_inner = 0; yy_outer_inner < 55; ++yy_outer_inner) {\n      for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 2; ++ff_c_outer_inner_init) {\n        for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 29; ++ff_c_inner_init) {\n          ((float*)conv3d_ncdhw_local)[((ff_c_outer_inner_init * 29) + ff_c_inner_init)] = 0.000000e+00f;\n        },\n      },\n      for (int32_t rc_outer = 0; rc_outer < 11; ++rc_outer) {\n        for (int32_t ry_outer = 0; ry_outer < 2; ++ry_outer) {\n          for (int32_t rz_outer = 0; rz_outer < 2; ++rz_outer) {\n            for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 2; ++ff_c_outer_inner) {\n              for (int32_t rc_inner = 0; rc_inner < 4; ++rc_inner) {\n                for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n                  for (int32_t rx_inner = 0; rx_inner < 4; ++rx_inner) {\n                    for (int32_t rz_inner = 0; rz_inner < 2; ++rz_inner) {\n                      for (int32_t ff_c_inner = 0; ff_c_inner < 29; ++ff_c_inner) {\n                        int32_t cse_var_9 = (ry_outer * 48);\n                        int32_t cse_var_8 = (ry_inner * 16);\n                        int32_t cse_var_7 = (rx_inner * 4);\n                        int32_t cse_var_6 = (rz_outer * 2);\n                        int32_t cse_var_5 = ((ff_c_outer_inner * 29) + ff_c_inner);\n                        ((float*)conv3d_ncdhw_local)[cse_var_5] = (((float*)conv3d_ncdhw_local)[cse_var_5] + (((float*)pad_temp)[((((((((rc_outer * 3840) + (rc_inner * 960)) + cse_var_9) + (yy_outer_inner * 16)) + cse_var_8) + cse_var_7) + cse_var_6) + rz_inner)] * ((float*)kernel_1)[(((((((((ff_c_outer_inner * 122496) + (ff_c_inner * 4224)) + (rc_outer * 384)) + (rc_inner * 96)) + cse_var_9) + cse_var_8) + cse_var_7) + cse_var_6) + rz_inner)]));\n                      },\n                    },\n                  },\n                },\n              },\n            },\n          },\n        },\n      },\n      for (int32_t ff_inner = 0; ff_inner < 58; ++ff_inner) {\n        ((float*)conv3d_ncdhw_1)[((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused / 108) * 344520) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused % 3) * 114840)) + (ff_inner * 1980)) + (yy_outer_inner * 36)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused % 108) / 3))] = ((float*)conv3d_ncdhw_local)[ff_inner];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv3d_ncdhw_local) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(174) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(174) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv3d_ncdhw_local[10];\n  __shared__ float pad_temp_shared[792];\n  __shared__ float kernel_shared[5104];\n  for (int yy_c_outer_inner_init = 0; yy_c_outer_inner_init < 5; ++yy_c_outer_inner_init) {\n    for (int zz_c_inner_init = 0; zz_c_inner_init < 2; ++zz_c_inner_init) {\n      conv3d_ncdhw_local[((yy_c_outer_inner_init * 2) + zz_c_inner_init)] = 0.000000e+00f;\n    },\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 4; ++rc_outer_outer) {\n    for (int ry_outer_outer = 0; ry_outer_outer < 3; ++ry_outer_outer) {\n      for (int rx_outer_outer = 0; rx_outer_outer < 2; ++rx_outer_outer) {\n        for (int rz_outer_outer = 0; rz_outer_outer < 2; ++rz_outer_outer) {\n          __syncthreads();\n          for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer < 5; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer) {\n            if (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 29) + (((int)threadIdx.x) / 6)) < 132) {\n              pad_temp_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 174) + ((int)threadIdx.x))] = (((((((1 <= (((((((int)blockIdx.x) % 66) / 6) * 5) + (ry_outer_outer * 2)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 5) + (((int)threadIdx.x) / 6)) % 12) >> 1))) && ((((((((int)blockIdx.x) % 66) / 6) * 5) + (ry_outer_outer * 2)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 5) + (((int)threadIdx.x) / 6)) % 12) >> 1)) < 59)) && (1 <= (((((((int)blockIdx.x) % 6) >> 1) * 3) + (rx_outer_outer * 2)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 2) + (((int)threadIdx.x) / 3)) & 3)))) && ((((((((int)blockIdx.x) % 6) >> 1) * 3) + (rx_outer_outer * 2)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 2) + (((int)threadIdx.x) / 3)) & 3)) < 11)) && (1 <= (((rz_outer_outer * 2) + ((((int)blockIdx.x) & 1) * 2)) + (((int)threadIdx.x) % 3)))) && (((((((int)threadIdx.x) % 3) >> 1) + rz_outer_outer) + (((int)blockIdx.x) & 1)) < 3)) ? data[((((((((((((((((int)blockIdx.x) / 66) * 127600) + (rc_outer_outer * 31900)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 29) + (((int)threadIdx.x) / 6)) / 12) * 2900)) + (((((int)blockIdx.x) % 66) / 6) * 250)) + (ry_outer_outer * 100)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 5) + (((int)threadIdx.x) / 6)) % 12) >> 1) * 50)) + (((((int)blockIdx.x) % 6) >> 1) * 15)) + (rx_outer_outer * 10)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 2) + (((int)threadIdx.x) / 3)) & 3) * 5)) + (rz_outer_outer * 2)) + ((((int)blockIdx.x) & 1) * 2)) + (((int)threadIdx.x) % 3)) - 56)] : 0.000000e+00f);\n            },\n          },\n          for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 < 30; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1) {\n            if (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 * 3) + (((int)threadIdx.x) / 58)) < 88) {\n              kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 * 174) + ((int)threadIdx.x))] = kernel[((((((((((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 * 87) + (((int)threadIdx.x) >> 1)) / 44) * 4224) + (rc_outer_outer * 1056)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 * 43) + (((int)threadIdx.x) >> 1)) % 44) >> 2) * 96)) + (ry_outer_outer * 32)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 * 3) + (((int)threadIdx.x) >> 1)) & 3) >> 1) * 16)) + (rx_outer_outer * 8)) + ((((((int)threadIdx.x) >> 1) + ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1) & 1) * 4)) + (rz_outer_outer * 2)) + (((int)threadIdx.x) & 1))];\n            },\n          },\n          __syncthreads();\n          for (int rc_outer_inner = 0; rc_outer_inner < 11; ++rc_outer_inner) {\n            for (int ry_outer_inner = 0; ry_outer_inner < 2; ++ry_outer_inner) {\n              for (int rz_outer_inner = 0; rz_outer_inner < 2; ++rz_outer_inner) {\n                for (int yy_c_outer_inner = 0; yy_c_outer_inner < 5; ++yy_c_outer_inner) {\n                  for (int rx_inner = 0; rx_inner < 2; ++rx_inner) {\n                    for (int zz_c_inner = 0; zz_c_inner < 2; ++zz_c_inner) {\n                      conv3d_ncdhw_local[((yy_c_outer_inner * 2) + zz_c_inner)] = (conv3d_ncdhw_local[((yy_c_outer_inner * 2) + zz_c_inner)] + (pad_temp_shared[(((((((rc_outer_inner * 72) + (yy_c_outer_inner * 12)) + (ry_outer_inner * 12)) + (rx_inner * 3)) + ((((int)threadIdx.x) % 3) * 3)) + zz_c_inner) + rz_outer_inner)] * kernel_shared[((((((((int)threadIdx.x) / 3) * 88) + (rc_outer_inner * 8)) + (ry_outer_inner * 4)) + (rx_inner * 2)) + rz_outer_inner)]));\n                    },\n                  },\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int yy_inner = 0; yy_inner < 5; ++yy_inner) {\n    for (int zz_inner = 0; zz_inner < 2; ++zz_inner) {\n      conv3d_ncdhw[(((((((((((int)blockIdx.x) / 66) * 114840) + ((((int)threadIdx.x) / 3) * 1980)) + (((((int)blockIdx.x) % 66) / 6) * 180)) + (yy_inner * 36)) + (((((int)blockIdx.x) % 6) >> 1) * 12)) + ((((int)threadIdx.x) % 3) * 4)) + ((((int)blockIdx.x) & 1) * 2)) + zz_inner)] = conv3d_ncdhw_local[((yy_inner * 2) + zz_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 44, 58, 10, 5), \"float32\"), kernel: T.Buffer((58, 44, 6, 4, 4), \"float32\"), conv3d_ncdhw: T.Buffer((6, 58, 55, 9, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused in T.parallel(216):\n            pad_temp = T.allocate([42240], \"float32\", \"global\")\n            conv3d_ncdhw_local = T.allocate([58], \"float32\", \"local\")\n            pad_temp_1 = T.Buffer((42240,), data=pad_temp)\n            for i1, i2, i3, i4_s in T.grid(44, 60, 4, 4):\n                cse_var_4: T.int32 = nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused % 108 // 12\n                cse_var_3: T.int32 = nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused % 12 // 3\n                cse_var_2: T.int32 = cse_var_4 + i3\n                cse_var_1: T.int32 = cse_var_3 + i4_s\n                data_1 = T.Buffer((765600,), data=data.data)\n                pad_temp_1[i1 * 960 + i2 * 16 + i3 * 4 + i4_s] = T.if_then_else(1 <= i2 and i2 < 59 and 1 <= cse_var_2 and cse_var_2 < 11 and 1 <= cse_var_1 and cse_var_1 < 6, data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused // 108 * 382800 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused % 3 * 127600 + i1 * 2900 + i2 * 50 + cse_var_4 * 5 + i3 * 5 + cse_var_3 + i4_s - 56], T.float32(0))\n            for yy_outer_inner in range(55):\n                conv3d_ncdhw_local_1 = T.Buffer((58,), data=conv3d_ncdhw_local, scope=\"local\")\n                for ff_c_outer_inner_init, ff_c_inner_init in T.grid(2, 29):\n                    conv3d_ncdhw_local_1[ff_c_outer_inner_init * 29 + ff_c_inner_init] = T.float32(0)\n                for rc_outer, ry_outer, rz_outer, ff_c_outer_inner, rc_inner, ry_inner, rx_inner, rz_inner, ff_c_inner in T.grid(11, 2, 2, 2, 4, 3, 4, 2, 29):\n                    cse_var_9: T.int32 = ry_outer * 48\n                    cse_var_8: T.int32 = ry_inner * 16\n                    cse_var_7: T.int32 = rx_inner * 4\n                    cse_var_6: T.int32 = rz_outer * 2\n                    cse_var_5: T.int32 = ff_c_outer_inner * 29 + ff_c_inner\n                    kernel_1 = T.Buffer((244992,), data=kernel.data)\n                    conv3d_ncdhw_local_1[cse_var_5] = conv3d_ncdhw_local_1[cse_var_5] + pad_temp_1[rc_outer * 3840 + rc_inner * 960 + cse_var_9 + yy_outer_inner * 16 + cse_var_8 + cse_var_7 + cse_var_6 + rz_inner] * kernel_1[ff_c_outer_inner * 122496 + ff_c_inner * 4224 + rc_outer * 384 + rc_inner * 96 + cse_var_9 + cse_var_8 + cse_var_7 + cse_var_6 + rz_inner]\n                for ff_inner in range(58):\n                    conv3d_ncdhw_1 = T.Buffer((689040,), data=conv3d_ncdhw.data)\n                    conv3d_ncdhw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused // 108 * 344520 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused % 3 * 114840 + ff_inner * 1980 + yy_outer_inner * 36 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused % 108 // 3] = conv3d_ncdhw_local_1[ff_inner]",
        "data": "6_44_58_10_5",
        "kernel": "58_44_6_4_4"
    },
    {
        "op_name": "conv3d_ncdhw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv3d_ncdhw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv3d_ncdhw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv3d_ncdhw_1 = (((DLTensor*)conv3d_ncdhw)[0].data);\n  void* default_function_conv3d_ncdhw_shape = (((DLTensor*)conv3d_ncdhw)[0].shape);\n  void* default_function_conv3d_ncdhw_strides = (((DLTensor*)conv3d_ncdhw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv3d_ncdhw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused < 720; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused) {\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)2467584, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t i0 = 0; i0 < 9; ++i0) {\n      for (int32_t i1 = 0; i1 < 68; ++i1) {\n        for (int32_t i2 = 0; i2 < 14; ++i2) {\n          for (int32_t i3 = 0; i3 < 6; ++i3) {\n            for (int32_t i4_s = 0; i4_s < 12; ++i4_s) {\n              int32_t cse_var_4 = (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused & 1);\n              int32_t cse_var_3 = ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused % 30) >> 1);\n              int32_t cse_var_2 = ((cse_var_4 * 3) + i3);\n              int32_t cse_var_1 = ((cse_var_3 * 6) + i2);\n              ((float*)pad_temp)[(((((i0 * 68544) + (i1 * 1008)) + (i2 * 72)) + (i3 * 12)) + i4_s)] = (((((((1 <= cse_var_1) && (cse_var_1 < 97)) && (1 <= cse_var_2)) && (cse_var_2 < 8)) && (1 <= i4_s)) && (i4_s < 11)) ? ((float*)data_1)[((((((((i0 * 456960) + (i1 * 6720)) + (cse_var_3 * 420)) + (i2 * 70)) + (cse_var_4 * 30)) + (i3 * 10)) + i4_s) - 81)] : 0.000000e+00f);\n            },\n          },\n        },\n      },\n    },\n    for (int32_t nn_outer_outer_inner = 0; nn_outer_outer_inner < 9; ++nn_outer_outer_inner) {\n      for (int32_t ff_outer_outer_inner = 0; ff_outer_outer_inner < 2; ++ff_outer_outer_inner) {\n        for (int32_t yy_outer_inner_init = 0; yy_outer_inner_init < 3; ++yy_outer_inner_init) {\n          for (int32_t xx_outer_inner_init = 0; xx_outer_inner_init < 3; ++xx_outer_inner_init) {\n            for (int32_t ff_inner_init = 0; ff_inner_init < 2; ++ff_inner_init) {\n              for (int32_t yy_inner_init = 0; yy_inner_init < 2; ++yy_inner_init) {\n                *(float9*)(((float*)conv3d_ncdhw_1) + (((((((((nn_outer_outer_inner * 466560) + ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused / 30) * 19440)) + (ff_outer_outer_inner * 9720)) + (ff_inner_init * 4860)) + (((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused % 30) >> 1) * 324)) + (yy_outer_inner_init * 108)) + (yy_inner_init * 54)) + ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused & 1) * 27)) + (xx_outer_inner_init * 9))) = ((float9)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n              },\n            },\n          },\n        },\n        for (int32_t rc_outer = 0; rc_outer < 17; ++rc_outer) {\n          for (int32_t rx_outer = 0; rx_outer < 4; ++rx_outer) {\n            for (int32_t rz_outer = 0; rz_outer < 2; ++rz_outer) {\n              for (int32_t yy_outer_inner = 0; yy_outer_inner < 3; ++yy_outer_inner) {\n                for (int32_t xx_outer_inner = 0; xx_outer_inner < 3; ++xx_outer_inner) {\n                  for (int32_t rc_inner = 0; rc_inner < 4; ++rc_inner) {\n                    for (int32_t ry_inner = 0; ry_inner < 9; ++ry_inner) {\n                      for (int32_t rz_inner = 0; rz_inner < 2; ++rz_inner) {\n                        for (int32_t ff_inner = 0; ff_inner < 2; ++ff_inner) {\n                          for (int32_t yy_inner = 0; yy_inner < 2; ++yy_inner) {\n                            int32_t cse_var_7 = (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused / 30);\n                            int32_t cse_var_6 = (rz_outer * 2);\n                            int32_t cse_var_5 = (((((((((nn_outer_outer_inner * 466560) + (cse_var_7 * 19440)) + (ff_outer_outer_inner * 9720)) + (ff_inner * 4860)) + (((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused % 30) >> 1) * 324)) + (yy_outer_inner * 108)) + (yy_inner * 54)) + ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused & 1) * 27)) + (xx_outer_inner * 9));\n                            int32_t9 v_ = int32_t9((cse_var_5)+(1*0), (cse_var_5)+(1*1), (cse_var_5)+(1*2), (cse_var_5)+(1*3), (cse_var_5)+(1*4), (cse_var_5)+(1*5), (cse_var_5)+(1*6), (cse_var_5)+(1*7), (cse_var_5)+(1*8));\n                            int32_t9 v__1 = int32_t9((((((((((((nn_outer_outer_inner * 68544) + (rc_outer * 4032)) + (rc_inner * 1008)) + (yy_outer_inner * 144)) + (yy_inner * 72)) + (ry_inner * 72)) + (xx_outer_inner * 12)) + (rx_outer * 12)) + cse_var_6) + rz_inner))+(1*0), (((((((((((nn_outer_outer_inner * 68544) + (rc_outer * 4032)) + (rc_inner * 1008)) + (yy_outer_inner * 144)) + (yy_inner * 72)) + (ry_inner * 72)) + (xx_outer_inner * 12)) + (rx_outer * 12)) + cse_var_6) + rz_inner))+(1*1), (((((((((((nn_outer_outer_inner * 68544) + (rc_outer * 4032)) + (rc_inner * 1008)) + (yy_outer_inner * 144)) + (yy_inner * 72)) + (ry_inner * 72)) + (xx_outer_inner * 12)) + (rx_outer * 12)) + cse_var_6) + rz_inner))+(1*2), (((((((((((nn_outer_outer_inner * 68544) + (rc_outer * 4032)) + (rc_inner * 1008)) + (yy_outer_inner * 144)) + (yy_inner * 72)) + (ry_inner * 72)) + (xx_outer_inner * 12)) + (rx_outer * 12)) + cse_var_6) + rz_inner))+(1*3), (((((((((((nn_outer_outer_inner * 68544) + (rc_outer * 4032)) + (rc_inner * 1008)) + (yy_outer_inner * 144)) + (yy_inner * 72)) + (ry_inner * 72)) + (xx_outer_inner * 12)) + (rx_outer * 12)) + cse_var_6) + rz_inner))+(1*4), (((((((((((nn_outer_outer_inner * 68544) + (rc_outer * 4032)) + (rc_inner * 1008)) + (yy_outer_inner * 144)) + (yy_inner * 72)) + (ry_inner * 72)) + (xx_outer_inner * 12)) + (rx_outer * 12)) + cse_var_6) + rz_inner))+(1*5), (((((((((((nn_outer_outer_inner * 68544) + (rc_outer * 4032)) + (rc_inner * 1008)) + (yy_outer_inner * 144)) + (yy_inner * 72)) + (ry_inner * 72)) + (xx_outer_inner * 12)) + (rx_outer * 12)) + cse_var_6) + rz_inner))+(1*6), (((((((((((nn_outer_outer_inner * 68544) + (rc_outer * 4032)) + (rc_inner * 1008)) + (yy_outer_inner * 144)) + (yy_inner * 72)) + (ry_inner * 72)) + (xx_outer_inner * 12)) + (rx_outer * 12)) + cse_var_6) + rz_inner))+(1*7), (((((((((((nn_outer_outer_inner * 68544) + (rc_outer * 4032)) + (rc_inner * 1008)) + (yy_outer_inner * 144)) + (yy_inner * 72)) + (ry_inner * 72)) + (xx_outer_inner * 12)) + (rx_outer * 12)) + cse_var_6) + rz_inner))+(1*8));\n                            *(float9*)(((float*)conv3d_ncdhw_1) + cse_var_5) = ((float9(((float*)conv3d_ncdhw_1)[v_.s0],((float*)conv3d_ncdhw_1)[v_.s1],((float*)conv3d_ncdhw_1)[v_.s2],((float*)conv3d_ncdhw_1)[v_.s3],((float*)conv3d_ncdhw_1)[v_.s4],((float*)conv3d_ncdhw_1)[v_.s5],((float*)conv3d_ncdhw_1)[v_.s6],((float*)conv3d_ncdhw_1)[v_.s7],((float*)conv3d_ncdhw_1)[v_.s8])) + ((float9(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1],((float*)pad_temp)[v__1.s2],((float*)pad_temp)[v__1.s3],((float*)pad_temp)[v__1.s4],((float*)pad_temp)[v__1.s5],((float*)pad_temp)[v__1.s6],((float*)pad_temp)[v__1.s7],((float*)pad_temp)[v__1.s8])) * ((float9)(((float*)kernel_1)[(((((((((cse_var_7 * 39168) + (ff_outer_outer_inner * 19584)) + (ff_inner * 9792)) + (rc_outer * 576)) + (rc_inner * 144)) + (ry_inner * 16)) + (rx_outer * 4)) + cse_var_6) + rz_inner)], ((float*)kernel_1)[(((((((((cse_var_7 * 39168) + (ff_outer_outer_inner * 19584)) + (ff_inner * 9792)) + (rc_outer * 576)) + (rc_inner * 144)) + (ry_inner * 16)) + (rx_outer * 4)) + cse_var_6) + rz_inner)], ((float*)kernel_1)[(((((((((cse_var_7 * 39168) + (ff_outer_outer_inner * 19584)) + (ff_inner * 9792)) + (rc_outer * 576)) + (rc_inner * 144)) + (ry_inner * 16)) + (rx_outer * 4)) + cse_var_6) + rz_inner)], ((float*)kernel_1)[(((((((((cse_var_7 * 39168) + (ff_outer_outer_inner * 19584)) + (ff_inner * 9792)) + (rc_outer * 576)) + (rc_inner * 144)) + (ry_inner * 16)) + (rx_outer * 4)) + cse_var_6) + rz_inner)], ((float*)kernel_1)[(((((((((cse_var_7 * 39168) + (ff_outer_outer_inner * 19584)) + (ff_inner * 9792)) + (rc_outer * 576)) + (rc_inner * 144)) + (ry_inner * 16)) + (rx_outer * 4)) + cse_var_6) + rz_inner)], ((float*)kernel_1)[(((((((((cse_var_7 * 39168) + (ff_outer_outer_inner * 19584)) + (ff_inner * 9792)) + (rc_outer * 576)) + (rc_inner * 144)) + (ry_inner * 16)) + (rx_outer * 4)) + cse_var_6) + rz_inner)], ((float*)kernel_1)[(((((((((cse_var_7 * 39168) + (ff_outer_outer_inner * 19584)) + (ff_inner * 9792)) + (rc_outer * 576)) + (rc_inner * 144)) + (ry_inner * 16)) + (rx_outer * 4)) + cse_var_6) + rz_inner)], ((float*)kernel_1)[(((((((((cse_var_7 * 39168) + (ff_outer_outer_inner * 19584)) + (ff_inner * 9792)) + (rc_outer * 576)) + (rc_inner * 144)) + (ry_inner * 16)) + (rx_outer * 4)) + cse_var_6) + rz_inner)], ((float*)kernel_1)[(((((((((cse_var_7 * 39168) + (ff_outer_outer_inner * 19584)) + (ff_inner * 9792)) + (rc_outer * 576)) + (rc_inner * 144)) + (ry_inner * 16)) + (rx_outer * 4)) + cse_var_6) + rz_inner)]))));\n                          },\n                        },\n                      },\n                    },\n                  },\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(360) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(360) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv3d_ncdhw_local[16];\n  __shared__ float pad_temp_shared[3024];\n  __shared__ float kernel_shared[9216];\n  for (int ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 2; ++ff_c_outer_inner_init) {\n    for (int ff_c_inner_init = 0; ff_c_inner_init < 4; ++ff_c_inner_init) {\n      for (int yy_c_inner_init = 0; yy_c_inner_init < 2; ++yy_c_inner_init) {\n        conv3d_ncdhw_local[(((ff_c_outer_inner_init * 8) + (ff_c_inner_init * 2)) + yy_c_inner_init)] = 0.000000e+00f;\n      },\n    },\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 17; ++rc_outer_outer) {\n    for (int rx_outer_outer = 0; rx_outer_outer < 2; ++rx_outer_outer) {\n      __syncthreads();\n      for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer < 9; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer) {\n        if (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 5) + (((int)threadIdx.x) / 72)) < 42) {\n          pad_temp_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 360) + ((int)threadIdx.x))] = (((((((1 <= ((((((int)blockIdx.x) % 27) / 3) * 10) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 60) + (((int)threadIdx.x) / 6)) % 126) / 7))) && (((((((int)blockIdx.x) % 27) / 3) * 10) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 60) + (((int)threadIdx.x) / 6)) % 126) / 7)) < 97)) && (1 <= ((rx_outer_outer * 2) + (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 4) + (((int)threadIdx.x) / 6)) % 7)))) && ((((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 4) + (((int)threadIdx.x) / 6)) % 7) >> 1) + rx_outer_outer) < 4)) && (1 <= (((((int)blockIdx.x) % 3) * 3) + (((int)threadIdx.x) % 6)))) && ((((((int)blockIdx.x) % 3) * 3) + (((int)threadIdx.x) % 6)) < 11)) ? data[((((((((((((int)blockIdx.x) / 81) * 456960) + (rc_outer_outer * 26880)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 10) + (((int)threadIdx.x) / 36)) / 21) * 6720)) + (((((int)blockIdx.x) % 27) / 3) * 700)) + (rx_outer_outer * 20)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 60) + (((int)threadIdx.x) / 6)) % 126) * 10)) + ((((int)blockIdx.x) % 3) * 3)) + (((int)threadIdx.x) % 6)) - 81)] : 0.000000e+00f);\n        },\n      },\n      for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 < 26; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1) {\n        if (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 * 5) + (((int)threadIdx.x) / 72)) < 128) {\n          kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 * 360) + ((int)threadIdx.x))] = kernel[(((((((((((int)blockIdx.x) % 81) / 27) * 313344) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 * 5) + (((int)threadIdx.x) / 72)) >> 2) * 9792)) + (rc_outer_outer * 576)) + ((((((int)threadIdx.x) / 72) + ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1) & 3) * 144)) + (((((int)threadIdx.x) % 72) >> 3) * 16)) + (rx_outer_outer * 8)) + (((int)threadIdx.x) & 7))];\n        },\n      },\n      __syncthreads();\n      for (int rc_outer_inner = 0; rc_outer_inner < 2; ++rc_outer_inner) {\n        for (int ff_c_outer_inner = 0; ff_c_outer_inner < 2; ++ff_c_outer_inner) {\n          for (int rc_inner = 0; rc_inner < 2; ++rc_inner) {\n            for (int ry_inner = 0; ry_inner < 9; ++ry_inner) {\n              for (int rx_inner = 0; rx_inner < 2; ++rx_inner) {\n                for (int rz_inner = 0; rz_inner < 4; ++rz_inner) {\n                  for (int ff_c_inner = 0; ff_c_inner < 4; ++ff_c_inner) {\n                    for (int yy_c_inner = 0; yy_c_inner < 2; ++yy_c_inner) {\n                      conv3d_ncdhw_local[(((ff_c_outer_inner * 8) + (ff_c_inner * 2)) + yy_c_inner)] = (conv3d_ncdhw_local[(((ff_c_outer_inner * 8) + (ff_c_inner * 2)) + yy_c_inner)] + (pad_temp_shared[(((((((((rc_outer_inner * 1512) + (rc_inner * 756)) + (((((int)threadIdx.x) % 90) / 18) * 84)) + (yy_c_inner * 42)) + (ry_inner * 42)) + (((((int)threadIdx.x) % 18) / 3) * 6)) + (rx_inner * 6)) + rz_inner) + (((int)threadIdx.x) % 3))] * kernel_shared[(((((((((((int)threadIdx.x) / 90) * 2304) + (ff_c_outer_inner * 1152)) + (ff_c_inner * 288)) + (rc_outer_inner * 144)) + (rc_inner * 72)) + (ry_inner * 8)) + (rx_inner * 4)) + rz_inner)]));\n                    },\n                  },\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 8; ++ff_inner) {\n    for (int yy_inner = 0; yy_inner < 2; ++yy_inner) {\n      conv3d_ncdhw[((((((((((((int)blockIdx.x) / 27) * 155520) + ((((int)threadIdx.x) / 90) * 38880)) + (ff_inner * 4860)) + (((((int)blockIdx.x) % 27) / 3) * 540)) + (((((int)threadIdx.x) % 90) / 18) * 108)) + (yy_inner * 54)) + (((((int)threadIdx.x) % 18) / 3) * 9)) + ((((int)blockIdx.x) % 3) * 3)) + (((int)threadIdx.x) % 3))] = conv3d_ncdhw_local[((ff_inner * 2) + yy_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 68, 96, 7, 10), \"float32\"), kernel: T.Buffer((96, 68, 9, 4, 4), \"float32\"), conv3d_ncdhw: T.Buffer((9, 96, 90, 6, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused in T.parallel(720):\n            pad_temp = T.allocate([616896], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((616896,), data=pad_temp)\n            for i0, i1, i2, i3, i4_s in T.grid(9, 68, 14, 6, 12):\n                cse_var_4: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused % 2\n                cse_var_3: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused % 30 // 2\n                cse_var_2: T.int32 = cse_var_4 * 3 + i3\n                cse_var_1: T.int32 = cse_var_3 * 6 + i2\n                data_1 = T.Buffer((4112640,), data=data.data)\n                pad_temp_1[i0 * 68544 + i1 * 1008 + i2 * 72 + i3 * 12 + i4_s] = T.if_then_else(1 <= cse_var_1 and cse_var_1 < 97 and 1 <= cse_var_2 and cse_var_2 < 8 and 1 <= i4_s and i4_s < 11, data_1[i0 * 456960 + i1 * 6720 + cse_var_3 * 420 + i2 * 70 + cse_var_4 * 30 + i3 * 10 + i4_s - 81], T.float32(0))\n            for nn_outer_outer_inner, ff_outer_outer_inner in T.grid(9, 2):\n                conv3d_ncdhw_1 = T.Buffer((4199040,), data=conv3d_ncdhw.data)\n                for yy_outer_inner_init, xx_outer_inner_init, ff_inner_init, yy_inner_init in T.grid(3, 3, 2, 2):\n                    conv3d_ncdhw_1[nn_outer_outer_inner * 466560 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused // 30 * 19440 + ff_outer_outer_inner * 9720 + ff_inner_init * 4860 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused % 30 // 2 * 324 + yy_outer_inner_init * 108 + yy_inner_init * 54 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused % 2 * 27 + xx_outer_inner_init * 9:nn_outer_outer_inner * 466560 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused // 30 * 19440 + ff_outer_outer_inner * 9720 + ff_inner_init * 4860 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused % 30 // 2 * 324 + yy_outer_inner_init * 108 + yy_inner_init * 54 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused % 2 * 27 + xx_outer_inner_init * 9 + 9] = T.Broadcast(T.float32(0), 9)\n                for rc_outer, rx_outer, rz_outer, yy_outer_inner, xx_outer_inner, rc_inner, ry_inner, rz_inner, ff_inner, yy_inner in T.grid(17, 4, 2, 3, 3, 4, 9, 2, 2, 2):\n                    cse_var_7: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused // 30\n                    cse_var_6: T.int32 = rz_outer * 2\n                    cse_var_5: T.int32 = nn_outer_outer_inner * 466560 + cse_var_7 * 19440 + ff_outer_outer_inner * 9720 + ff_inner * 4860 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused % 30 // 2 * 324 + yy_outer_inner * 108 + yy_inner * 54 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused % 2 * 27 + xx_outer_inner * 9\n                    kernel_1 = T.Buffer((940032,), data=kernel.data)\n                    conv3d_ncdhw_1[cse_var_5:cse_var_5 + 9] = conv3d_ncdhw_1[cse_var_5:cse_var_5 + 9] + pad_temp_1[nn_outer_outer_inner * 68544 + rc_outer * 4032 + rc_inner * 1008 + yy_outer_inner * 144 + yy_inner * 72 + ry_inner * 72 + xx_outer_inner * 12 + rx_outer * 12 + cse_var_6 + rz_inner:nn_outer_outer_inner * 68544 + rc_outer * 4032 + rc_inner * 1008 + yy_outer_inner * 144 + yy_inner * 72 + ry_inner * 72 + xx_outer_inner * 12 + rx_outer * 12 + cse_var_6 + rz_inner + 9] * T.Broadcast(kernel_1[cse_var_7 * 39168 + ff_outer_outer_inner * 19584 + ff_inner * 9792 + rc_outer * 576 + rc_inner * 144 + ry_inner * 16 + rx_outer * 4 + cse_var_6 + rz_inner], 9)",
        "data": "9_68_96_7_10",
        "kernel": "96_68_9_4_4"
    },
    {
        "op_name": "conv3d_ncdhw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv3d_ncdhw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv3d_ncdhw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv3d_ncdhw_1 = (((DLTensor*)conv3d_ncdhw)[0].data);\n  void* default_function_conv3d_ncdhw_shape = (((DLTensor*)conv3d_ncdhw)[0].shape);\n  void* default_function_conv3d_ncdhw_strides = (((DLTensor*)conv3d_ncdhw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv3d_ncdhw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused < 304; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused) {\n    void* conv3d_ncdhw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)5504, 2, 32);\n    if (conv3d_ncdhw_local == NULL) {\n      return -1;\n    },\n    void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)4032, 2, 32);\n    if (pad_temp == NULL) {\n      return -1;\n    },\n    for (int32_t yy_c_outer_inner_init = 0; yy_c_outer_inner_init < 2; ++yy_c_outer_inner_init) {\n      for (int32_t xx_c_outer_inner_init = 0; xx_c_outer_inner_init < 4; ++xx_c_outer_inner_init) {\n        for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 43; ++ff_c_inner_init) {\n          ((float4*)conv3d_ncdhw_local)[(((ff_c_inner_init * 8) + (yy_c_outer_inner_init * 4)) + xx_c_outer_inner_init)] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n        },\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 4; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 4; ++ry_outer) {\n        for (int32_t rz_outer = 0; rz_outer < 3; ++rz_outer) {\n          for (int32_t i1 = 0; i1 < 12; ++i1) {\n            for (int32_t i2 = 0; i2 < 3; ++i2) {\n              for (int32_t i3 = 0; i3 < 7; ++i3) {\n                for (int32_t i4_s = 0; i4_s < 4; ++i4_s) {\n                  int32_t cse_var_4 = (nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused & 1);\n                  int32_t cse_var_3 = (i4_s + rz_outer);\n                  int32_t cse_var_2 = (i3 * 4);\n                  int32_t cse_var_1 = ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 38) >> 1);\n                  ((float*)pad_temp)[((((i1 * 84) + (i2 * 28)) + cse_var_2) + i4_s)] = (((((((1 <= (((cse_var_1 * 2) + (ry_outer * 2)) + i2)) && ((((i2 >> 1) + cse_var_1) + ry_outer) < 22)) && (1 <= ((cse_var_4 * 4) + i3))) && (((cse_var_4 * 2) + (i3 >> 1)) < 5)) && (1 <= cse_var_3)) && (cse_var_3 < 5)) ? ((float*)data_1)[((((((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused / 38) * 74304) + (rc_outer * 18576)) + (i1 * 1548)) + (cse_var_1 * 72)) + (ry_outer * 72)) + (i2 * 36)) + (cse_var_4 * 16)) + cse_var_2) + i4_s) + rz_outer) - 41)] : 0.000000e+00f);\n                },\n              },\n            },\n          },\n          for (int32_t yy_c_outer_inner = 0; yy_c_outer_inner < 2; ++yy_c_outer_inner) {\n            for (int32_t xx_c_outer_inner = 0; xx_c_outer_inner < 4; ++xx_c_outer_inner) {\n              for (int32_t rc_inner = 0; rc_inner < 12; ++rc_inner) {\n                for (int32_t ry_inner = 0; ry_inner < 2; ++ry_inner) {\n                  for (int32_t rx_inner = 0; rx_inner < 4; ++rx_inner) {\n                    for (int32_t ff_c_inner = 0; ff_c_inner < 43; ++ff_c_inner) {\n                      int32_t cse_var_5 = (((ff_c_inner * 8) + (yy_c_outer_inner * 4)) + xx_c_outer_inner);\n                      ((float4*)conv3d_ncdhw_local)[cse_var_5] = (((float4*)conv3d_ncdhw_local)[cse_var_5] + (*(float4*)(((float*)pad_temp) + (((((rc_inner * 84) + (yy_c_outer_inner * 28)) + (ry_inner * 28)) + (xx_c_outer_inner * 4)) + (rx_inner * 4))) * ((float4)(((float*)kernel_1)[(((((((ff_c_inner * 4608) + (rc_outer * 1152)) + (rc_inner * 96)) + (ry_outer * 24)) + (ry_inner * 12)) + (rx_inner * 3)) + rz_outer)], ((float*)kernel_1)[(((((((ff_c_inner * 4608) + (rc_outer * 1152)) + (rc_inner * 96)) + (ry_outer * 24)) + (ry_inner * 12)) + (rx_inner * 3)) + rz_outer)], ((float*)kernel_1)[(((((((ff_c_inner * 4608) + (rc_outer * 1152)) + (rc_inner * 96)) + (ry_outer * 24)) + (ry_inner * 12)) + (rx_inner * 3)) + rz_outer)], ((float*)kernel_1)[(((((((ff_c_inner * 4608) + (rc_outer * 1152)) + (rc_inner * 96)) + (ry_outer * 24)) + (ry_inner * 12)) + (rx_inner * 3)) + rz_outer)]))));\n                    },\n                  },\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t ff_inner = 0; ff_inner < 43; ++ff_inner) {\n      for (int32_t yy_inner = 0; yy_inner < 2; ++yy_inner) {\n        for (int32_t xx_inner = 0; xx_inner < 4; ++xx_inner) {\n          *(float4*)(((float*)conv3d_ncdhw_1) + (((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused / 38) * 52288) + (ff_inner * 1216)) + (((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 38) >> 1) * 64)) + (yy_inner * 32)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused & 1) * 16)) + (xx_inner * 4))) = ((float4*)conv3d_ncdhw_local)[(((ff_inner * 8) + (yy_inner * 4)) + xx_inner)];\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n      return -1;\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv3d_ncdhw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(344) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(344) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv3d_ncdhw_local[16];\n  __shared__ float pad_temp_shared[336];\n  __shared__ float kernel_shared[344];\n  for (int nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 2; ++nn_c_outer_inner_init) {\n    for (int yy_c_outer_inner_init = 0; yy_c_outer_inner_init < 2; ++yy_c_outer_inner_init) {\n      for (int nn_c_inner_init = 0; nn_c_inner_init < 2; ++nn_c_inner_init) {\n        conv3d_ncdhw_local[(((nn_c_outer_inner_init * 4) + (nn_c_inner_init * 2)) + yy_c_outer_inner_init)] = 0.000000e+00f;\n        conv3d_ncdhw_local[((((nn_c_outer_inner_init * 4) + (nn_c_inner_init * 2)) + yy_c_outer_inner_init) + 8)] = 0.000000e+00f;\n      },\n    },\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 48; ++rc_outer_outer) {\n    for (int ry_outer_outer = 0; ry_outer_outer < 4; ++ry_outer_outer) {\n      for (int rz_outer_outer = 0; rz_outer_outer < 3; ++rz_outer_outer) {\n        __syncthreads();\n        if (((int)threadIdx.x) < 336) {\n          pad_temp_shared[((int)threadIdx.x)] = (((((((1 <= (((((((int)blockIdx.x) % 38) >> 1) * 2) + (ry_outer_outer * 2)) + ((((int)threadIdx.x) % 84) / 28))) && (((((((int)threadIdx.x) % 84) / 56) + ((((int)blockIdx.x) % 38) >> 1)) + ry_outer_outer) < 22)) && (1 <= (((((int)blockIdx.x) & 1) * 4) + ((((int)threadIdx.x) % 28) >> 2)))) && ((((((int)blockIdx.x) & 1) * 2) + ((((int)threadIdx.x) % 28) >> 3)) < 5)) && (1 <= (rz_outer_outer + (((int)threadIdx.x) & 3)))) && ((rz_outer_outer + (((int)threadIdx.x) & 3)) < 5)) ? data[(((((((((((((int)blockIdx.x) / 38) * 297216) + ((((int)threadIdx.x) / 84) * 74304)) + (rc_outer_outer * 1548)) + (((((int)blockIdx.x) % 38) >> 1) * 72)) + (ry_outer_outer * 72)) + (((((int)threadIdx.x) % 84) / 28) * 36)) + ((((int)blockIdx.x) & 1) * 16)) + rz_outer_outer) + (((int)threadIdx.x) % 28)) - 41)] : 0.000000e+00f);\n        },\n        kernel_shared[((int)threadIdx.x)] = kernel[((((((((int)threadIdx.x) >> 3) * 4608) + (rc_outer_outer * 96)) + (ry_outer_outer * 24)) + ((((int)threadIdx.x) & 7) * 3)) + rz_outer_outer)];\n        __syncthreads();\n        for (int nn_c_outer_inner = 0; nn_c_outer_inner < 2; ++nn_c_outer_inner) {\n          for (int yy_c_outer_inner = 0; yy_c_outer_inner < 2; ++yy_c_outer_inner) {\n            for (int ry_inner = 0; ry_inner < 2; ++ry_inner) {\n              for (int rx_inner = 0; rx_inner < 4; ++rx_inner) {\n                for (int nn_c_inner = 0; nn_c_inner < 2; ++nn_c_inner) {\n                  conv3d_ncdhw_local[(((nn_c_outer_inner * 4) + (nn_c_inner * 2)) + yy_c_outer_inner)] = (conv3d_ncdhw_local[(((nn_c_outer_inner * 4) + (nn_c_inner * 2)) + yy_c_outer_inner)] + (pad_temp_shared[((((((nn_c_outer_inner * 168) + (nn_c_inner * 84)) + (yy_c_outer_inner * 28)) + (ry_inner * 28)) + (rx_inner * 4)) + (((int)threadIdx.x) & 7))] * kernel_shared[((((((int)threadIdx.x) >> 3) * 8) + (ry_inner * 4)) + rx_inner)]));\n                  conv3d_ncdhw_local[((((nn_c_outer_inner * 4) + (nn_c_inner * 2)) + yy_c_outer_inner) + 8)] = (conv3d_ncdhw_local[((((nn_c_outer_inner * 4) + (nn_c_inner * 2)) + yy_c_outer_inner) + 8)] + (pad_temp_shared[(((((((nn_c_outer_inner * 168) + (nn_c_inner * 84)) + (yy_c_outer_inner * 28)) + (ry_inner * 28)) + (rx_inner * 4)) + (((int)threadIdx.x) & 7)) + 8)] * kernel_shared[((((((int)threadIdx.x) >> 3) * 8) + (ry_inner * 4)) + rx_inner)]));\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int nn_inner = 0; nn_inner < 4; ++nn_inner) {\n    for (int yy_inner = 0; yy_inner < 2; ++yy_inner) {\n      conv3d_ncdhw[((((((((((int)blockIdx.x) / 38) * 209152) + (nn_inner * 52288)) + ((((int)threadIdx.x) >> 3) * 1216)) + (((((int)blockIdx.x) % 38) >> 1) * 64)) + (yy_inner * 32)) + ((((int)blockIdx.x) & 1) * 16)) + (((int)threadIdx.x) & 7))] = conv3d_ncdhw_local[((nn_inner * 2) + yy_inner)];\n      conv3d_ncdhw[(((((((((((int)blockIdx.x) / 38) * 209152) + (nn_inner * 52288)) + ((((int)threadIdx.x) >> 3) * 1216)) + (((((int)blockIdx.x) % 38) >> 1) * 64)) + (yy_inner * 32)) + ((((int)blockIdx.x) & 1) * 16)) + (((int)threadIdx.x) & 7)) + 8)] = conv3d_ncdhw_local[(((nn_inner * 2) + yy_inner) + 8)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 48, 43, 9, 4), \"float32\"), kernel: T.Buffer((43, 48, 8, 4, 3), \"float32\"), conv3d_ncdhw: T.Buffer((8, 43, 38, 8, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused in T.parallel(304):\n            conv3d_ncdhw_local = T.allocate([344], \"float32x4\", \"local\")\n            pad_temp = T.allocate([1008], \"float32\", \"global\")\n            conv3d_ncdhw_local_1 = T.Buffer((344,), \"float32x4\", data=conv3d_ncdhw_local, scope=\"local\")\n            for yy_c_outer_inner_init, xx_c_outer_inner_init, ff_c_inner_init in T.grid(2, 4, 43):\n                conv3d_ncdhw_local_1[ff_c_inner_init * 8 + yy_c_outer_inner_init * 4 + xx_c_outer_inner_init] = T.Broadcast(T.float32(0), 4)\n            for rc_outer, ry_outer, rz_outer in T.grid(4, 4, 3):\n                pad_temp_1 = T.Buffer((1008,), data=pad_temp)\n                for i1, i2, i3, i4_s in T.grid(12, 3, 7, 4):\n                    cse_var_4: T.int32 = nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 2\n                    cse_var_3: T.int32 = i4_s + rz_outer\n                    cse_var_2: T.int32 = i3 * 4\n                    cse_var_1: T.int32 = nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 38 // 2\n                    data_1 = T.Buffer((594432,), data=data.data)\n                    pad_temp_1[i1 * 84 + i2 * 28 + cse_var_2 + i4_s] = T.if_then_else(1 <= cse_var_1 * 2 + ry_outer * 2 + i2 and i2 // 2 + cse_var_1 + ry_outer < 22 and 1 <= cse_var_4 * 4 + i3 and cse_var_4 * 2 + i3 // 2 < 5 and 1 <= cse_var_3 and cse_var_3 < 5, data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused // 38 * 74304 + rc_outer * 18576 + i1 * 1548 + cse_var_1 * 72 + ry_outer * 72 + i2 * 36 + cse_var_4 * 16 + cse_var_2 + i4_s + rz_outer - 41], T.float32(0))\n                for yy_c_outer_inner, xx_c_outer_inner, rc_inner, ry_inner, rx_inner, ff_c_inner in T.grid(2, 4, 12, 2, 4, 43):\n                    cse_var_5: T.int32 = ff_c_inner * 8 + yy_c_outer_inner * 4 + xx_c_outer_inner\n                    kernel_1 = T.Buffer((198144,), data=kernel.data)\n                    conv3d_ncdhw_local_1[cse_var_5] = conv3d_ncdhw_local_1[cse_var_5] + pad_temp_1[rc_inner * 84 + yy_c_outer_inner * 28 + ry_inner * 28 + xx_c_outer_inner * 4 + rx_inner * 4:rc_inner * 84 + yy_c_outer_inner * 28 + ry_inner * 28 + xx_c_outer_inner * 4 + rx_inner * 4 + 4] * T.Broadcast(kernel_1[ff_c_inner * 4608 + rc_outer * 1152 + rc_inner * 96 + ry_outer * 24 + ry_inner * 12 + rx_inner * 3 + rz_outer], 4)\n            for ff_inner, yy_inner, xx_inner in T.grid(43, 2, 4):\n                conv3d_ncdhw_1 = T.Buffer((418304,), data=conv3d_ncdhw.data)\n                conv3d_ncdhw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused // 38 * 52288 + ff_inner * 1216 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 38 // 2 * 64 + yy_inner * 32 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 2 * 16 + xx_inner * 4:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused // 38 * 52288 + ff_inner * 1216 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 38 // 2 * 64 + yy_inner * 32 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 2 * 16 + xx_inner * 4 + 4] = conv3d_ncdhw_local_1[ff_inner * 8 + yy_inner * 4 + xx_inner]",
        "data": "8_48_43_9_4",
        "kernel": "43_48_8_4_3"
    },
    {
        "op_name": "conv3d_ncdhw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv3d_ncdhw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv3d_ncdhw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv3d_ncdhw_1 = (((DLTensor*)conv3d_ncdhw)[0].data);\n  void* default_function_conv3d_ncdhw_shape = (((DLTensor*)conv3d_ncdhw)[0].shape);\n  void* default_function_conv3d_ncdhw_strides = (((DLTensor*)conv3d_ncdhw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv3d_ncdhw_strides == NULL)) {\n  },\n  void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)13171200, 2, 32);\n  if (pad_temp == NULL) {\n    return -1;\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 54880; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 6; ++i3) {\n      for (int32_t i4_s = 0; i4_s < 10; ++i4_s) {\n        int32_t cse_var_1 = (i0_i1_fused_i2_fused % 98);\n        ((float*)pad_temp)[(((i0_i1_fused_i2_fused * 60) + (i3 * 10)) + i4_s)] = (((((((1 <= cse_var_1) && (cse_var_1 < 97)) && (1 <= i3)) && (i3 < 5)) && (1 <= i4_s)) && (i4_s < 9)) ? ((float*)data_1)[((((((i0_i1_fused_i2_fused / 98) * 3072) + (cse_var_1 * 32)) + (i3 * 8)) + i4_s) - 41)] : 0.000000e+00f);\n      },\n    },\n  },\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused < 7120; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused) {\n    for (int32_t ff_outer_inner_init = 0; ff_outer_inner_init < 4; ++ff_outer_inner_init) {\n      for (int32_t ff_inner_init = 0; ff_inner_init < 6; ++ff_inner_init) {\n        for (int32_t xx_inner_init = 0; xx_inner_init < 4; ++xx_inner_init) {\n          *(float4*)(((float*)conv3d_ncdhw_1) + (((((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused / 1424) * 546816) + (((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 178) / 89) * 273408)) + (((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 1424) / 356) * 68352)) + (ff_outer_inner_init * 17088)) + (ff_inner_init * 2848)) + ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 89) * 32)) + (xx_inner_init * 8)) + (((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 356) / 178) * 4))) = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n        },\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 14; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 2; ++ry_outer) {\n        for (int32_t rx_outer = 0; rx_outer < 3; ++rx_outer) {\n          for (int32_t rz_outer = 0; rz_outer < 3; ++rz_outer) {\n            for (int32_t ff_outer_inner = 0; ff_outer_inner < 4; ++ff_outer_inner) {\n              for (int32_t rc_inner = 0; rc_inner < 4; ++rc_inner) {\n                for (int32_t ry_inner = 0; ry_inner < 5; ++ry_inner) {\n                  for (int32_t ff_inner = 0; ff_inner < 6; ++ff_inner) {\n                    for (int32_t xx_inner = 0; xx_inner < 4; ++xx_inner) {\n                      int32_t cse_var_7 = (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused / 1424);\n                      int32_t cse_var_6 = (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 89);\n                      int32_t cse_var_5 = ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 178) / 89);\n                      int32_t cse_var_4 = ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 1424) / 356);\n                      int32_t cse_var_3 = (((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 356) / 178) * 4);\n                      int32_t cse_var_2 = ((((((((cse_var_7 * 546816) + (cse_var_5 * 273408)) + (cse_var_4 * 68352)) + (ff_outer_inner * 17088)) + (ff_inner * 2848)) + (cse_var_6 * 32)) + (xx_inner * 8)) + cse_var_3);\n                      int32_t4 v_ = int32_t4((cse_var_2)+(1*0), (cse_var_2)+(1*1), (cse_var_2)+(1*2), (cse_var_2)+(1*3));\n                      int32_t4 v__1 = int32_t4(((((((((((((cse_var_7 * 658560) + (cse_var_5 * 329280)) + (rc_outer * 23520)) + (rc_inner * 5880)) + (ry_outer * 300)) + (ry_inner * 60)) + (cse_var_6 * 60)) + (xx_inner * 10)) + (rx_outer * 10)) + cse_var_3) + rz_outer))+(1*0), ((((((((((((cse_var_7 * 658560) + (cse_var_5 * 329280)) + (rc_outer * 23520)) + (rc_inner * 5880)) + (ry_outer * 300)) + (ry_inner * 60)) + (cse_var_6 * 60)) + (xx_inner * 10)) + (rx_outer * 10)) + cse_var_3) + rz_outer))+(1*1), ((((((((((((cse_var_7 * 658560) + (cse_var_5 * 329280)) + (rc_outer * 23520)) + (rc_inner * 5880)) + (ry_outer * 300)) + (ry_inner * 60)) + (cse_var_6 * 60)) + (xx_inner * 10)) + (rx_outer * 10)) + cse_var_3) + rz_outer))+(1*2), ((((((((((((cse_var_7 * 658560) + (cse_var_5 * 329280)) + (rc_outer * 23520)) + (rc_inner * 5880)) + (ry_outer * 300)) + (ry_inner * 60)) + (cse_var_6 * 60)) + (xx_inner * 10)) + (rx_outer * 10)) + cse_var_3) + rz_outer))+(1*3));\n                      *(float4*)(((float*)conv3d_ncdhw_1) + cse_var_2) = ((float4(((float*)conv3d_ncdhw_1)[v_.s0],((float*)conv3d_ncdhw_1)[v_.s1],((float*)conv3d_ncdhw_1)[v_.s2],((float*)conv3d_ncdhw_1)[v_.s3])) + ((float4(((float*)pad_temp)[v__1.s0],((float*)pad_temp)[v__1.s1],((float*)pad_temp)[v__1.s2],((float*)pad_temp)[v__1.s3])) * ((float4)(((float*)kernel_1)[(((((((((cse_var_4 * 120960) + (ff_outer_inner * 30240)) + (ff_inner * 5040)) + (rc_outer * 360)) + (rc_inner * 90)) + (ry_outer * 45)) + (ry_inner * 9)) + (rx_outer * 3)) + rz_outer)], ((float*)kernel_1)[(((((((((cse_var_4 * 120960) + (ff_outer_inner * 30240)) + (ff_inner * 5040)) + (rc_outer * 360)) + (rc_inner * 90)) + (ry_outer * 45)) + (ry_inner * 9)) + (rx_outer * 3)) + rz_outer)], ((float*)kernel_1)[(((((((((cse_var_4 * 120960) + (ff_outer_inner * 30240)) + (ff_inner * 5040)) + (rc_outer * 360)) + (rc_inner * 90)) + (ry_outer * 45)) + (ry_inner * 9)) + (rx_outer * 3)) + rz_outer)], ((float*)kernel_1)[(((((((((cse_var_4 * 120960) + (ff_outer_inner * 30240)) + (ff_inner * 5040)) + (rc_outer * 360)) + (rc_inner * 90)) + (ry_outer * 45)) + (ry_inner * 9)) + (rx_outer * 3)) + rz_outer)]))));\n                    },\n                  },\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv3d_ncdhw_local[16];\n  __shared__ float pad_temp_shared[4200];\n  __shared__ float kernel_shared[5040];\n  for (int ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 2; ++ff_c_outer_inner_init) {\n    for (int zz_c_inner_init = 0; zz_c_inner_init < 2; ++zz_c_inner_init) {\n      conv3d_ncdhw_local[((ff_c_outer_inner_init * 2) + zz_c_inner_init)] = 0.000000e+00f;\n      conv3d_ncdhw_local[(((ff_c_outer_inner_init * 2) + zz_c_inner_init) + 4)] = 0.000000e+00f;\n      conv3d_ncdhw_local[(((ff_c_outer_inner_init * 2) + zz_c_inner_init) + 8)] = 0.000000e+00f;\n      conv3d_ncdhw_local[(((ff_c_outer_inner_init * 2) + zz_c_inner_init) + 12)] = 0.000000e+00f;\n    },\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 4; ++rc_outer_outer) {\n    for (int ry_outer_outer = 0; ry_outer_outer < 2; ++ry_outer_outer) {\n      for (int rz_outer_outer = 0; rz_outer_outer < 3; ++rz_outer_outer) {\n        __syncthreads();\n        for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer < 70; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer) {\n          pad_temp_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 60) + ((int)threadIdx.x))] = (((((((1 <= (((ry_outer_outer * 5) + (((int)threadIdx.x) / 12)) + ((((int)blockIdx.x) % 356) >> 2))) && ((((ry_outer_outer * 5) + (((int)threadIdx.x) / 12)) + ((((int)blockIdx.x) % 356) >> 2)) < 97)) && (2 <= (((int)threadIdx.x) % 12))) && ((((int)threadIdx.x) % 12) < 10)) && (1 <= ((((((int)blockIdx.x) & 3) * 2) + rz_outer_outer) + (((int)threadIdx.x) & 1)))) && (((((((int)blockIdx.x) & 3) * 2) + rz_outer_outer) + (((int)threadIdx.x) & 1)) < 9)) ? data[(((((((((((((((int)blockIdx.x) / 1424) * 860160) + ((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer / 14) * 172032)) + (rc_outer_outer * 43008)) + ((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer % 14) * 3072)) + (ry_outer_outer * 160)) + ((((int)threadIdx.x) / 12) * 32)) + (((((int)blockIdx.x) % 356) >> 2) * 32)) + (((((int)threadIdx.x) % 12) >> 1) * 8)) + ((((int)blockIdx.x) & 3) * 2)) + rz_outer_outer) + (((int)threadIdx.x) & 1)) - 41)] : 0.000000e+00f);\n        },\n        for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 < 84; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1) {\n          kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 * 60) + ((int)threadIdx.x))] = kernel[(((((((((((int)blockIdx.x) % 1424) / 356) * 120960) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 * 2) + (((int)threadIdx.x) / 30)) / 7) * 5040)) + (rc_outer_outer * 1260)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer_1 * 4) + (((int)threadIdx.x) / 15)) % 14) * 90)) + (ry_outer_outer * 45)) + ((((int)threadIdx.x) % 15) * 3)) + rz_outer_outer)];\n        },\n        __syncthreads();\n        for (int rc_outer_inner = 0; rc_outer_inner < 14; ++rc_outer_inner) {\n          for (int rx_outer_inner = 0; rx_outer_inner < 3; ++rx_outer_inner) {\n            for (int ff_c_outer_inner = 0; ff_c_outer_inner < 2; ++ff_c_outer_inner) {\n              for (int ry_inner = 0; ry_inner < 5; ++ry_inner) {\n                for (int zz_c_inner = 0; zz_c_inner < 2; ++zz_c_inner) {\n                  conv3d_ncdhw_local[((ff_c_outer_inner * 2) + zz_c_inner)] = (conv3d_ncdhw_local[((ff_c_outer_inner * 2) + zz_c_inner)] + (pad_temp_shared[(((((((((int)threadIdx.x) / 12) * 840) + (rc_outer_inner * 60)) + (ry_inner * 12)) + (rx_outer_inner * 2)) + ((((int)threadIdx.x) & 3) * 2)) + zz_c_inner)] * kernel_shared[(((((((((int)threadIdx.x) % 12) >> 2) * 420) + (ff_c_outer_inner * 210)) + (rc_outer_inner * 15)) + (ry_inner * 3)) + rx_outer_inner)]));\n                  conv3d_ncdhw_local[(((ff_c_outer_inner * 2) + zz_c_inner) + 4)] = (conv3d_ncdhw_local[(((ff_c_outer_inner * 2) + zz_c_inner) + 4)] + (pad_temp_shared[(((((((((int)threadIdx.x) / 12) * 840) + (rc_outer_inner * 60)) + (ry_inner * 12)) + (rx_outer_inner * 2)) + ((((int)threadIdx.x) & 3) * 2)) + zz_c_inner)] * kernel_shared[((((((((((int)threadIdx.x) % 12) >> 2) * 420) + (ff_c_outer_inner * 210)) + (rc_outer_inner * 15)) + (ry_inner * 3)) + rx_outer_inner) + 1260)]));\n                  conv3d_ncdhw_local[(((ff_c_outer_inner * 2) + zz_c_inner) + 8)] = (conv3d_ncdhw_local[(((ff_c_outer_inner * 2) + zz_c_inner) + 8)] + (pad_temp_shared[(((((((((int)threadIdx.x) / 12) * 840) + (rc_outer_inner * 60)) + (ry_inner * 12)) + (rx_outer_inner * 2)) + ((((int)threadIdx.x) & 3) * 2)) + zz_c_inner)] * kernel_shared[((((((((((int)threadIdx.x) % 12) >> 2) * 420) + (ff_c_outer_inner * 210)) + (rc_outer_inner * 15)) + (ry_inner * 3)) + rx_outer_inner) + 2520)]));\n                  conv3d_ncdhw_local[(((ff_c_outer_inner * 2) + zz_c_inner) + 12)] = (conv3d_ncdhw_local[(((ff_c_outer_inner * 2) + zz_c_inner) + 12)] + (pad_temp_shared[(((((((((int)threadIdx.x) / 12) * 840) + (rc_outer_inner * 60)) + (ry_inner * 12)) + (rx_outer_inner * 2)) + ((((int)threadIdx.x) & 3) * 2)) + zz_c_inner)] * kernel_shared[((((((((((int)threadIdx.x) % 12) >> 2) * 420) + (ff_c_outer_inner * 210)) + (rc_outer_inner * 15)) + (ry_inner * 3)) + rx_outer_inner) + 3780)]));\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 2; ++ff_inner) {\n    for (int zz_inner = 0; zz_inner < 2; ++zz_inner) {\n      conv3d_ncdhw[((((((((((((int)blockIdx.x) / 1424) * 1367040) + ((((int)threadIdx.x) / 12) * 273408)) + (((((int)blockIdx.x) % 1424) / 356) * 68352)) + (((((int)threadIdx.x) % 12) >> 2) * 5696)) + (ff_inner * 2848)) + (((((int)blockIdx.x) % 356) >> 2) * 32)) + ((((int)threadIdx.x) & 3) * 8)) + ((((int)blockIdx.x) & 3) * 2)) + zz_inner)] = conv3d_ncdhw_local[((ff_inner * 2) + zz_inner)];\n      conv3d_ncdhw[(((((((((((((int)blockIdx.x) / 1424) * 1367040) + ((((int)threadIdx.x) / 12) * 273408)) + (((((int)blockIdx.x) % 1424) / 356) * 68352)) + (((((int)threadIdx.x) % 12) >> 2) * 5696)) + (ff_inner * 2848)) + (((((int)blockIdx.x) % 356) >> 2) * 32)) + ((((int)threadIdx.x) & 3) * 8)) + ((((int)blockIdx.x) & 3) * 2)) + zz_inner) + 17088)] = conv3d_ncdhw_local[(((ff_inner * 2) + zz_inner) + 4)];\n      conv3d_ncdhw[(((((((((((((int)blockIdx.x) / 1424) * 1367040) + ((((int)threadIdx.x) / 12) * 273408)) + (((((int)blockIdx.x) % 1424) / 356) * 68352)) + (((((int)threadIdx.x) % 12) >> 2) * 5696)) + (ff_inner * 2848)) + (((((int)blockIdx.x) % 356) >> 2) * 32)) + ((((int)threadIdx.x) & 3) * 8)) + ((((int)blockIdx.x) & 3) * 2)) + zz_inner) + 34176)] = conv3d_ncdhw_local[(((ff_inner * 2) + zz_inner) + 8)];\n      conv3d_ncdhw[(((((((((((((int)blockIdx.x) / 1424) * 1367040) + ((((int)threadIdx.x) / 12) * 273408)) + (((((int)blockIdx.x) % 1424) / 356) * 68352)) + (((((int)threadIdx.x) % 12) >> 2) * 5696)) + (ff_inner * 2848)) + (((((int)blockIdx.x) % 356) >> 2) * 32)) + ((((int)threadIdx.x) & 3) * 8)) + ((((int)blockIdx.x) & 3) * 2)) + zz_inner) + 51264)] = conv3d_ncdhw_local[(((ff_inner * 2) + zz_inner) + 12)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 56, 96, 4, 8), \"float32\"), kernel: T.Buffer((96, 56, 10, 3, 3), \"float32\"), conv3d_ncdhw: T.Buffer((10, 96, 89, 4, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        pad_temp = T.allocate([3292800], \"float32\", \"global\")\n        pad_temp_1 = T.Buffer((3292800,), data=pad_temp)\n        for i0_i1_fused_i2_fused in T.parallel(54880):\n            for i3, i4_s in T.grid(6, 10):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused % 98\n                data_1 = T.Buffer((1720320,), data=data.data)\n                pad_temp_1[i0_i1_fused_i2_fused * 60 + i3 * 10 + i4_s] = T.if_then_else(1 <= cse_var_1 and cse_var_1 < 97 and 1 <= i3 and i3 < 5 and 1 <= i4_s and i4_s < 9, data_1[i0_i1_fused_i2_fused // 98 * 3072 + cse_var_1 * 32 + i3 * 8 + i4_s - 41], T.float32(0))\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused in T.parallel(7120):\n            conv3d_ncdhw_1 = T.Buffer((2734080,), data=conv3d_ncdhw.data)\n            for ff_outer_inner_init, ff_inner_init, xx_inner_init in T.grid(4, 6, 4):\n                conv3d_ncdhw_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 1424 * 546816 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 178 // 89 * 273408 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 1424 // 356 * 68352 + ff_outer_inner_init * 17088 + ff_inner_init * 2848 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 89 * 32 + xx_inner_init * 8 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 356 // 178 * 4:nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 1424 * 546816 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 178 // 89 * 273408 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 1424 // 356 * 68352 + ff_outer_inner_init * 17088 + ff_inner_init * 2848 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 89 * 32 + xx_inner_init * 8 + nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 356 // 178 * 4 + 4] = T.Broadcast(T.float32(0), 4)\n            for rc_outer, ry_outer, rx_outer, rz_outer, ff_outer_inner, rc_inner, ry_inner, ff_inner, xx_inner in T.grid(14, 2, 3, 3, 4, 4, 5, 6, 4):\n                cse_var_7: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 1424\n                cse_var_6: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 89\n                cse_var_5: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 178 // 89\n                cse_var_4: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 1424 // 356\n                cse_var_3: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_xx_outer_outer_outer_fused_zz_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 356 // 178 * 4\n                cse_var_2: T.int32 = cse_var_7 * 546816 + cse_var_5 * 273408 + cse_var_4 * 68352 + ff_outer_inner * 17088 + ff_inner * 2848 + cse_var_6 * 32 + xx_inner * 8 + cse_var_3\n                kernel_1 = T.Buffer((483840,), data=kernel.data)\n                conv3d_ncdhw_1[cse_var_2:cse_var_2 + 4] = conv3d_ncdhw_1[cse_var_2:cse_var_2 + 4] + pad_temp_1[cse_var_7 * 658560 + cse_var_5 * 329280 + rc_outer * 23520 + rc_inner * 5880 + ry_outer * 300 + ry_inner * 60 + cse_var_6 * 60 + xx_inner * 10 + rx_outer * 10 + cse_var_3 + rz_outer:cse_var_7 * 658560 + cse_var_5 * 329280 + rc_outer * 23520 + rc_inner * 5880 + ry_outer * 300 + ry_inner * 60 + cse_var_6 * 60 + xx_inner * 10 + rx_outer * 10 + cse_var_3 + rz_outer + 4] * T.Broadcast(kernel_1[cse_var_4 * 120960 + ff_outer_inner * 30240 + ff_inner * 5040 + rc_outer * 360 + rc_inner * 90 + ry_outer * 45 + ry_inner * 9 + rx_outer * 3 + rz_outer], 4)",
        "data": "10_56_96_4_8",
        "kernel": "96_56_10_3_3"
    },
    {
        "op_name": "conv3d_ncdhw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv3d_ncdhw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv3d_ncdhw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv3d_ncdhw_1 = (((DLTensor*)conv3d_ncdhw)[0].data);\n  void* default_function_conv3d_ncdhw_shape = (((DLTensor*)conv3d_ncdhw)[0].shape);\n  void* default_function_conv3d_ncdhw_strides = (((DLTensor*)conv3d_ncdhw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv3d_ncdhw_strides == NULL)) {\n  },\n  void* pad_temp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)2619008, 2, 32);\n  if (pad_temp == NULL) {\n    return -1;\n  },\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 20461; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 4; ++i3) {\n      for (int32_t i4_s = 0; i4_s < 8; ++i4_s) {\n        int32_t cse_var_1 = (i0_i1_fused_i2_fused % 37);\n        ((float*)pad_temp)[(((i0_i1_fused_i2_fused * 32) + (i3 * 8)) + i4_s)] = (((((((1 <= cse_var_1) && (cse_var_1 < 36)) && (1 <= i3)) && (i3 < 3)) && (1 <= i4_s)) && (i4_s < 7)) ? ((float*)data_1)[((((((i0_i1_fused_i2_fused / 37) * 420) + (cse_var_1 * 12)) + (i3 * 6)) + i4_s) - 19)] : 0.000000e+00f);\n      },\n    },\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused < 98; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused) {\n    void* conv3d_ncdhw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)3720, 2, 32);\n    if (conv3d_ncdhw_local == NULL) {\n      return -1;\n    },\n    for (int32_t yy_c_outer_inner_init = 0; yy_c_outer_inner_init < 31; ++yy_c_outer_inner_init) {\n      for (int32_t xx_c_outer_inner_init = 0; xx_c_outer_inner_init < 2; ++xx_c_outer_inner_init) {\n        for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 5; ++ff_c_inner_init) {\n          ((float3*)conv3d_ncdhw_local)[(((ff_c_inner_init * 62) + (yy_c_outer_inner_init * 2)) + xx_c_outer_inner_init)] = ((float3)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n        },\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 79; ++rc_outer) {\n      for (int32_t rz_outer = 0; rz_outer < 3; ++rz_outer) {\n        for (int32_t yy_c_outer_inner = 0; yy_c_outer_inner < 31; ++yy_c_outer_inner) {\n          for (int32_t xx_c_outer_inner = 0; xx_c_outer_inner < 2; ++xx_c_outer_inner) {\n            for (int32_t ry_inner = 0; ry_inner < 7; ++ry_inner) {\n              for (int32_t rx_inner = 0; rx_inner < 3; ++rx_inner) {\n                for (int32_t ff_c_inner = 0; ff_c_inner < 5; ++ff_c_inner) {\n                  int32_t cse_var_2 = (((ff_c_inner * 62) + (yy_c_outer_inner * 2)) + xx_c_outer_inner);\n                  int32_t3 v_ = int32_t3(((((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused / 14) * 93536) + (rc_outer * 1184)) + (yy_c_outer_inner * 32)) + (ry_inner * 32)) + (xx_c_outer_inner * 8)) + (rx_inner * 8)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused & 1) * 3)) + rz_outer))+(1*0), ((((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused / 14) * 93536) + (rc_outer * 1184)) + (yy_c_outer_inner * 32)) + (ry_inner * 32)) + (xx_c_outer_inner * 8)) + (rx_inner * 8)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused & 1) * 3)) + rz_outer))+(1*1), ((((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused / 14) * 93536) + (rc_outer * 1184)) + (yy_c_outer_inner * 32)) + (ry_inner * 32)) + (xx_c_outer_inner * 8)) + (rx_inner * 8)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused & 1) * 3)) + rz_outer))+(1*2));\n                  ((float3*)conv3d_ncdhw_local)[cse_var_2] = (((float3*)conv3d_ncdhw_local)[cse_var_2] + ((float3(((float*)pad_temp)[v_.s0],((float*)pad_temp)[v_.s1],((float*)pad_temp)[v_.s2])) * ((float3)(((float*)kernel_1)[((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 14) >> 1) * 24885) + (ff_c_inner * 4977)) + (rc_outer * 63)) + (ry_inner * 9)) + (rx_inner * 3)) + rz_outer)], ((float*)kernel_1)[((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 14) >> 1) * 24885) + (ff_c_inner * 4977)) + (rc_outer * 63)) + (ry_inner * 9)) + (rx_inner * 3)) + rz_outer)], ((float*)kernel_1)[((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 14) >> 1) * 24885) + (ff_c_inner * 4977)) + (rc_outer * 63)) + (ry_inner * 9)) + (rx_inner * 3)) + rz_outer)]))));\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t ff_inner = 0; ff_inner < 5; ++ff_inner) {\n      for (int32_t yy_inner = 0; yy_inner < 31; ++yy_inner) {\n        for (int32_t xx_inner = 0; xx_inner < 2; ++xx_inner) {\n          *(float3*)(((float*)conv3d_ncdhw_1) + ((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused >> 1) * 1860) + (ff_inner * 372)) + (yy_inner * 12)) + (xx_inner * 6)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused & 1) * 3))) = ((float3*)conv3d_ncdhw_local)[(((ff_inner * 62) + (yy_inner * 2)) + xx_inner)];\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv3d_ncdhw_local) != 0) {\n      return -1;\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, pad_temp) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(217) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(217) default_function_kernel(float* __restrict__ conv3d_ncdhw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv3d_ncdhw_local[6];\n  __shared__ float pad_temp_shared[5180];\n  __shared__ float kernel_shared[63];\n  for (int zz_c_outer_inner_init = 0; zz_c_outer_inner_init < 3; ++zz_c_outer_inner_init) {\n    conv3d_ncdhw_local[zz_c_outer_inner_init] = 0.000000e+00f;\n    conv3d_ncdhw_local[(zz_c_outer_inner_init + 3)] = 0.000000e+00f;\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 79; ++rc_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer < 5; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 31) + (((int)threadIdx.x) / 7)) < 148) {\n        for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s < 5; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s) {\n          pad_temp_shared[(((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 1085) + (((int)threadIdx.x) * 5)) + ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s)] = (((((((4 <= (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 69) + ((int)threadIdx.x)) % 148)) && ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 69) + ((int)threadIdx.x)) % 148) < 144)) && (1 <= ((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer + ((int)threadIdx.x)) & 3))) && (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer + ((int)threadIdx.x)) & 3) < 3)) && (1 <= (((((int)blockIdx.x) & 1) * 3) + ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s))) && ((((((int)blockIdx.x) & 1) * 3) + ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s) < 7)) ? data[((((((((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 217) + ((int)threadIdx.x)) / 148) * 33180) + (rc_outer_outer * 420)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 69) + ((int)threadIdx.x)) % 148) >> 2) * 12)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer + ((int)threadIdx.x)) & 3) * 6)) + ((((int)blockIdx.x) & 1) * 3)) + ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s) - 19)] : 0.000000e+00f);\n        },\n      },\n    },\n    if (((int)threadIdx.x) < 63) {\n      kernel_shared[((int)threadIdx.x)] = kernel[((((((int)blockIdx.x) >> 1) * 4977) + (rc_outer_outer * 63)) + ((int)threadIdx.x))];\n    },\n    __syncthreads();\n    for (int rx_outer_inner = 0; rx_outer_inner < 3; ++rx_outer_inner) {\n      for (int rz_outer_inner = 0; rz_outer_inner < 3; ++rz_outer_inner) {\n        for (int zz_c_outer_inner = 0; zz_c_outer_inner < 3; ++zz_c_outer_inner) {\n          for (int ry_inner = 0; ry_inner < 7; ++ry_inner) {\n            conv3d_ncdhw_local[zz_c_outer_inner] = (conv3d_ncdhw_local[zz_c_outer_inner] + (pad_temp_shared[(((((((((int)threadIdx.x) / 31) * 740) + (ry_inner * 20)) + ((((int)threadIdx.x) % 31) * 20)) + (rx_outer_inner * 5)) + zz_c_outer_inner) + rz_outer_inner)] * kernel_shared[(((ry_inner * 9) + (rx_outer_inner * 3)) + rz_outer_inner)]));\n            conv3d_ncdhw_local[(zz_c_outer_inner + 3)] = (conv3d_ncdhw_local[(zz_c_outer_inner + 3)] + (pad_temp_shared[((((((((((int)threadIdx.x) / 31) * 740) + (ry_inner * 20)) + ((((int)threadIdx.x) % 31) * 20)) + (rx_outer_inner * 5)) + zz_c_outer_inner) + rz_outer_inner) + 5)] * kernel_shared[(((ry_inner * 9) + (rx_outer_inner * 3)) + rz_outer_inner)]));\n          },\n        },\n      },\n    },\n  },\n  for (int zz_inner = 0; zz_inner < 3; ++zz_inner) {\n    conv3d_ncdhw[((((((((int)threadIdx.x) / 31) * 13020) + ((((int)blockIdx.x) >> 1) * 372)) + ((((int)threadIdx.x) % 31) * 12)) + ((((int)blockIdx.x) & 1) * 3)) + zz_inner)] = conv3d_ncdhw_local[zz_inner];\n    conv3d_ncdhw[(((((((((int)threadIdx.x) / 31) * 13020) + ((((int)blockIdx.x) >> 1) * 372)) + ((((int)threadIdx.x) % 31) * 12)) + ((((int)blockIdx.x) & 1) * 3)) + zz_inner) + 6)] = conv3d_ncdhw_local[(zz_inner + 3)];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 79, 35, 2, 6), \"float32\"), kernel: T.Buffer((35, 79, 7, 3, 3), \"float32\"), conv3d_ncdhw: T.Buffer((7, 35, 31, 2, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        pad_temp = T.allocate([654752], \"float32\", \"global\")\n        pad_temp_1 = T.Buffer((654752,), data=pad_temp)\n        for i0_i1_fused_i2_fused in T.parallel(20461):\n            for i3, i4_s in T.grid(4, 8):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused % 37\n                data_1 = T.Buffer((232260,), data=data.data)\n                pad_temp_1[i0_i1_fused_i2_fused * 32 + i3 * 8 + i4_s] = T.if_then_else(1 <= cse_var_1 and cse_var_1 < 36 and 1 <= i3 and i3 < 3 and 1 <= i4_s and i4_s < 7, data_1[i0_i1_fused_i2_fused // 37 * 420 + cse_var_1 * 12 + i3 * 6 + i4_s - 19], T.float32(0))\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused in T.parallel(98):\n            conv3d_ncdhw_local = T.allocate([310], \"float32x3\", \"local\")\n            conv3d_ncdhw_local_1 = T.Buffer((310,), \"float32x3\", data=conv3d_ncdhw_local, scope=\"local\")\n            for yy_c_outer_inner_init, xx_c_outer_inner_init, ff_c_inner_init in T.grid(31, 2, 5):\n                conv3d_ncdhw_local_1[ff_c_inner_init * 62 + yy_c_outer_inner_init * 2 + xx_c_outer_inner_init] = T.Broadcast(T.float32(0), 3)\n            for rc_outer, rz_outer, yy_c_outer_inner, xx_c_outer_inner, ry_inner, rx_inner, ff_c_inner in T.grid(79, 3, 31, 2, 7, 3, 5):\n                cse_var_2: T.int32 = ff_c_inner * 62 + yy_c_outer_inner * 2 + xx_c_outer_inner\n                kernel_1 = T.Buffer((174195,), data=kernel.data)\n                conv3d_ncdhw_local_1[cse_var_2] = conv3d_ncdhw_local_1[cse_var_2] + pad_temp_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused // 14 * 93536 + rc_outer * 1184 + yy_c_outer_inner * 32 + ry_inner * 32 + xx_c_outer_inner * 8 + rx_inner * 8 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 2 * 3 + rz_outer:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused // 14 * 93536 + rc_outer * 1184 + yy_c_outer_inner * 32 + ry_inner * 32 + xx_c_outer_inner * 8 + rx_inner * 8 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 2 * 3 + rz_outer + 3] * T.Broadcast(kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 14 // 2 * 24885 + ff_c_inner * 4977 + rc_outer * 63 + ry_inner * 9 + rx_inner * 3 + rz_outer], 3)\n            for ff_inner, yy_inner, xx_inner in T.grid(5, 31, 2):\n                conv3d_ncdhw_1 = T.Buffer((91140,), data=conv3d_ncdhw.data)\n                conv3d_ncdhw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused // 2 * 1860 + ff_inner * 372 + yy_inner * 12 + xx_inner * 6 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 2 * 3:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused // 2 * 1860 + ff_inner * 372 + yy_inner * 12 + xx_inner * 6 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_xx_outer_outer_fused_zz_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused_xx_outer_inner_fused_zz_outer_inner_fused % 2 * 3 + 3] = conv3d_ncdhw_local_1[ff_inner * 62 + yy_inner * 2 + xx_inner]",
        "data": "7_79_35_2_6",
        "kernel": "35_79_7_3_3"
    },
    {
        "op_name": "conv3d_winograd_weight_transform",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t transform_weight_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* transform_weight = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* transform_weight_1 = (((DLTensor*)transform_weight)[0].data);\n  void* default_function_transform_weight_shape = (((DLTensor*)transform_weight)[0].shape);\n  void* default_function_transform_weight_strides = (((DLTensor*)transform_weight)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_transform_weight_strides == NULL)) {\n  },\n  for (int32_t eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused = 0; eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused < 39675; ++eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused) {\n    for (int32_t d_outer_inner_init = 0; d_outer_inner_init < 4; ++d_outer_inner_init) {\n      for (int32_t co_outer_inner_init = 0; co_outer_inner_init < 7; ++co_outer_inner_init) {\n        ((float*)transform_weight_1)[(((((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused / 69) * 1932) + (d_outer_inner_init * 483)) + (co_outer_inner_init * 69)) + (eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused % 69))] = 0.000000e+00f;\n      },\n    },\n    for (int32_t r_kw_outer = 0; r_kw_outer < 2; ++r_kw_outer) {\n      for (int32_t d_outer_inner = 0; d_outer_inner < 4; ++d_outer_inner) {\n        for (int32_t co_outer_inner = 0; co_outer_inner < 7; ++co_outer_inner) {\n          for (int32_t r_kh_inner = 0; r_kh_inner < 4; ++r_kh_inner) {\n            for (int32_t r_kw_inner = 0; r_kw_inner < 2; ++r_kw_inner) {\n              int32_t cse_var_24 = (r_kw_outer * 2);\n              int32_t cse_var_23 = (eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused / 7935);\n              int32_t cse_var_22 = (eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused % 69);\n              int32_t cse_var_21 = ((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused % 7935) / 1587);\n              int32_t cse_var_20 = (cse_var_24 + r_kw_inner);\n              bool cse_var_19 = (r_kh_inner == 0);\n              bool cse_var_18 = (r_kh_inner == 1);\n              bool cse_var_17 = (r_kh_inner == 2);\n              bool cse_var_16 = (r_kh_inner == 3);\n              bool cse_var_15 = (cse_var_23 == 0);\n              bool cse_var_14 = (cse_var_23 == 1);\n              bool cse_var_13 = (cse_var_23 == 2);\n              bool cse_var_12 = (cse_var_23 == 3);\n              bool cse_var_11 = (cse_var_23 == 4);\n              bool cse_var_10 = (cse_var_21 == 0);\n              bool cse_var_9 = (cse_var_21 == 1);\n              bool cse_var_8 = (cse_var_21 == 2);\n              bool cse_var_7 = (cse_var_21 == 3);\n              bool cse_var_6 = (cse_var_20 == 0);\n              bool cse_var_5 = (cse_var_20 == 1);\n              bool cse_var_4 = (cse_var_20 == 2);\n              bool cse_var_3 = (cse_var_20 == 3);\n              bool cse_var_2 = (cse_var_21 == 4);\n              int32_t cse_var_1 = (((((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused / 69) * 1932) + (d_outer_inner * 483)) + (co_outer_inner * 69)) + cse_var_22);\n              ((float*)transform_weight_1)[cse_var_1] = (((float*)transform_weight_1)[cse_var_1] + ((((float*)data_1)[(((((((co_outer_inner * 101568) + (cse_var_22 * 1472)) + (((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused % 1587) / 69) * 64)) + (d_outer_inner * 16)) + (r_kh_inner * 4)) + cse_var_24) + r_kw_inner)] * ((cse_var_11 && cse_var_16) ? 1.000000e+00f : ((cse_var_11 && cse_var_17) ? 0.000000e+00f : ((cse_var_11 && cse_var_18) ? 0.000000e+00f : ((cse_var_11 && cse_var_19) ? 0.000000e+00f : ((cse_var_12 && cse_var_16) ? -3.333333e-01f : ((cse_var_12 && cse_var_17) ? -6.666667e-01f : ((cse_var_12 && cse_var_18) ? -1.333333e+00f : ((cse_var_12 && cse_var_19) ? -2.666667e+00f : ((cse_var_13 && cse_var_16) ? 1.000000e+00f : ((cse_var_13 && cse_var_17) ? 1.000000e+00f : ((cse_var_13 && cse_var_18) ? 1.000000e+00f : ((cse_var_13 && cse_var_19) ? 1.000000e+00f : ((cse_var_14 && cse_var_16) ? 3.333333e-01f : ((cse_var_14 && cse_var_17) ? -3.333333e-01f : ((cse_var_14 && cse_var_18) ? 3.333333e-01f : ((cse_var_14 && cse_var_19) ? -3.333333e-01f : ((cse_var_15 && cse_var_16) ? 0.000000e+00f : ((cse_var_15 && cse_var_17) ? 0.000000e+00f : ((cse_var_15 && cse_var_18) ? 0.000000e+00f : ((cse_var_15 && cse_var_19) ? 2.000000e+00f : 0.000000e+00f))))))))))))))))))))) * ((cse_var_2 && cse_var_3) ? 1.000000e+00f : ((cse_var_2 && cse_var_4) ? 0.000000e+00f : ((cse_var_2 && cse_var_5) ? 0.000000e+00f : ((cse_var_2 && cse_var_6) ? 0.000000e+00f : ((cse_var_7 && cse_var_3) ? -3.333333e-01f : ((cse_var_7 && cse_var_4) ? -6.666667e-01f : ((cse_var_7 && cse_var_5) ? -1.333333e+00f : ((cse_var_7 && cse_var_6) ? -2.666667e+00f : ((cse_var_8 && cse_var_3) ? 1.000000e+00f : ((cse_var_8 && cse_var_4) ? 1.000000e+00f : ((cse_var_8 && cse_var_5) ? 1.000000e+00f : ((cse_var_8 && cse_var_6) ? 1.000000e+00f : ((cse_var_9 && cse_var_3) ? 3.333333e-01f : ((cse_var_9 && cse_var_4) ? -3.333333e-01f : ((cse_var_9 && cse_var_5) ? 3.333333e-01f : ((cse_var_9 && cse_var_6) ? -3.333333e-01f : ((cse_var_10 && cse_var_3) ? 0.000000e+00f : ((cse_var_10 && cse_var_4) ? 0.000000e+00f : ((cse_var_10 && cse_var_5) ? 0.000000e+00f : ((cse_var_10 && cse_var_6) ? 2.000000e+00f : 0.000000e+00f))))))))))))))))))))));\n            },\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(230) default_function_kernel(float* __restrict__ data, float* __restrict__ transform_weight);\nextern \"C\" __global__ void __launch_bounds__(230) default_function_kernel(float* __restrict__ data, float* __restrict__ transform_weight) {\n  float transform_weight_local[7];\n  __shared__ float data_shared[1288];\n  __shared__ float G_shared[20];\n  transform_weight_local[0] = 0.000000e+00f;\n  transform_weight_local[1] = 0.000000e+00f;\n  transform_weight_local[2] = 0.000000e+00f;\n  transform_weight_local[3] = 0.000000e+00f;\n  transform_weight_local[4] = 0.000000e+00f;\n  transform_weight_local[5] = 0.000000e+00f;\n  transform_weight_local[6] = 0.000000e+00f;\n  for (int r_kh_outer_outer = 0; r_kh_outer_outer < 4; ++r_kh_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer < 6; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 5) + (((int)threadIdx.x) / 46)) < 28) {\n        data_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 230) + ((int)threadIdx.x))] = data[((((((((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 5) + (((int)threadIdx.x) / 46)) >> 2) * 101568) + ((((int)blockIdx.x) % 3) * 33856)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 23) + (((int)threadIdx.x) >> 1)) % 92) >> 2) * 1472)) + (((((int)blockIdx.x) % 138) / 3) * 32)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 3) + (((int)threadIdx.x) >> 1)) & 3) >> 1) * 16)) + (r_kh_outer_outer * 4)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 2) + ((int)threadIdx.x)) & 3))];\n      },\n    },\n    if (((int)threadIdx.x) < 20) {\n      G_shared[((int)threadIdx.x)] = ((((int)threadIdx.x) == 19) ? 1.000000e+00f : ((((int)threadIdx.x) == 18) ? 0.000000e+00f : ((((int)threadIdx.x) == 17) ? 0.000000e+00f : ((((int)threadIdx.x) == 16) ? 0.000000e+00f : ((((int)threadIdx.x) == 15) ? -3.333333e-01f : ((((int)threadIdx.x) == 14) ? -6.666667e-01f : ((((int)threadIdx.x) == 13) ? -1.333333e+00f : ((((int)threadIdx.x) == 12) ? -2.666667e+00f : ((((int)threadIdx.x) == 11) ? 1.000000e+00f : ((((int)threadIdx.x) == 10) ? 1.000000e+00f : ((((int)threadIdx.x) == 9) ? 1.000000e+00f : ((((int)threadIdx.x) == 8) ? 1.000000e+00f : ((((int)threadIdx.x) == 7) ? 3.333333e-01f : ((((int)threadIdx.x) == 6) ? -3.333333e-01f : ((((int)threadIdx.x) == 5) ? 3.333333e-01f : ((((int)threadIdx.x) == 4) ? -3.333333e-01f : ((((int)threadIdx.x) == 3) ? 0.000000e+00f : ((((int)threadIdx.x) == 2) ? 0.000000e+00f : ((((int)threadIdx.x) == 1) ? 0.000000e+00f : 2.000000e+00f)))))))))))))))))));\n    },\n    __syncthreads();\n    for (int r_kw_outer_inner = 0; r_kw_outer_inner < 4; ++r_kw_outer_inner) {\n      transform_weight_local[0] = (transform_weight_local[0] + ((data_shared[((((((int)threadIdx.x) % 23) * 8) + (((((int)threadIdx.x) % 46) / 23) * 4)) + r_kw_outer_inner)] * G_shared[(((((int)threadIdx.x) / 46) * 4) + r_kh_outer_outer)]) * G_shared[(((((int)blockIdx.x) / 138) * 4) + r_kw_outer_inner)]));\n      transform_weight_local[1] = (transform_weight_local[1] + ((data_shared[(((((((int)threadIdx.x) % 23) * 8) + (((((int)threadIdx.x) % 46) / 23) * 4)) + r_kw_outer_inner) + 184)] * G_shared[(((((int)threadIdx.x) / 46) * 4) + r_kh_outer_outer)]) * G_shared[(((((int)blockIdx.x) / 138) * 4) + r_kw_outer_inner)]));\n      transform_weight_local[2] = (transform_weight_local[2] + ((data_shared[(((((((int)threadIdx.x) % 23) * 8) + (((((int)threadIdx.x) % 46) / 23) * 4)) + r_kw_outer_inner) + 368)] * G_shared[(((((int)threadIdx.x) / 46) * 4) + r_kh_outer_outer)]) * G_shared[(((((int)blockIdx.x) / 138) * 4) + r_kw_outer_inner)]));\n      transform_weight_local[3] = (transform_weight_local[3] + ((data_shared[(((((((int)threadIdx.x) % 23) * 8) + (((((int)threadIdx.x) % 46) / 23) * 4)) + r_kw_outer_inner) + 552)] * G_shared[(((((int)threadIdx.x) / 46) * 4) + r_kh_outer_outer)]) * G_shared[(((((int)blockIdx.x) / 138) * 4) + r_kw_outer_inner)]));\n      transform_weight_local[4] = (transform_weight_local[4] + ((data_shared[(((((((int)threadIdx.x) % 23) * 8) + (((((int)threadIdx.x) % 46) / 23) * 4)) + r_kw_outer_inner) + 736)] * G_shared[(((((int)threadIdx.x) / 46) * 4) + r_kh_outer_outer)]) * G_shared[(((((int)blockIdx.x) / 138) * 4) + r_kw_outer_inner)]));\n      transform_weight_local[5] = (transform_weight_local[5] + ((data_shared[(((((((int)threadIdx.x) % 23) * 8) + (((((int)threadIdx.x) % 46) / 23) * 4)) + r_kw_outer_inner) + 920)] * G_shared[(((((int)threadIdx.x) / 46) * 4) + r_kh_outer_outer)]) * G_shared[(((((int)blockIdx.x) / 138) * 4) + r_kw_outer_inner)]));\n      transform_weight_local[6] = (transform_weight_local[6] + ((data_shared[(((((((int)threadIdx.x) % 23) * 8) + (((((int)threadIdx.x) % 46) / 23) * 4)) + r_kw_outer_inner) + 1104)] * G_shared[(((((int)threadIdx.x) / 46) * 4) + r_kh_outer_outer)]) * G_shared[(((((int)blockIdx.x) / 138) * 4) + r_kw_outer_inner)]));\n    },\n  },\n  transform_weight[((((((((int)threadIdx.x) / 46) * 222180) + ((((int)blockIdx.x) / 3) * 966)) + (((((int)threadIdx.x) % 46) / 23) * 483)) + ((((int)blockIdx.x) % 3) * 23)) + (((int)threadIdx.x) % 23))] = transform_weight_local[0];\n  transform_weight[(((((((((int)threadIdx.x) / 46) * 222180) + ((((int)blockIdx.x) / 3) * 966)) + (((((int)threadIdx.x) % 46) / 23) * 483)) + ((((int)blockIdx.x) % 3) * 23)) + (((int)threadIdx.x) % 23)) + 69)] = transform_weight_local[1];\n  transform_weight[(((((((((int)threadIdx.x) / 46) * 222180) + ((((int)blockIdx.x) / 3) * 966)) + (((((int)threadIdx.x) % 46) / 23) * 483)) + ((((int)blockIdx.x) % 3) * 23)) + (((int)threadIdx.x) % 23)) + 138)] = transform_weight_local[2];\n  transform_weight[(((((((((int)threadIdx.x) / 46) * 222180) + ((((int)blockIdx.x) / 3) * 966)) + (((((int)threadIdx.x) % 46) / 23) * 483)) + ((((int)blockIdx.x) % 3) * 23)) + (((int)threadIdx.x) % 23)) + 207)] = transform_weight_local[3];\n  transform_weight[(((((((((int)threadIdx.x) / 46) * 222180) + ((((int)blockIdx.x) / 3) * 966)) + (((((int)threadIdx.x) % 46) / 23) * 483)) + ((((int)blockIdx.x) % 3) * 23)) + (((int)threadIdx.x) % 23)) + 276)] = transform_weight_local[4];\n  transform_weight[(((((((((int)threadIdx.x) / 46) * 222180) + ((((int)blockIdx.x) / 3) * 966)) + (((((int)threadIdx.x) % 46) / 23) * 483)) + ((((int)blockIdx.x) % 3) * 23)) + (((int)threadIdx.x) % 23)) + 345)] = transform_weight_local[5];\n  transform_weight[(((((((((int)threadIdx.x) / 46) * 222180) + ((((int)blockIdx.x) / 3) * 966)) + (((((int)threadIdx.x) % 46) / 23) * 483)) + ((((int)blockIdx.x) % 3) * 23)) + (((int)threadIdx.x) % 23)) + 414)] = transform_weight_local[6];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 69, 92, 4, 4), \"float32\"), transform_weight: T.Buffer((5, 5, 92, 7, 69), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused in T.parallel(39675):\n            transform_weight_1 = T.Buffer((1110900,), data=transform_weight.data)\n            for d_outer_inner_init, co_outer_inner_init in T.grid(4, 7):\n                transform_weight_1[eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused // 69 * 1932 + d_outer_inner_init * 483 + co_outer_inner_init * 69 + eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused % 69] = T.float32(0)\n            for r_kw_outer, d_outer_inner, co_outer_inner, r_kh_inner, r_kw_inner in T.grid(2, 4, 7, 4, 2):\n                cse_var_24: T.int32 = r_kw_outer * 2\n                cse_var_23: T.int32 = eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused // 7935\n                cse_var_22: T.int32 = eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused % 69\n                cse_var_21: T.int32 = eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused % 7935 // 1587\n                cse_var_20: T.int32 = cse_var_24 + r_kw_inner\n                cse_var_19: T.bool = r_kh_inner == 0\n                cse_var_18: T.bool = r_kh_inner == 1\n                cse_var_17: T.bool = r_kh_inner == 2\n                cse_var_16: T.bool = r_kh_inner == 3\n                cse_var_15: T.bool = cse_var_23 == 0\n                cse_var_14: T.bool = cse_var_23 == 1\n                cse_var_13: T.bool = cse_var_23 == 2\n                cse_var_12: T.bool = cse_var_23 == 3\n                cse_var_11: T.bool = cse_var_23 == 4\n                cse_var_10: T.bool = cse_var_21 == 0\n                cse_var_9: T.bool = cse_var_21 == 1\n                cse_var_8: T.bool = cse_var_21 == 2\n                cse_var_7: T.bool = cse_var_21 == 3\n                cse_var_6: T.bool = cse_var_20 == 0\n                cse_var_5: T.bool = cse_var_20 == 1\n                cse_var_4: T.bool = cse_var_20 == 2\n                cse_var_3: T.bool = cse_var_20 == 3\n                cse_var_2: T.bool = cse_var_21 == 4\n                cse_var_1: T.int32 = eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused // 69 * 1932 + d_outer_inner * 483 + co_outer_inner * 69 + cse_var_22\n                data_1 = T.Buffer((710976,), data=data.data)\n                transform_weight_1[cse_var_1] = transform_weight_1[cse_var_1] + data_1[co_outer_inner * 101568 + cse_var_22 * 1472 + eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_d_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused % 1587 // 69 * 64 + d_outer_inner * 16 + r_kh_inner * 4 + cse_var_24 + r_kw_inner] * T.Select(cse_var_11 and cse_var_16, T.float32(1), T.Select(cse_var_11 and cse_var_17, T.float32(0), T.Select(cse_var_11 and cse_var_18, T.float32(0), T.Select(cse_var_11 and cse_var_19, T.float32(0), T.Select(cse_var_12 and cse_var_16, T.float32(-0.3333333432674408), T.Select(cse_var_12 and cse_var_17, T.float32(-0.66666668653488159), T.Select(cse_var_12 and cse_var_18, T.float32(-1.3333333730697632), T.Select(cse_var_12 and cse_var_19, T.float32(-2.6666667461395264), T.Select(cse_var_13 and cse_var_16, T.float32(1), T.Select(cse_var_13 and cse_var_17, T.float32(1), T.Select(cse_var_13 and cse_var_18, T.float32(1), T.Select(cse_var_13 and cse_var_19, T.float32(1), T.Select(cse_var_14 and cse_var_16, T.float32(0.3333333432674408), T.Select(cse_var_14 and cse_var_17, T.float32(-0.3333333432674408), T.Select(cse_var_14 and cse_var_18, T.float32(0.3333333432674408), T.Select(cse_var_14 and cse_var_19, T.float32(-0.3333333432674408), T.Select(cse_var_15 and cse_var_16, T.float32(0), T.Select(cse_var_15 and cse_var_17, T.float32(0), T.Select(cse_var_15 and cse_var_18, T.float32(0), T.Select(cse_var_15 and cse_var_19, T.float32(2), T.float32(0))))))))))))))))))))) * T.Select(cse_var_2 and cse_var_3, T.float32(1), T.Select(cse_var_2 and cse_var_4, T.float32(0), T.Select(cse_var_2 and cse_var_5, T.float32(0), T.Select(cse_var_2 and cse_var_6, T.float32(0), T.Select(cse_var_7 and cse_var_3, T.float32(-0.3333333432674408), T.Select(cse_var_7 and cse_var_4, T.float32(-0.66666668653488159), T.Select(cse_var_7 and cse_var_5, T.float32(-1.3333333730697632), T.Select(cse_var_7 and cse_var_6, T.float32(-2.6666667461395264), T.Select(cse_var_8 and cse_var_3, T.float32(1), T.Select(cse_var_8 and cse_var_4, T.float32(1), T.Select(cse_var_8 and cse_var_5, T.float32(1), T.Select(cse_var_8 and cse_var_6, T.float32(1), T.Select(cse_var_9 and cse_var_3, T.float32(0.3333333432674408), T.Select(cse_var_9 and cse_var_4, T.float32(-0.3333333432674408), T.Select(cse_var_9 and cse_var_5, T.float32(0.3333333432674408), T.Select(cse_var_9 and cse_var_6, T.float32(-0.3333333432674408), T.Select(cse_var_10 and cse_var_3, T.float32(0), T.Select(cse_var_10 and cse_var_4, T.float32(0), T.Select(cse_var_10 and cse_var_5, T.float32(0), T.Select(cse_var_10 and cse_var_6, T.float32(2), T.float32(0)))))))))))))))))))))",
        "data": "7_69_92_4_4"
    },
    {
        "op_name": "conv3d_winograd_weight_transform",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t transform_weight_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* transform_weight = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* transform_weight_1 = (((DLTensor*)transform_weight)[0].data);\n  void* default_function_transform_weight_shape = (((DLTensor*)transform_weight)[0].shape);\n  void* default_function_transform_weight_strides = (((DLTensor*)transform_weight)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_transform_weight_strides == NULL)) {\n  },\n  for (int32_t eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused = 0; eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused < 1274; ++eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused) {\n    for (int32_t co_outer_outer_inner = 0; co_outer_outer_inner < 2; ++co_outer_outer_inner) {\n      for (int32_t ci_outer_inner_init = 0; ci_outer_inner_init < 11; ++ci_outer_inner_init) {\n        for (int32_t co_inner_init = 0; co_inner_init < 2; ++co_inner_init) {\n          *(float9*)(((float*)transform_weight_1) + ((((((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused % 7) * 72072) + ((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused / 7) * 396)) + (co_outer_outer_inner * 198)) + (co_inner_init * 99)) + (ci_outer_inner_init * 9))) = ((float9)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n        },\n      },\n      for (int32_t r_kh_outer = 0; r_kh_outer < 3; ++r_kh_outer) {\n        for (int32_t r_kw_outer = 0; r_kw_outer < 6; ++r_kw_outer) {\n          for (int32_t ci_outer_inner = 0; ci_outer_inner < 11; ++ci_outer_inner) {\n            for (int32_t r_kh_inner = 0; r_kh_inner < 2; ++r_kh_inner) {\n              for (int32_t co_inner = 0; co_inner < 2; ++co_inner) {\n                int32_t cse_var_30 = (eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused % 7);\n                int32_t cse_var_29 = (eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused / 182);\n                int32_t cse_var_28 = ((r_kh_outer * 2) + r_kh_inner);\n                bool cse_var_27 = (r_kw_outer == 0);\n                bool cse_var_26 = (r_kw_outer == 1);\n                bool cse_var_25 = (r_kw_outer == 2);\n                bool cse_var_24 = (r_kw_outer == 3);\n                bool cse_var_23 = (r_kw_outer == 4);\n                bool cse_var_22 = (r_kw_outer == 5);\n                bool cse_var_21 = (cse_var_29 == 0);\n                bool cse_var_20 = (cse_var_29 == 1);\n                bool cse_var_19 = (cse_var_29 == 2);\n                bool cse_var_18 = (cse_var_29 == 3);\n                bool cse_var_17 = (cse_var_29 == 4);\n                bool cse_var_16 = (cse_var_29 == 5);\n                bool cse_var_15 = (cse_var_29 == 6);\n                bool cse_var_14 = (cse_var_30 == 0);\n                bool cse_var_13 = (cse_var_30 == 1);\n                bool cse_var_12 = (cse_var_30 == 2);\n                bool cse_var_11 = (cse_var_30 == 3);\n                bool cse_var_10 = (cse_var_30 == 4);\n                bool cse_var_9 = (cse_var_30 == 5);\n                bool cse_var_8 = (cse_var_30 == 6);\n                bool cse_var_7 = (cse_var_28 == 0);\n                bool cse_var_6 = (cse_var_28 == 1);\n                bool cse_var_5 = (cse_var_28 == 2);\n                bool cse_var_4 = (cse_var_28 == 3);\n                bool cse_var_3 = (cse_var_28 == 4);\n                bool cse_var_2 = (cse_var_28 == 5);\n                int32_t cse_var_1 = (((((cse_var_30 * 72072) + ((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused / 7) * 396)) + (co_outer_outer_inner * 198)) + (co_inner * 99)) + (ci_outer_inner * 9));\n                int32_t9 v_ = int32_t9((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7), (cse_var_1)+(1*8));\n                int32_t9 v__1 = int32_t9(((((((((co_outer_outer_inner * 185328) + (co_inner * 92664)) + (ci_outer_inner * 8424)) + (((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused % 182) / 7) * 36)) + (r_kh_outer * 12)) + (r_kh_inner * 6)) + r_kw_outer))+(936*0), ((((((((co_outer_outer_inner * 185328) + (co_inner * 92664)) + (ci_outer_inner * 8424)) + (((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused % 182) / 7) * 36)) + (r_kh_outer * 12)) + (r_kh_inner * 6)) + r_kw_outer))+(936*1), ((((((((co_outer_outer_inner * 185328) + (co_inner * 92664)) + (ci_outer_inner * 8424)) + (((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused % 182) / 7) * 36)) + (r_kh_outer * 12)) + (r_kh_inner * 6)) + r_kw_outer))+(936*2), ((((((((co_outer_outer_inner * 185328) + (co_inner * 92664)) + (ci_outer_inner * 8424)) + (((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused % 182) / 7) * 36)) + (r_kh_outer * 12)) + (r_kh_inner * 6)) + r_kw_outer))+(936*3), ((((((((co_outer_outer_inner * 185328) + (co_inner * 92664)) + (ci_outer_inner * 8424)) + (((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused % 182) / 7) * 36)) + (r_kh_outer * 12)) + (r_kh_inner * 6)) + r_kw_outer))+(936*4), ((((((((co_outer_outer_inner * 185328) + (co_inner * 92664)) + (ci_outer_inner * 8424)) + (((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused % 182) / 7) * 36)) + (r_kh_outer * 12)) + (r_kh_inner * 6)) + r_kw_outer))+(936*5), ((((((((co_outer_outer_inner * 185328) + (co_inner * 92664)) + (ci_outer_inner * 8424)) + (((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused % 182) / 7) * 36)) + (r_kh_outer * 12)) + (r_kh_inner * 6)) + r_kw_outer))+(936*6), ((((((((co_outer_outer_inner * 185328) + (co_inner * 92664)) + (ci_outer_inner * 8424)) + (((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused % 182) / 7) * 36)) + (r_kh_outer * 12)) + (r_kh_inner * 6)) + r_kw_outer))+(936*7), ((((((((co_outer_outer_inner * 185328) + (co_inner * 92664)) + (ci_outer_inner * 8424)) + (((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused % 182) / 7) * 36)) + (r_kh_outer * 12)) + (r_kh_inner * 6)) + r_kw_outer))+(936*8));\n                *(float9*)(((float*)transform_weight_1) + cse_var_1) = ((float9(((float*)transform_weight_1)[v_.s0],((float*)transform_weight_1)[v_.s1],((float*)transform_weight_1)[v_.s2],((float*)transform_weight_1)[v_.s3],((float*)transform_weight_1)[v_.s4],((float*)transform_weight_1)[v_.s5],((float*)transform_weight_1)[v_.s6],((float*)transform_weight_1)[v_.s7],((float*)transform_weight_1)[v_.s8])) + (((float9(((float*)data_1)[v__1.s0],((float*)data_1)[v__1.s1],((float*)data_1)[v__1.s2],((float*)data_1)[v__1.s3],((float*)data_1)[v__1.s4],((float*)data_1)[v__1.s5],((float*)data_1)[v__1.s6],((float*)data_1)[v__1.s7],((float*)data_1)[v__1.s8])) * ((float9)(((cse_var_8 && cse_var_2) ? 1.000000e+00f : ((cse_var_8 && cse_var_3) ? 0.000000e+00f : ((cse_var_8 && cse_var_4) ? 0.000000e+00f : ((cse_var_8 && cse_var_5) ? 0.000000e+00f : ((cse_var_8 && cse_var_6) ? 0.000000e+00f : ((cse_var_8 && cse_var_7) ? 0.000000e+00f : ((cse_var_9 && cse_var_2) ? 5.555556e-02f : ((cse_var_9 && cse_var_3) ? -1.111111e-01f : ((cse_var_9 && cse_var_4) ? 2.222222e-01f : ((cse_var_9 && cse_var_5) ? -4.444444e-01f : ((cse_var_9 && cse_var_6) ? 8.888889e-01f : ((cse_var_9 && cse_var_7) ? -1.777778e+00f : ((cse_var_10 && cse_var_2) ? 1.422222e+00f : ((cse_var_10 && cse_var_3) ? -7.111111e-01f : ((cse_var_10 && cse_var_4) ? 3.555556e-01f : ((cse_var_10 && cse_var_5) ? -1.777778e-01f : ((cse_var_10 && cse_var_6) ? 8.888889e-02f : ((cse_var_10 && cse_var_7) ? -4.444445e-02f : ((cse_var_11 && cse_var_2) ? -3.333334e-02f : ((cse_var_11 && cse_var_3) ? -6.666667e-02f : ((cse_var_11 && cse_var_4) ? -1.333333e-01f : ((cse_var_11 && cse_var_5) ? -2.666667e-01f : ((cse_var_11 && cse_var_6) ? -5.333334e-01f : ((cse_var_11 && cse_var_7) ? -1.066667e+00f : ((cse_var_12 && cse_var_2) ? 2.222222e-01f : ((cse_var_12 && cse_var_3) ? 2.222222e-01f : ((cse_var_12 && cse_var_4) ? 2.222222e-01f : ((cse_var_12 && cse_var_5) ? 2.222222e-01f : ((cse_var_12 && cse_var_6) ? 2.222222e-01f : ((cse_var_12 && cse_var_7) ? 2.222222e-01f : ((cse_var_13 && cse_var_2) ? -6.666667e-01f : ((cse_var_13 && cse_var_3) ? 6.666667e-01f : ((cse_var_13 && cse_var_4) ? -6.666667e-01f : ((cse_var_13 && cse_var_5) ? 6.666667e-01f : ((cse_var_13 && cse_var_6) ? -6.666667e-01f : ((cse_var_13 && cse_var_7) ? 6.666667e-01f : ((cse_var_14 && cse_var_2) ? 0.000000e+00f : ((cse_var_14 && cse_var_3) ? 0.000000e+00f : ((cse_var_14 && cse_var_4) ? 0.000000e+00f : ((cse_var_14 && cse_var_5) ? 0.000000e+00f : ((cse_var_14 && cse_var_6) ? 0.000000e+00f : ((cse_var_14 && cse_var_7) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_8 && cse_var_2) ? 1.000000e+00f : ((cse_var_8 && cse_var_3) ? 0.000000e+00f : ((cse_var_8 && cse_var_4) ? 0.000000e+00f : ((cse_var_8 && cse_var_5) ? 0.000000e+00f : ((cse_var_8 && cse_var_6) ? 0.000000e+00f : ((cse_var_8 && cse_var_7) ? 0.000000e+00f : ((cse_var_9 && cse_var_2) ? 5.555556e-02f : ((cse_var_9 && cse_var_3) ? -1.111111e-01f : ((cse_var_9 && cse_var_4) ? 2.222222e-01f : ((cse_var_9 && cse_var_5) ? -4.444444e-01f : ((cse_var_9 && cse_var_6) ? 8.888889e-01f : ((cse_var_9 && cse_var_7) ? -1.777778e+00f : ((cse_var_10 && cse_var_2) ? 1.422222e+00f : ((cse_var_10 && cse_var_3) ? -7.111111e-01f : ((cse_var_10 && cse_var_4) ? 3.555556e-01f : ((cse_var_10 && cse_var_5) ? -1.777778e-01f : ((cse_var_10 && cse_var_6) ? 8.888889e-02f : ((cse_var_10 && cse_var_7) ? -4.444445e-02f : ((cse_var_11 && cse_var_2) ? -3.333334e-02f : ((cse_var_11 && cse_var_3) ? -6.666667e-02f : ((cse_var_11 && cse_var_4) ? -1.333333e-01f : ((cse_var_11 && cse_var_5) ? -2.666667e-01f : ((cse_var_11 && cse_var_6) ? -5.333334e-01f : ((cse_var_11 && cse_var_7) ? -1.066667e+00f : ((cse_var_12 && cse_var_2) ? 2.222222e-01f : ((cse_var_12 && cse_var_3) ? 2.222222e-01f : ((cse_var_12 && cse_var_4) ? 2.222222e-01f : ((cse_var_12 && cse_var_5) ? 2.222222e-01f : ((cse_var_12 && cse_var_6) ? 2.222222e-01f : ((cse_var_12 && cse_var_7) ? 2.222222e-01f : ((cse_var_13 && cse_var_2) ? -6.666667e-01f : ((cse_var_13 && cse_var_3) ? 6.666667e-01f : ((cse_var_13 && cse_var_4) ? -6.666667e-01f : ((cse_var_13 && cse_var_5) ? 6.666667e-01f : ((cse_var_13 && cse_var_6) ? -6.666667e-01f : ((cse_var_13 && cse_var_7) ? 6.666667e-01f : ((cse_var_14 && cse_var_2) ? 0.000000e+00f : ((cse_var_14 && cse_var_3) ? 0.000000e+00f : ((cse_var_14 && cse_var_4) ? 0.000000e+00f : ((cse_var_14 && cse_var_5) ? 0.000000e+00f : ((cse_var_14 && cse_var_6) ? 0.000000e+00f : ((cse_var_14 && cse_var_7) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_8 && cse_var_2) ? 1.000000e+00f : ((cse_var_8 && cse_var_3) ? 0.000000e+00f : ((cse_var_8 && cse_var_4) ? 0.000000e+00f : ((cse_var_8 && cse_var_5) ? 0.000000e+00f : ((cse_var_8 && cse_var_6) ? 0.000000e+00f : ((cse_var_8 && cse_var_7) ? 0.000000e+00f : ((cse_var_9 && cse_var_2) ? 5.555556e-02f : ((cse_var_9 && cse_var_3) ? -1.111111e-01f : ((cse_var_9 && cse_var_4) ? 2.222222e-01f : ((cse_var_9 && cse_var_5) ? -4.444444e-01f : ((cse_var_9 && cse_var_6) ? 8.888889e-01f : ((cse_var_9 && cse_var_7) ? -1.777778e+00f : ((cse_var_10 && cse_var_2) ? 1.422222e+00f : ((cse_var_10 && cse_var_3) ? -7.111111e-01f : ((cse_var_10 && cse_var_4) ? 3.555556e-01f : ((cse_var_10 && cse_var_5) ? -1.777778e-01f : ((cse_var_10 && cse_var_6) ? 8.888889e-02f : ((cse_var_10 && cse_var_7) ? -4.444445e-02f : ((cse_var_11 && cse_var_2) ? -3.333334e-02f : ((cse_var_11 && cse_var_3) ? -6.666667e-02f : ((cse_var_11 && cse_var_4) ? -1.333333e-01f : ((cse_var_11 && cse_var_5) ? -2.666667e-01f : ((cse_var_11 && cse_var_6) ? -5.333334e-01f : ((cse_var_11 && cse_var_7) ? -1.066667e+00f : ((cse_var_12 && cse_var_2) ? 2.222222e-01f : ((cse_var_12 && cse_var_3) ? 2.222222e-01f : ((cse_var_12 && cse_var_4) ? 2.222222e-01f : ((cse_var_12 && cse_var_5) ? 2.222222e-01f : ((cse_var_12 && cse_var_6) ? 2.222222e-01f : ((cse_var_12 && cse_var_7) ? 2.222222e-01f : ((cse_var_13 && cse_var_2) ? -6.666667e-01f : ((cse_var_13 && cse_var_3) ? 6.666667e-01f : ((cse_var_13 && cse_var_4) ? -6.666667e-01f : ((cse_var_13 && cse_var_5) ? 6.666667e-01f : ((cse_var_13 && cse_var_6) ? -6.666667e-01f : ((cse_var_13 && cse_var_7) ? 6.666667e-01f : ((cse_var_14 && cse_var_2) ? 0.000000e+00f : ((cse_var_14 && cse_var_3) ? 0.000000e+00f : ((cse_var_14 && cse_var_4) ? 0.000000e+00f : ((cse_var_14 && cse_var_5) ? 0.000000e+00f : ((cse_var_14 && cse_var_6) ? 0.000000e+00f : ((cse_var_14 && cse_var_7) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_8 && cse_var_2) ? 1.000000e+00f : ((cse_var_8 && cse_var_3) ? 0.000000e+00f : ((cse_var_8 && cse_var_4) ? 0.000000e+00f : ((cse_var_8 && cse_var_5) ? 0.000000e+00f : ((cse_var_8 && cse_var_6) ? 0.000000e+00f : ((cse_var_8 && cse_var_7) ? 0.000000e+00f : ((cse_var_9 && cse_var_2) ? 5.555556e-02f : ((cse_var_9 && cse_var_3) ? -1.111111e-01f : ((cse_var_9 && cse_var_4) ? 2.222222e-01f : ((cse_var_9 && cse_var_5) ? -4.444444e-01f : ((cse_var_9 && cse_var_6) ? 8.888889e-01f : ((cse_var_9 && cse_var_7) ? -1.777778e+00f : ((cse_var_10 && cse_var_2) ? 1.422222e+00f : ((cse_var_10 && cse_var_3) ? -7.111111e-01f : ((cse_var_10 && cse_var_4) ? 3.555556e-01f : ((cse_var_10 && cse_var_5) ? -1.777778e-01f : ((cse_var_10 && cse_var_6) ? 8.888889e-02f : ((cse_var_10 && cse_var_7) ? -4.444445e-02f : ((cse_var_11 && cse_var_2) ? -3.333334e-02f : ((cse_var_11 && cse_var_3) ? -6.666667e-02f : ((cse_var_11 && cse_var_4) ? -1.333333e-01f : ((cse_var_11 && cse_var_5) ? -2.666667e-01f : ((cse_var_11 && cse_var_6) ? -5.333334e-01f : ((cse_var_11 && cse_var_7) ? -1.066667e+00f : ((cse_var_12 && cse_var_2) ? 2.222222e-01f : ((cse_var_12 && cse_var_3) ? 2.222222e-01f : ((cse_var_12 && cse_var_4) ? 2.222222e-01f : ((cse_var_12 && cse_var_5) ? 2.222222e-01f : ((cse_var_12 && cse_var_6) ? 2.222222e-01f : ((cse_var_12 && cse_var_7) ? 2.222222e-01f : ((cse_var_13 && cse_var_2) ? -6.666667e-01f : ((cse_var_13 && cse_var_3) ? 6.666667e-01f : ((cse_var_13 && cse_var_4) ? -6.666667e-01f : ((cse_var_13 && cse_var_5) ? 6.666667e-01f : ((cse_var_13 && cse_var_6) ? -6.666667e-01f : ((cse_var_13 && cse_var_7) ? 6.666667e-01f : ((cse_var_14 && cse_var_2) ? 0.000000e+00f : ((cse_var_14 && cse_var_3) ? 0.000000e+00f : ((cse_var_14 && cse_var_4) ? 0.000000e+00f : ((cse_var_14 && cse_var_5) ? 0.000000e+00f : ((cse_var_14 && cse_var_6) ? 0.000000e+00f : ((cse_var_14 && cse_var_7) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_8 && cse_var_2) ? 1.000000e+00f : ((cse_var_8 && cse_var_3) ? 0.000000e+00f : ((cse_var_8 && cse_var_4) ? 0.000000e+00f : ((cse_var_8 && cse_var_5) ? 0.000000e+00f : ((cse_var_8 && cse_var_6) ? 0.000000e+00f : ((cse_var_8 && cse_var_7) ? 0.000000e+00f : ((cse_var_9 && cse_var_2) ? 5.555556e-02f : ((cse_var_9 && cse_var_3) ? -1.111111e-01f : ((cse_var_9 && cse_var_4) ? 2.222222e-01f : ((cse_var_9 && cse_var_5) ? -4.444444e-01f : ((cse_var_9 && cse_var_6) ? 8.888889e-01f : ((cse_var_9 && cse_var_7) ? -1.777778e+00f : ((cse_var_10 && cse_var_2) ? 1.422222e+00f : ((cse_var_10 && cse_var_3) ? -7.111111e-01f : ((cse_var_10 && cse_var_4) ? 3.555556e-01f : ((cse_var_10 && cse_var_5) ? -1.777778e-01f : ((cse_var_10 && cse_var_6) ? 8.888889e-02f : ((cse_var_10 && cse_var_7) ? -4.444445e-02f : ((cse_var_11 && cse_var_2) ? -3.333334e-02f : ((cse_var_11 && cse_var_3) ? -6.666667e-02f : ((cse_var_11 && cse_var_4) ? -1.333333e-01f : ((cse_var_11 && cse_var_5) ? -2.666667e-01f : ((cse_var_11 && cse_var_6) ? -5.333334e-01f : ((cse_var_11 && cse_var_7) ? -1.066667e+00f : ((cse_var_12 && cse_var_2) ? 2.222222e-01f : ((cse_var_12 && cse_var_3) ? 2.222222e-01f : ((cse_var_12 && cse_var_4) ? 2.222222e-01f : ((cse_var_12 && cse_var_5) ? 2.222222e-01f : ((cse_var_12 && cse_var_6) ? 2.222222e-01f : ((cse_var_12 && cse_var_7) ? 2.222222e-01f : ((cse_var_13 && cse_var_2) ? -6.666667e-01f : ((cse_var_13 && cse_var_3) ? 6.666667e-01f : ((cse_var_13 && cse_var_4) ? -6.666667e-01f : ((cse_var_13 && cse_var_5) ? 6.666667e-01f : ((cse_var_13 && cse_var_6) ? -6.666667e-01f : ((cse_var_13 && cse_var_7) ? 6.666667e-01f : ((cse_var_14 && cse_var_2) ? 0.000000e+00f : ((cse_var_14 && cse_var_3) ? 0.000000e+00f : ((cse_var_14 && cse_var_4) ? 0.000000e+00f : ((cse_var_14 && cse_var_5) ? 0.000000e+00f : ((cse_var_14 && cse_var_6) ? 0.000000e+00f : ((cse_var_14 && cse_var_7) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_8 && cse_var_2) ? 1.000000e+00f : ((cse_var_8 && cse_var_3) ? 0.000000e+00f : ((cse_var_8 && cse_var_4) ? 0.000000e+00f : ((cse_var_8 && cse_var_5) ? 0.000000e+00f : ((cse_var_8 && cse_var_6) ? 0.000000e+00f : ((cse_var_8 && cse_var_7) ? 0.000000e+00f : ((cse_var_9 && cse_var_2) ? 5.555556e-02f : ((cse_var_9 && cse_var_3) ? -1.111111e-01f : ((cse_var_9 && cse_var_4) ? 2.222222e-01f : ((cse_var_9 && cse_var_5) ? -4.444444e-01f : ((cse_var_9 && cse_var_6) ? 8.888889e-01f : ((cse_var_9 && cse_var_7) ? -1.777778e+00f : ((cse_var_10 && cse_var_2) ? 1.422222e+00f : ((cse_var_10 && cse_var_3) ? -7.111111e-01f : ((cse_var_10 && cse_var_4) ? 3.555556e-01f : ((cse_var_10 && cse_var_5) ? -1.777778e-01f : ((cse_var_10 && cse_var_6) ? 8.888889e-02f : ((cse_var_10 && cse_var_7) ? -4.444445e-02f : ((cse_var_11 && cse_var_2) ? -3.333334e-02f : ((cse_var_11 && cse_var_3) ? -6.666667e-02f : ((cse_var_11 && cse_var_4) ? -1.333333e-01f : ((cse_var_11 && cse_var_5) ? -2.666667e-01f : ((cse_var_11 && cse_var_6) ? -5.333334e-01f : ((cse_var_11 && cse_var_7) ? -1.066667e+00f : ((cse_var_12 && cse_var_2) ? 2.222222e-01f : ((cse_var_12 && cse_var_3) ? 2.222222e-01f : ((cse_var_12 && cse_var_4) ? 2.222222e-01f : ((cse_var_12 && cse_var_5) ? 2.222222e-01f : ((cse_var_12 && cse_var_6) ? 2.222222e-01f : ((cse_var_12 && cse_var_7) ? 2.222222e-01f : ((cse_var_13 && cse_var_2) ? -6.666667e-01f : ((cse_var_13 && cse_var_3) ? 6.666667e-01f : ((cse_var_13 && cse_var_4) ? -6.666667e-01f : ((cse_var_13 && cse_var_5) ? 6.666667e-01f : ((cse_var_13 && cse_var_6) ? -6.666667e-01f : ((cse_var_13 && cse_var_7) ? 6.666667e-01f : ((cse_var_14 && cse_var_2) ? 0.000000e+00f : ((cse_var_14 && cse_var_3) ? 0.000000e+00f : ((cse_var_14 && cse_var_4) ? 0.000000e+00f : ((cse_var_14 && cse_var_5) ? 0.000000e+00f : ((cse_var_14 && cse_var_6) ? 0.000000e+00f : ((cse_var_14 && cse_var_7) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_8 && cse_var_2) ? 1.000000e+00f : ((cse_var_8 && cse_var_3) ? 0.000000e+00f : ((cse_var_8 && cse_var_4) ? 0.000000e+00f : ((cse_var_8 && cse_var_5) ? 0.000000e+00f : ((cse_var_8 && cse_var_6) ? 0.000000e+00f : ((cse_var_8 && cse_var_7) ? 0.000000e+00f : ((cse_var_9 && cse_var_2) ? 5.555556e-02f : ((cse_var_9 && cse_var_3) ? -1.111111e-01f : ((cse_var_9 && cse_var_4) ? 2.222222e-01f : ((cse_var_9 && cse_var_5) ? -4.444444e-01f : ((cse_var_9 && cse_var_6) ? 8.888889e-01f : ((cse_var_9 && cse_var_7) ? -1.777778e+00f : ((cse_var_10 && cse_var_2) ? 1.422222e+00f : ((cse_var_10 && cse_var_3) ? -7.111111e-01f : ((cse_var_10 && cse_var_4) ? 3.555556e-01f : ((cse_var_10 && cse_var_5) ? -1.777778e-01f : ((cse_var_10 && cse_var_6) ? 8.888889e-02f : ((cse_var_10 && cse_var_7) ? -4.444445e-02f : ((cse_var_11 && cse_var_2) ? -3.333334e-02f : ((cse_var_11 && cse_var_3) ? -6.666667e-02f : ((cse_var_11 && cse_var_4) ? -1.333333e-01f : ((cse_var_11 && cse_var_5) ? -2.666667e-01f : ((cse_var_11 && cse_var_6) ? -5.333334e-01f : ((cse_var_11 && cse_var_7) ? -1.066667e+00f : ((cse_var_12 && cse_var_2) ? 2.222222e-01f : ((cse_var_12 && cse_var_3) ? 2.222222e-01f : ((cse_var_12 && cse_var_4) ? 2.222222e-01f : ((cse_var_12 && cse_var_5) ? 2.222222e-01f : ((cse_var_12 && cse_var_6) ? 2.222222e-01f : ((cse_var_12 && cse_var_7) ? 2.222222e-01f : ((cse_var_13 && cse_var_2) ? -6.666667e-01f : ((cse_var_13 && cse_var_3) ? 6.666667e-01f : ((cse_var_13 && cse_var_4) ? -6.666667e-01f : ((cse_var_13 && cse_var_5) ? 6.666667e-01f : ((cse_var_13 && cse_var_6) ? -6.666667e-01f : ((cse_var_13 && cse_var_7) ? 6.666667e-01f : ((cse_var_14 && cse_var_2) ? 0.000000e+00f : ((cse_var_14 && cse_var_3) ? 0.000000e+00f : ((cse_var_14 && cse_var_4) ? 0.000000e+00f : ((cse_var_14 && cse_var_5) ? 0.000000e+00f : ((cse_var_14 && cse_var_6) ? 0.000000e+00f : ((cse_var_14 && cse_var_7) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_8 && cse_var_2) ? 1.000000e+00f : ((cse_var_8 && cse_var_3) ? 0.000000e+00f : ((cse_var_8 && cse_var_4) ? 0.000000e+00f : ((cse_var_8 && cse_var_5) ? 0.000000e+00f : ((cse_var_8 && cse_var_6) ? 0.000000e+00f : ((cse_var_8 && cse_var_7) ? 0.000000e+00f : ((cse_var_9 && cse_var_2) ? 5.555556e-02f : ((cse_var_9 && cse_var_3) ? -1.111111e-01f : ((cse_var_9 && cse_var_4) ? 2.222222e-01f : ((cse_var_9 && cse_var_5) ? -4.444444e-01f : ((cse_var_9 && cse_var_6) ? 8.888889e-01f : ((cse_var_9 && cse_var_7) ? -1.777778e+00f : ((cse_var_10 && cse_var_2) ? 1.422222e+00f : ((cse_var_10 && cse_var_3) ? -7.111111e-01f : ((cse_var_10 && cse_var_4) ? 3.555556e-01f : ((cse_var_10 && cse_var_5) ? -1.777778e-01f : ((cse_var_10 && cse_var_6) ? 8.888889e-02f : ((cse_var_10 && cse_var_7) ? -4.444445e-02f : ((cse_var_11 && cse_var_2) ? -3.333334e-02f : ((cse_var_11 && cse_var_3) ? -6.666667e-02f : ((cse_var_11 && cse_var_4) ? -1.333333e-01f : ((cse_var_11 && cse_var_5) ? -2.666667e-01f : ((cse_var_11 && cse_var_6) ? -5.333334e-01f : ((cse_var_11 && cse_var_7) ? -1.066667e+00f : ((cse_var_12 && cse_var_2) ? 2.222222e-01f : ((cse_var_12 && cse_var_3) ? 2.222222e-01f : ((cse_var_12 && cse_var_4) ? 2.222222e-01f : ((cse_var_12 && cse_var_5) ? 2.222222e-01f : ((cse_var_12 && cse_var_6) ? 2.222222e-01f : ((cse_var_12 && cse_var_7) ? 2.222222e-01f : ((cse_var_13 && cse_var_2) ? -6.666667e-01f : ((cse_var_13 && cse_var_3) ? 6.666667e-01f : ((cse_var_13 && cse_var_4) ? -6.666667e-01f : ((cse_var_13 && cse_var_5) ? 6.666667e-01f : ((cse_var_13 && cse_var_6) ? -6.666667e-01f : ((cse_var_13 && cse_var_7) ? 6.666667e-01f : ((cse_var_14 && cse_var_2) ? 0.000000e+00f : ((cse_var_14 && cse_var_3) ? 0.000000e+00f : ((cse_var_14 && cse_var_4) ? 0.000000e+00f : ((cse_var_14 && cse_var_5) ? 0.000000e+00f : ((cse_var_14 && cse_var_6) ? 0.000000e+00f : ((cse_var_14 && cse_var_7) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_8 && cse_var_2) ? 1.000000e+00f : ((cse_var_8 && cse_var_3) ? 0.000000e+00f : ((cse_var_8 && cse_var_4) ? 0.000000e+00f : ((cse_var_8 && cse_var_5) ? 0.000000e+00f : ((cse_var_8 && cse_var_6) ? 0.000000e+00f : ((cse_var_8 && cse_var_7) ? 0.000000e+00f : ((cse_var_9 && cse_var_2) ? 5.555556e-02f : ((cse_var_9 && cse_var_3) ? -1.111111e-01f : ((cse_var_9 && cse_var_4) ? 2.222222e-01f : ((cse_var_9 && cse_var_5) ? -4.444444e-01f : ((cse_var_9 && cse_var_6) ? 8.888889e-01f : ((cse_var_9 && cse_var_7) ? -1.777778e+00f : ((cse_var_10 && cse_var_2) ? 1.422222e+00f : ((cse_var_10 && cse_var_3) ? -7.111111e-01f : ((cse_var_10 && cse_var_4) ? 3.555556e-01f : ((cse_var_10 && cse_var_5) ? -1.777778e-01f : ((cse_var_10 && cse_var_6) ? 8.888889e-02f : ((cse_var_10 && cse_var_7) ? -4.444445e-02f : ((cse_var_11 && cse_var_2) ? -3.333334e-02f : ((cse_var_11 && cse_var_3) ? -6.666667e-02f : ((cse_var_11 && cse_var_4) ? -1.333333e-01f : ((cse_var_11 && cse_var_5) ? -2.666667e-01f : ((cse_var_11 && cse_var_6) ? -5.333334e-01f : ((cse_var_11 && cse_var_7) ? -1.066667e+00f : ((cse_var_12 && cse_var_2) ? 2.222222e-01f : ((cse_var_12 && cse_var_3) ? 2.222222e-01f : ((cse_var_12 && cse_var_4) ? 2.222222e-01f : ((cse_var_12 && cse_var_5) ? 2.222222e-01f : ((cse_var_12 && cse_var_6) ? 2.222222e-01f : ((cse_var_12 && cse_var_7) ? 2.222222e-01f : ((cse_var_13 && cse_var_2) ? -6.666667e-01f : ((cse_var_13 && cse_var_3) ? 6.666667e-01f : ((cse_var_13 && cse_var_4) ? -6.666667e-01f : ((cse_var_13 && cse_var_5) ? 6.666667e-01f : ((cse_var_13 && cse_var_6) ? -6.666667e-01f : ((cse_var_13 && cse_var_7) ? 6.666667e-01f : ((cse_var_14 && cse_var_2) ? 0.000000e+00f : ((cse_var_14 && cse_var_3) ? 0.000000e+00f : ((cse_var_14 && cse_var_4) ? 0.000000e+00f : ((cse_var_14 && cse_var_5) ? 0.000000e+00f : ((cse_var_14 && cse_var_6) ? 0.000000e+00f : ((cse_var_14 && cse_var_7) ? 2.000000e+00f : 0.000000e+00f))))))))))))))))))))))))))))))))))))))))))))) * ((float9)(((cse_var_15 && cse_var_22) ? 1.000000e+00f : ((cse_var_15 && cse_var_23) ? 0.000000e+00f : ((cse_var_15 && cse_var_24) ? 0.000000e+00f : ((cse_var_15 && cse_var_25) ? 0.000000e+00f : ((cse_var_15 && cse_var_26) ? 0.000000e+00f : ((cse_var_15 && cse_var_27) ? 0.000000e+00f : ((cse_var_16 && cse_var_22) ? 5.555556e-02f : ((cse_var_16 && cse_var_23) ? -1.111111e-01f : ((cse_var_16 && cse_var_24) ? 2.222222e-01f : ((cse_var_16 && cse_var_25) ? -4.444444e-01f : ((cse_var_16 && cse_var_26) ? 8.888889e-01f : ((cse_var_16 && cse_var_27) ? -1.777778e+00f : ((cse_var_17 && cse_var_22) ? 1.422222e+00f : ((cse_var_17 && cse_var_23) ? -7.111111e-01f : ((cse_var_17 && cse_var_24) ? 3.555556e-01f : ((cse_var_17 && cse_var_25) ? -1.777778e-01f : ((cse_var_17 && cse_var_26) ? 8.888889e-02f : ((cse_var_17 && cse_var_27) ? -4.444445e-02f : ((cse_var_18 && cse_var_22) ? -3.333334e-02f : ((cse_var_18 && cse_var_23) ? -6.666667e-02f : ((cse_var_18 && cse_var_24) ? -1.333333e-01f : ((cse_var_18 && cse_var_25) ? -2.666667e-01f : ((cse_var_18 && cse_var_26) ? -5.333334e-01f : ((cse_var_18 && cse_var_27) ? -1.066667e+00f : ((cse_var_19 && cse_var_22) ? 2.222222e-01f : ((cse_var_19 && cse_var_23) ? 2.222222e-01f : ((cse_var_19 && cse_var_24) ? 2.222222e-01f : ((cse_var_19 && cse_var_25) ? 2.222222e-01f : ((cse_var_19 && cse_var_26) ? 2.222222e-01f : ((cse_var_19 && cse_var_27) ? 2.222222e-01f : ((cse_var_20 && cse_var_22) ? -6.666667e-01f : ((cse_var_20 && cse_var_23) ? 6.666667e-01f : ((cse_var_20 && cse_var_24) ? -6.666667e-01f : ((cse_var_20 && cse_var_25) ? 6.666667e-01f : ((cse_var_20 && cse_var_26) ? -6.666667e-01f : ((cse_var_20 && cse_var_27) ? 6.666667e-01f : ((cse_var_21 && cse_var_22) ? 0.000000e+00f : ((cse_var_21 && cse_var_23) ? 0.000000e+00f : ((cse_var_21 && cse_var_24) ? 0.000000e+00f : ((cse_var_21 && cse_var_25) ? 0.000000e+00f : ((cse_var_21 && cse_var_26) ? 0.000000e+00f : ((cse_var_21 && cse_var_27) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_15 && cse_var_22) ? 1.000000e+00f : ((cse_var_15 && cse_var_23) ? 0.000000e+00f : ((cse_var_15 && cse_var_24) ? 0.000000e+00f : ((cse_var_15 && cse_var_25) ? 0.000000e+00f : ((cse_var_15 && cse_var_26) ? 0.000000e+00f : ((cse_var_15 && cse_var_27) ? 0.000000e+00f : ((cse_var_16 && cse_var_22) ? 5.555556e-02f : ((cse_var_16 && cse_var_23) ? -1.111111e-01f : ((cse_var_16 && cse_var_24) ? 2.222222e-01f : ((cse_var_16 && cse_var_25) ? -4.444444e-01f : ((cse_var_16 && cse_var_26) ? 8.888889e-01f : ((cse_var_16 && cse_var_27) ? -1.777778e+00f : ((cse_var_17 && cse_var_22) ? 1.422222e+00f : ((cse_var_17 && cse_var_23) ? -7.111111e-01f : ((cse_var_17 && cse_var_24) ? 3.555556e-01f : ((cse_var_17 && cse_var_25) ? -1.777778e-01f : ((cse_var_17 && cse_var_26) ? 8.888889e-02f : ((cse_var_17 && cse_var_27) ? -4.444445e-02f : ((cse_var_18 && cse_var_22) ? -3.333334e-02f : ((cse_var_18 && cse_var_23) ? -6.666667e-02f : ((cse_var_18 && cse_var_24) ? -1.333333e-01f : ((cse_var_18 && cse_var_25) ? -2.666667e-01f : ((cse_var_18 && cse_var_26) ? -5.333334e-01f : ((cse_var_18 && cse_var_27) ? -1.066667e+00f : ((cse_var_19 && cse_var_22) ? 2.222222e-01f : ((cse_var_19 && cse_var_23) ? 2.222222e-01f : ((cse_var_19 && cse_var_24) ? 2.222222e-01f : ((cse_var_19 && cse_var_25) ? 2.222222e-01f : ((cse_var_19 && cse_var_26) ? 2.222222e-01f : ((cse_var_19 && cse_var_27) ? 2.222222e-01f : ((cse_var_20 && cse_var_22) ? -6.666667e-01f : ((cse_var_20 && cse_var_23) ? 6.666667e-01f : ((cse_var_20 && cse_var_24) ? -6.666667e-01f : ((cse_var_20 && cse_var_25) ? 6.666667e-01f : ((cse_var_20 && cse_var_26) ? -6.666667e-01f : ((cse_var_20 && cse_var_27) ? 6.666667e-01f : ((cse_var_21 && cse_var_22) ? 0.000000e+00f : ((cse_var_21 && cse_var_23) ? 0.000000e+00f : ((cse_var_21 && cse_var_24) ? 0.000000e+00f : ((cse_var_21 && cse_var_25) ? 0.000000e+00f : ((cse_var_21 && cse_var_26) ? 0.000000e+00f : ((cse_var_21 && cse_var_27) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_15 && cse_var_22) ? 1.000000e+00f : ((cse_var_15 && cse_var_23) ? 0.000000e+00f : ((cse_var_15 && cse_var_24) ? 0.000000e+00f : ((cse_var_15 && cse_var_25) ? 0.000000e+00f : ((cse_var_15 && cse_var_26) ? 0.000000e+00f : ((cse_var_15 && cse_var_27) ? 0.000000e+00f : ((cse_var_16 && cse_var_22) ? 5.555556e-02f : ((cse_var_16 && cse_var_23) ? -1.111111e-01f : ((cse_var_16 && cse_var_24) ? 2.222222e-01f : ((cse_var_16 && cse_var_25) ? -4.444444e-01f : ((cse_var_16 && cse_var_26) ? 8.888889e-01f : ((cse_var_16 && cse_var_27) ? -1.777778e+00f : ((cse_var_17 && cse_var_22) ? 1.422222e+00f : ((cse_var_17 && cse_var_23) ? -7.111111e-01f : ((cse_var_17 && cse_var_24) ? 3.555556e-01f : ((cse_var_17 && cse_var_25) ? -1.777778e-01f : ((cse_var_17 && cse_var_26) ? 8.888889e-02f : ((cse_var_17 && cse_var_27) ? -4.444445e-02f : ((cse_var_18 && cse_var_22) ? -3.333334e-02f : ((cse_var_18 && cse_var_23) ? -6.666667e-02f : ((cse_var_18 && cse_var_24) ? -1.333333e-01f : ((cse_var_18 && cse_var_25) ? -2.666667e-01f : ((cse_var_18 && cse_var_26) ? -5.333334e-01f : ((cse_var_18 && cse_var_27) ? -1.066667e+00f : ((cse_var_19 && cse_var_22) ? 2.222222e-01f : ((cse_var_19 && cse_var_23) ? 2.222222e-01f : ((cse_var_19 && cse_var_24) ? 2.222222e-01f : ((cse_var_19 && cse_var_25) ? 2.222222e-01f : ((cse_var_19 && cse_var_26) ? 2.222222e-01f : ((cse_var_19 && cse_var_27) ? 2.222222e-01f : ((cse_var_20 && cse_var_22) ? -6.666667e-01f : ((cse_var_20 && cse_var_23) ? 6.666667e-01f : ((cse_var_20 && cse_var_24) ? -6.666667e-01f : ((cse_var_20 && cse_var_25) ? 6.666667e-01f : ((cse_var_20 && cse_var_26) ? -6.666667e-01f : ((cse_var_20 && cse_var_27) ? 6.666667e-01f : ((cse_var_21 && cse_var_22) ? 0.000000e+00f : ((cse_var_21 && cse_var_23) ? 0.000000e+00f : ((cse_var_21 && cse_var_24) ? 0.000000e+00f : ((cse_var_21 && cse_var_25) ? 0.000000e+00f : ((cse_var_21 && cse_var_26) ? 0.000000e+00f : ((cse_var_21 && cse_var_27) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_15 && cse_var_22) ? 1.000000e+00f : ((cse_var_15 && cse_var_23) ? 0.000000e+00f : ((cse_var_15 && cse_var_24) ? 0.000000e+00f : ((cse_var_15 && cse_var_25) ? 0.000000e+00f : ((cse_var_15 && cse_var_26) ? 0.000000e+00f : ((cse_var_15 && cse_var_27) ? 0.000000e+00f : ((cse_var_16 && cse_var_22) ? 5.555556e-02f : ((cse_var_16 && cse_var_23) ? -1.111111e-01f : ((cse_var_16 && cse_var_24) ? 2.222222e-01f : ((cse_var_16 && cse_var_25) ? -4.444444e-01f : ((cse_var_16 && cse_var_26) ? 8.888889e-01f : ((cse_var_16 && cse_var_27) ? -1.777778e+00f : ((cse_var_17 && cse_var_22) ? 1.422222e+00f : ((cse_var_17 && cse_var_23) ? -7.111111e-01f : ((cse_var_17 && cse_var_24) ? 3.555556e-01f : ((cse_var_17 && cse_var_25) ? -1.777778e-01f : ((cse_var_17 && cse_var_26) ? 8.888889e-02f : ((cse_var_17 && cse_var_27) ? -4.444445e-02f : ((cse_var_18 && cse_var_22) ? -3.333334e-02f : ((cse_var_18 && cse_var_23) ? -6.666667e-02f : ((cse_var_18 && cse_var_24) ? -1.333333e-01f : ((cse_var_18 && cse_var_25) ? -2.666667e-01f : ((cse_var_18 && cse_var_26) ? -5.333334e-01f : ((cse_var_18 && cse_var_27) ? -1.066667e+00f : ((cse_var_19 && cse_var_22) ? 2.222222e-01f : ((cse_var_19 && cse_var_23) ? 2.222222e-01f : ((cse_var_19 && cse_var_24) ? 2.222222e-01f : ((cse_var_19 && cse_var_25) ? 2.222222e-01f : ((cse_var_19 && cse_var_26) ? 2.222222e-01f : ((cse_var_19 && cse_var_27) ? 2.222222e-01f : ((cse_var_20 && cse_var_22) ? -6.666667e-01f : ((cse_var_20 && cse_var_23) ? 6.666667e-01f : ((cse_var_20 && cse_var_24) ? -6.666667e-01f : ((cse_var_20 && cse_var_25) ? 6.666667e-01f : ((cse_var_20 && cse_var_26) ? -6.666667e-01f : ((cse_var_20 && cse_var_27) ? 6.666667e-01f : ((cse_var_21 && cse_var_22) ? 0.000000e+00f : ((cse_var_21 && cse_var_23) ? 0.000000e+00f : ((cse_var_21 && cse_var_24) ? 0.000000e+00f : ((cse_var_21 && cse_var_25) ? 0.000000e+00f : ((cse_var_21 && cse_var_26) ? 0.000000e+00f : ((cse_var_21 && cse_var_27) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_15 && cse_var_22) ? 1.000000e+00f : ((cse_var_15 && cse_var_23) ? 0.000000e+00f : ((cse_var_15 && cse_var_24) ? 0.000000e+00f : ((cse_var_15 && cse_var_25) ? 0.000000e+00f : ((cse_var_15 && cse_var_26) ? 0.000000e+00f : ((cse_var_15 && cse_var_27) ? 0.000000e+00f : ((cse_var_16 && cse_var_22) ? 5.555556e-02f : ((cse_var_16 && cse_var_23) ? -1.111111e-01f : ((cse_var_16 && cse_var_24) ? 2.222222e-01f : ((cse_var_16 && cse_var_25) ? -4.444444e-01f : ((cse_var_16 && cse_var_26) ? 8.888889e-01f : ((cse_var_16 && cse_var_27) ? -1.777778e+00f : ((cse_var_17 && cse_var_22) ? 1.422222e+00f : ((cse_var_17 && cse_var_23) ? -7.111111e-01f : ((cse_var_17 && cse_var_24) ? 3.555556e-01f : ((cse_var_17 && cse_var_25) ? -1.777778e-01f : ((cse_var_17 && cse_var_26) ? 8.888889e-02f : ((cse_var_17 && cse_var_27) ? -4.444445e-02f : ((cse_var_18 && cse_var_22) ? -3.333334e-02f : ((cse_var_18 && cse_var_23) ? -6.666667e-02f : ((cse_var_18 && cse_var_24) ? -1.333333e-01f : ((cse_var_18 && cse_var_25) ? -2.666667e-01f : ((cse_var_18 && cse_var_26) ? -5.333334e-01f : ((cse_var_18 && cse_var_27) ? -1.066667e+00f : ((cse_var_19 && cse_var_22) ? 2.222222e-01f : ((cse_var_19 && cse_var_23) ? 2.222222e-01f : ((cse_var_19 && cse_var_24) ? 2.222222e-01f : ((cse_var_19 && cse_var_25) ? 2.222222e-01f : ((cse_var_19 && cse_var_26) ? 2.222222e-01f : ((cse_var_19 && cse_var_27) ? 2.222222e-01f : ((cse_var_20 && cse_var_22) ? -6.666667e-01f : ((cse_var_20 && cse_var_23) ? 6.666667e-01f : ((cse_var_20 && cse_var_24) ? -6.666667e-01f : ((cse_var_20 && cse_var_25) ? 6.666667e-01f : ((cse_var_20 && cse_var_26) ? -6.666667e-01f : ((cse_var_20 && cse_var_27) ? 6.666667e-01f : ((cse_var_21 && cse_var_22) ? 0.000000e+00f : ((cse_var_21 && cse_var_23) ? 0.000000e+00f : ((cse_var_21 && cse_var_24) ? 0.000000e+00f : ((cse_var_21 && cse_var_25) ? 0.000000e+00f : ((cse_var_21 && cse_var_26) ? 0.000000e+00f : ((cse_var_21 && cse_var_27) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_15 && cse_var_22) ? 1.000000e+00f : ((cse_var_15 && cse_var_23) ? 0.000000e+00f : ((cse_var_15 && cse_var_24) ? 0.000000e+00f : ((cse_var_15 && cse_var_25) ? 0.000000e+00f : ((cse_var_15 && cse_var_26) ? 0.000000e+00f : ((cse_var_15 && cse_var_27) ? 0.000000e+00f : ((cse_var_16 && cse_var_22) ? 5.555556e-02f : ((cse_var_16 && cse_var_23) ? -1.111111e-01f : ((cse_var_16 && cse_var_24) ? 2.222222e-01f : ((cse_var_16 && cse_var_25) ? -4.444444e-01f : ((cse_var_16 && cse_var_26) ? 8.888889e-01f : ((cse_var_16 && cse_var_27) ? -1.777778e+00f : ((cse_var_17 && cse_var_22) ? 1.422222e+00f : ((cse_var_17 && cse_var_23) ? -7.111111e-01f : ((cse_var_17 && cse_var_24) ? 3.555556e-01f : ((cse_var_17 && cse_var_25) ? -1.777778e-01f : ((cse_var_17 && cse_var_26) ? 8.888889e-02f : ((cse_var_17 && cse_var_27) ? -4.444445e-02f : ((cse_var_18 && cse_var_22) ? -3.333334e-02f : ((cse_var_18 && cse_var_23) ? -6.666667e-02f : ((cse_var_18 && cse_var_24) ? -1.333333e-01f : ((cse_var_18 && cse_var_25) ? -2.666667e-01f : ((cse_var_18 && cse_var_26) ? -5.333334e-01f : ((cse_var_18 && cse_var_27) ? -1.066667e+00f : ((cse_var_19 && cse_var_22) ? 2.222222e-01f : ((cse_var_19 && cse_var_23) ? 2.222222e-01f : ((cse_var_19 && cse_var_24) ? 2.222222e-01f : ((cse_var_19 && cse_var_25) ? 2.222222e-01f : ((cse_var_19 && cse_var_26) ? 2.222222e-01f : ((cse_var_19 && cse_var_27) ? 2.222222e-01f : ((cse_var_20 && cse_var_22) ? -6.666667e-01f : ((cse_var_20 && cse_var_23) ? 6.666667e-01f : ((cse_var_20 && cse_var_24) ? -6.666667e-01f : ((cse_var_20 && cse_var_25) ? 6.666667e-01f : ((cse_var_20 && cse_var_26) ? -6.666667e-01f : ((cse_var_20 && cse_var_27) ? 6.666667e-01f : ((cse_var_21 && cse_var_22) ? 0.000000e+00f : ((cse_var_21 && cse_var_23) ? 0.000000e+00f : ((cse_var_21 && cse_var_24) ? 0.000000e+00f : ((cse_var_21 && cse_var_25) ? 0.000000e+00f : ((cse_var_21 && cse_var_26) ? 0.000000e+00f : ((cse_var_21 && cse_var_27) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_15 && cse_var_22) ? 1.000000e+00f : ((cse_var_15 && cse_var_23) ? 0.000000e+00f : ((cse_var_15 && cse_var_24) ? 0.000000e+00f : ((cse_var_15 && cse_var_25) ? 0.000000e+00f : ((cse_var_15 && cse_var_26) ? 0.000000e+00f : ((cse_var_15 && cse_var_27) ? 0.000000e+00f : ((cse_var_16 && cse_var_22) ? 5.555556e-02f : ((cse_var_16 && cse_var_23) ? -1.111111e-01f : ((cse_var_16 && cse_var_24) ? 2.222222e-01f : ((cse_var_16 && cse_var_25) ? -4.444444e-01f : ((cse_var_16 && cse_var_26) ? 8.888889e-01f : ((cse_var_16 && cse_var_27) ? -1.777778e+00f : ((cse_var_17 && cse_var_22) ? 1.422222e+00f : ((cse_var_17 && cse_var_23) ? -7.111111e-01f : ((cse_var_17 && cse_var_24) ? 3.555556e-01f : ((cse_var_17 && cse_var_25) ? -1.777778e-01f : ((cse_var_17 && cse_var_26) ? 8.888889e-02f : ((cse_var_17 && cse_var_27) ? -4.444445e-02f : ((cse_var_18 && cse_var_22) ? -3.333334e-02f : ((cse_var_18 && cse_var_23) ? -6.666667e-02f : ((cse_var_18 && cse_var_24) ? -1.333333e-01f : ((cse_var_18 && cse_var_25) ? -2.666667e-01f : ((cse_var_18 && cse_var_26) ? -5.333334e-01f : ((cse_var_18 && cse_var_27) ? -1.066667e+00f : ((cse_var_19 && cse_var_22) ? 2.222222e-01f : ((cse_var_19 && cse_var_23) ? 2.222222e-01f : ((cse_var_19 && cse_var_24) ? 2.222222e-01f : ((cse_var_19 && cse_var_25) ? 2.222222e-01f : ((cse_var_19 && cse_var_26) ? 2.222222e-01f : ((cse_var_19 && cse_var_27) ? 2.222222e-01f : ((cse_var_20 && cse_var_22) ? -6.666667e-01f : ((cse_var_20 && cse_var_23) ? 6.666667e-01f : ((cse_var_20 && cse_var_24) ? -6.666667e-01f : ((cse_var_20 && cse_var_25) ? 6.666667e-01f : ((cse_var_20 && cse_var_26) ? -6.666667e-01f : ((cse_var_20 && cse_var_27) ? 6.666667e-01f : ((cse_var_21 && cse_var_22) ? 0.000000e+00f : ((cse_var_21 && cse_var_23) ? 0.000000e+00f : ((cse_var_21 && cse_var_24) ? 0.000000e+00f : ((cse_var_21 && cse_var_25) ? 0.000000e+00f : ((cse_var_21 && cse_var_26) ? 0.000000e+00f : ((cse_var_21 && cse_var_27) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_15 && cse_var_22) ? 1.000000e+00f : ((cse_var_15 && cse_var_23) ? 0.000000e+00f : ((cse_var_15 && cse_var_24) ? 0.000000e+00f : ((cse_var_15 && cse_var_25) ? 0.000000e+00f : ((cse_var_15 && cse_var_26) ? 0.000000e+00f : ((cse_var_15 && cse_var_27) ? 0.000000e+00f : ((cse_var_16 && cse_var_22) ? 5.555556e-02f : ((cse_var_16 && cse_var_23) ? -1.111111e-01f : ((cse_var_16 && cse_var_24) ? 2.222222e-01f : ((cse_var_16 && cse_var_25) ? -4.444444e-01f : ((cse_var_16 && cse_var_26) ? 8.888889e-01f : ((cse_var_16 && cse_var_27) ? -1.777778e+00f : ((cse_var_17 && cse_var_22) ? 1.422222e+00f : ((cse_var_17 && cse_var_23) ? -7.111111e-01f : ((cse_var_17 && cse_var_24) ? 3.555556e-01f : ((cse_var_17 && cse_var_25) ? -1.777778e-01f : ((cse_var_17 && cse_var_26) ? 8.888889e-02f : ((cse_var_17 && cse_var_27) ? -4.444445e-02f : ((cse_var_18 && cse_var_22) ? -3.333334e-02f : ((cse_var_18 && cse_var_23) ? -6.666667e-02f : ((cse_var_18 && cse_var_24) ? -1.333333e-01f : ((cse_var_18 && cse_var_25) ? -2.666667e-01f : ((cse_var_18 && cse_var_26) ? -5.333334e-01f : ((cse_var_18 && cse_var_27) ? -1.066667e+00f : ((cse_var_19 && cse_var_22) ? 2.222222e-01f : ((cse_var_19 && cse_var_23) ? 2.222222e-01f : ((cse_var_19 && cse_var_24) ? 2.222222e-01f : ((cse_var_19 && cse_var_25) ? 2.222222e-01f : ((cse_var_19 && cse_var_26) ? 2.222222e-01f : ((cse_var_19 && cse_var_27) ? 2.222222e-01f : ((cse_var_20 && cse_var_22) ? -6.666667e-01f : ((cse_var_20 && cse_var_23) ? 6.666667e-01f : ((cse_var_20 && cse_var_24) ? -6.666667e-01f : ((cse_var_20 && cse_var_25) ? 6.666667e-01f : ((cse_var_20 && cse_var_26) ? -6.666667e-01f : ((cse_var_20 && cse_var_27) ? 6.666667e-01f : ((cse_var_21 && cse_var_22) ? 0.000000e+00f : ((cse_var_21 && cse_var_23) ? 0.000000e+00f : ((cse_var_21 && cse_var_24) ? 0.000000e+00f : ((cse_var_21 && cse_var_25) ? 0.000000e+00f : ((cse_var_21 && cse_var_26) ? 0.000000e+00f : ((cse_var_21 && cse_var_27) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_15 && cse_var_22) ? 1.000000e+00f : ((cse_var_15 && cse_var_23) ? 0.000000e+00f : ((cse_var_15 && cse_var_24) ? 0.000000e+00f : ((cse_var_15 && cse_var_25) ? 0.000000e+00f : ((cse_var_15 && cse_var_26) ? 0.000000e+00f : ((cse_var_15 && cse_var_27) ? 0.000000e+00f : ((cse_var_16 && cse_var_22) ? 5.555556e-02f : ((cse_var_16 && cse_var_23) ? -1.111111e-01f : ((cse_var_16 && cse_var_24) ? 2.222222e-01f : ((cse_var_16 && cse_var_25) ? -4.444444e-01f : ((cse_var_16 && cse_var_26) ? 8.888889e-01f : ((cse_var_16 && cse_var_27) ? -1.777778e+00f : ((cse_var_17 && cse_var_22) ? 1.422222e+00f : ((cse_var_17 && cse_var_23) ? -7.111111e-01f : ((cse_var_17 && cse_var_24) ? 3.555556e-01f : ((cse_var_17 && cse_var_25) ? -1.777778e-01f : ((cse_var_17 && cse_var_26) ? 8.888889e-02f : ((cse_var_17 && cse_var_27) ? -4.444445e-02f : ((cse_var_18 && cse_var_22) ? -3.333334e-02f : ((cse_var_18 && cse_var_23) ? -6.666667e-02f : ((cse_var_18 && cse_var_24) ? -1.333333e-01f : ((cse_var_18 && cse_var_25) ? -2.666667e-01f : ((cse_var_18 && cse_var_26) ? -5.333334e-01f : ((cse_var_18 && cse_var_27) ? -1.066667e+00f : ((cse_var_19 && cse_var_22) ? 2.222222e-01f : ((cse_var_19 && cse_var_23) ? 2.222222e-01f : ((cse_var_19 && cse_var_24) ? 2.222222e-01f : ((cse_var_19 && cse_var_25) ? 2.222222e-01f : ((cse_var_19 && cse_var_26) ? 2.222222e-01f : ((cse_var_19 && cse_var_27) ? 2.222222e-01f : ((cse_var_20 && cse_var_22) ? -6.666667e-01f : ((cse_var_20 && cse_var_23) ? 6.666667e-01f : ((cse_var_20 && cse_var_24) ? -6.666667e-01f : ((cse_var_20 && cse_var_25) ? 6.666667e-01f : ((cse_var_20 && cse_var_26) ? -6.666667e-01f : ((cse_var_20 && cse_var_27) ? 6.666667e-01f : ((cse_var_21 && cse_var_22) ? 0.000000e+00f : ((cse_var_21 && cse_var_23) ? 0.000000e+00f : ((cse_var_21 && cse_var_24) ? 0.000000e+00f : ((cse_var_21 && cse_var_25) ? 0.000000e+00f : ((cse_var_21 && cse_var_26) ? 0.000000e+00f : ((cse_var_21 && cse_var_27) ? 2.000000e+00f : 0.000000e+00f))))))))))))))))))))))))))))))))))))))))))))));\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(39) default_function_kernel(float* __restrict__ data, float* __restrict__ transform_weight);\nextern \"C\" __global__ void __launch_bounds__(39) default_function_kernel(float* __restrict__ data, float* __restrict__ transform_weight) {\n  float transform_weight_local[56];\n  __shared__ float data_shared[3744];\n  __shared__ float G_shared[42];\n  for (int d_c_inner_init = 0; d_c_inner_init < 2; ++d_c_inner_init) {\n    for (int co_c_inner_init = 0; co_c_inner_init < 2; ++co_c_inner_init) {\n      transform_weight_local[((d_c_inner_init * 2) + co_c_inner_init)] = 0.000000e+00f;\n      transform_weight_local[(((d_c_inner_init * 2) + co_c_inner_init) + 4)] = 0.000000e+00f;\n      transform_weight_local[(((d_c_inner_init * 2) + co_c_inner_init) + 8)] = 0.000000e+00f;\n      transform_weight_local[(((d_c_inner_init * 2) + co_c_inner_init) + 12)] = 0.000000e+00f;\n      transform_weight_local[(((d_c_inner_init * 2) + co_c_inner_init) + 16)] = 0.000000e+00f;\n      transform_weight_local[(((d_c_inner_init * 2) + co_c_inner_init) + 20)] = 0.000000e+00f;\n      transform_weight_local[(((d_c_inner_init * 2) + co_c_inner_init) + 24)] = 0.000000e+00f;\n      transform_weight_local[(((d_c_inner_init * 2) + co_c_inner_init) + 28)] = 0.000000e+00f;\n      transform_weight_local[(((d_c_inner_init * 2) + co_c_inner_init) + 32)] = 0.000000e+00f;\n      transform_weight_local[(((d_c_inner_init * 2) + co_c_inner_init) + 36)] = 0.000000e+00f;\n      transform_weight_local[(((d_c_inner_init * 2) + co_c_inner_init) + 40)] = 0.000000e+00f;\n      transform_weight_local[(((d_c_inner_init * 2) + co_c_inner_init) + 44)] = 0.000000e+00f;\n      transform_weight_local[(((d_c_inner_init * 2) + co_c_inner_init) + 48)] = 0.000000e+00f;\n      transform_weight_local[(((d_c_inner_init * 2) + co_c_inner_init) + 52)] = 0.000000e+00f;\n    },\n  },\n  for (int r_kw_outer_outer = 0; r_kw_outer_outer < 3; ++r_kw_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer < 96; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer) {\n      data_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 39) + ((int)threadIdx.x))] = data[((((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer / 24) * 92664) + ((((int)blockIdx.x) % 33) * 2808)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer % 24) * 39) + ((int)threadIdx.x)) >> 1) * 6)) + (r_kw_outer_outer * 2)) + ((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer + ((int)threadIdx.x)) & 1))];\n    },\n    for (int ax0_ax1_fused_outer_outer = 0; ax0_ax1_fused_outer_outer < 2; ++ax0_ax1_fused_outer_outer) {\n      if (((ax0_ax1_fused_outer_outer * 13) + (((int)threadIdx.x) / 3)) < 14) {\n        G_shared[((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x))] = ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 41) ? 1.000000e+00f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 40) ? 0.000000e+00f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 39) ? 0.000000e+00f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 38) ? 0.000000e+00f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 37) ? 0.000000e+00f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 36) ? 0.000000e+00f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 35) ? 5.555556e-02f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 34) ? -1.111111e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 33) ? 2.222222e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 32) ? -4.444444e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 31) ? 8.888889e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 30) ? -1.777778e+00f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 29) ? 1.422222e+00f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 28) ? -7.111111e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 27) ? 3.555556e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 26) ? -1.777778e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 25) ? 8.888889e-02f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 24) ? -4.444445e-02f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 23) ? -3.333334e-02f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 22) ? -6.666667e-02f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 21) ? -1.333333e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 20) ? -2.666667e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 19) ? -5.333334e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 18) ? -1.066667e+00f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 17) ? 2.222222e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 16) ? 2.222222e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 15) ? 2.222222e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 14) ? 2.222222e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 13) ? 2.222222e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 12) ? 2.222222e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 11) ? -6.666667e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 10) ? 6.666667e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 9) ? -6.666667e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 8) ? 6.666667e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 7) ? -6.666667e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 6) ? 6.666667e-01f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 5) ? 0.000000e+00f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 4) ? 0.000000e+00f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 3) ? 0.000000e+00f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 2) ? 0.000000e+00f : ((((ax0_ax1_fused_outer_outer * 39) + ((int)threadIdx.x)) == 1) ? 0.000000e+00f : 2.000000e+00f)))))))))))))))))))))))))))))))))))))))));\n      },\n    },\n    __syncthreads();\n    for (int r_kw_outer_inner = 0; r_kw_outer_inner < 2; ++r_kw_outer_inner) {\n      for (int r_kh_inner = 0; r_kh_inner < 6; ++r_kh_inner) {\n        for (int d_c_inner = 0; d_c_inner < 2; ++d_c_inner) {\n          for (int co_c_inner = 0; co_c_inner < 2; ++co_c_inner) {\n            transform_weight_local[((d_c_inner * 2) + co_c_inner)] = (transform_weight_local[((d_c_inner * 2) + co_c_inner)] + ((data_shared[((((((co_c_inner * 936) + ((((int)threadIdx.x) % 3) * 312)) + ((((int)threadIdx.x) / 3) * 24)) + (d_c_inner * 12)) + (r_kh_inner * 2)) + r_kw_outer_inner)] * G_shared[r_kh_inner]) * G_shared[((((((int)blockIdx.x) / 33) * 6) + (r_kw_outer_outer * 2)) + r_kw_outer_inner)]));\n            transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 4)] = (transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 4)] + ((data_shared[(((((((co_c_inner * 936) + ((((int)threadIdx.x) % 3) * 312)) + ((((int)threadIdx.x) / 3) * 24)) + (d_c_inner * 12)) + (r_kh_inner * 2)) + r_kw_outer_inner) + 1872)] * G_shared[r_kh_inner]) * G_shared[((((((int)blockIdx.x) / 33) * 6) + (r_kw_outer_outer * 2)) + r_kw_outer_inner)]));\n            transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 8)] = (transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 8)] + ((data_shared[((((((co_c_inner * 936) + ((((int)threadIdx.x) % 3) * 312)) + ((((int)threadIdx.x) / 3) * 24)) + (d_c_inner * 12)) + (r_kh_inner * 2)) + r_kw_outer_inner)] * G_shared[(r_kh_inner + 6)]) * G_shared[((((((int)blockIdx.x) / 33) * 6) + (r_kw_outer_outer * 2)) + r_kw_outer_inner)]));\n            transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 12)] = (transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 12)] + ((data_shared[(((((((co_c_inner * 936) + ((((int)threadIdx.x) % 3) * 312)) + ((((int)threadIdx.x) / 3) * 24)) + (d_c_inner * 12)) + (r_kh_inner * 2)) + r_kw_outer_inner) + 1872)] * G_shared[(r_kh_inner + 6)]) * G_shared[((((((int)blockIdx.x) / 33) * 6) + (r_kw_outer_outer * 2)) + r_kw_outer_inner)]));\n            transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 16)] = (transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 16)] + ((data_shared[((((((co_c_inner * 936) + ((((int)threadIdx.x) % 3) * 312)) + ((((int)threadIdx.x) / 3) * 24)) + (d_c_inner * 12)) + (r_kh_inner * 2)) + r_kw_outer_inner)] * G_shared[(r_kh_inner + 12)]) * G_shared[((((((int)blockIdx.x) / 33) * 6) + (r_kw_outer_outer * 2)) + r_kw_outer_inner)]));\n            transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 20)] = (transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 20)] + ((data_shared[(((((((co_c_inner * 936) + ((((int)threadIdx.x) % 3) * 312)) + ((((int)threadIdx.x) / 3) * 24)) + (d_c_inner * 12)) + (r_kh_inner * 2)) + r_kw_outer_inner) + 1872)] * G_shared[(r_kh_inner + 12)]) * G_shared[((((((int)blockIdx.x) / 33) * 6) + (r_kw_outer_outer * 2)) + r_kw_outer_inner)]));\n            transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 24)] = (transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 24)] + ((data_shared[((((((co_c_inner * 936) + ((((int)threadIdx.x) % 3) * 312)) + ((((int)threadIdx.x) / 3) * 24)) + (d_c_inner * 12)) + (r_kh_inner * 2)) + r_kw_outer_inner)] * G_shared[(r_kh_inner + 18)]) * G_shared[((((((int)blockIdx.x) / 33) * 6) + (r_kw_outer_outer * 2)) + r_kw_outer_inner)]));\n            transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 28)] = (transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 28)] + ((data_shared[(((((((co_c_inner * 936) + ((((int)threadIdx.x) % 3) * 312)) + ((((int)threadIdx.x) / 3) * 24)) + (d_c_inner * 12)) + (r_kh_inner * 2)) + r_kw_outer_inner) + 1872)] * G_shared[(r_kh_inner + 18)]) * G_shared[((((((int)blockIdx.x) / 33) * 6) + (r_kw_outer_outer * 2)) + r_kw_outer_inner)]));\n            transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 32)] = (transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 32)] + ((data_shared[((((((co_c_inner * 936) + ((((int)threadIdx.x) % 3) * 312)) + ((((int)threadIdx.x) / 3) * 24)) + (d_c_inner * 12)) + (r_kh_inner * 2)) + r_kw_outer_inner)] * G_shared[(r_kh_inner + 24)]) * G_shared[((((((int)blockIdx.x) / 33) * 6) + (r_kw_outer_outer * 2)) + r_kw_outer_inner)]));\n            transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 36)] = (transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 36)] + ((data_shared[(((((((co_c_inner * 936) + ((((int)threadIdx.x) % 3) * 312)) + ((((int)threadIdx.x) / 3) * 24)) + (d_c_inner * 12)) + (r_kh_inner * 2)) + r_kw_outer_inner) + 1872)] * G_shared[(r_kh_inner + 24)]) * G_shared[((((((int)blockIdx.x) / 33) * 6) + (r_kw_outer_outer * 2)) + r_kw_outer_inner)]));\n            transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 40)] = (transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 40)] + ((data_shared[((((((co_c_inner * 936) + ((((int)threadIdx.x) % 3) * 312)) + ((((int)threadIdx.x) / 3) * 24)) + (d_c_inner * 12)) + (r_kh_inner * 2)) + r_kw_outer_inner)] * G_shared[(r_kh_inner + 30)]) * G_shared[((((((int)blockIdx.x) / 33) * 6) + (r_kw_outer_outer * 2)) + r_kw_outer_inner)]));\n            transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 44)] = (transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 44)] + ((data_shared[(((((((co_c_inner * 936) + ((((int)threadIdx.x) % 3) * 312)) + ((((int)threadIdx.x) / 3) * 24)) + (d_c_inner * 12)) + (r_kh_inner * 2)) + r_kw_outer_inner) + 1872)] * G_shared[(r_kh_inner + 30)]) * G_shared[((((((int)blockIdx.x) / 33) * 6) + (r_kw_outer_outer * 2)) + r_kw_outer_inner)]));\n            transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 48)] = (transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 48)] + ((data_shared[((((((co_c_inner * 936) + ((((int)threadIdx.x) % 3) * 312)) + ((((int)threadIdx.x) / 3) * 24)) + (d_c_inner * 12)) + (r_kh_inner * 2)) + r_kw_outer_inner)] * G_shared[(r_kh_inner + 36)]) * G_shared[((((((int)blockIdx.x) / 33) * 6) + (r_kw_outer_outer * 2)) + r_kw_outer_inner)]));\n            transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 52)] = (transform_weight_local[(((d_c_inner * 2) + co_c_inner) + 52)] + ((data_shared[(((((((co_c_inner * 936) + ((((int)threadIdx.x) % 3) * 312)) + ((((int)threadIdx.x) / 3) * 24)) + (d_c_inner * 12)) + (r_kh_inner * 2)) + r_kw_outer_inner) + 1872)] * G_shared[(r_kh_inner + 36)]) * G_shared[((((((int)blockIdx.x) / 33) * 6) + (r_kw_outer_outer * 2)) + r_kw_outer_inner)]));\n          },\n        },\n      },\n    },\n  },\n  for (int d_inner = 0; d_inner < 2; ++d_inner) {\n    for (int co_inner = 0; co_inner < 2; ++co_inner) {\n      transform_weight[(((((((((int)blockIdx.x) / 33) * 10296) + ((((int)threadIdx.x) / 3) * 792)) + (d_inner * 396)) + (co_inner * 99)) + ((((int)blockIdx.x) % 33) * 3)) + (((int)threadIdx.x) % 3))] = transform_weight_local[((d_inner * 2) + co_inner)];\n      transform_weight[((((((((((int)blockIdx.x) / 33) * 10296) + ((((int)threadIdx.x) / 3) * 792)) + (d_inner * 396)) + (co_inner * 99)) + ((((int)blockIdx.x) % 33) * 3)) + (((int)threadIdx.x) % 3)) + 198)] = transform_weight_local[(((d_inner * 2) + co_inner) + 4)];\n      transform_weight[((((((((((int)blockIdx.x) / 33) * 10296) + ((((int)threadIdx.x) / 3) * 792)) + (d_inner * 396)) + (co_inner * 99)) + ((((int)blockIdx.x) % 33) * 3)) + (((int)threadIdx.x) % 3)) + 72072)] = transform_weight_local[(((d_inner * 2) + co_inner) + 8)];\n      transform_weight[((((((((((int)blockIdx.x) / 33) * 10296) + ((((int)threadIdx.x) / 3) * 792)) + (d_inner * 396)) + (co_inner * 99)) + ((((int)blockIdx.x) % 33) * 3)) + (((int)threadIdx.x) % 3)) + 72270)] = transform_weight_local[(((d_inner * 2) + co_inner) + 12)];\n      transform_weight[((((((((((int)blockIdx.x) / 33) * 10296) + ((((int)threadIdx.x) / 3) * 792)) + (d_inner * 396)) + (co_inner * 99)) + ((((int)blockIdx.x) % 33) * 3)) + (((int)threadIdx.x) % 3)) + 144144)] = transform_weight_local[(((d_inner * 2) + co_inner) + 16)];\n      transform_weight[((((((((((int)blockIdx.x) / 33) * 10296) + ((((int)threadIdx.x) / 3) * 792)) + (d_inner * 396)) + (co_inner * 99)) + ((((int)blockIdx.x) % 33) * 3)) + (((int)threadIdx.x) % 3)) + 144342)] = transform_weight_local[(((d_inner * 2) + co_inner) + 20)];\n      transform_weight[((((((((((int)blockIdx.x) / 33) * 10296) + ((((int)threadIdx.x) / 3) * 792)) + (d_inner * 396)) + (co_inner * 99)) + ((((int)blockIdx.x) % 33) * 3)) + (((int)threadIdx.x) % 3)) + 216216)] = transform_weight_local[(((d_inner * 2) + co_inner) + 24)];\n      transform_weight[((((((((((int)blockIdx.x) / 33) * 10296) + ((((int)threadIdx.x) / 3) * 792)) + (d_inner * 396)) + (co_inner * 99)) + ((((int)blockIdx.x) % 33) * 3)) + (((int)threadIdx.x) % 3)) + 216414)] = transform_weight_local[(((d_inner * 2) + co_inner) + 28)];\n      transform_weight[((((((((((int)blockIdx.x) / 33) * 10296) + ((((int)threadIdx.x) / 3) * 792)) + (d_inner * 396)) + (co_inner * 99)) + ((((int)blockIdx.x) % 33) * 3)) + (((int)threadIdx.x) % 3)) + 288288)] = transform_weight_local[(((d_inner * 2) + co_inner) + 32)];\n      transform_weight[((((((((((int)blockIdx.x) / 33) * 10296) + ((((int)threadIdx.x) / 3) * 792)) + (d_inner * 396)) + (co_inner * 99)) + ((((int)blockIdx.x) % 33) * 3)) + (((int)threadIdx.x) % 3)) + 288486)] = transform_weight_local[(((d_inner * 2) + co_inner) + 36)];\n      transform_weight[((((((((((int)blockIdx.x) / 33) * 10296) + ((((int)threadIdx.x) / 3) * 792)) + (d_inner * 396)) + (co_inner * 99)) + ((((int)blockIdx.x) % 33) * 3)) + (((int)threadIdx.x) % 3)) + 360360)] = transform_weight_local[(((d_inner * 2) + co_inner) + 40)];\n      transform_weight[((((((((((int)blockIdx.x) / 33) * 10296) + ((((int)threadIdx.x) / 3) * 792)) + (d_inner * 396)) + (co_inner * 99)) + ((((int)blockIdx.x) % 33) * 3)) + (((int)threadIdx.x) % 3)) + 360558)] = transform_weight_local[(((d_inner * 2) + co_inner) + 44)];\n      transform_weight[((((((((((int)blockIdx.x) / 33) * 10296) + ((((int)threadIdx.x) / 3) * 792)) + (d_inner * 396)) + (co_inner * 99)) + ((((int)blockIdx.x) % 33) * 3)) + (((int)threadIdx.x) % 3)) + 432432)] = transform_weight_local[(((d_inner * 2) + co_inner) + 48)];\n      transform_weight[((((((((((int)blockIdx.x) / 33) * 10296) + ((((int)threadIdx.x) / 3) * 792)) + (d_inner * 396)) + (co_inner * 99)) + ((((int)blockIdx.x) % 33) * 3)) + (((int)threadIdx.x) % 3)) + 432630)] = transform_weight_local[(((d_inner * 2) + co_inner) + 52)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 99, 26, 6, 6), \"float32\"), transform_weight: T.Buffer((7, 7, 26, 4, 99), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused in T.parallel(1274):\n            for co_outer_outer_inner in range(2):\n                transform_weight_1 = T.Buffer((504504,), data=transform_weight.data)\n                for ci_outer_inner_init, co_inner_init in T.grid(11, 2):\n                    transform_weight_1[eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused % 7 * 72072 + eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused // 7 * 396 + co_outer_outer_inner * 198 + co_inner_init * 99 + ci_outer_inner_init * 9:eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused % 7 * 72072 + eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused // 7 * 396 + co_outer_outer_inner * 198 + co_inner_init * 99 + ci_outer_inner_init * 9 + 9] = T.Broadcast(T.float32(0), 9)\n                for r_kh_outer, r_kw_outer, ci_outer_inner, r_kh_inner, co_inner in T.grid(3, 6, 11, 2, 2):\n                    cse_var_30: T.int32 = eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused % 7\n                    cse_var_29: T.int32 = eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused // 182\n                    cse_var_28: T.int32 = r_kh_outer * 2 + r_kh_inner\n                    cse_var_27: T.bool = r_kw_outer == 0\n                    cse_var_26: T.bool = r_kw_outer == 1\n                    cse_var_25: T.bool = r_kw_outer == 2\n                    cse_var_24: T.bool = r_kw_outer == 3\n                    cse_var_23: T.bool = r_kw_outer == 4\n                    cse_var_22: T.bool = r_kw_outer == 5\n                    cse_var_21: T.bool = cse_var_29 == 0\n                    cse_var_20: T.bool = cse_var_29 == 1\n                    cse_var_19: T.bool = cse_var_29 == 2\n                    cse_var_18: T.bool = cse_var_29 == 3\n                    cse_var_17: T.bool = cse_var_29 == 4\n                    cse_var_16: T.bool = cse_var_29 == 5\n                    cse_var_15: T.bool = cse_var_29 == 6\n                    cse_var_14: T.bool = cse_var_30 == 0\n                    cse_var_13: T.bool = cse_var_30 == 1\n                    cse_var_12: T.bool = cse_var_30 == 2\n                    cse_var_11: T.bool = cse_var_30 == 3\n                    cse_var_10: T.bool = cse_var_30 == 4\n                    cse_var_9: T.bool = cse_var_30 == 5\n                    cse_var_8: T.bool = cse_var_30 == 6\n                    cse_var_7: T.bool = cse_var_28 == 0\n                    cse_var_6: T.bool = cse_var_28 == 1\n                    cse_var_5: T.bool = cse_var_28 == 2\n                    cse_var_4: T.bool = cse_var_28 == 3\n                    cse_var_3: T.bool = cse_var_28 == 4\n                    cse_var_2: T.bool = cse_var_28 == 5\n                    cse_var_1: T.int32 = cse_var_30 * 72072 + eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused // 7 * 396 + co_outer_outer_inner * 198 + co_inner * 99 + ci_outer_inner * 9\n                    data_1 = T.Buffer((370656,), data=data.data)\n                    transform_weight_1[cse_var_1:cse_var_1 + 9] = transform_weight_1[cse_var_1:cse_var_1 + 9] + data_1[co_outer_outer_inner * 185328 + co_inner * 92664 + ci_outer_inner * 8424 + eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused % 182 // 7 * 36 + r_kh_outer * 12 + r_kh_inner * 6 + r_kw_outer:co_outer_outer_inner * 185328 + co_inner * 92664 + ci_outer_inner * 8424 + eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused % 182 // 7 * 36 + r_kh_outer * 12 + r_kh_inner * 6 + r_kw_outer + 8424:936] * T.Broadcast(T.Select(cse_var_8 and cse_var_2, T.float32(1), T.Select(cse_var_8 and cse_var_3, T.float32(0), T.Select(cse_var_8 and cse_var_4, T.float32(0), T.Select(cse_var_8 and cse_var_5, T.float32(0), T.Select(cse_var_8 and cse_var_6, T.float32(0), T.Select(cse_var_8 and cse_var_7, T.float32(0), T.Select(cse_var_9 and cse_var_2, T.float32(0.0555555559694767), T.Select(cse_var_9 and cse_var_3, T.float32(-0.1111111119389534), T.Select(cse_var_9 and cse_var_4, T.float32(0.2222222238779068), T.Select(cse_var_9 and cse_var_5, T.float32(-0.4444444477558136), T.Select(cse_var_9 and cse_var_6, T.float32(0.8888888955116272), T.Select(cse_var_9 and cse_var_7, T.float32(-1.7777777910232544), T.Select(cse_var_10 and cse_var_2, T.float32(1.4222222566604614), T.Select(cse_var_10 and cse_var_3, T.float32(-0.71111112833023071), T.Select(cse_var_10 and cse_var_4, T.float32(0.35555556416511536), T.Select(cse_var_10 and cse_var_5, T.float32(-0.17777778208255768), T.Select(cse_var_10 and cse_var_6, T.float32(0.088888891041278839), T.Select(cse_var_10 and cse_var_7, T.float32(-0.04444444552063942), T.Select(cse_var_11 and cse_var_2, T.float32(-0.033333335071802139), T.Select(cse_var_11 and cse_var_3, T.float32(-0.066666670143604279), T.Select(cse_var_11 and cse_var_4, T.float32(-0.13333334028720856), T.Select(cse_var_11 and cse_var_5, T.float32(-0.26666668057441711), T.Select(cse_var_11 and cse_var_6, T.float32(-0.53333336114883423), T.Select(cse_var_11 and cse_var_7, T.float32(-1.0666667222976685), T.Select(cse_var_12 and cse_var_2, T.float32(0.2222222238779068), T.Select(cse_var_12 and cse_var_3, T.float32(0.2222222238779068), T.Select(cse_var_12 and cse_var_4, T.float32(0.2222222238779068), T.Select(cse_var_12 and cse_var_5, T.float32(0.2222222238779068), T.Select(cse_var_12 and cse_var_6, T.float32(0.2222222238779068), T.Select(cse_var_12 and cse_var_7, T.float32(0.2222222238779068), T.Select(cse_var_13 and cse_var_2, T.float32(-0.66666668653488159), T.Select(cse_var_13 and cse_var_3, T.float32(0.66666668653488159), T.Select(cse_var_13 and cse_var_4, T.float32(-0.66666668653488159), T.Select(cse_var_13 and cse_var_5, T.float32(0.66666668653488159), T.Select(cse_var_13 and cse_var_6, T.float32(-0.66666668653488159), T.Select(cse_var_13 and cse_var_7, T.float32(0.66666668653488159), T.Select(cse_var_14 and cse_var_2, T.float32(0), T.Select(cse_var_14 and cse_var_3, T.float32(0), T.Select(cse_var_14 and cse_var_4, T.float32(0), T.Select(cse_var_14 and cse_var_5, T.float32(0), T.Select(cse_var_14 and cse_var_6, T.float32(0), T.Select(cse_var_14 and cse_var_7, T.float32(2), T.float32(0))))))))))))))))))))))))))))))))))))))))))), 9) * T.Broadcast(T.Select(cse_var_15 and cse_var_22, T.float32(1), T.Select(cse_var_15 and cse_var_23, T.float32(0), T.Select(cse_var_15 and cse_var_24, T.float32(0), T.Select(cse_var_15 and cse_var_25, T.float32(0), T.Select(cse_var_15 and cse_var_26, T.float32(0), T.Select(cse_var_15 and cse_var_27, T.float32(0), T.Select(cse_var_16 and cse_var_22, T.float32(0.0555555559694767), T.Select(cse_var_16 and cse_var_23, T.float32(-0.1111111119389534), T.Select(cse_var_16 and cse_var_24, T.float32(0.2222222238779068), T.Select(cse_var_16 and cse_var_25, T.float32(-0.4444444477558136), T.Select(cse_var_16 and cse_var_26, T.float32(0.8888888955116272), T.Select(cse_var_16 and cse_var_27, T.float32(-1.7777777910232544), T.Select(cse_var_17 and cse_var_22, T.float32(1.4222222566604614), T.Select(cse_var_17 and cse_var_23, T.float32(-0.71111112833023071), T.Select(cse_var_17 and cse_var_24, T.float32(0.35555556416511536), T.Select(cse_var_17 and cse_var_25, T.float32(-0.17777778208255768), T.Select(cse_var_17 and cse_var_26, T.float32(0.088888891041278839), T.Select(cse_var_17 and cse_var_27, T.float32(-0.04444444552063942), T.Select(cse_var_18 and cse_var_22, T.float32(-0.033333335071802139), T.Select(cse_var_18 and cse_var_23, T.float32(-0.066666670143604279), T.Select(cse_var_18 and cse_var_24, T.float32(-0.13333334028720856), T.Select(cse_var_18 and cse_var_25, T.float32(-0.26666668057441711), T.Select(cse_var_18 and cse_var_26, T.float32(-0.53333336114883423), T.Select(cse_var_18 and cse_var_27, T.float32(-1.0666667222976685), T.Select(cse_var_19 and cse_var_22, T.float32(0.2222222238779068), T.Select(cse_var_19 and cse_var_23, T.float32(0.2222222238779068), T.Select(cse_var_19 and cse_var_24, T.float32(0.2222222238779068), T.Select(cse_var_19 and cse_var_25, T.float32(0.2222222238779068), T.Select(cse_var_19 and cse_var_26, T.float32(0.2222222238779068), T.Select(cse_var_19 and cse_var_27, T.float32(0.2222222238779068), T.Select(cse_var_20 and cse_var_22, T.float32(-0.66666668653488159), T.Select(cse_var_20 and cse_var_23, T.float32(0.66666668653488159), T.Select(cse_var_20 and cse_var_24, T.float32(-0.66666668653488159), T.Select(cse_var_20 and cse_var_25, T.float32(0.66666668653488159), T.Select(cse_var_20 and cse_var_26, T.float32(-0.66666668653488159), T.Select(cse_var_20 and cse_var_27, T.float32(0.66666668653488159), T.Select(cse_var_21 and cse_var_22, T.float32(0), T.Select(cse_var_21 and cse_var_23, T.float32(0), T.Select(cse_var_21 and cse_var_24, T.float32(0), T.Select(cse_var_21 and cse_var_25, T.float32(0), T.Select(cse_var_21 and cse_var_26, T.float32(0), T.Select(cse_var_21 and cse_var_27, T.float32(2), T.float32(0))))))))))))))))))))))))))))))))))))))))))), 9)",
        "data": "4_99_26_6_6"
    },
    {
        "op_name": "conv3d_winograd_weight_transform",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t transform_weight_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* transform_weight = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* transform_weight_1 = (((DLTensor*)transform_weight)[0].data);\n  void* default_function_transform_weight_shape = (((DLTensor*)transform_weight)[0].shape);\n  void* default_function_transform_weight_strides = (((DLTensor*)transform_weight)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_transform_weight_strides == NULL)) {\n  },\n  for (int32_t eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused = 0; eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused < 6118; ++eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused) {\n    for (int32_t eps_inner_init = 0; eps_inner_init < 7; ++eps_inner_init) {\n      for (int32_t d_inner_init = 0; d_inner_init < 2; ++d_inner_init) {\n        for (int32_t co_inner_init = 0; co_inner_init < 2; ++co_inner_init) {\n          *(float3*)(((float*)transform_weight_1) + ((((((eps_inner_init * 73416) + ((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused % 7) * 10488)) + ((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused / 133) * 228)) + (d_inner_init * 114)) + (co_inner_init * 57)) + (((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused % 133) / 7) * 3))) = ((float3)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n        },\n      },\n    },\n    for (int32_t r_kh_outer = 0; r_kh_outer < 2; ++r_kh_outer) {\n      for (int32_t r_kw_outer = 0; r_kw_outer < 3; ++r_kw_outer) {\n        for (int32_t r_kh_inner = 0; r_kh_inner < 3; ++r_kh_inner) {\n          for (int32_t r_kw_inner = 0; r_kw_inner < 2; ++r_kw_inner) {\n            for (int32_t eps_inner = 0; eps_inner < 7; ++eps_inner) {\n              for (int32_t d_inner = 0; d_inner < 2; ++d_inner) {\n                for (int32_t co_inner = 0; co_inner < 2; ++co_inner) {\n                  int32_t cse_var_33 = (r_kw_outer * 2);\n                  int32_t cse_var_32 = (eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused / 133);\n                  int32_t cse_var_31 = (eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused % 7);\n                  int32_t cse_var_30 = ((eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused % 133) / 7);\n                  int32_t cse_var_29 = (cse_var_33 + r_kw_inner);\n                  int32_t cse_var_28 = ((r_kh_outer * 3) + r_kh_inner);\n                  bool cse_var_27 = (eps_inner == 0);\n                  bool cse_var_26 = (eps_inner == 1);\n                  bool cse_var_25 = (eps_inner == 2);\n                  bool cse_var_24 = (eps_inner == 3);\n                  bool cse_var_23 = (eps_inner == 4);\n                  bool cse_var_22 = (eps_inner == 5);\n                  bool cse_var_21 = (eps_inner == 6);\n                  bool cse_var_20 = (cse_var_31 == 0);\n                  bool cse_var_19 = (cse_var_31 == 1);\n                  bool cse_var_18 = (cse_var_31 == 2);\n                  bool cse_var_17 = (cse_var_31 == 3);\n                  bool cse_var_16 = (cse_var_31 == 4);\n                  bool cse_var_15 = (cse_var_31 == 5);\n                  bool cse_var_14 = (cse_var_31 == 6);\n                  bool cse_var_13 = (cse_var_28 == 0);\n                  bool cse_var_12 = (cse_var_28 == 1);\n                  bool cse_var_11 = (cse_var_28 == 2);\n                  bool cse_var_10 = (cse_var_28 == 3);\n                  bool cse_var_9 = (cse_var_28 == 4);\n                  bool cse_var_8 = (cse_var_28 == 5);\n                  bool cse_var_7 = (cse_var_29 == 0);\n                  bool cse_var_6 = (cse_var_29 == 1);\n                  bool cse_var_5 = (cse_var_29 == 2);\n                  bool cse_var_4 = (cse_var_29 == 3);\n                  bool cse_var_3 = (cse_var_29 == 4);\n                  bool cse_var_2 = (cse_var_29 == 5);\n                  int32_t cse_var_1 = ((((((eps_inner * 73416) + (cse_var_31 * 10488)) + (cse_var_32 * 228)) + (d_inner * 114)) + (co_inner * 57)) + (cse_var_30 * 3));\n                  int32_t3 v_ = int32_t3((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2));\n                  int32_t3 v__1 = int32_t3((((((((((co_inner * 188784) + (cse_var_30 * 9936)) + (cse_var_32 * 72)) + (d_inner * 36)) + (r_kh_outer * 18)) + (r_kh_inner * 6)) + cse_var_33) + r_kw_inner))+(3312*0), (((((((((co_inner * 188784) + (cse_var_30 * 9936)) + (cse_var_32 * 72)) + (d_inner * 36)) + (r_kh_outer * 18)) + (r_kh_inner * 6)) + cse_var_33) + r_kw_inner))+(3312*1), (((((((((co_inner * 188784) + (cse_var_30 * 9936)) + (cse_var_32 * 72)) + (d_inner * 36)) + (r_kh_outer * 18)) + (r_kh_inner * 6)) + cse_var_33) + r_kw_inner))+(3312*2));\n                  *(float3*)(((float*)transform_weight_1) + cse_var_1) = ((float3(((float*)transform_weight_1)[v_.s0],((float*)transform_weight_1)[v_.s1],((float*)transform_weight_1)[v_.s2])) + (((float3(((float*)data_1)[v__1.s0],((float*)data_1)[v__1.s1],((float*)data_1)[v__1.s2])) * ((float3)(((cse_var_21 && cse_var_8) ? 1.000000e+00f : ((cse_var_21 && cse_var_9) ? 0.000000e+00f : ((cse_var_21 && cse_var_10) ? 0.000000e+00f : ((cse_var_21 && cse_var_11) ? 0.000000e+00f : ((cse_var_21 && cse_var_12) ? 0.000000e+00f : ((cse_var_21 && cse_var_13) ? 0.000000e+00f : ((cse_var_22 && cse_var_8) ? 5.555556e-02f : ((cse_var_22 && cse_var_9) ? -1.111111e-01f : ((cse_var_22 && cse_var_10) ? 2.222222e-01f : ((cse_var_22 && cse_var_11) ? -4.444444e-01f : ((cse_var_22 && cse_var_12) ? 8.888889e-01f : ((cse_var_22 && cse_var_13) ? -1.777778e+00f : ((cse_var_23 && cse_var_8) ? 1.422222e+00f : ((cse_var_23 && cse_var_9) ? -7.111111e-01f : ((cse_var_23 && cse_var_10) ? 3.555556e-01f : ((cse_var_23 && cse_var_11) ? -1.777778e-01f : ((cse_var_23 && cse_var_12) ? 8.888889e-02f : ((cse_var_23 && cse_var_13) ? -4.444445e-02f : ((cse_var_24 && cse_var_8) ? -3.333334e-02f : ((cse_var_24 && cse_var_9) ? -6.666667e-02f : ((cse_var_24 && cse_var_10) ? -1.333333e-01f : ((cse_var_24 && cse_var_11) ? -2.666667e-01f : ((cse_var_24 && cse_var_12) ? -5.333334e-01f : ((cse_var_24 && cse_var_13) ? -1.066667e+00f : ((cse_var_25 && cse_var_8) ? 2.222222e-01f : ((cse_var_25 && cse_var_9) ? 2.222222e-01f : ((cse_var_25 && cse_var_10) ? 2.222222e-01f : ((cse_var_25 && cse_var_11) ? 2.222222e-01f : ((cse_var_25 && cse_var_12) ? 2.222222e-01f : ((cse_var_25 && cse_var_13) ? 2.222222e-01f : ((cse_var_26 && cse_var_8) ? -6.666667e-01f : ((cse_var_26 && cse_var_9) ? 6.666667e-01f : ((cse_var_26 && cse_var_10) ? -6.666667e-01f : ((cse_var_26 && cse_var_11) ? 6.666667e-01f : ((cse_var_26 && cse_var_12) ? -6.666667e-01f : ((cse_var_26 && cse_var_13) ? 6.666667e-01f : ((cse_var_27 && cse_var_8) ? 0.000000e+00f : ((cse_var_27 && cse_var_9) ? 0.000000e+00f : ((cse_var_27 && cse_var_10) ? 0.000000e+00f : ((cse_var_27 && cse_var_11) ? 0.000000e+00f : ((cse_var_27 && cse_var_12) ? 0.000000e+00f : ((cse_var_27 && cse_var_13) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_21 && cse_var_8) ? 1.000000e+00f : ((cse_var_21 && cse_var_9) ? 0.000000e+00f : ((cse_var_21 && cse_var_10) ? 0.000000e+00f : ((cse_var_21 && cse_var_11) ? 0.000000e+00f : ((cse_var_21 && cse_var_12) ? 0.000000e+00f : ((cse_var_21 && cse_var_13) ? 0.000000e+00f : ((cse_var_22 && cse_var_8) ? 5.555556e-02f : ((cse_var_22 && cse_var_9) ? -1.111111e-01f : ((cse_var_22 && cse_var_10) ? 2.222222e-01f : ((cse_var_22 && cse_var_11) ? -4.444444e-01f : ((cse_var_22 && cse_var_12) ? 8.888889e-01f : ((cse_var_22 && cse_var_13) ? -1.777778e+00f : ((cse_var_23 && cse_var_8) ? 1.422222e+00f : ((cse_var_23 && cse_var_9) ? -7.111111e-01f : ((cse_var_23 && cse_var_10) ? 3.555556e-01f : ((cse_var_23 && cse_var_11) ? -1.777778e-01f : ((cse_var_23 && cse_var_12) ? 8.888889e-02f : ((cse_var_23 && cse_var_13) ? -4.444445e-02f : ((cse_var_24 && cse_var_8) ? -3.333334e-02f : ((cse_var_24 && cse_var_9) ? -6.666667e-02f : ((cse_var_24 && cse_var_10) ? -1.333333e-01f : ((cse_var_24 && cse_var_11) ? -2.666667e-01f : ((cse_var_24 && cse_var_12) ? -5.333334e-01f : ((cse_var_24 && cse_var_13) ? -1.066667e+00f : ((cse_var_25 && cse_var_8) ? 2.222222e-01f : ((cse_var_25 && cse_var_9) ? 2.222222e-01f : ((cse_var_25 && cse_var_10) ? 2.222222e-01f : ((cse_var_25 && cse_var_11) ? 2.222222e-01f : ((cse_var_25 && cse_var_12) ? 2.222222e-01f : ((cse_var_25 && cse_var_13) ? 2.222222e-01f : ((cse_var_26 && cse_var_8) ? -6.666667e-01f : ((cse_var_26 && cse_var_9) ? 6.666667e-01f : ((cse_var_26 && cse_var_10) ? -6.666667e-01f : ((cse_var_26 && cse_var_11) ? 6.666667e-01f : ((cse_var_26 && cse_var_12) ? -6.666667e-01f : ((cse_var_26 && cse_var_13) ? 6.666667e-01f : ((cse_var_27 && cse_var_8) ? 0.000000e+00f : ((cse_var_27 && cse_var_9) ? 0.000000e+00f : ((cse_var_27 && cse_var_10) ? 0.000000e+00f : ((cse_var_27 && cse_var_11) ? 0.000000e+00f : ((cse_var_27 && cse_var_12) ? 0.000000e+00f : ((cse_var_27 && cse_var_13) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_21 && cse_var_8) ? 1.000000e+00f : ((cse_var_21 && cse_var_9) ? 0.000000e+00f : ((cse_var_21 && cse_var_10) ? 0.000000e+00f : ((cse_var_21 && cse_var_11) ? 0.000000e+00f : ((cse_var_21 && cse_var_12) ? 0.000000e+00f : ((cse_var_21 && cse_var_13) ? 0.000000e+00f : ((cse_var_22 && cse_var_8) ? 5.555556e-02f : ((cse_var_22 && cse_var_9) ? -1.111111e-01f : ((cse_var_22 && cse_var_10) ? 2.222222e-01f : ((cse_var_22 && cse_var_11) ? -4.444444e-01f : ((cse_var_22 && cse_var_12) ? 8.888889e-01f : ((cse_var_22 && cse_var_13) ? -1.777778e+00f : ((cse_var_23 && cse_var_8) ? 1.422222e+00f : ((cse_var_23 && cse_var_9) ? -7.111111e-01f : ((cse_var_23 && cse_var_10) ? 3.555556e-01f : ((cse_var_23 && cse_var_11) ? -1.777778e-01f : ((cse_var_23 && cse_var_12) ? 8.888889e-02f : ((cse_var_23 && cse_var_13) ? -4.444445e-02f : ((cse_var_24 && cse_var_8) ? -3.333334e-02f : ((cse_var_24 && cse_var_9) ? -6.666667e-02f : ((cse_var_24 && cse_var_10) ? -1.333333e-01f : ((cse_var_24 && cse_var_11) ? -2.666667e-01f : ((cse_var_24 && cse_var_12) ? -5.333334e-01f : ((cse_var_24 && cse_var_13) ? -1.066667e+00f : ((cse_var_25 && cse_var_8) ? 2.222222e-01f : ((cse_var_25 && cse_var_9) ? 2.222222e-01f : ((cse_var_25 && cse_var_10) ? 2.222222e-01f : ((cse_var_25 && cse_var_11) ? 2.222222e-01f : ((cse_var_25 && cse_var_12) ? 2.222222e-01f : ((cse_var_25 && cse_var_13) ? 2.222222e-01f : ((cse_var_26 && cse_var_8) ? -6.666667e-01f : ((cse_var_26 && cse_var_9) ? 6.666667e-01f : ((cse_var_26 && cse_var_10) ? -6.666667e-01f : ((cse_var_26 && cse_var_11) ? 6.666667e-01f : ((cse_var_26 && cse_var_12) ? -6.666667e-01f : ((cse_var_26 && cse_var_13) ? 6.666667e-01f : ((cse_var_27 && cse_var_8) ? 0.000000e+00f : ((cse_var_27 && cse_var_9) ? 0.000000e+00f : ((cse_var_27 && cse_var_10) ? 0.000000e+00f : ((cse_var_27 && cse_var_11) ? 0.000000e+00f : ((cse_var_27 && cse_var_12) ? 0.000000e+00f : ((cse_var_27 && cse_var_13) ? 2.000000e+00f : 0.000000e+00f))))))))))))))))))))))))))))))))))))))))))))) * ((float3)(((cse_var_14 && cse_var_2) ? 1.000000e+00f : ((cse_var_14 && cse_var_3) ? 0.000000e+00f : ((cse_var_14 && cse_var_4) ? 0.000000e+00f : ((cse_var_14 && cse_var_5) ? 0.000000e+00f : ((cse_var_14 && cse_var_6) ? 0.000000e+00f : ((cse_var_14 && cse_var_7) ? 0.000000e+00f : ((cse_var_15 && cse_var_2) ? 5.555556e-02f : ((cse_var_15 && cse_var_3) ? -1.111111e-01f : ((cse_var_15 && cse_var_4) ? 2.222222e-01f : ((cse_var_15 && cse_var_5) ? -4.444444e-01f : ((cse_var_15 && cse_var_6) ? 8.888889e-01f : ((cse_var_15 && cse_var_7) ? -1.777778e+00f : ((cse_var_16 && cse_var_2) ? 1.422222e+00f : ((cse_var_16 && cse_var_3) ? -7.111111e-01f : ((cse_var_16 && cse_var_4) ? 3.555556e-01f : ((cse_var_16 && cse_var_5) ? -1.777778e-01f : ((cse_var_16 && cse_var_6) ? 8.888889e-02f : ((cse_var_16 && cse_var_7) ? -4.444445e-02f : ((cse_var_17 && cse_var_2) ? -3.333334e-02f : ((cse_var_17 && cse_var_3) ? -6.666667e-02f : ((cse_var_17 && cse_var_4) ? -1.333333e-01f : ((cse_var_17 && cse_var_5) ? -2.666667e-01f : ((cse_var_17 && cse_var_6) ? -5.333334e-01f : ((cse_var_17 && cse_var_7) ? -1.066667e+00f : ((cse_var_18 && cse_var_2) ? 2.222222e-01f : ((cse_var_18 && cse_var_3) ? 2.222222e-01f : ((cse_var_18 && cse_var_4) ? 2.222222e-01f : ((cse_var_18 && cse_var_5) ? 2.222222e-01f : ((cse_var_18 && cse_var_6) ? 2.222222e-01f : ((cse_var_18 && cse_var_7) ? 2.222222e-01f : ((cse_var_19 && cse_var_2) ? -6.666667e-01f : ((cse_var_19 && cse_var_3) ? 6.666667e-01f : ((cse_var_19 && cse_var_4) ? -6.666667e-01f : ((cse_var_19 && cse_var_5) ? 6.666667e-01f : ((cse_var_19 && cse_var_6) ? -6.666667e-01f : ((cse_var_19 && cse_var_7) ? 6.666667e-01f : ((cse_var_20 && cse_var_2) ? 0.000000e+00f : ((cse_var_20 && cse_var_3) ? 0.000000e+00f : ((cse_var_20 && cse_var_4) ? 0.000000e+00f : ((cse_var_20 && cse_var_5) ? 0.000000e+00f : ((cse_var_20 && cse_var_6) ? 0.000000e+00f : ((cse_var_20 && cse_var_7) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_14 && cse_var_2) ? 1.000000e+00f : ((cse_var_14 && cse_var_3) ? 0.000000e+00f : ((cse_var_14 && cse_var_4) ? 0.000000e+00f : ((cse_var_14 && cse_var_5) ? 0.000000e+00f : ((cse_var_14 && cse_var_6) ? 0.000000e+00f : ((cse_var_14 && cse_var_7) ? 0.000000e+00f : ((cse_var_15 && cse_var_2) ? 5.555556e-02f : ((cse_var_15 && cse_var_3) ? -1.111111e-01f : ((cse_var_15 && cse_var_4) ? 2.222222e-01f : ((cse_var_15 && cse_var_5) ? -4.444444e-01f : ((cse_var_15 && cse_var_6) ? 8.888889e-01f : ((cse_var_15 && cse_var_7) ? -1.777778e+00f : ((cse_var_16 && cse_var_2) ? 1.422222e+00f : ((cse_var_16 && cse_var_3) ? -7.111111e-01f : ((cse_var_16 && cse_var_4) ? 3.555556e-01f : ((cse_var_16 && cse_var_5) ? -1.777778e-01f : ((cse_var_16 && cse_var_6) ? 8.888889e-02f : ((cse_var_16 && cse_var_7) ? -4.444445e-02f : ((cse_var_17 && cse_var_2) ? -3.333334e-02f : ((cse_var_17 && cse_var_3) ? -6.666667e-02f : ((cse_var_17 && cse_var_4) ? -1.333333e-01f : ((cse_var_17 && cse_var_5) ? -2.666667e-01f : ((cse_var_17 && cse_var_6) ? -5.333334e-01f : ((cse_var_17 && cse_var_7) ? -1.066667e+00f : ((cse_var_18 && cse_var_2) ? 2.222222e-01f : ((cse_var_18 && cse_var_3) ? 2.222222e-01f : ((cse_var_18 && cse_var_4) ? 2.222222e-01f : ((cse_var_18 && cse_var_5) ? 2.222222e-01f : ((cse_var_18 && cse_var_6) ? 2.222222e-01f : ((cse_var_18 && cse_var_7) ? 2.222222e-01f : ((cse_var_19 && cse_var_2) ? -6.666667e-01f : ((cse_var_19 && cse_var_3) ? 6.666667e-01f : ((cse_var_19 && cse_var_4) ? -6.666667e-01f : ((cse_var_19 && cse_var_5) ? 6.666667e-01f : ((cse_var_19 && cse_var_6) ? -6.666667e-01f : ((cse_var_19 && cse_var_7) ? 6.666667e-01f : ((cse_var_20 && cse_var_2) ? 0.000000e+00f : ((cse_var_20 && cse_var_3) ? 0.000000e+00f : ((cse_var_20 && cse_var_4) ? 0.000000e+00f : ((cse_var_20 && cse_var_5) ? 0.000000e+00f : ((cse_var_20 && cse_var_6) ? 0.000000e+00f : ((cse_var_20 && cse_var_7) ? 2.000000e+00f : 0.000000e+00f)))))))))))))))))))))))))))))))))))))))))), ((cse_var_14 && cse_var_2) ? 1.000000e+00f : ((cse_var_14 && cse_var_3) ? 0.000000e+00f : ((cse_var_14 && cse_var_4) ? 0.000000e+00f : ((cse_var_14 && cse_var_5) ? 0.000000e+00f : ((cse_var_14 && cse_var_6) ? 0.000000e+00f : ((cse_var_14 && cse_var_7) ? 0.000000e+00f : ((cse_var_15 && cse_var_2) ? 5.555556e-02f : ((cse_var_15 && cse_var_3) ? -1.111111e-01f : ((cse_var_15 && cse_var_4) ? 2.222222e-01f : ((cse_var_15 && cse_var_5) ? -4.444444e-01f : ((cse_var_15 && cse_var_6) ? 8.888889e-01f : ((cse_var_15 && cse_var_7) ? -1.777778e+00f : ((cse_var_16 && cse_var_2) ? 1.422222e+00f : ((cse_var_16 && cse_var_3) ? -7.111111e-01f : ((cse_var_16 && cse_var_4) ? 3.555556e-01f : ((cse_var_16 && cse_var_5) ? -1.777778e-01f : ((cse_var_16 && cse_var_6) ? 8.888889e-02f : ((cse_var_16 && cse_var_7) ? -4.444445e-02f : ((cse_var_17 && cse_var_2) ? -3.333334e-02f : ((cse_var_17 && cse_var_3) ? -6.666667e-02f : ((cse_var_17 && cse_var_4) ? -1.333333e-01f : ((cse_var_17 && cse_var_5) ? -2.666667e-01f : ((cse_var_17 && cse_var_6) ? -5.333334e-01f : ((cse_var_17 && cse_var_7) ? -1.066667e+00f : ((cse_var_18 && cse_var_2) ? 2.222222e-01f : ((cse_var_18 && cse_var_3) ? 2.222222e-01f : ((cse_var_18 && cse_var_4) ? 2.222222e-01f : ((cse_var_18 && cse_var_5) ? 2.222222e-01f : ((cse_var_18 && cse_var_6) ? 2.222222e-01f : ((cse_var_18 && cse_var_7) ? 2.222222e-01f : ((cse_var_19 && cse_var_2) ? -6.666667e-01f : ((cse_var_19 && cse_var_3) ? 6.666667e-01f : ((cse_var_19 && cse_var_4) ? -6.666667e-01f : ((cse_var_19 && cse_var_5) ? 6.666667e-01f : ((cse_var_19 && cse_var_6) ? -6.666667e-01f : ((cse_var_19 && cse_var_7) ? 6.666667e-01f : ((cse_var_20 && cse_var_2) ? 0.000000e+00f : ((cse_var_20 && cse_var_3) ? 0.000000e+00f : ((cse_var_20 && cse_var_4) ? 0.000000e+00f : ((cse_var_20 && cse_var_5) ? 0.000000e+00f : ((cse_var_20 && cse_var_6) ? 0.000000e+00f : ((cse_var_20 && cse_var_7) ? 2.000000e+00f : 0.000000e+00f))))))))))))))))))))))))))))))))))))))))))))));\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(483) default_function_kernel(float* __restrict__ data, float* __restrict__ transform_weight);\nextern \"C\" __global__ void __launch_bounds__(483) default_function_kernel(float* __restrict__ data, float* __restrict__ transform_weight) {\n  float transform_weight_local[28];\n  __shared__ float data_shared[9936];\n  __shared__ float G_shared[42];\n  for (int eps_c_outer_inner_init = 0; eps_c_outer_inner_init < 7; ++eps_c_outer_inner_init) {\n    for (int d_c_inner_init = 0; d_c_inner_init < 2; ++d_c_inner_init) {\n      transform_weight_local[((eps_c_outer_inner_init * 2) + d_c_inner_init)] = 0.000000e+00f;\n      transform_weight_local[(((eps_c_outer_inner_init * 2) + d_c_inner_init) + 14)] = 0.000000e+00f;\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer < 7; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 7) + (((int)threadIdx.x) / 69)) < 48) {\n      for (int ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s = 0; ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s < 3; ++ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s) {\n        data_shared[(((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 1449) + (((int)threadIdx.x) * 3)) + ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s)] = data[((((int)blockIdx.x) * 9936) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_outer_outer * 1449) + (((int)threadIdx.x) * 3)) + ax0_ax1_fused_ax2_fused_ax3_fused_ax4_fused_inner_s) % 9936))];\n      },\n    },\n  },\n  if (((int)threadIdx.x) < 42) {\n    G_shared[((int)threadIdx.x)] = ((((int)threadIdx.x) == 41) ? 1.000000e+00f : ((((int)threadIdx.x) == 40) ? 0.000000e+00f : ((((int)threadIdx.x) == 39) ? 0.000000e+00f : ((((int)threadIdx.x) == 38) ? 0.000000e+00f : ((((int)threadIdx.x) == 37) ? 0.000000e+00f : ((((int)threadIdx.x) == 36) ? 0.000000e+00f : ((((int)threadIdx.x) == 35) ? 5.555556e-02f : ((((int)threadIdx.x) == 34) ? -1.111111e-01f : ((((int)threadIdx.x) == 33) ? 2.222222e-01f : ((((int)threadIdx.x) == 32) ? -4.444444e-01f : ((((int)threadIdx.x) == 31) ? 8.888889e-01f : ((((int)threadIdx.x) == 30) ? -1.777778e+00f : ((((int)threadIdx.x) == 29) ? 1.422222e+00f : ((((int)threadIdx.x) == 28) ? -7.111111e-01f : ((((int)threadIdx.x) == 27) ? 3.555556e-01f : ((((int)threadIdx.x) == 26) ? -1.777778e-01f : ((((int)threadIdx.x) == 25) ? 8.888889e-02f : ((((int)threadIdx.x) == 24) ? -4.444445e-02f : ((((int)threadIdx.x) == 23) ? -3.333334e-02f : ((((int)threadIdx.x) == 22) ? -6.666667e-02f : ((((int)threadIdx.x) == 21) ? -1.333333e-01f : ((((int)threadIdx.x) == 20) ? -2.666667e-01f : ((((int)threadIdx.x) == 19) ? -5.333334e-01f : ((((int)threadIdx.x) == 18) ? -1.066667e+00f : ((((int)threadIdx.x) == 17) ? 2.222222e-01f : ((((int)threadIdx.x) == 16) ? 2.222222e-01f : ((((int)threadIdx.x) == 15) ? 2.222222e-01f : ((((int)threadIdx.x) == 14) ? 2.222222e-01f : ((((int)threadIdx.x) == 13) ? 2.222222e-01f : ((((int)threadIdx.x) == 12) ? 2.222222e-01f : ((((int)threadIdx.x) == 11) ? -6.666667e-01f : ((((int)threadIdx.x) == 10) ? 6.666667e-01f : ((((int)threadIdx.x) == 9) ? -6.666667e-01f : ((((int)threadIdx.x) == 8) ? 6.666667e-01f : ((((int)threadIdx.x) == 7) ? -6.666667e-01f : ((((int)threadIdx.x) == 6) ? 6.666667e-01f : ((((int)threadIdx.x) == 5) ? 0.000000e+00f : ((((int)threadIdx.x) == 4) ? 0.000000e+00f : ((((int)threadIdx.x) == 3) ? 0.000000e+00f : ((((int)threadIdx.x) == 2) ? 0.000000e+00f : ((((int)threadIdx.x) == 1) ? 0.000000e+00f : 2.000000e+00f)))))))))))))))))))))))))))))))))))))))));\n  },\n  __syncthreads();\n  for (int r_kh_outer_inner = 0; r_kh_outer_inner < 3; ++r_kh_outer_inner) {\n    for (int eps_c_outer_inner = 0; eps_c_outer_inner < 7; ++eps_c_outer_inner) {\n      for (int r_kh_inner = 0; r_kh_inner < 2; ++r_kh_inner) {\n        for (int r_kw_inner = 0; r_kw_inner < 6; ++r_kw_inner) {\n          for (int d_c_inner = 0; d_c_inner < 2; ++d_c_inner) {\n            transform_weight_local[((eps_c_outer_inner * 2) + d_c_inner)] = (transform_weight_local[((eps_c_outer_inner * 2) + d_c_inner)] + ((data_shared[(((((((((int)threadIdx.x) % 3) * 3312) + (((((int)threadIdx.x) % 69) / 3) * 72)) + (d_c_inner * 36)) + (r_kh_outer_inner * 12)) + (r_kh_inner * 6)) + r_kw_inner)] * G_shared[(((eps_c_outer_inner * 6) + (r_kh_outer_inner * 2)) + r_kh_inner)]) * G_shared[(((((int)threadIdx.x) / 69) * 6) + r_kw_inner)]));\n            transform_weight_local[(((eps_c_outer_inner * 2) + d_c_inner) + 14)] = (transform_weight_local[(((eps_c_outer_inner * 2) + d_c_inner) + 14)] + ((data_shared[((((((((((int)threadIdx.x) % 3) * 3312) + (((((int)threadIdx.x) % 69) / 3) * 72)) + (d_c_inner * 36)) + (r_kh_outer_inner * 12)) + (r_kh_inner * 6)) + r_kw_inner) + 1656)] * G_shared[(((eps_c_outer_inner * 6) + (r_kh_outer_inner * 2)) + r_kh_inner)]) * G_shared[(((((int)threadIdx.x) / 69) * 6) + r_kw_inner)]));\n          },\n        },\n      },\n    },\n  },\n  for (int eps_inner = 0; eps_inner < 7; ++eps_inner) {\n    for (int d_inner = 0; d_inner < 2; ++d_inner) {\n      transform_weight[((((((eps_inner * 73416) + ((((int)threadIdx.x) / 69) * 10488)) + (((((int)threadIdx.x) % 69) / 3) * 228)) + (d_inner * 114)) + (((int)blockIdx.x) * 3)) + (((int)threadIdx.x) % 3))] = transform_weight_local[((eps_inner * 2) + d_inner)];\n      transform_weight[(((((((eps_inner * 73416) + ((((int)threadIdx.x) / 69) * 10488)) + (((((int)threadIdx.x) % 69) / 3) * 228)) + (d_inner * 114)) + (((int)blockIdx.x) * 3)) + (((int)threadIdx.x) % 3)) + 5244)] = transform_weight_local[(((eps_inner * 2) + d_inner) + 14)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 57, 92, 6, 6), \"float32\"), transform_weight: T.Buffer((7, 7, 92, 2, 57), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused in T.parallel(6118):\n            transform_weight_1 = T.Buffer((513912,), data=transform_weight.data)\n            for eps_inner_init, d_inner_init, co_inner_init in T.grid(7, 2, 2):\n                transform_weight_1[eps_inner_init * 73416 + eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused % 7 * 10488 + eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused // 133 * 228 + d_inner_init * 114 + co_inner_init * 57 + eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused % 133 // 7 * 3:eps_inner_init * 73416 + eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused % 7 * 10488 + eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused // 133 * 228 + d_inner_init * 114 + co_inner_init * 57 + eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused % 133 // 7 * 3 + 3] = T.Broadcast(T.float32(0), 3)\n            for r_kh_outer, r_kw_outer, r_kh_inner, r_kw_inner, eps_inner, d_inner, co_inner in T.grid(2, 3, 3, 2, 7, 2, 2):\n                cse_var_33: T.int32 = r_kw_outer * 2\n                cse_var_32: T.int32 = eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused // 133\n                cse_var_31: T.int32 = eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused % 7\n                cse_var_30: T.int32 = eps_outer_outer_outer_nu_outer_outer_outer_fused_d_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused % 133 // 7\n                cse_var_29: T.int32 = cse_var_33 + r_kw_inner\n                cse_var_28: T.int32 = r_kh_outer * 3 + r_kh_inner\n                cse_var_27: T.bool = eps_inner == 0\n                cse_var_26: T.bool = eps_inner == 1\n                cse_var_25: T.bool = eps_inner == 2\n                cse_var_24: T.bool = eps_inner == 3\n                cse_var_23: T.bool = eps_inner == 4\n                cse_var_22: T.bool = eps_inner == 5\n                cse_var_21: T.bool = eps_inner == 6\n                cse_var_20: T.bool = cse_var_31 == 0\n                cse_var_19: T.bool = cse_var_31 == 1\n                cse_var_18: T.bool = cse_var_31 == 2\n                cse_var_17: T.bool = cse_var_31 == 3\n                cse_var_16: T.bool = cse_var_31 == 4\n                cse_var_15: T.bool = cse_var_31 == 5\n                cse_var_14: T.bool = cse_var_31 == 6\n                cse_var_13: T.bool = cse_var_28 == 0\n                cse_var_12: T.bool = cse_var_28 == 1\n                cse_var_11: T.bool = cse_var_28 == 2\n                cse_var_10: T.bool = cse_var_28 == 3\n                cse_var_9: T.bool = cse_var_28 == 4\n                cse_var_8: T.bool = cse_var_28 == 5\n                cse_var_7: T.bool = cse_var_29 == 0\n                cse_var_6: T.bool = cse_var_29 == 1\n                cse_var_5: T.bool = cse_var_29 == 2\n                cse_var_4: T.bool = cse_var_29 == 3\n                cse_var_3: T.bool = cse_var_29 == 4\n                cse_var_2: T.bool = cse_var_29 == 5\n                cse_var_1: T.int32 = eps_inner * 73416 + cse_var_31 * 10488 + cse_var_32 * 228 + d_inner * 114 + co_inner * 57 + cse_var_30 * 3\n                data_1 = T.Buffer((377568,), data=data.data)\n                transform_weight_1[cse_var_1:cse_var_1 + 3] = transform_weight_1[cse_var_1:cse_var_1 + 3] + data_1[co_inner * 188784 + cse_var_30 * 9936 + cse_var_32 * 72 + d_inner * 36 + r_kh_outer * 18 + r_kh_inner * 6 + cse_var_33 + r_kw_inner:co_inner * 188784 + cse_var_30 * 9936 + cse_var_32 * 72 + d_inner * 36 + r_kh_outer * 18 + r_kh_inner * 6 + cse_var_33 + r_kw_inner + 9936:3312] * T.Broadcast(T.Select(cse_var_21 and cse_var_8, T.float32(1), T.Select(cse_var_21 and cse_var_9, T.float32(0), T.Select(cse_var_21 and cse_var_10, T.float32(0), T.Select(cse_var_21 and cse_var_11, T.float32(0), T.Select(cse_var_21 and cse_var_12, T.float32(0), T.Select(cse_var_21 and cse_var_13, T.float32(0), T.Select(cse_var_22 and cse_var_8, T.float32(0.0555555559694767), T.Select(cse_var_22 and cse_var_9, T.float32(-0.1111111119389534), T.Select(cse_var_22 and cse_var_10, T.float32(0.2222222238779068), T.Select(cse_var_22 and cse_var_11, T.float32(-0.4444444477558136), T.Select(cse_var_22 and cse_var_12, T.float32(0.8888888955116272), T.Select(cse_var_22 and cse_var_13, T.float32(-1.7777777910232544), T.Select(cse_var_23 and cse_var_8, T.float32(1.4222222566604614), T.Select(cse_var_23 and cse_var_9, T.float32(-0.71111112833023071), T.Select(cse_var_23 and cse_var_10, T.float32(0.35555556416511536), T.Select(cse_var_23 and cse_var_11, T.float32(-0.17777778208255768), T.Select(cse_var_23 and cse_var_12, T.float32(0.088888891041278839), T.Select(cse_var_23 and cse_var_13, T.float32(-0.04444444552063942), T.Select(cse_var_24 and cse_var_8, T.float32(-0.033333335071802139), T.Select(cse_var_24 and cse_var_9, T.float32(-0.066666670143604279), T.Select(cse_var_24 and cse_var_10, T.float32(-0.13333334028720856), T.Select(cse_var_24 and cse_var_11, T.float32(-0.26666668057441711), T.Select(cse_var_24 and cse_var_12, T.float32(-0.53333336114883423), T.Select(cse_var_24 and cse_var_13, T.float32(-1.0666667222976685), T.Select(cse_var_25 and cse_var_8, T.float32(0.2222222238779068), T.Select(cse_var_25 and cse_var_9, T.float32(0.2222222238779068), T.Select(cse_var_25 and cse_var_10, T.float32(0.2222222238779068), T.Select(cse_var_25 and cse_var_11, T.float32(0.2222222238779068), T.Select(cse_var_25 and cse_var_12, T.float32(0.2222222238779068), T.Select(cse_var_25 and cse_var_13, T.float32(0.2222222238779068), T.Select(cse_var_26 and cse_var_8, T.float32(-0.66666668653488159), T.Select(cse_var_26 and cse_var_9, T.float32(0.66666668653488159), T.Select(cse_var_26 and cse_var_10, T.float32(-0.66666668653488159), T.Select(cse_var_26 and cse_var_11, T.float32(0.66666668653488159), T.Select(cse_var_26 and cse_var_12, T.float32(-0.66666668653488159), T.Select(cse_var_26 and cse_var_13, T.float32(0.66666668653488159), T.Select(cse_var_27 and cse_var_8, T.float32(0), T.Select(cse_var_27 and cse_var_9, T.float32(0), T.Select(cse_var_27 and cse_var_10, T.float32(0), T.Select(cse_var_27 and cse_var_11, T.float32(0), T.Select(cse_var_27 and cse_var_12, T.float32(0), T.Select(cse_var_27 and cse_var_13, T.float32(2), T.float32(0))))))))))))))))))))))))))))))))))))))))))), 3) * T.Broadcast(T.Select(cse_var_14 and cse_var_2, T.float32(1), T.Select(cse_var_14 and cse_var_3, T.float32(0), T.Select(cse_var_14 and cse_var_4, T.float32(0), T.Select(cse_var_14 and cse_var_5, T.float32(0), T.Select(cse_var_14 and cse_var_6, T.float32(0), T.Select(cse_var_14 and cse_var_7, T.float32(0), T.Select(cse_var_15 and cse_var_2, T.float32(0.0555555559694767), T.Select(cse_var_15 and cse_var_3, T.float32(-0.1111111119389534), T.Select(cse_var_15 and cse_var_4, T.float32(0.2222222238779068), T.Select(cse_var_15 and cse_var_5, T.float32(-0.4444444477558136), T.Select(cse_var_15 and cse_var_6, T.float32(0.8888888955116272), T.Select(cse_var_15 and cse_var_7, T.float32(-1.7777777910232544), T.Select(cse_var_16 and cse_var_2, T.float32(1.4222222566604614), T.Select(cse_var_16 and cse_var_3, T.float32(-0.71111112833023071), T.Select(cse_var_16 and cse_var_4, T.float32(0.35555556416511536), T.Select(cse_var_16 and cse_var_5, T.float32(-0.17777778208255768), T.Select(cse_var_16 and cse_var_6, T.float32(0.088888891041278839), T.Select(cse_var_16 and cse_var_7, T.float32(-0.04444444552063942), T.Select(cse_var_17 and cse_var_2, T.float32(-0.033333335071802139), T.Select(cse_var_17 and cse_var_3, T.float32(-0.066666670143604279), T.Select(cse_var_17 and cse_var_4, T.float32(-0.13333334028720856), T.Select(cse_var_17 and cse_var_5, T.float32(-0.26666668057441711), T.Select(cse_var_17 and cse_var_6, T.float32(-0.53333336114883423), T.Select(cse_var_17 and cse_var_7, T.float32(-1.0666667222976685), T.Select(cse_var_18 and cse_var_2, T.float32(0.2222222238779068), T.Select(cse_var_18 and cse_var_3, T.float32(0.2222222238779068), T.Select(cse_var_18 and cse_var_4, T.float32(0.2222222238779068), T.Select(cse_var_18 and cse_var_5, T.float32(0.2222222238779068), T.Select(cse_var_18 and cse_var_6, T.float32(0.2222222238779068), T.Select(cse_var_18 and cse_var_7, T.float32(0.2222222238779068), T.Select(cse_var_19 and cse_var_2, T.float32(-0.66666668653488159), T.Select(cse_var_19 and cse_var_3, T.float32(0.66666668653488159), T.Select(cse_var_19 and cse_var_4, T.float32(-0.66666668653488159), T.Select(cse_var_19 and cse_var_5, T.float32(0.66666668653488159), T.Select(cse_var_19 and cse_var_6, T.float32(-0.66666668653488159), T.Select(cse_var_19 and cse_var_7, T.float32(0.66666668653488159), T.Select(cse_var_20 and cse_var_2, T.float32(0), T.Select(cse_var_20 and cse_var_3, T.float32(0), T.Select(cse_var_20 and cse_var_4, T.float32(0), T.Select(cse_var_20 and cse_var_5, T.float32(0), T.Select(cse_var_20 and cse_var_6, T.float32(0), T.Select(cse_var_20 and cse_var_7, T.float32(2), T.float32(0))))))))))))))))))))))))))))))))))))))))))), 3)",
        "data": "2_57_92_6_6"
    },
    {
        "op_name": "depthwise_conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t DepthwiseConv2d_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* DepthwiseConv2d = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* DepthwiseConv2d_1 = (((DLTensor*)DepthwiseConv2d)[0].data);\n  void* default_function_DepthwiseConv2d_shape = (((DLTensor*)DepthwiseConv2d)[0].shape);\n  void* default_function_DepthwiseConv2d_strides = (((DLTensor*)DepthwiseConv2d)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_DepthwiseConv2d_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 606515; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t c_inner_init = 0; c_inner_init < 5; ++c_inner_init) {\n      for (int32_t i_inner_init = 0; i_inner_init < 2; ++i_inner_init) {\n        ((float*)DepthwiseConv2d_1)[((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 155) * 1550) + (c_inner_init * 310)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 155) / 5) * 10)) + (i_inner_init * 5)) + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5))] = 0.000000e+00f;\n      },\n    },\n    for (int32_t di_outer = 0; di_outer < 4; ++di_outer) {\n      for (int32_t dj_inner = 0; dj_inner < 3; ++dj_inner) {\n        for (int32_t c_inner = 0; c_inner < 5; ++c_inner) {\n          for (int32_t i_inner = 0; i_inner < 2; ++i_inner) {\n            int32_t cse_var_3 = (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5);\n            int32_t cse_var_2 = ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 155) / 5);\n            int32_t cse_var_1 = ((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 155) * 1550) + (c_inner * 310)) + (cse_var_2 * 10)) + (i_inner * 5)) + cse_var_3);\n            ((float*)DepthwiseConv2d_1)[cse_var_1] = (((float*)DepthwiseConv2d_1)[cse_var_1] + (((float*)data_1)[(((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 2015) * 455) + (cse_var_2 * 14)) + (i_inner * 7)) + (di_outer * 7)) + dj_inner) + cse_var_3)] * ((float*)kernel_1)[((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 86645) / 155) * 60) + (c_inner * 12)) + (di_outer * 3)) + dj_inner)]));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel) {\n  float DepthwiseConv2d_local[1];\n  __shared__ float PaddedInput_shared[196];\n  __shared__ float kernel_shared[12];\n  DepthwiseConv2d_local[0] = 0.000000e+00f;\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 6; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 5) + (((int)threadIdx.x) / 7)) < 28) {\n      PaddedInput_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 35) + ((int)threadIdx.x))] = data[((((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 5) + (((int)threadIdx.x) / 7)) >> 2) * 19565) + ((((int)blockIdx.x) / 4030) * 455)) + ((((int)blockIdx.x) % 62) * 7)) + ((((((int)threadIdx.x) / 7) + ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) & 3) * 7)) + (((int)threadIdx.x) % 7))];\n    },\n  },\n  if (((int)threadIdx.x) < 12) {\n    if (0 <= ((((int)blockIdx.x) / 62) - ((((int)blockIdx.x) / 4030) * 65))) {\n      kernel_shared[((int)threadIdx.x)] = kernel[(((((int)blockIdx.x) / 62) * 12) + ((int)threadIdx.x))];\n    },\n  },\n  __syncthreads();\n  for (int di_outer_inner = 0; di_outer_inner < 2; ++di_outer_inner) {\n    for (int di_inner = 0; di_inner < 2; ++di_inner) {\n      for (int dj_inner = 0; dj_inner < 3; ++dj_inner) {\n        DepthwiseConv2d_local[0] = (DepthwiseConv2d_local[0] + (PaddedInput_shared[((((((((int)threadIdx.x) / 5) * 28) + (di_outer_inner * 14)) + (di_inner * 7)) + dj_inner) + (((int)threadIdx.x) % 5))] * kernel_shared[(((di_outer_inner * 6) + (di_inner * 3)) + dj_inner)]));\n      },\n    },\n  },\n  DepthwiseConv2d[((((((int)threadIdx.x) / 5) * 866450) + (((int)blockIdx.x) * 5)) + (((int)threadIdx.x) % 5))] = DepthwiseConv2d_local[0];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 43, 65, 7), \"float32\"), kernel: T.Buffer((43, 65, 4, 3), \"float32\"), DepthwiseConv2d: T.Buffer((7, 2795, 62, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(606515):\n            DepthwiseConv2d_1 = T.Buffer((6065150,), data=DepthwiseConv2d.data)\n            for c_inner_init, i_inner_init in T.grid(5, 2):\n                DepthwiseConv2d_1[b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 155 * 1550 + c_inner_init * 310 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 155 // 5 * 10 + i_inner_init * 5 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5] = T.float32(0)\n            for di_outer, dj_inner, c_inner, i_inner in T.grid(4, 3, 5, 2):\n                cse_var_3: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5\n                cse_var_2: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 155 // 5\n                cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 155 * 1550 + c_inner * 310 + cse_var_2 * 10 + i_inner * 5 + cse_var_3\n                data_1 = T.Buffer((136955,), data=data.data)\n                kernel_1 = T.Buffer((33540,), data=kernel.data)\n                DepthwiseConv2d_1[cse_var_1] = DepthwiseConv2d_1[cse_var_1] + data_1[b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 2015 * 455 + cse_var_2 * 14 + i_inner * 7 + di_outer * 7 + dj_inner + cse_var_3] * kernel_1[b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 86645 // 155 * 60 + c_inner * 12 + di_outer * 3 + dj_inner]",
        "data": "7_43_65_7",
        "kernel": "43_65_4_3"
    },
    {
        "op_name": "depthwise_conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t DepthwiseConv2d_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* DepthwiseConv2d = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* DepthwiseConv2d_1 = (((DLTensor*)DepthwiseConv2d)[0].data);\n  void* default_function_DepthwiseConv2d_shape = (((DLTensor*)DepthwiseConv2d)[0].shape);\n  void* default_function_DepthwiseConv2d_strides = (((DLTensor*)DepthwiseConv2d)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_DepthwiseConv2d_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused = 0; b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused < 484; ++b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused) {\n    void* DepthwiseConv2d_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)8400, 2, 32);\n    if (DepthwiseConv2d_local == NULL) {\n      return -1;\n    },\n    for (int32_t c_c_outer_inner_init = 0; c_c_outer_inner_init < 10; ++c_c_outer_inner_init) {\n      for (int32_t c_c_inner_init = 0; c_c_inner_init < 10; ++c_c_inner_init) {\n        for (int32_t i_c_inner_init = 0; i_c_inner_init < 7; ++i_c_inner_init) {\n          ((float3*)DepthwiseConv2d_local)[(((c_c_outer_inner_init * 70) + (c_c_inner_init * 7)) + i_c_inner_init)] = ((float3)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n        },\n      },\n    },\n    for (int32_t dj_outer = 0; dj_outer < 4; ++dj_outer) {\n      for (int32_t c_c_outer_inner = 0; c_c_outer_inner < 10; ++c_c_outer_inner) {\n        for (int32_t di_inner = 0; di_inner < 4; ++di_inner) {\n          for (int32_t c_c_inner = 0; c_c_inner < 10; ++c_c_inner) {\n            for (int32_t i_c_inner = 0; i_c_inner < 7; ++i_c_inner) {\n              int32_t cse_var_2 = (b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused / 11);\n              int32_t cse_var_1 = (((c_c_outer_inner * 70) + (c_c_inner * 7)) + i_c_inner);\n              int32_t3 v_ = int32_t3((((((((((cse_var_2 * 5) + (c_c_outer_inner >> 1)) >> 2) * 480) + ((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 11) * 42)) + (i_c_inner * 6)) + (di_inner * 6)) + dj_outer))+(1*0), (((((((((cse_var_2 * 5) + (c_c_outer_inner >> 1)) >> 2) * 480) + ((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 11) * 42)) + (i_c_inner * 6)) + (di_inner * 6)) + dj_outer))+(1*1), (((((((((cse_var_2 * 5) + (c_c_outer_inner >> 1)) >> 2) * 480) + ((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 11) * 42)) + (i_c_inner * 6)) + (di_inner * 6)) + dj_outer))+(1*2));\n              ((float3*)DepthwiseConv2d_local)[cse_var_1] = (((float3*)DepthwiseConv2d_local)[cse_var_1] + ((float3(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2])) * ((float3)(((float*)kernel_1)[(((((cse_var_2 * 1600) + (c_c_outer_inner * 160)) + (c_c_inner * 16)) + (di_inner * 4)) + dj_outer)], ((float*)kernel_1)[(((((cse_var_2 * 1600) + (c_c_outer_inner * 160)) + (c_c_inner * 16)) + (di_inner * 4)) + dj_outer)], ((float*)kernel_1)[(((((cse_var_2 * 1600) + (c_c_outer_inner * 160)) + (c_c_inner * 16)) + (di_inner * 4)) + dj_outer)]))));\n            },\n          },\n        },\n      },\n    },\n    for (int32_t c_inner = 0; c_inner < 100; ++c_inner) {\n      for (int32_t i_inner = 0; i_inner < 7; ++i_inner) {\n        *(float3*)(((float*)DepthwiseConv2d_1) + (((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused / 11) * 23100) + (c_inner * 231)) + ((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 11) * 21)) + (i_inner * 3))) = ((float3*)DepthwiseConv2d_local)[((c_inner * 7) + i_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, DepthwiseConv2d_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(616) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(616) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel) {\n  float DepthwiseConv2d_local[10];\n  __shared__ float PaddedInput_shared[176];\n  __shared__ float kernel_shared[3520];\n  for (int c_c_inner_init = 0; c_c_inner_init < 10; ++c_c_inner_init) {\n    DepthwiseConv2d_local[c_c_inner_init] = 0.000000e+00f;\n  },\n  for (int di_outer_outer = 0; di_outer_outer < 2; ++di_outer_outer) {\n    for (int dj_outer_outer = 0; dj_outer_outer < 2; ++dj_outer_outer) {\n      __syncthreads();\n      if (((int)threadIdx.x) < 88) {\n        int2 v_ = make_int2((((((((((((int)blockIdx.x) / 33) * 5280) + ((((int)threadIdx.x) >> 3) * 480)) + (((((int)blockIdx.x) % 33) / 3) * 42)) + (di_outer_outer * 12)) + ((((int)threadIdx.x) & 7) * 6)) + (dj_outer_outer * 2)) + (((int)blockIdx.x) % 3)))+(1*0), (((((((((((int)blockIdx.x) / 33) * 5280) + ((((int)threadIdx.x) >> 3) * 480)) + (((((int)blockIdx.x) % 33) / 3) * 42)) + (di_outer_outer * 12)) + ((((int)threadIdx.x) & 7) * 6)) + (dj_outer_outer * 2)) + (((int)blockIdx.x) % 3)))+(1*1));\n        *(float2*)(PaddedInput_shared + (((int)threadIdx.x) * 2)) = make_float2(data[v_.x],data[v_.y]);\n      },\n      for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 6; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n        if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 7) + (((int)threadIdx.x) / 88)) < 40) {\n          kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 616) + ((int)threadIdx.x))] = kernel[((((((((((int)blockIdx.x) / 33) * 14080) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 2464)) + ((((int)threadIdx.x) >> 2) * 16)) + (di_outer_outer * 8)) + (((((int)threadIdx.x) & 3) >> 1) * 4)) + (dj_outer_outer * 2)) + (((int)threadIdx.x) & 1))];\n        },\n      },\n      __syncthreads();\n      for (int dj_outer_inner = 0; dj_outer_inner < 2; ++dj_outer_inner) {\n        for (int di_inner = 0; di_inner < 2; ++di_inner) {\n          for (int c_c_inner = 0; c_c_inner < 10; ++c_c_inner) {\n            DepthwiseConv2d_local[c_c_inner] = (DepthwiseConv2d_local[c_c_inner] + (PaddedInput_shared[(((((((int)threadIdx.x) / 56) * 16) + (di_inner * 2)) + ((((int)threadIdx.x) % 7) * 2)) + dj_outer_inner)] * kernel_shared[(((((((int)threadIdx.x) / 7) * 40) + (c_c_inner * 4)) + (di_inner * 2)) + dj_outer_inner)]));\n          },\n        },\n      },\n    },\n  },\n  for (int c_inner = 0; c_inner < 10; ++c_inner) {\n    DepthwiseConv2d[(((((((((int)blockIdx.x) / 33) * 203280) + ((((int)threadIdx.x) / 7) * 2310)) + (c_inner * 231)) + (((((int)blockIdx.x) % 33) / 3) * 21)) + ((((int)threadIdx.x) % 7) * 3)) + (((int)blockIdx.x) % 3))] = DepthwiseConv2d_local[c_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 55, 80, 6), \"float32\"), kernel: T.Buffer((55, 80, 4, 4), \"float32\"), DepthwiseConv2d: T.Buffer((1, 4400, 77, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused in T.parallel(484):\n            DepthwiseConv2d_local = T.allocate([700], \"float32x3\", \"local\")\n            DepthwiseConv2d_local_1 = T.Buffer((700,), \"float32x3\", data=DepthwiseConv2d_local, scope=\"local\")\n            for c_c_outer_inner_init, c_c_inner_init, i_c_inner_init in T.grid(10, 10, 7):\n                DepthwiseConv2d_local_1[c_c_outer_inner_init * 70 + c_c_inner_init * 7 + i_c_inner_init] = T.Broadcast(T.float32(0), 3)\n            for dj_outer, c_c_outer_inner, di_inner, c_c_inner, i_c_inner in T.grid(4, 10, 4, 10, 7):\n                cse_var_2: T.int32 = b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused // 11\n                cse_var_1: T.int32 = c_c_outer_inner * 70 + c_c_inner * 7 + i_c_inner\n                data_1 = T.Buffer((26400,), data=data.data)\n                kernel_1 = T.Buffer((70400,), data=kernel.data)\n                DepthwiseConv2d_local_1[cse_var_1] = DepthwiseConv2d_local_1[cse_var_1] + data_1[(cse_var_2 * 5 + c_c_outer_inner // 2) // 4 * 480 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 11 * 42 + i_c_inner * 6 + di_inner * 6 + dj_outer:(cse_var_2 * 5 + c_c_outer_inner // 2) // 4 * 480 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 11 * 42 + i_c_inner * 6 + di_inner * 6 + dj_outer + 3] * T.Broadcast(kernel_1[cse_var_2 * 1600 + c_c_outer_inner * 160 + c_c_inner * 16 + di_inner * 4 + dj_outer], 3)\n            for c_inner, i_inner in T.grid(100, 7):\n                DepthwiseConv2d_1 = T.Buffer((1016400,), data=DepthwiseConv2d.data)\n                DepthwiseConv2d_1[b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused // 11 * 23100 + c_inner * 231 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 11 * 21 + i_inner * 3:b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused // 11 * 23100 + c_inner * 231 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 11 * 21 + i_inner * 3 + 3] = DepthwiseConv2d_local_1[c_inner * 7 + i_inner]",
        "data": "1_55_80_6",
        "kernel": "55_80_4_4"
    },
    {
        "op_name": "depthwise_conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t DepthwiseConv2d_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* DepthwiseConv2d = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* DepthwiseConv2d_1 = (((DLTensor*)DepthwiseConv2d)[0].data);\n  void* default_function_DepthwiseConv2d_shape = (((DLTensor*)DepthwiseConv2d)[0].shape);\n  void* default_function_DepthwiseConv2d_strides = (((DLTensor*)DepthwiseConv2d)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_DepthwiseConv2d_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused < 990; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused) {\n    for (int32_t c_outer_inner_init = 0; c_outer_inner_init < 97; ++c_outer_inner_init) {\n      for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 19; ++i_outer_inner_init) {\n        *(float2*)(((float*)DepthwiseConv2d_1) + (((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused / 495) * 1824570) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused % 3) * 608190)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused % 495) / 15) * 18430)) + (c_outer_inner_init * 190)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused % 15) / 3) * 38)) + (i_outer_inner_init * 2))) = ((float2)(0.000000e+00f, 0.000000e+00f));\n      },\n    },\n    for (int32_t dj_outer = 0; dj_outer < 3; ++dj_outer) {\n      for (int32_t c_outer_inner = 0; c_outer_inner < 97; ++c_outer_inner) {\n        for (int32_t i_outer_inner = 0; i_outer_inner < 19; ++i_outer_inner) {\n          for (int32_t di_inner = 0; di_inner < 3; ++di_inner) {\n            int32_t cse_var_5 = (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused % 3);\n            int32_t cse_var_4 = (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused / 495);\n            int32_t cse_var_3 = ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused % 15) / 3);\n            int32_t cse_var_2 = ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused % 495) / 15);\n            int32_t cse_var_1 = ((((((cse_var_4 * 1824570) + (cse_var_5 * 608190)) + (cse_var_2 * 18430)) + (c_outer_inner * 190)) + (cse_var_3 * 38)) + (i_outer_inner * 2));\n            int32_t2 v_ = int32_t2((cse_var_1)+(1*0), (cse_var_1)+(1*1));\n            int32_t2 v__1 = int32_t2(((((((((cse_var_4 * 38412) + (cse_var_5 * 12804)) + (cse_var_2 * 388)) + (cse_var_3 * 76)) + (i_outer_inner * 4)) + (di_inner * 4)) + dj_outer))+(1*0), ((((((((cse_var_4 * 38412) + (cse_var_5 * 12804)) + (cse_var_2 * 388)) + (cse_var_3 * 76)) + (i_outer_inner * 4)) + (di_inner * 4)) + dj_outer))+(1*1));\n            *(float2*)(((float*)DepthwiseConv2d_1) + cse_var_1) = ((float2(((float*)DepthwiseConv2d_1)[v_.s0],((float*)DepthwiseConv2d_1)[v_.s1])) + ((float2(((float*)data_1)[v__1.s0],((float*)data_1)[v__1.s1])) * ((float2)(((float*)kernel_1)[((((cse_var_2 * 873) + (c_outer_inner * 9)) + (di_inner * 3)) + dj_outer)], ((float*)kernel_1)[((((cse_var_2 * 873) + (c_outer_inner * 9)) + (di_inner * 3)) + dj_outer)]))));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(194) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(194) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel) {\n  float DepthwiseConv2d_local[165];\n  __shared__ float PaddedInput_shared[660];\n  __shared__ float kernel_shared[9603];\n  for (int c_c_inner_init = 0; c_c_inner_init < 11; ++c_c_inner_init) {\n    for (int i_c_inner_init = 0; i_c_inner_init < 5; ++i_c_inner_init) {\n      DepthwiseConv2d_local[((c_c_inner_init * 5) + i_c_inner_init)] = 0.000000e+00f;\n      DepthwiseConv2d_local[(((c_c_inner_init * 5) + i_c_inner_init) + 55)] = 0.000000e+00f;\n      DepthwiseConv2d_local[(((c_c_inner_init * 5) + i_c_inner_init) + 110)] = 0.000000e+00f;\n    },\n  },\n  for (int di_outer_outer = 0; di_outer_outer < 3; ++di_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 97) + (((int)threadIdx.x) >> 1)) < 330) {\n        PaddedInput_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 194) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) / 19) * 12804) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 97) + (((int)threadIdx.x) >> 1)) / 10) * 388)) + ((((int)blockIdx.x) % 19) * 20)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 7) + (((int)threadIdx.x) >> 1)) % 10) >> 1) * 4)) + (di_outer_outer * 4)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 2) + ((int)threadIdx.x)) & 3))];\n      },\n    },\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 < 50; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) {\n      if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 2) + (((int)threadIdx.x) / 97)) < 99) {\n        kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 194) + ((int)threadIdx.x))] = kernel[((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 194) + ((int)threadIdx.x)) / 3) * 9) + (di_outer_outer * 3)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 2) + ((int)threadIdx.x)) % 3))];\n      },\n    },\n    __syncthreads();\n    for (int dj_outer_inner = 0; dj_outer_inner < 3; ++dj_outer_inner) {\n      for (int c_c_inner = 0; c_c_inner < 11; ++c_c_inner) {\n        for (int i_c_inner = 0; i_c_inner < 5; ++i_c_inner) {\n          DepthwiseConv2d_local[((c_c_inner * 5) + i_c_inner)] = (DepthwiseConv2d_local[((c_c_inner * 5) + i_c_inner)] + (PaddedInput_shared[((((((((((int)threadIdx.x) >> 1) * 11) + c_c_inner) / 97) * 20) + (i_c_inner * 4)) + dj_outer_inner) + (((int)threadIdx.x) & 1))] * kernel_shared[((((((int)threadIdx.x) >> 1) * 33) + (c_c_inner * 3)) + dj_outer_inner)]));\n          DepthwiseConv2d_local[(((c_c_inner * 5) + i_c_inner) + 55)] = (DepthwiseConv2d_local[(((c_c_inner * 5) + i_c_inner) + 55)] + (PaddedInput_shared[(((((((((((int)threadIdx.x) >> 1) * 11) + c_c_inner) / 97) * 20) + (i_c_inner * 4)) + dj_outer_inner) + (((int)threadIdx.x) & 1)) + 220)] * kernel_shared[(((((((int)threadIdx.x) >> 1) * 33) + (c_c_inner * 3)) + dj_outer_inner) + 3201)]));\n          DepthwiseConv2d_local[(((c_c_inner * 5) + i_c_inner) + 110)] = (DepthwiseConv2d_local[(((c_c_inner * 5) + i_c_inner) + 110)] + (PaddedInput_shared[(((((((((((int)threadIdx.x) >> 1) * 11) + c_c_inner) / 97) * 20) + (i_c_inner * 4)) + dj_outer_inner) + (((int)threadIdx.x) & 1)) + 440)] * kernel_shared[(((((((int)threadIdx.x) >> 1) * 33) + (c_c_inner * 3)) + dj_outer_inner) + 6402)]));\n        },\n      },\n    },\n  },\n  for (int c_inner = 0; c_inner < 11; ++c_inner) {\n    for (int i_inner = 0; i_inner < 5; ++i_inner) {\n      DepthwiseConv2d[(((((((((int)blockIdx.x) / 19) * 608190) + ((((int)threadIdx.x) >> 1) * 2090)) + (c_inner * 190)) + ((((int)blockIdx.x) % 19) * 10)) + (i_inner * 2)) + (((int)threadIdx.x) & 1))] = DepthwiseConv2d_local[((c_inner * 5) + i_inner)];\n      DepthwiseConv2d[((((((((((int)blockIdx.x) / 19) * 608190) + ((((int)threadIdx.x) >> 1) * 2090)) + (c_inner * 190)) + ((((int)blockIdx.x) % 19) * 10)) + (i_inner * 2)) + (((int)threadIdx.x) & 1)) + 202730)] = DepthwiseConv2d_local[(((c_inner * 5) + i_inner) + 55)];\n      DepthwiseConv2d[((((((((((int)blockIdx.x) / 19) * 608190) + ((((int)threadIdx.x) >> 1) * 2090)) + (c_inner * 190)) + ((((int)blockIdx.x) % 19) * 10)) + (i_inner * 2)) + (((int)threadIdx.x) & 1)) + 405460)] = DepthwiseConv2d_local[(((c_inner * 5) + i_inner) + 110)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 33, 97, 4), \"float32\"), kernel: T.Buffer((33, 97, 3, 3), \"float32\"), DepthwiseConv2d: T.Buffer((6, 3201, 95, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused in T.parallel(990):\n            DepthwiseConv2d_1 = T.Buffer((3649140,), data=DepthwiseConv2d.data)\n            for c_outer_inner_init, i_outer_inner_init in T.grid(97, 19):\n                DepthwiseConv2d_1[b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused // 495 * 1824570 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused % 3 * 608190 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused % 495 // 15 * 18430 + c_outer_inner_init * 190 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused % 15 // 3 * 38 + i_outer_inner_init * 2:b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused // 495 * 1824570 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused % 3 * 608190 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused % 495 // 15 * 18430 + c_outer_inner_init * 190 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused % 15 // 3 * 38 + i_outer_inner_init * 2 + 2] = T.Broadcast(T.float32(0), 2)\n            for dj_outer, c_outer_inner, i_outer_inner, di_inner in T.grid(3, 97, 19, 3):\n                cse_var_5: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused % 3\n                cse_var_4: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused // 495\n                cse_var_3: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused % 15 // 3\n                cse_var_2: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused % 495 // 15\n                cse_var_1: T.int32 = cse_var_4 * 1824570 + cse_var_5 * 608190 + cse_var_2 * 18430 + c_outer_inner * 190 + cse_var_3 * 38 + i_outer_inner * 2\n                data_1 = T.Buffer((76824,), data=data.data)\n                kernel_1 = T.Buffer((28809,), data=kernel.data)\n                DepthwiseConv2d_1[cse_var_1:cse_var_1 + 2] = DepthwiseConv2d_1[cse_var_1:cse_var_1 + 2] + data_1[cse_var_4 * 38412 + cse_var_5 * 12804 + cse_var_2 * 388 + cse_var_3 * 76 + i_outer_inner * 4 + di_inner * 4 + dj_outer:cse_var_4 * 38412 + cse_var_5 * 12804 + cse_var_2 * 388 + cse_var_3 * 76 + i_outer_inner * 4 + di_inner * 4 + dj_outer + 2] * T.Broadcast(kernel_1[cse_var_2 * 873 + c_outer_inner * 9 + di_inner * 3 + dj_outer], 2)",
        "data": "6_33_97_4",
        "kernel": "33_97_3_3"
    },
    {
        "op_name": "depthwise_conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t DepthwiseConv2d_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* DepthwiseConv2d = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* DepthwiseConv2d_1 = (((DLTensor*)DepthwiseConv2d)[0].data);\n  void* default_function_DepthwiseConv2d_shape = (((DLTensor*)DepthwiseConv2d)[0].shape);\n  void* default_function_DepthwiseConv2d_strides = (((DLTensor*)DepthwiseConv2d)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_DepthwiseConv2d_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused < 4148; ++b_outer_outer_outer_c_outer_outer_outer_fused) {\n    for (int32_t i_outer_outer_inner = 0; i_outer_outer_inner < 59; ++i_outer_outer_inner) {\n      for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 2; ++j_outer_inner_init) {\n        for (int32_t b_inner_init = 0; b_inner_init < 3; ++b_inner_init) {\n          ((float*)DepthwiseConv2d_1)[((((b_inner_init * 489464) + (b_outer_outer_outer_c_outer_outer_outer_fused * 118)) + (i_outer_outer_inner * 2)) + j_outer_inner_init)] = 0.000000e+00f;\n        },\n      },\n      for (int32_t di_outer = 0; di_outer < 3; ++di_outer) {\n        for (int32_t j_outer_inner = 0; j_outer_inner < 2; ++j_outer_inner) {\n          for (int32_t dj_inner = 0; dj_inner < 3; ++dj_inner) {\n            for (int32_t b_inner = 0; b_inner < 3; ++b_inner) {\n              int32_t cse_var_1 = ((((b_inner * 489464) + (b_outer_outer_outer_c_outer_outer_outer_fused * 118)) + (i_outer_outer_inner * 2)) + j_outer_inner);\n              ((float*)DepthwiseConv2d_1)[cse_var_1] = (((float*)DepthwiseConv2d_1)[cse_var_1] + (((float*)data_1)[((((((b_inner * 16592) + ((b_outer_outer_outer_c_outer_outer_outer_fused / 61) * 244)) + (i_outer_outer_inner * 4)) + (di_outer * 4)) + j_outer_inner) + dj_inner)] * ((float*)kernel_1)[(((b_outer_outer_outer_c_outer_outer_outer_fused * 9) + (di_outer * 3)) + dj_inner)]));\n            },\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(61) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(61) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel) {\n  float DepthwiseConv2d_local[6];\n  __shared__ float PaddedInput_shared[54];\n  __shared__ float kernel_shared[1098];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 3; ++b_c_outer_inner_init) {\n    for (int c_c_outer_inner_init = 0; c_c_outer_inner_init < 2; ++c_c_outer_inner_init) {\n      DepthwiseConv2d_local[((b_c_outer_inner_init * 2) + c_c_outer_inner_init)] = 0.000000e+00f;\n    },\n  },\n  if (((int)threadIdx.x) < 54) {\n    PaddedInput_shared[((int)threadIdx.x)] = data[((((((((((int)threadIdx.x) / 18) * 16592) + ((((int)blockIdx.x) / 118) * 488)) + (((((int)threadIdx.x) % 18) / 9) * 244)) + (((((int)threadIdx.x) % 9) / 3) * 4)) + (((((int)blockIdx.x) % 118) >> 1) * 4)) + (((int)threadIdx.x) % 3)) + (((int)blockIdx.x) & 1))];\n  },\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 18; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n    kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 61) + ((int)threadIdx.x))] = kernel[((((((int)blockIdx.x) / 118) * 1098) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 61)) + ((int)threadIdx.x))];\n  },\n  __syncthreads();\n  for (int di_outer_inner = 0; di_outer_inner < 3; ++di_outer_inner) {\n    for (int dj_outer_inner = 0; dj_outer_inner < 3; ++dj_outer_inner) {\n      for (int b_c_outer_inner = 0; b_c_outer_inner < 3; ++b_c_outer_inner) {\n        for (int c_c_outer_inner = 0; c_c_outer_inner < 2; ++c_c_outer_inner) {\n          DepthwiseConv2d_local[((b_c_outer_inner * 2) + c_c_outer_inner)] = (DepthwiseConv2d_local[((b_c_outer_inner * 2) + c_c_outer_inner)] + (PaddedInput_shared[((((b_c_outer_inner * 18) + ((((((int)threadIdx.x) * 2) + c_c_outer_inner) / 61) * 9)) + (di_outer_inner * 3)) + dj_outer_inner)] * kernel_shared[((((((int)threadIdx.x) * 18) + (c_c_outer_inner * 9)) + (di_outer_inner * 3)) + dj_outer_inner)]));\n        },\n      },\n    },\n  },\n  for (int b_inner = 0; b_inner < 3; ++b_inner) {\n    for (int c_inner = 0; c_inner < 2; ++c_inner) {\n      DepthwiseConv2d[(((((b_inner * 489464) + ((((int)blockIdx.x) / 118) * 14396)) + (((int)threadIdx.x) * 236)) + (c_inner * 118)) + (((int)blockIdx.x) % 118))] = DepthwiseConv2d_local[((b_inner * 2) + c_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 68, 61, 4), \"float32\"), kernel: T.Buffer((68, 61, 3, 3), \"float32\"), DepthwiseConv2d: T.Buffer((3, 4148, 59, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused in T.parallel(4148):\n            for i_outer_outer_inner in range(59):\n                DepthwiseConv2d_1 = T.Buffer((1468392,), data=DepthwiseConv2d.data)\n                for j_outer_inner_init, b_inner_init in T.grid(2, 3):\n                    DepthwiseConv2d_1[b_inner_init * 489464 + b_outer_outer_outer_c_outer_outer_outer_fused * 118 + i_outer_outer_inner * 2 + j_outer_inner_init] = T.float32(0)\n                for di_outer, j_outer_inner, dj_inner, b_inner in T.grid(3, 2, 3, 3):\n                    cse_var_1: T.int32 = b_inner * 489464 + b_outer_outer_outer_c_outer_outer_outer_fused * 118 + i_outer_outer_inner * 2 + j_outer_inner\n                    data_1 = T.Buffer((49776,), data=data.data)\n                    kernel_1 = T.Buffer((37332,), data=kernel.data)\n                    DepthwiseConv2d_1[cse_var_1] = DepthwiseConv2d_1[cse_var_1] + data_1[b_inner * 16592 + b_outer_outer_outer_c_outer_outer_outer_fused // 61 * 244 + i_outer_outer_inner * 4 + di_outer * 4 + j_outer_inner + dj_inner] * kernel_1[b_outer_outer_outer_c_outer_outer_outer_fused * 9 + di_outer * 3 + dj_inner]",
        "data": "3_68_61_4",
        "kernel": "68_61_3_3"
    },
    {
        "op_name": "depthwise_conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t DepthwiseConv2d_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* DepthwiseConv2d = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* DepthwiseConv2d_1 = (((DLTensor*)DepthwiseConv2d)[0].data);\n  void* default_function_DepthwiseConv2d_shape = (((DLTensor*)DepthwiseConv2d)[0].shape);\n  void* default_function_DepthwiseConv2d_strides = (((DLTensor*)DepthwiseConv2d)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_DepthwiseConv2d_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused < 9568; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused) {\n    for (int32_t b_outer_outer_inner = 0; b_outer_outer_inner < 4; ++b_outer_outer_inner) {\n      for (int32_t c_outer_outer_inner = 0; c_outer_outer_inner < 9; ++c_outer_outer_inner) {\n        for (int32_t i_inner_init = 0; i_inner_init < 3; ++i_inner_init) {\n          *(float6*)(((float*)DepthwiseConv2d_1) + (((((b_outer_outer_inner * 1550016) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused / 23) * 3726)) + (c_outer_outer_inner * 414)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused % 23) * 18)) + (i_inner_init * 6))) = ((float6)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n        },\n        for (int32_t di_outer = 0; di_outer < 2; ++di_outer) {\n          for (int32_t dj_outer = 0; dj_outer < 3; ++dj_outer) {\n            for (int32_t di_inner = 0; di_inner < 2; ++di_inner) {\n              for (int32_t i_inner = 0; i_inner < 3; ++i_inner) {\n                int32_t cse_var_3 = (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused / 23);\n                int32_t cse_var_2 = (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused % 23);\n                int32_t cse_var_1 = (((((b_outer_outer_inner * 1550016) + (cse_var_3 * 3726)) + (c_outer_outer_inner * 414)) + (cse_var_2 * 18)) + (i_inner * 6));\n                int32_t6 v_ = int32_t6((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5));\n                int32_t6 v__1 = int32_t6(((((((((b_outer_outer_inner * 29952) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused / 184) * 576)) + (cse_var_2 * 24)) + (di_outer * 16)) + (i_inner * 8)) + (di_inner * 8)) + dj_outer))+(1*0), ((((((((b_outer_outer_inner * 29952) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused / 184) * 576)) + (cse_var_2 * 24)) + (di_outer * 16)) + (i_inner * 8)) + (di_inner * 8)) + dj_outer))+(1*1), ((((((((b_outer_outer_inner * 29952) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused / 184) * 576)) + (cse_var_2 * 24)) + (di_outer * 16)) + (i_inner * 8)) + (di_inner * 8)) + dj_outer))+(1*2), ((((((((b_outer_outer_inner * 29952) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused / 184) * 576)) + (cse_var_2 * 24)) + (di_outer * 16)) + (i_inner * 8)) + (di_inner * 8)) + dj_outer))+(1*3), ((((((((b_outer_outer_inner * 29952) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused / 184) * 576)) + (cse_var_2 * 24)) + (di_outer * 16)) + (i_inner * 8)) + (di_inner * 8)) + dj_outer))+(1*4), ((((((((b_outer_outer_inner * 29952) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused / 184) * 576)) + (cse_var_2 * 24)) + (di_outer * 16)) + (i_inner * 8)) + (di_inner * 8)) + dj_outer))+(1*5));\n                *(float6*)(((float*)DepthwiseConv2d_1) + cse_var_1) = ((float6(((float*)DepthwiseConv2d_1)[v_.s0],((float*)DepthwiseConv2d_1)[v_.s1],((float*)DepthwiseConv2d_1)[v_.s2],((float*)DepthwiseConv2d_1)[v_.s3],((float*)DepthwiseConv2d_1)[v_.s4],((float*)DepthwiseConv2d_1)[v_.s5])) + ((float6(((float*)data_1)[v__1.s0],((float*)data_1)[v__1.s1],((float*)data_1)[v__1.s2],((float*)data_1)[v__1.s3],((float*)data_1)[v__1.s4],((float*)data_1)[v__1.s5])) * ((float6)(((float*)kernel_1)[(((((cse_var_3 * 108) + (c_outer_outer_inner * 12)) + (di_outer * 6)) + (di_inner * 3)) + dj_outer)], ((float*)kernel_1)[(((((cse_var_3 * 108) + (c_outer_outer_inner * 12)) + (di_outer * 6)) + (di_inner * 3)) + dj_outer)], ((float*)kernel_1)[(((((cse_var_3 * 108) + (c_outer_outer_inner * 12)) + (di_outer * 6)) + (di_inner * 3)) + dj_outer)], ((float*)kernel_1)[(((((cse_var_3 * 108) + (c_outer_outer_inner * 12)) + (di_outer * 6)) + (di_inner * 3)) + dj_outer)], ((float*)kernel_1)[(((((cse_var_3 * 108) + (c_outer_outer_inner * 12)) + (di_outer * 6)) + (di_inner * 3)) + dj_outer)], ((float*)kernel_1)[(((((cse_var_3 * 108) + (c_outer_outer_inner * 12)) + (di_outer * 6)) + (di_inner * 3)) + dj_outer)]))));\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(207) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(207) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel) {\n  float DepthwiseConv2d_local[576];\n  __shared__ float PaddedInput_shared[2880];\n  __shared__ float kernel_shared[3456];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 2; ++b_c_outer_inner_init) {\n    for (int c_c_outer_inner_init = 0; c_c_outer_inner_init < 4; ++c_c_outer_inner_init) {\n      for (int c_c_inner_init = 0; c_c_inner_init < 2; ++c_c_inner_init) {\n        for (int i_c_inner_init = 0; i_c_inner_init < 3; ++i_c_inner_init) {\n          for (int j_c_inner_init = 0; j_c_inner_init < 3; ++j_c_inner_init) {\n            DepthwiseConv2d_local[(((((b_c_outer_inner_init * 72) + (c_c_outer_inner_init * 18)) + (c_c_inner_init * 9)) + (i_c_inner_init * 3)) + j_c_inner_init)] = 0.000000e+00f;\n            DepthwiseConv2d_local[((((((b_c_outer_inner_init * 72) + (c_c_outer_inner_init * 18)) + (c_c_inner_init * 9)) + (i_c_inner_init * 3)) + j_c_inner_init) + 144)] = 0.000000e+00f;\n            DepthwiseConv2d_local[((((((b_c_outer_inner_init * 72) + (c_c_outer_inner_init * 18)) + (c_c_inner_init * 9)) + (i_c_inner_init * 3)) + j_c_inner_init) + 288)] = 0.000000e+00f;\n            DepthwiseConv2d_local[((((((b_c_outer_inner_init * 72) + (c_c_outer_inner_init * 18)) + (c_c_inner_init * 9)) + (i_c_inner_init * 3)) + j_c_inner_init) + 432)] = 0.000000e+00f;\n          },\n        },\n      },\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 14; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 23) + (((int)threadIdx.x) / 9)) < 320) {\n      PaddedInput_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 207) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) / 26) * 59904) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 23) + (((int)threadIdx.x) / 9)) / 160) * 29952)) + (((((int)blockIdx.x) % 26) >> 1) * 2304)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 207) + ((int)threadIdx.x)) % 1440) / 5) * 8)) + ((((int)blockIdx.x) & 1) * 3)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 2) + ((int)threadIdx.x)) % 5))];\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 < 17; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 23) + (((int)threadIdx.x) / 9)) < 384) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 207) + ((int)threadIdx.x))] = kernel[(((((((int)blockIdx.x) % 26) >> 1) * 3456) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 207)) + ((int)threadIdx.x))];\n    },\n  },\n  __syncthreads();\n  for (int di_outer_inner = 0; di_outer_inner < 2; ++di_outer_inner) {\n    for (int dj_outer_inner = 0; dj_outer_inner < 3; ++dj_outer_inner) {\n      for (int b_c_outer_inner = 0; b_c_outer_inner < 2; ++b_c_outer_inner) {\n        for (int c_c_outer_inner = 0; c_c_outer_inner < 4; ++c_c_outer_inner) {\n          for (int di_inner = 0; di_inner < 2; ++di_inner) {\n            for (int c_c_inner = 0; c_c_inner < 2; ++c_c_inner) {\n              for (int i_c_inner = 0; i_c_inner < 3; ++i_c_inner) {\n                for (int j_c_inner = 0; j_c_inner < 3; ++j_c_inner) {\n                  DepthwiseConv2d_local[(((((b_c_outer_inner * 72) + (c_c_outer_inner * 18)) + (c_c_inner * 9)) + (i_c_inner * 3)) + j_c_inner)] = (DepthwiseConv2d_local[(((((b_c_outer_inner * 72) + (c_c_outer_inner * 18)) + (c_c_inner * 9)) + (i_c_inner * 3)) + j_c_inner)] + (PaddedInput_shared[(((((((b_c_outer_inner * 1440) + ((((int)threadIdx.x) % 23) * 15)) + (di_outer_inner * 10)) + (i_c_inner * 5)) + (di_inner * 5)) + j_c_inner) + dj_outer_inner)] * kernel_shared[(((((((((int)threadIdx.x) / 23) * 96) + (c_c_outer_inner * 24)) + (c_c_inner * 12)) + (di_outer_inner * 6)) + (di_inner * 3)) + dj_outer_inner)]));\n                  DepthwiseConv2d_local[((((((b_c_outer_inner * 72) + (c_c_outer_inner * 18)) + (c_c_inner * 9)) + (i_c_inner * 3)) + j_c_inner) + 144)] = (DepthwiseConv2d_local[((((((b_c_outer_inner * 72) + (c_c_outer_inner * 18)) + (c_c_inner * 9)) + (i_c_inner * 3)) + j_c_inner) + 144)] + (PaddedInput_shared[((((((((b_c_outer_inner * 1440) + ((((int)threadIdx.x) % 23) * 15)) + (di_outer_inner * 10)) + (i_c_inner * 5)) + (di_inner * 5)) + j_c_inner) + dj_outer_inner) + 360)] * kernel_shared[((((((((((int)threadIdx.x) / 23) * 96) + (c_c_outer_inner * 24)) + (c_c_inner * 12)) + (di_outer_inner * 6)) + (di_inner * 3)) + dj_outer_inner) + 864)]));\n                  DepthwiseConv2d_local[((((((b_c_outer_inner * 72) + (c_c_outer_inner * 18)) + (c_c_inner * 9)) + (i_c_inner * 3)) + j_c_inner) + 288)] = (DepthwiseConv2d_local[((((((b_c_outer_inner * 72) + (c_c_outer_inner * 18)) + (c_c_inner * 9)) + (i_c_inner * 3)) + j_c_inner) + 288)] + (PaddedInput_shared[((((((((b_c_outer_inner * 1440) + ((((int)threadIdx.x) % 23) * 15)) + (di_outer_inner * 10)) + (i_c_inner * 5)) + (di_inner * 5)) + j_c_inner) + dj_outer_inner) + 720)] * kernel_shared[((((((((((int)threadIdx.x) / 23) * 96) + (c_c_outer_inner * 24)) + (c_c_inner * 12)) + (di_outer_inner * 6)) + (di_inner * 3)) + dj_outer_inner) + 1728)]));\n                  DepthwiseConv2d_local[((((((b_c_outer_inner * 72) + (c_c_outer_inner * 18)) + (c_c_inner * 9)) + (i_c_inner * 3)) + j_c_inner) + 432)] = (DepthwiseConv2d_local[((((((b_c_outer_inner * 72) + (c_c_outer_inner * 18)) + (c_c_inner * 9)) + (i_c_inner * 3)) + j_c_inner) + 432)] + (PaddedInput_shared[((((((((b_c_outer_inner * 1440) + ((((int)threadIdx.x) % 23) * 15)) + (di_outer_inner * 10)) + (i_c_inner * 5)) + (di_inner * 5)) + j_c_inner) + dj_outer_inner) + 1080)] * kernel_shared[((((((((((int)threadIdx.x) / 23) * 96) + (c_c_outer_inner * 24)) + (c_c_inner * 12)) + (di_outer_inner * 6)) + (di_inner * 3)) + dj_outer_inner) + 2592)]));\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    for (int c_inner = 0; c_inner < 8; ++c_inner) {\n      for (int i_inner = 0; i_inner < 3; ++i_inner) {\n        for (int j_inner = 0; j_inner < 3; ++j_inner) {\n          DepthwiseConv2d[((((((((((((int)blockIdx.x) / 26) * 3100032) + (b_inner * 1550016)) + (((((int)blockIdx.x) % 26) >> 1) * 119232)) + ((((int)threadIdx.x) / 23) * 3312)) + (c_inner * 414)) + ((((int)threadIdx.x) % 23) * 18)) + (i_inner * 6)) + ((((int)blockIdx.x) & 1) * 3)) + j_inner)] = DepthwiseConv2d_local[((((b_inner * 72) + (c_inner * 9)) + (i_inner * 3)) + j_inner)];\n          DepthwiseConv2d[(((((((((((((int)blockIdx.x) / 26) * 3100032) + (b_inner * 1550016)) + (((((int)blockIdx.x) % 26) >> 1) * 119232)) + ((((int)threadIdx.x) / 23) * 3312)) + (c_inner * 414)) + ((((int)threadIdx.x) % 23) * 18)) + (i_inner * 6)) + ((((int)blockIdx.x) & 1) * 3)) + j_inner) + 29808)] = DepthwiseConv2d_local[(((((b_inner * 72) + (c_inner * 9)) + (i_inner * 3)) + j_inner) + 144)];\n          DepthwiseConv2d[(((((((((((((int)blockIdx.x) / 26) * 3100032) + (b_inner * 1550016)) + (((((int)blockIdx.x) % 26) >> 1) * 119232)) + ((((int)threadIdx.x) / 23) * 3312)) + (c_inner * 414)) + ((((int)threadIdx.x) % 23) * 18)) + (i_inner * 6)) + ((((int)blockIdx.x) & 1) * 3)) + j_inner) + 59616)] = DepthwiseConv2d_local[(((((b_inner * 72) + (c_inner * 9)) + (i_inner * 3)) + j_inner) + 288)];\n          DepthwiseConv2d[(((((((((((((int)blockIdx.x) / 26) * 3100032) + (b_inner * 1550016)) + (((((int)blockIdx.x) % 26) >> 1) * 119232)) + ((((int)threadIdx.x) / 23) * 3312)) + (c_inner * 414)) + ((((int)threadIdx.x) % 23) * 18)) + (i_inner * 6)) + ((((int)blockIdx.x) & 1) * 3)) + j_inner) + 89424)] = DepthwiseConv2d_local[(((((b_inner * 72) + (c_inner * 9)) + (i_inner * 3)) + j_inner) + 432)];\n        },\n      },\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 52, 72, 8), \"float32\"), kernel: T.Buffer((52, 72, 4, 3), \"float32\"), DepthwiseConv2d: T.Buffer((4, 3744, 69, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused in T.parallel(9568):\n            for b_outer_outer_inner, c_outer_outer_inner in T.grid(4, 9):\n                DepthwiseConv2d_1 = T.Buffer((6200064,), data=DepthwiseConv2d.data)\n                for i_inner_init in range(3):\n                    DepthwiseConv2d_1[b_outer_outer_inner * 1550016 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused // 23 * 3726 + c_outer_outer_inner * 414 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused % 23 * 18 + i_inner_init * 6:b_outer_outer_inner * 1550016 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused // 23 * 3726 + c_outer_outer_inner * 414 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused % 23 * 18 + i_inner_init * 6 + 6] = T.Broadcast(T.float32(0), 6)\n                for di_outer, dj_outer, di_inner, i_inner in T.grid(2, 3, 2, 3):\n                    cse_var_3: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused // 23\n                    cse_var_2: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused % 23\n                    cse_var_1: T.int32 = b_outer_outer_inner * 1550016 + cse_var_3 * 3726 + c_outer_outer_inner * 414 + cse_var_2 * 18 + i_inner * 6\n                    data_1 = T.Buffer((119808,), data=data.data)\n                    kernel_1 = T.Buffer((44928,), data=kernel.data)\n                    DepthwiseConv2d_1[cse_var_1:cse_var_1 + 6] = DepthwiseConv2d_1[cse_var_1:cse_var_1 + 6] + data_1[b_outer_outer_inner * 29952 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused // 184 * 576 + cse_var_2 * 24 + di_outer * 16 + i_inner * 8 + di_inner * 8 + dj_outer:b_outer_outer_inner * 29952 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused // 184 * 576 + cse_var_2 * 24 + di_outer * 16 + i_inner * 8 + di_inner * 8 + dj_outer + 6] * T.Broadcast(kernel_1[cse_var_3 * 108 + c_outer_outer_inner * 12 + di_outer * 6 + di_inner * 3 + dj_outer], 6)",
        "data": "4_52_72_8",
        "kernel": "52_72_4_3"
    },
    {
        "op_name": "depthwise_conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t DepthwiseConv2d_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* DepthwiseConv2d = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* DepthwiseConv2d_1 = (((DLTensor*)DepthwiseConv2d)[0].data);\n  void* default_function_DepthwiseConv2d_shape = (((DLTensor*)DepthwiseConv2d)[0].shape);\n  void* default_function_DepthwiseConv2d_strides = (((DLTensor*)DepthwiseConv2d)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_DepthwiseConv2d_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused = 0; b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused < 2397; ++b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused) {\n    void* DepthwiseConv2d_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1400, 2, 32);\n    if (DepthwiseConv2d_local == NULL) {\n      return -1;\n    },\n    for (int32_t i_outer_inner = 0; i_outer_inner < 13; ++i_outer_inner) {\n      for (int32_t c_c_outer_inner_init = 0; c_c_outer_inner_init < 2; ++c_c_outer_inner_init) {\n        for (int32_t i_c_outer_inner_init = 0; i_c_outer_inner_init < 7; ++i_c_outer_inner_init) {\n          for (int32_t c_c_inner_init = 0; c_c_inner_init < 5; ++c_c_inner_init) {\n            ((float5*)DepthwiseConv2d_local)[(((c_c_outer_inner_init * 35) + (c_c_inner_init * 7)) + i_c_outer_inner_init)] = ((float5)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n          },\n        },\n      },\n      for (int32_t di_outer = 0; di_outer < 2; ++di_outer) {\n        for (int32_t dj_outer = 0; dj_outer < 4; ++dj_outer) {\n          for (int32_t c_c_outer_inner = 0; c_c_outer_inner < 2; ++c_c_outer_inner) {\n            for (int32_t i_c_outer_inner = 0; i_c_outer_inner < 7; ++i_c_outer_inner) {\n              for (int32_t di_inner = 0; di_inner < 2; ++di_inner) {\n                for (int32_t c_c_inner = 0; c_c_inner < 5; ++c_c_inner) {\n                  int32_t cse_var_2 = (b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 799);\n                  int32_t cse_var_1 = (((c_c_outer_inner * 35) + (c_c_inner * 7)) + i_c_outer_inner);\n                  int32_t5 v_ = int32_t5((((((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused / 799) * 63920) + ((((cse_var_2 * 5) + (((c_c_outer_inner * 5) + c_c_inner) >> 1)) / 47) * 752)) + (i_outer_inner * 56)) + (di_outer * 16)) + (i_c_outer_inner * 8)) + (di_inner * 8)) + dj_outer))+(1*0), (((((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused / 799) * 63920) + ((((cse_var_2 * 5) + (((c_c_outer_inner * 5) + c_c_inner) >> 1)) / 47) * 752)) + (i_outer_inner * 56)) + (di_outer * 16)) + (i_c_outer_inner * 8)) + (di_inner * 8)) + dj_outer))+(1*1), (((((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused / 799) * 63920) + ((((cse_var_2 * 5) + (((c_c_outer_inner * 5) + c_c_inner) >> 1)) / 47) * 752)) + (i_outer_inner * 56)) + (di_outer * 16)) + (i_c_outer_inner * 8)) + (di_inner * 8)) + dj_outer))+(1*2), (((((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused / 799) * 63920) + ((((cse_var_2 * 5) + (((c_c_outer_inner * 5) + c_c_inner) >> 1)) / 47) * 752)) + (i_outer_inner * 56)) + (di_outer * 16)) + (i_c_outer_inner * 8)) + (di_inner * 8)) + dj_outer))+(1*3), (((((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused / 799) * 63920) + ((((cse_var_2 * 5) + (((c_c_outer_inner * 5) + c_c_inner) >> 1)) / 47) * 752)) + (i_outer_inner * 56)) + (di_outer * 16)) + (i_c_outer_inner * 8)) + (di_inner * 8)) + dj_outer))+(1*4));\n                  ((float5*)DepthwiseConv2d_local)[cse_var_1] = (((float5*)DepthwiseConv2d_local)[cse_var_1] + ((float5(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4])) * ((float5)(((float*)kernel_1)[((((((cse_var_2 * 160) + (c_c_outer_inner * 80)) + (c_c_inner * 16)) + (di_outer * 8)) + (di_inner * 4)) + dj_outer)], ((float*)kernel_1)[((((((cse_var_2 * 160) + (c_c_outer_inner * 80)) + (c_c_inner * 16)) + (di_outer * 8)) + (di_inner * 4)) + dj_outer)], ((float*)kernel_1)[((((((cse_var_2 * 160) + (c_c_outer_inner * 80)) + (c_c_inner * 16)) + (di_outer * 8)) + (di_inner * 4)) + dj_outer)], ((float*)kernel_1)[((((((cse_var_2 * 160) + (c_c_outer_inner * 80)) + (c_c_inner * 16)) + (di_outer * 8)) + (di_inner * 4)) + dj_outer)], ((float*)kernel_1)[((((((cse_var_2 * 160) + (c_c_outer_inner * 80)) + (c_c_inner * 16)) + (di_outer * 8)) + (di_inner * 4)) + dj_outer)]))));\n                },\n              },\n            },\n          },\n        },\n      },\n      for (int32_t c_inner = 0; c_inner < 10; ++c_inner) {\n        for (int32_t i_inner = 0; i_inner < 7; ++i_inner) {\n          *(float5*)(((float*)DepthwiseConv2d_1) + ((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused * 4550) + (c_inner * 455)) + (i_outer_inner * 35)) + (i_inner * 5))) = ((float5*)DepthwiseConv2d_local)[((c_inner * 7) + i_inner)];\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, DepthwiseConv2d_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(611) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(611) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel) {\n  float DepthwiseConv2d_local[14];\n  __shared__ float PaddedInput_shared[368];\n  __shared__ float kernel_shared[752];\n  for (int c_c_outer_inner_init = 0; c_c_outer_inner_init < 2; ++c_c_outer_inner_init) {\n    for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 7; ++i_c_outer_inner_init) {\n      DepthwiseConv2d_local[((c_c_outer_inner_init * 7) + i_c_outer_inner_init)] = 0.000000e+00f;\n    },\n  },\n  for (int di_outer_outer = 0; di_outer_outer < 2; ++di_outer_outer) {\n    __syncthreads();\n    if (((int)threadIdx.x) < 368) {\n      PaddedInput_shared[((int)threadIdx.x)] = data[((((((((int)blockIdx.x) / 5) * 752) + (di_outer_outer * 16)) + ((((int)threadIdx.x) >> 2) * 8)) + (((int)blockIdx.x) % 5)) + (((int)threadIdx.x) & 3))];\n    },\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 13) + (((int)threadIdx.x) / 47)) < 16) {\n        kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 611) + ((int)threadIdx.x))] = kernel[((((((((int)blockIdx.x) % 425) / 5) * 1504) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 611) + ((int)threadIdx.x)) >> 3) * 16)) + (di_outer_outer * 8)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 3) + ((int)threadIdx.x)) & 7))];\n      },\n    },\n    __syncthreads();\n    for (int di_outer_inner = 0; di_outer_inner < 2; ++di_outer_inner) {\n      for (int c_c_outer_inner = 0; c_c_outer_inner < 2; ++c_c_outer_inner) {\n        for (int i_c_outer_inner = 0; i_c_outer_inner < 7; ++i_c_outer_inner) {\n          for (int dj_inner = 0; dj_inner < 4; ++dj_inner) {\n            DepthwiseConv2d_local[((c_c_outer_inner * 7) + i_c_outer_inner)] = (DepthwiseConv2d_local[((c_c_outer_inner * 7) + i_c_outer_inner)] + (PaddedInput_shared[(((((((int)threadIdx.x) % 13) * 28) + (i_c_outer_inner * 4)) + (di_outer_inner * 4)) + dj_inner)] * kernel_shared[(((((((int)threadIdx.x) / 13) * 16) + (c_c_outer_inner * 8)) + (di_outer_inner * 4)) + dj_inner)]));\n          },\n        },\n      },\n    },\n  },\n  for (int c_inner = 0; c_inner < 2; ++c_inner) {\n    for (int i_inner = 0; i_inner < 7; ++i_inner) {\n      DepthwiseConv2d[(((((((((int)blockIdx.x) / 5) * 42770) + ((((int)threadIdx.x) / 13) * 910)) + (c_inner * 455)) + ((((int)threadIdx.x) % 13) * 35)) + (i_inner * 5)) + (((int)blockIdx.x) % 5))] = DepthwiseConv2d_local[((c_inner * 7) + i_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 85, 94, 8), \"float32\"), kernel: T.Buffer((85, 94, 4, 4), \"float32\"), DepthwiseConv2d: T.Buffer((3, 7990, 91, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused in T.parallel(2397):\n            DepthwiseConv2d_local = T.allocate([70], \"float32x5\", \"local\")\n            for i_outer_inner in range(13):\n                DepthwiseConv2d_local_1 = T.Buffer((70,), \"float32x5\", data=DepthwiseConv2d_local, scope=\"local\")\n                for c_c_outer_inner_init, i_c_outer_inner_init, c_c_inner_init in T.grid(2, 7, 5):\n                    DepthwiseConv2d_local_1[c_c_outer_inner_init * 35 + c_c_inner_init * 7 + i_c_outer_inner_init] = T.Broadcast(T.float32(0), 5)\n                for di_outer, dj_outer, c_c_outer_inner, i_c_outer_inner, di_inner, c_c_inner in T.grid(2, 4, 2, 7, 2, 5):\n                    cse_var_2: T.int32 = b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 799\n                    cse_var_1: T.int32 = c_c_outer_inner * 35 + c_c_inner * 7 + i_c_outer_inner\n                    data_1 = T.Buffer((191760,), data=data.data)\n                    kernel_1 = T.Buffer((127840,), data=kernel.data)\n                    DepthwiseConv2d_local_1[cse_var_1] = DepthwiseConv2d_local_1[cse_var_1] + data_1[b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused // 799 * 63920 + (cse_var_2 * 5 + (c_c_outer_inner * 5 + c_c_inner) // 2) // 47 * 752 + i_outer_inner * 56 + di_outer * 16 + i_c_outer_inner * 8 + di_inner * 8 + dj_outer:b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused // 799 * 63920 + (cse_var_2 * 5 + (c_c_outer_inner * 5 + c_c_inner) // 2) // 47 * 752 + i_outer_inner * 56 + di_outer * 16 + i_c_outer_inner * 8 + di_inner * 8 + dj_outer + 5] * T.Broadcast(kernel_1[cse_var_2 * 160 + c_c_outer_inner * 80 + c_c_inner * 16 + di_outer * 8 + di_inner * 4 + dj_outer], 5)\n                for c_inner, i_inner in T.grid(10, 7):\n                    DepthwiseConv2d_1 = T.Buffer((10906350,), data=DepthwiseConv2d.data)\n                    DepthwiseConv2d_1[b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused * 4550 + c_inner * 455 + i_outer_inner * 35 + i_inner * 5:b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused * 4550 + c_inner * 455 + i_outer_inner * 35 + i_inner * 5 + 5] = DepthwiseConv2d_local_1[c_inner * 7 + i_inner]",
        "data": "3_85_94_8",
        "kernel": "85_94_4_4"
    },
    {
        "op_name": "depthwise_conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t DepthwiseConv2d_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* DepthwiseConv2d = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* DepthwiseConv2d_1 = (((DLTensor*)DepthwiseConv2d)[0].data);\n  void* default_function_DepthwiseConv2d_shape = (((DLTensor*)DepthwiseConv2d)[0].shape);\n  void* default_function_DepthwiseConv2d_strides = (((DLTensor*)DepthwiseConv2d)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_DepthwiseConv2d_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused = 0; b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused < 8580; ++b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused) {\n    void* DepthwiseConv2d_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)152, 2, 32);\n    if (DepthwiseConv2d_local == NULL) {\n      return -1;\n    },\n    for (int32_t i_c_outer_inner_init = 0; i_c_outer_inner_init < 19; ++i_c_outer_inner_init) {\n      for (int32_t i_c_inner_init = 0; i_c_inner_init < 2; ++i_c_inner_init) {\n        ((float*)DepthwiseConv2d_local)[((i_c_outer_inner_init * 2) + i_c_inner_init)] = 0.000000e+00f;\n      },\n    },\n    for (int32_t dj_outer = 0; dj_outer < 3; ++dj_outer) {\n      for (int32_t i_c_outer_inner = 0; i_c_outer_inner < 19; ++i_c_outer_inner) {\n        for (int32_t di_inner = 0; di_inner < 3; ++di_inner) {\n          for (int32_t i_c_inner = 0; i_c_inner < 2; ++i_c_inner) {\n            int32_t cse_var_3 = (b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused / 110);\n            int32_t cse_var_2 = (b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 11);\n            int32_t cse_var_1 = ((i_c_outer_inner * 2) + i_c_inner);\n            ((float*)DepthwiseConv2d_local)[cse_var_1] = (((float*)DepthwiseConv2d_local)[cse_var_1] + (((float*)data_1)[((((((((((cse_var_3 * 11) + cse_var_2) / 78) * 546) + (((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 110) / 55) * 266)) + (i_c_outer_inner * 14)) + (i_c_inner * 7)) + (di_inner * 7)) + ((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 55) / 11)) + dj_outer)] * ((float*)kernel_1)[((((cse_var_3 * 99) + (cse_var_2 * 9)) + (di_inner * 3)) + dj_outer)]));\n          },\n        },\n      },\n    },\n    for (int32_t i_inner = 0; i_inner < 38; ++i_inner) {\n      ((float*)DepthwiseConv2d_1)[((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused / 110) * 4180) + ((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 11) * 380)) + (((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 110) / 55) * 190)) + (i_inner * 5)) + ((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 55) / 11))] = ((float*)DepthwiseConv2d_local)[i_inner];\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, DepthwiseConv2d_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(76) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(76) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel) {\n  float DepthwiseConv2d_local[5];\n  __shared__ float PaddedInput_shared[532];\n  __shared__ float kernel_shared[3];\n  DepthwiseConv2d_local[0] = 0.000000e+00f;\n  DepthwiseConv2d_local[1] = 0.000000e+00f;\n  DepthwiseConv2d_local[2] = 0.000000e+00f;\n  DepthwiseConv2d_local[3] = 0.000000e+00f;\n  DepthwiseConv2d_local[4] = 0.000000e+00f;\n  for (int di_outer_outer = 0; di_outer_outer < 3; ++di_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 7; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n      PaddedInput_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 76) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) / 78) * 546) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 76)) + (di_outer_outer * 7)) + ((int)threadIdx.x))];\n    },\n    if (((int)threadIdx.x) < 3) {\n      kernel_shared[((int)threadIdx.x)] = kernel[(((((int)blockIdx.x) * 9) + (di_outer_outer * 3)) + ((int)threadIdx.x))];\n    },\n    __syncthreads();\n    for (int dj_inner = 0; dj_inner < 3; ++dj_inner) {\n      DepthwiseConv2d_local[0] = (DepthwiseConv2d_local[0] + (PaddedInput_shared[((((int)threadIdx.x) * 7) + dj_inner)] * kernel_shared[dj_inner]));\n      DepthwiseConv2d_local[1] = (DepthwiseConv2d_local[1] + (PaddedInput_shared[(((((int)threadIdx.x) * 7) + dj_inner) + 1)] * kernel_shared[dj_inner]));\n      DepthwiseConv2d_local[2] = (DepthwiseConv2d_local[2] + (PaddedInput_shared[(((((int)threadIdx.x) * 7) + dj_inner) + 2)] * kernel_shared[dj_inner]));\n      DepthwiseConv2d_local[3] = (DepthwiseConv2d_local[3] + (PaddedInput_shared[(((((int)threadIdx.x) * 7) + dj_inner) + 3)] * kernel_shared[dj_inner]));\n      DepthwiseConv2d_local[4] = (DepthwiseConv2d_local[4] + (PaddedInput_shared[(((((int)threadIdx.x) * 7) + dj_inner) + 4)] * kernel_shared[dj_inner]));\n    },\n  },\n  DepthwiseConv2d[((((int)blockIdx.x) * 380) + (((int)threadIdx.x) * 5))] = DepthwiseConv2d_local[0];\n  DepthwiseConv2d[(((((int)blockIdx.x) * 380) + (((int)threadIdx.x) * 5)) + 1)] = DepthwiseConv2d_local[1];\n  DepthwiseConv2d[(((((int)blockIdx.x) * 380) + (((int)threadIdx.x) * 5)) + 2)] = DepthwiseConv2d_local[2];\n  DepthwiseConv2d[(((((int)blockIdx.x) * 380) + (((int)threadIdx.x) * 5)) + 3)] = DepthwiseConv2d_local[3];\n  DepthwiseConv2d[(((((int)blockIdx.x) * 380) + (((int)threadIdx.x) * 5)) + 4)] = DepthwiseConv2d_local[4];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 11, 78, 7), \"float32\"), kernel: T.Buffer((11, 78, 3, 3), \"float32\"), DepthwiseConv2d: T.Buffer((1, 858, 76, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused in T.parallel(8580):\n            DepthwiseConv2d_local = T.allocate([38], \"float32\", \"local\")\n            DepthwiseConv2d_local_1 = T.Buffer((38,), data=DepthwiseConv2d_local, scope=\"local\")\n            for i_c_outer_inner_init, i_c_inner_init in T.grid(19, 2):\n                DepthwiseConv2d_local_1[i_c_outer_inner_init * 2 + i_c_inner_init] = T.float32(0)\n            for dj_outer, i_c_outer_inner, di_inner, i_c_inner in T.grid(3, 19, 3, 2):\n                cse_var_3: T.int32 = b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused // 110\n                cse_var_2: T.int32 = b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 11\n                cse_var_1: T.int32 = i_c_outer_inner * 2 + i_c_inner\n                data_1 = T.Buffer((6006,), data=data.data)\n                kernel_1 = T.Buffer((7722,), data=kernel.data)\n                DepthwiseConv2d_local_1[cse_var_1] = DepthwiseConv2d_local_1[cse_var_1] + data_1[(cse_var_3 * 11 + cse_var_2) // 78 * 546 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 110 // 55 * 266 + i_c_outer_inner * 14 + i_c_inner * 7 + di_inner * 7 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 55 // 11 + dj_outer] * kernel_1[cse_var_3 * 99 + cse_var_2 * 9 + di_inner * 3 + dj_outer]\n            for i_inner in range(38):\n                DepthwiseConv2d_1 = T.Buffer((326040,), data=DepthwiseConv2d.data)\n                DepthwiseConv2d_1[b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused // 110 * 4180 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 11 * 380 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 110 // 55 * 190 + i_inner * 5 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 55 // 11] = DepthwiseConv2d_local_1[i_inner]",
        "data": "1_11_78_7",
        "kernel": "11_78_3_3"
    },
    {
        "op_name": "depthwise_conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t DepthwiseConv2d_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* DepthwiseConv2d = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* DepthwiseConv2d_1 = (((DLTensor*)DepthwiseConv2d)[0].data);\n  void* default_function_DepthwiseConv2d_shape = (((DLTensor*)DepthwiseConv2d)[0].shape);\n  void* default_function_DepthwiseConv2d_strides = (((DLTensor*)DepthwiseConv2d)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_DepthwiseConv2d_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused = 0; b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused < 1974; ++b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused) {\n    void* DepthwiseConv2d_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)504, 2, 32);\n    if (DepthwiseConv2d_local == NULL) {\n      return -1;\n    },\n    for (int32_t i_outer_inner = 0; i_outer_inner < 81; ++i_outer_inner) {\n      for (int32_t b_c_outer_inner_init = 0; b_c_outer_inner_init < 3; ++b_c_outer_inner_init) {\n        for (int32_t c_c_outer_inner_init = 0; c_c_outer_inner_init < 6; ++c_c_outer_inner_init) {\n          for (int32_t j_c_outer_inner_init = 0; j_c_outer_inner_init < 7; ++j_c_outer_inner_init) {\n            ((float*)DepthwiseConv2d_local)[(((b_c_outer_inner_init * 42) + (c_c_outer_inner_init * 7)) + j_c_outer_inner_init)] = 0.000000e+00f;\n          },\n        },\n      },\n      for (int32_t di_outer = 0; di_outer < 4; ++di_outer) {\n        for (int32_t b_c_outer_inner = 0; b_c_outer_inner < 3; ++b_c_outer_inner) {\n          for (int32_t c_c_outer_inner = 0; c_c_outer_inner < 6; ++c_c_outer_inner) {\n            for (int32_t j_c_outer_inner = 0; j_c_outer_inner < 7; ++j_c_outer_inner) {\n              for (int32_t dj_inner = 0; dj_inner < 3; ++dj_inner) {\n                int32_t cse_var_2 = (b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 658);\n                int32_t cse_var_1 = (((b_c_outer_inner * 42) + (c_c_outer_inner * 7)) + j_c_outer_inner);\n                ((float*)DepthwiseConv2d_local)[cse_var_1] = (((float*)DepthwiseConv2d_local)[cse_var_1] + (((float*)data_1)[((((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused / 658) * 106596) + (b_c_outer_inner * 35532)) + ((cse_var_2 / 14) * 756)) + (i_outer_inner * 9)) + (di_outer * 9)) + j_c_outer_inner) + dj_inner)] * ((float*)kernel_1)[((((cse_var_2 * 72) + (c_c_outer_inner * 12)) + (di_outer * 3)) + dj_inner)]));\n              },\n            },\n          },\n        },\n      },\n      for (int32_t b_inner = 0; b_inner < 3; ++b_inner) {\n        for (int32_t c_inner = 0; c_inner < 6; ++c_inner) {\n          *(float7*)(((float*)DepthwiseConv2d_1) + ((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused / 658) * 6715548) + (b_inner * 2238516)) + ((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 658) * 3402)) + (c_inner * 567)) + (i_outer_inner * 7))) = *(float7*)(((float*)DepthwiseConv2d_local) + ((b_inner * 42) + (c_inner * 7)));\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, DepthwiseConv2d_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(567) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(567) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel) {\n  float DepthwiseConv2d_local[108];\n  __shared__ float PaddedInput_shared[5292];\n  __shared__ float kernel_shared[48];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 3; ++b_c_outer_inner_init) {\n    for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 3; ++i_c_outer_inner_init) {\n      for (int c_c_inner_init = 0; c_c_inner_init < 4; ++c_c_inner_init) {\n        DepthwiseConv2d_local[(((b_c_outer_inner_init * 12) + (c_c_inner_init * 3)) + i_c_outer_inner_init)] = 0.000000e+00f;\n        DepthwiseConv2d_local[((((b_c_outer_inner_init * 12) + (c_c_inner_init * 3)) + i_c_outer_inner_init) + 36)] = 0.000000e+00f;\n        DepthwiseConv2d_local[((((b_c_outer_inner_init * 12) + (c_c_inner_init * 3)) + i_c_outer_inner_init) + 72)] = 0.000000e+00f;\n      },\n    },\n  },\n  for (int dj_outer_outer = 0; dj_outer_outer < 3; ++dj_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 10; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 3) + (((int)threadIdx.x) / 189)) < 28) {\n        PaddedInput_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 567) + ((int)threadIdx.x))] = data[((((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 27) + (((int)threadIdx.x) / 21)) / 28) * 35532) + ((((int)blockIdx.x) / 7) * 756)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 81) + (((int)threadIdx.x) / 7)) % 84) * 9)) + dj_outer_outer) + (((int)threadIdx.x) % 7))];\n      },\n    },\n    if (((int)threadIdx.x) < 48) {\n      kernel_shared[((int)threadIdx.x)] = kernel[(((((int)blockIdx.x) * 144) + (((int)threadIdx.x) * 3)) + dj_outer_outer)];\n    },\n    __syncthreads();\n    for (int di_outer_inner = 0; di_outer_inner < 4; ++di_outer_inner) {\n      for (int b_c_outer_inner = 0; b_c_outer_inner < 3; ++b_c_outer_inner) {\n        for (int i_c_outer_inner = 0; i_c_outer_inner < 3; ++i_c_outer_inner) {\n          for (int c_c_inner = 0; c_c_inner < 4; ++c_c_inner) {\n            DepthwiseConv2d_local[(((b_c_outer_inner * 12) + (c_c_inner * 3)) + i_c_outer_inner)] = (DepthwiseConv2d_local[(((b_c_outer_inner * 12) + (c_c_inner * 3)) + i_c_outer_inner)] + (PaddedInput_shared[(((((((((int)threadIdx.x) / 189) * 1764) + (b_c_outer_inner * 588)) + (((((int)threadIdx.x) % 63) / 7) * 21)) + (i_c_outer_inner * 7)) + (di_outer_inner * 7)) + (((int)threadIdx.x) % 7))] * kernel_shared[(((((((((int)blockIdx.x) * 12) + (((((int)threadIdx.x) % 189) / 63) * 4)) + c_c_inner) % 84) * 4) + di_outer_inner) - ((((int)blockIdx.x) % 7) * 48))]));\n            DepthwiseConv2d_local[((((b_c_outer_inner * 12) + (c_c_inner * 3)) + i_c_outer_inner) + 36)] = (DepthwiseConv2d_local[((((b_c_outer_inner * 12) + (c_c_inner * 3)) + i_c_outer_inner) + 36)] + (PaddedInput_shared[((((((((((int)threadIdx.x) / 189) * 1764) + (b_c_outer_inner * 588)) + (((((int)threadIdx.x) % 63) / 7) * 21)) + (i_c_outer_inner * 7)) + (di_outer_inner * 7)) + (((int)threadIdx.x) % 7)) + 189)] * kernel_shared[(((((((((int)blockIdx.x) * 12) + (((((int)threadIdx.x) % 189) / 63) * 4)) + c_c_inner) % 84) * 4) + di_outer_inner) - ((((int)blockIdx.x) % 7) * 48))]));\n            DepthwiseConv2d_local[((((b_c_outer_inner * 12) + (c_c_inner * 3)) + i_c_outer_inner) + 72)] = (DepthwiseConv2d_local[((((b_c_outer_inner * 12) + (c_c_inner * 3)) + i_c_outer_inner) + 72)] + (PaddedInput_shared[((((((((((int)threadIdx.x) / 189) * 1764) + (b_c_outer_inner * 588)) + (((((int)threadIdx.x) % 63) / 7) * 21)) + (i_c_outer_inner * 7)) + (di_outer_inner * 7)) + (((int)threadIdx.x) % 7)) + 378)] * kernel_shared[(((((((((int)blockIdx.x) * 12) + (((((int)threadIdx.x) % 189) / 63) * 4)) + c_c_inner) % 84) * 4) + di_outer_inner) - ((((int)blockIdx.x) % 7) * 48))]));\n          },\n        },\n      },\n    },\n  },\n  for (int b_inner = 0; b_inner < 3; ++b_inner) {\n    for (int c_inner = 0; c_inner < 4; ++c_inner) {\n      for (int i_inner = 0; i_inner < 3; ++i_inner) {\n        DepthwiseConv2d[(((((((((((int)threadIdx.x) / 189) * 6715548) + (b_inner * 2238516)) + (((int)blockIdx.x) * 6804)) + (((((int)threadIdx.x) % 189) / 63) * 2268)) + (c_inner * 567)) + (((((int)threadIdx.x) % 63) / 7) * 21)) + (i_inner * 7)) + (((int)threadIdx.x) % 7))] = DepthwiseConv2d_local[(((b_inner * 12) + (c_inner * 3)) + i_inner)];\n        DepthwiseConv2d[((((((((((((int)threadIdx.x) / 189) * 6715548) + (b_inner * 2238516)) + (((int)blockIdx.x) * 6804)) + (((((int)threadIdx.x) % 189) / 63) * 2268)) + (c_inner * 567)) + (((((int)threadIdx.x) % 63) / 7) * 21)) + (i_inner * 7)) + (((int)threadIdx.x) % 7)) + 189)] = DepthwiseConv2d_local[((((b_inner * 12) + (c_inner * 3)) + i_inner) + 36)];\n        DepthwiseConv2d[((((((((((((int)threadIdx.x) / 189) * 6715548) + (b_inner * 2238516)) + (((int)blockIdx.x) * 6804)) + (((((int)threadIdx.x) % 189) / 63) * 2268)) + (c_inner * 567)) + (((((int)threadIdx.x) % 63) / 7) * 21)) + (i_inner * 7)) + (((int)threadIdx.x) % 7)) + 378)] = DepthwiseConv2d_local[((((b_inner * 12) + (c_inner * 3)) + i_inner) + 72)];\n      },\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 47, 84, 9), \"float32\"), kernel: T.Buffer((47, 84, 4, 3), \"float32\"), DepthwiseConv2d: T.Buffer((9, 3948, 81, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused in T.parallel(1974):\n            DepthwiseConv2d_local = T.allocate([126], \"float32\", \"local\")\n            for i_outer_inner in range(81):\n                DepthwiseConv2d_local_1 = T.Buffer((126,), data=DepthwiseConv2d_local, scope=\"local\")\n                for b_c_outer_inner_init, c_c_outer_inner_init, j_c_outer_inner_init in T.grid(3, 6, 7):\n                    DepthwiseConv2d_local_1[b_c_outer_inner_init * 42 + c_c_outer_inner_init * 7 + j_c_outer_inner_init] = T.float32(0)\n                for di_outer, b_c_outer_inner, c_c_outer_inner, j_c_outer_inner, dj_inner in T.grid(4, 3, 6, 7, 3):\n                    cse_var_2: T.int32 = b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 658\n                    cse_var_1: T.int32 = b_c_outer_inner * 42 + c_c_outer_inner * 7 + j_c_outer_inner\n                    data_1 = T.Buffer((319788,), data=data.data)\n                    kernel_1 = T.Buffer((47376,), data=kernel.data)\n                    DepthwiseConv2d_local_1[cse_var_1] = DepthwiseConv2d_local_1[cse_var_1] + data_1[b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused // 658 * 106596 + b_c_outer_inner * 35532 + cse_var_2 // 14 * 756 + i_outer_inner * 9 + di_outer * 9 + j_c_outer_inner + dj_inner] * kernel_1[cse_var_2 * 72 + c_c_outer_inner * 12 + di_outer * 3 + dj_inner]\n                for b_inner, c_inner in T.grid(3, 6):\n                    DepthwiseConv2d_1 = T.Buffer((20146644,), data=DepthwiseConv2d.data)\n                    DepthwiseConv2d_1[b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused // 658 * 6715548 + b_inner * 2238516 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 658 * 3402 + c_inner * 567 + i_outer_inner * 7:b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused // 658 * 6715548 + b_inner * 2238516 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused % 658 * 3402 + c_inner * 567 + i_outer_inner * 7 + 7] = DepthwiseConv2d_local_1[b_inner * 42 + c_inner * 7:b_inner * 42 + c_inner * 7 + 7]",
        "data": "9_47_84_9",
        "kernel": "47_84_4_3"
    },
    {
        "op_name": "depthwise_conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t DepthwiseConv2d_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* DepthwiseConv2d = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* DepthwiseConv2d_1 = (((DLTensor*)DepthwiseConv2d)[0].data);\n  void* default_function_DepthwiseConv2d_shape = (((DLTensor*)DepthwiseConv2d)[0].shape);\n  void* default_function_DepthwiseConv2d_strides = (((DLTensor*)DepthwiseConv2d)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_DepthwiseConv2d_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused = 0; b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused < 396; ++b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused) {\n    void* DepthwiseConv2d_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)15600, 2, 32);\n    if (DepthwiseConv2d_local == NULL) {\n      return -1;\n    },\n    for (int32_t c_c_outer_inner_init = 0; c_c_outer_inner_init < 13; ++c_c_outer_inner_init) {\n      for (int32_t i_c_outer_inner_init = 0; i_c_outer_inner_init < 5; ++i_c_outer_inner_init) {\n        for (int32_t i_c_inner_init = 0; i_c_inner_init < 15; ++i_c_inner_init) {\n          ((float4*)DepthwiseConv2d_local)[(((c_c_outer_inner_init * 75) + (i_c_outer_inner_init * 15)) + i_c_inner_init)] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n        },\n      },\n    },\n    for (int32_t dj_outer = 0; dj_outer < 3; ++dj_outer) {\n      for (int32_t c_c_outer_inner = 0; c_c_outer_inner < 13; ++c_c_outer_inner) {\n        for (int32_t i_c_outer_inner = 0; i_c_outer_inner < 5; ++i_c_outer_inner) {\n          for (int32_t di_inner = 0; di_inner < 4; ++di_inner) {\n            for (int32_t i_c_inner = 0; i_c_inner < 15; ++i_c_inner) {\n              int32_t cse_var_1 = (((c_c_outer_inner * 75) + (i_c_outer_inner * 15)) + i_c_inner);\n              int32_t4 v_ = int32_t4(((((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused / 12) * 780) + (i_c_outer_inner * 150)) + (i_c_inner * 10)) + (di_inner * 10)) + ((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused & 1) * 4)) + dj_outer))+(1*0), ((((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused / 12) * 780) + (i_c_outer_inner * 150)) + (i_c_inner * 10)) + (di_inner * 10)) + ((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused & 1) * 4)) + dj_outer))+(1*1), ((((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused / 12) * 780) + (i_c_outer_inner * 150)) + (i_c_inner * 10)) + (di_inner * 10)) + ((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused & 1) * 4)) + dj_outer))+(1*2), ((((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused / 12) * 780) + (i_c_outer_inner * 150)) + (i_c_inner * 10)) + (di_inner * 10)) + ((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused & 1) * 4)) + dj_outer))+(1*3));\n              ((float4*)DepthwiseConv2d_local)[cse_var_1] = (((float4*)DepthwiseConv2d_local)[cse_var_1] + ((float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3])) * ((float4)(((float*)kernel_1)[(((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused >> 1) * 156) + (c_c_outer_inner * 12)) + (di_inner * 3)) + dj_outer)], ((float*)kernel_1)[(((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused >> 1) * 156) + (c_c_outer_inner * 12)) + (di_inner * 3)) + dj_outer)], ((float*)kernel_1)[(((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused >> 1) * 156) + (c_c_outer_inner * 12)) + (di_inner * 3)) + dj_outer)], ((float*)kernel_1)[(((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused >> 1) * 156) + (c_c_outer_inner * 12)) + (di_inner * 3)) + dj_outer)]))));\n            },\n          },\n        },\n      },\n    },\n    for (int32_t c_inner = 0; c_inner < 13; ++c_inner) {\n      for (int32_t i_inner = 0; i_inner < 75; ++i_inner) {\n        *(float4*)(((float*)DepthwiseConv2d_1) + (((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused >> 1) * 7800) + (c_inner * 600)) + (i_inner * 8)) + ((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused & 1) * 4))) = ((float4*)DepthwiseConv2d_local)[((c_inner * 75) + i_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, DepthwiseConv2d_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel) {\n  float DepthwiseConv2d_local[390];\n  __shared__ float PaddedInput_shared[750];\n  __shared__ float kernel_shared[117];\n  for (int c_c_inner_init = 0; c_c_inner_init < 39; ++c_c_inner_init) {\n    for (int i_c_inner_init = 0; i_c_inner_init < 5; ++i_c_inner_init) {\n      for (int j_c_inner_init = 0; j_c_inner_init < 2; ++j_c_inner_init) {\n        DepthwiseConv2d_local[(((c_c_inner_init * 10) + (i_c_inner_init * 2)) + j_c_inner_init)] = 0.000000e+00f;\n      },\n    },\n  },\n  for (int di_outer_outer = 0; di_outer_outer < 4; ++di_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 13; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 2) + (((int)threadIdx.x) / 30)) < 25) {\n        PaddedInput_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 60) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) >> 1) * 780) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 60)) + (di_outer_outer * 10)) + ((int)threadIdx.x))];\n      },\n    },\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) {\n      if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 20) + (((int)threadIdx.x) / 3)) < 39) {\n        kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 60) + ((int)threadIdx.x))] = kernel[(((((((int)blockIdx.x) * 468) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 240)) + ((((int)threadIdx.x) / 3) * 12)) + (di_outer_outer * 3)) + (((int)threadIdx.x) % 3))];\n      },\n    },\n    __syncthreads();\n    for (int dj_inner = 0; dj_inner < 3; ++dj_inner) {\n      for (int c_c_inner = 0; c_c_inner < 39; ++c_c_inner) {\n        for (int i_c_inner = 0; i_c_inner < 5; ++i_c_inner) {\n          for (int j_c_inner = 0; j_c_inner < 2; ++j_c_inner) {\n            DepthwiseConv2d_local[(((c_c_inner * 10) + (i_c_inner * 2)) + j_c_inner)] = (DepthwiseConv2d_local[(((c_c_inner * 10) + (i_c_inner * 2)) + j_c_inner)] + (PaddedInput_shared[((((((((int)threadIdx.x) >> 2) * 50) + (i_c_inner * 10)) + ((((int)threadIdx.x) & 3) * 2)) + j_c_inner) + dj_inner)] * kernel_shared[((c_c_inner * 3) + dj_inner)]));\n          },\n        },\n      },\n    },\n  },\n  for (int c_inner = 0; c_inner < 39; ++c_inner) {\n    for (int i_inner = 0; i_inner < 5; ++i_inner) {\n      for (int j_inner = 0; j_inner < 2; ++j_inner) {\n        DepthwiseConv2d[((((((((int)blockIdx.x) * 23400) + (c_inner * 600)) + ((((int)threadIdx.x) >> 2) * 40)) + (i_inner * 8)) + ((((int)threadIdx.x) & 3) * 2)) + j_inner)] = DepthwiseConv2d_local[(((c_inner * 10) + (i_inner * 2)) + j_inner)];\n      },\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 33, 78, 10), \"float32\"), kernel: T.Buffer((33, 78, 4, 3), \"float32\"), DepthwiseConv2d: T.Buffer((1, 2574, 75, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused in T.parallel(396):\n            DepthwiseConv2d_local = T.allocate([975], \"float32x4\", \"local\")\n            DepthwiseConv2d_local_1 = T.Buffer((975,), \"float32x4\", data=DepthwiseConv2d_local, scope=\"local\")\n            for c_c_outer_inner_init, i_c_outer_inner_init, i_c_inner_init in T.grid(13, 5, 15):\n                DepthwiseConv2d_local_1[c_c_outer_inner_init * 75 + i_c_outer_inner_init * 15 + i_c_inner_init] = T.Broadcast(T.float32(0), 4)\n            for dj_outer, c_c_outer_inner, i_c_outer_inner, di_inner, i_c_inner in T.grid(3, 13, 5, 4, 15):\n                cse_var_1: T.int32 = c_c_outer_inner * 75 + i_c_outer_inner * 15 + i_c_inner\n                data_1 = T.Buffer((25740,), data=data.data)\n                kernel_1 = T.Buffer((30888,), data=kernel.data)\n                DepthwiseConv2d_local_1[cse_var_1] = DepthwiseConv2d_local_1[cse_var_1] + data_1[b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused // 12 * 780 + i_c_outer_inner * 150 + i_c_inner * 10 + di_inner * 10 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 2 * 4 + dj_outer:b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused // 12 * 780 + i_c_outer_inner * 150 + i_c_inner * 10 + di_inner * 10 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 2 * 4 + dj_outer + 4] * T.Broadcast(kernel_1[b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused // 2 * 156 + c_c_outer_inner * 12 + di_inner * 3 + dj_outer], 4)\n            for c_inner, i_inner in T.grid(13, 75):\n                DepthwiseConv2d_1 = T.Buffer((1544400,), data=DepthwiseConv2d.data)\n                DepthwiseConv2d_1[b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused // 2 * 7800 + c_inner * 600 + i_inner * 8 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 2 * 4:b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused // 2 * 7800 + c_inner * 600 + i_inner * 8 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 2 * 4 + 4] = DepthwiseConv2d_local_1[c_inner * 75 + i_inner]",
        "data": "1_33_78_10",
        "kernel": "33_78_4_3"
    },
    {
        "op_name": "depthwise_conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t DepthwiseConv2d_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* DepthwiseConv2d = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* DepthwiseConv2d_1 = (((DLTensor*)DepthwiseConv2d)[0].data);\n  void* default_function_DepthwiseConv2d_shape = (((DLTensor*)DepthwiseConv2d)[0].shape);\n  void* default_function_DepthwiseConv2d_strides = (((DLTensor*)DepthwiseConv2d)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_DepthwiseConv2d_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused < 27000; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused) {\n    for (int32_t i_inner_init = 0; i_inner_init < 58; ++i_inner_init) {\n      *(float2*)(((float*)DepthwiseConv2d_1) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused * 116) + (i_inner_init * 2))) = ((float2)(0.000000e+00f, 0.000000e+00f));\n    },\n    for (int32_t di_inner = 0; di_inner < 3; ++di_inner) {\n      for (int32_t dj_inner = 0; dj_inner < 3; ++dj_inner) {\n        for (int32_t i_inner = 0; i_inner < 58; ++i_inner) {\n          int32_t cse_var_1 = ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused * 116) + (i_inner * 2));\n          int32_t2 v_ = int32_t2((cse_var_1)+(1*0), (cse_var_1)+(1*1));\n          int32_t2 v__1 = int32_t2(((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused / 60) * 240) + (i_inner * 4)) + (di_inner * 4)) + dj_inner))+(1*0), ((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused / 60) * 240) + (i_inner * 4)) + (di_inner * 4)) + dj_inner))+(1*1));\n          *(float2*)(((float*)DepthwiseConv2d_1) + cse_var_1) = ((float2(((float*)DepthwiseConv2d_1)[v_.s0],((float*)DepthwiseConv2d_1)[v_.s1])) + ((float2(((float*)data_1)[v__1.s0],((float*)data_1)[v__1.s1])) * ((float2)(((float*)kernel_1)[((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 4500) * 9) + (di_inner * 3)) + dj_inner)], ((float*)kernel_1)[((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 4500) * 9) + (di_inner * 3)) + dj_inner)]))));\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(58) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(58) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel) {\n  float DepthwiseConv2d_local[24];\n  __shared__ float PaddedInput_shared[232];\n  __shared__ float kernel_shared[6];\n  for (int c_c_outer_inner_init = 0; c_c_outer_inner_init < 3; ++c_c_outer_inner_init) {\n    for (int j_c_inner_init = 0; j_c_inner_init < 2; ++j_c_inner_init) {\n      DepthwiseConv2d_local[((c_c_outer_inner_init * 2) + j_c_inner_init)] = 0.000000e+00f;\n      DepthwiseConv2d_local[(((c_c_outer_inner_init * 2) + j_c_inner_init) + 6)] = 0.000000e+00f;\n      DepthwiseConv2d_local[(((c_c_outer_inner_init * 2) + j_c_inner_init) + 12)] = 0.000000e+00f;\n      DepthwiseConv2d_local[(((c_c_outer_inner_init * 2) + j_c_inner_init) + 18)] = 0.000000e+00f;\n    },\n  },\n  for (int di_outer_outer = 0; di_outer_outer < 3; ++di_outer_outer) {\n    for (int dj_outer_outer = 0; dj_outer_outer < 3; ++dj_outer_outer) {\n      __syncthreads();\n      for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n        PaddedInput_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 58) + ((int)threadIdx.x))] = data[(((((((((((int)blockIdx.x) / 750) * 36000) + ((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer >> 1) * 18000)) + (((((int)blockIdx.x) % 750) / 10) * 240)) + ((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer & 1) * 116)) + ((((int)threadIdx.x) >> 1) * 4)) + (di_outer_outer * 4)) + dj_outer_outer) + (((int)threadIdx.x) & 1))];\n      },\n      if (((int)threadIdx.x) < 6) {\n        kernel_shared[((int)threadIdx.x)] = kernel[(((((((int)blockIdx.x) % 750) * 54) + (((int)threadIdx.x) * 9)) + (di_outer_outer * 3)) + dj_outer_outer)];\n      },\n      __syncthreads();\n      for (int c_c_outer_inner = 0; c_c_outer_inner < 3; ++c_c_outer_inner) {\n        for (int j_c_inner = 0; j_c_inner < 2; ++j_c_inner) {\n          DepthwiseConv2d_local[((c_c_outer_inner * 2) + j_c_inner)] = (DepthwiseConv2d_local[((c_c_outer_inner * 2) + j_c_inner)] + (PaddedInput_shared[((((int)threadIdx.x) * 2) + j_c_inner)] * kernel_shared[c_c_outer_inner]));\n          DepthwiseConv2d_local[(((c_c_outer_inner * 2) + j_c_inner) + 6)] = (DepthwiseConv2d_local[(((c_c_outer_inner * 2) + j_c_inner) + 6)] + (PaddedInput_shared[((((int)threadIdx.x) * 2) + j_c_inner)] * kernel_shared[((((((((int)blockIdx.x) % 750) * 6) + c_c_outer_inner) + 3) % 60) - ((((int)blockIdx.x) % 10) * 6))]));\n          DepthwiseConv2d_local[(((c_c_outer_inner * 2) + j_c_inner) + 12)] = (DepthwiseConv2d_local[(((c_c_outer_inner * 2) + j_c_inner) + 12)] + (PaddedInput_shared[(((((int)threadIdx.x) * 2) + j_c_inner) + 116)] * kernel_shared[c_c_outer_inner]));\n          DepthwiseConv2d_local[(((c_c_outer_inner * 2) + j_c_inner) + 18)] = (DepthwiseConv2d_local[(((c_c_outer_inner * 2) + j_c_inner) + 18)] + (PaddedInput_shared[(((((int)threadIdx.x) * 2) + j_c_inner) + 116)] * kernel_shared[((((((((int)blockIdx.x) % 750) * 6) + c_c_outer_inner) + 3) % 60) - ((((int)blockIdx.x) % 10) * 6))]));\n        },\n      },\n    },\n  },\n  for (int c_inner = 0; c_inner < 3; ++c_inner) {\n    for (int j_inner = 0; j_inner < 2; ++j_inner) {\n      DepthwiseConv2d[((((((((int)blockIdx.x) / 750) * 1044000) + ((((int)blockIdx.x) % 750) * 696)) + (c_inner * 116)) + (((int)threadIdx.x) * 2)) + j_inner)] = DepthwiseConv2d_local[((c_inner * 2) + j_inner)];\n      DepthwiseConv2d[(((((((((int)blockIdx.x) / 750) * 1044000) + ((((int)blockIdx.x) % 750) * 696)) + (c_inner * 116)) + (((int)threadIdx.x) * 2)) + j_inner) + 348)] = DepthwiseConv2d_local[(((c_inner * 2) + j_inner) + 6)];\n      DepthwiseConv2d[(((((((((int)blockIdx.x) / 750) * 1044000) + ((((int)blockIdx.x) % 750) * 696)) + (c_inner * 116)) + (((int)threadIdx.x) * 2)) + j_inner) + 522000)] = DepthwiseConv2d_local[(((c_inner * 2) + j_inner) + 12)];\n      DepthwiseConv2d[(((((((((int)blockIdx.x) / 750) * 1044000) + ((((int)blockIdx.x) % 750) * 696)) + (c_inner * 116)) + (((int)threadIdx.x) * 2)) + j_inner) + 522348)] = DepthwiseConv2d_local[(((c_inner * 2) + j_inner) + 18)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 75, 60, 4), \"float32\"), kernel: T.Buffer((75, 60, 3, 3), \"float32\"), DepthwiseConv2d: T.Buffer((6, 4500, 58, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused in T.parallel(27000):\n            DepthwiseConv2d_1 = T.Buffer((3132000,), data=DepthwiseConv2d.data)\n            for i_inner_init in range(58):\n                DepthwiseConv2d_1[b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused * 116 + i_inner_init * 2:b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused * 116 + i_inner_init * 2 + 2] = T.Broadcast(T.float32(0), 2)\n            for di_inner, dj_inner, i_inner in T.grid(3, 3, 58):\n                cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused * 116 + i_inner * 2\n                data_1 = T.Buffer((108000,), data=data.data)\n                kernel_1 = T.Buffer((40500,), data=kernel.data)\n                DepthwiseConv2d_1[cse_var_1:cse_var_1 + 2] = DepthwiseConv2d_1[cse_var_1:cse_var_1 + 2] + data_1[b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused // 60 * 240 + i_inner * 4 + di_inner * 4 + dj_inner:b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused // 60 * 240 + i_inner * 4 + di_inner * 4 + dj_inner + 2] * T.Broadcast(kernel_1[b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused % 4500 * 9 + di_inner * 3 + dj_inner], 2)",
        "data": "6_75_60_4",
        "kernel": "75_60_3_3"
    },
    {
        "op_name": "depthwise_conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t DepthwiseConv2d_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* DepthwiseConv2d = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* DepthwiseConv2d_1 = (((DLTensor*)DepthwiseConv2d)[0].data);\n  void* default_function_DepthwiseConv2d_shape = (((DLTensor*)DepthwiseConv2d)[0].shape);\n  void* default_function_DepthwiseConv2d_strides = (((DLTensor*)DepthwiseConv2d)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_DepthwiseConv2d_strides == NULL)) {\n  },\n  for (int32_t b_outer_c_outer_fused = 0; b_outer_c_outer_fused < 8722; ++b_outer_c_outer_fused) {\n    void* DepthwiseConv2d_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)3584, 2, 32);\n    if (DepthwiseConv2d_local == NULL) {\n      return -1;\n    },\n    for (int32_t i_outer = 0; i_outer < 3; ++i_outer) {\n      for (int32_t i_c_outer_inner_init = 0; i_c_outer_inner_init < 16; ++i_c_outer_inner_init) {\n        for (int32_t b_c_inner_init = 0; b_c_inner_init < 4; ++b_c_inner_init) {\n          for (int32_t i_c_inner_init = 0; i_c_inner_init < 2; ++i_c_inner_init) {\n            ((float7*)DepthwiseConv2d_local)[(((b_c_inner_init * 32) + (i_c_outer_inner_init * 2)) + i_c_inner_init)] = ((float7)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n          },\n        },\n      },\n      for (int32_t dj_outer = 0; dj_outer < 4; ++dj_outer) {\n        for (int32_t i_c_outer_inner = 0; i_c_outer_inner < 16; ++i_c_outer_inner) {\n          for (int32_t di_inner = 0; di_inner < 3; ++di_inner) {\n            for (int32_t b_c_inner = 0; b_c_inner < 4; ++b_c_inner) {\n              for (int32_t i_c_inner = 0; i_c_inner < 2; ++i_c_inner) {\n                int32_t cse_var_1 = (((b_c_inner * 32) + (i_c_outer_inner * 2)) + i_c_inner);\n                int32_t7 v_ = int32_t7(((((((((b_c_inner * 87220) + ((b_outer_c_outer_fused / 98) * 980)) + (i_outer * 320)) + (i_c_outer_inner * 20)) + (i_c_inner * 10)) + (di_inner * 10)) + dj_outer))+(1*0), ((((((((b_c_inner * 87220) + ((b_outer_c_outer_fused / 98) * 980)) + (i_outer * 320)) + (i_c_outer_inner * 20)) + (i_c_inner * 10)) + (di_inner * 10)) + dj_outer))+(1*1), ((((((((b_c_inner * 87220) + ((b_outer_c_outer_fused / 98) * 980)) + (i_outer * 320)) + (i_c_outer_inner * 20)) + (i_c_inner * 10)) + (di_inner * 10)) + dj_outer))+(1*2), ((((((((b_c_inner * 87220) + ((b_outer_c_outer_fused / 98) * 980)) + (i_outer * 320)) + (i_c_outer_inner * 20)) + (i_c_inner * 10)) + (di_inner * 10)) + dj_outer))+(1*3), ((((((((b_c_inner * 87220) + ((b_outer_c_outer_fused / 98) * 980)) + (i_outer * 320)) + (i_c_outer_inner * 20)) + (i_c_inner * 10)) + (di_inner * 10)) + dj_outer))+(1*4), ((((((((b_c_inner * 87220) + ((b_outer_c_outer_fused / 98) * 980)) + (i_outer * 320)) + (i_c_outer_inner * 20)) + (i_c_inner * 10)) + (di_inner * 10)) + dj_outer))+(1*5), ((((((((b_c_inner * 87220) + ((b_outer_c_outer_fused / 98) * 980)) + (i_outer * 320)) + (i_c_outer_inner * 20)) + (i_c_inner * 10)) + (di_inner * 10)) + dj_outer))+(1*6));\n                ((float7*)DepthwiseConv2d_local)[cse_var_1] = (((float7*)DepthwiseConv2d_local)[cse_var_1] + ((float7(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6])) * ((float7)(((float*)kernel_1)[(((b_outer_c_outer_fused * 12) + (di_inner * 4)) + dj_outer)], ((float*)kernel_1)[(((b_outer_c_outer_fused * 12) + (di_inner * 4)) + dj_outer)], ((float*)kernel_1)[(((b_outer_c_outer_fused * 12) + (di_inner * 4)) + dj_outer)], ((float*)kernel_1)[(((b_outer_c_outer_fused * 12) + (di_inner * 4)) + dj_outer)], ((float*)kernel_1)[(((b_outer_c_outer_fused * 12) + (di_inner * 4)) + dj_outer)], ((float*)kernel_1)[(((b_outer_c_outer_fused * 12) + (di_inner * 4)) + dj_outer)], ((float*)kernel_1)[(((b_outer_c_outer_fused * 12) + (di_inner * 4)) + dj_outer)]))));\n              },\n            },\n          },\n        },\n      },\n      for (int32_t b_inner = 0; b_inner < 4; ++b_inner) {\n        for (int32_t i_inner = 0; i_inner < 32; ++i_inner) {\n          *(float7*)(((float*)DepthwiseConv2d_1) + ((((b_inner * 5861184) + (b_outer_c_outer_fused * 672)) + (i_outer * 224)) + (i_inner * 7))) = ((float7*)DepthwiseConv2d_local)[((b_inner * 32) + i_inner)];\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, DepthwiseConv2d_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(294) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(294) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel) {\n  float DepthwiseConv2d_local[112];\n  __shared__ float PaddedInput_shared[960];\n  __shared__ float kernel_shared[196];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 2; ++b_c_outer_inner_init) {\n    for (int c_c_outer_inner_init = 0; c_c_outer_inner_init < 7; ++c_c_outer_inner_init) {\n      for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 2; ++i_c_outer_inner_init) {\n        for (int b_c_inner_init = 0; b_c_inner_init < 2; ++b_c_inner_init) {\n          for (int i_c_inner_init = 0; i_c_inner_init < 2; ++i_c_inner_init) {\n            DepthwiseConv2d_local[(((((b_c_outer_inner_init * 56) + (b_c_inner_init * 28)) + (c_c_outer_inner_init * 4)) + (i_c_outer_inner_init * 2)) + i_c_inner_init)] = 0.000000e+00f;\n          },\n        },\n      },\n    },\n  },\n  for (int di_outer_outer = 0; di_outer_outer < 3; ++di_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 49) + (((int)threadIdx.x) / 6)) < 160) {\n        PaddedInput_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 294) + ((int)threadIdx.x))] = data[(((((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 49) + (((int)threadIdx.x) / 6)) / 40) * 87220) + ((((int)blockIdx.x) >> 3) * 980)) + ((((int)blockIdx.x) & 3) * 240)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 27) + (((int)threadIdx.x) >> 1)) % 120) / 5) * 10)) + (di_outer_outer * 10)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 4) + ((int)threadIdx.x)) % 10))];\n      },\n    },\n    if (((int)threadIdx.x) < 196) {\n      if (0 <= ((((((int)blockIdx.x) >> 2) * 49) + (((int)threadIdx.x) >> 2)) - ((((int)blockIdx.x) >> 3) * 98))) {\n        kernel_shared[((int)threadIdx.x)] = kernel[(((((((int)blockIdx.x) >> 2) * 588) + ((((int)threadIdx.x) >> 2) * 12)) + (di_outer_outer * 4)) + (((int)threadIdx.x) & 3))];\n      },\n    },\n    __syncthreads();\n    for (int dj_outer_inner = 0; dj_outer_inner < 2; ++dj_outer_inner) {\n      for (int b_c_outer_inner = 0; b_c_outer_inner < 2; ++b_c_outer_inner) {\n        for (int c_c_outer_inner = 0; c_c_outer_inner < 7; ++c_c_outer_inner) {\n          for (int i_c_outer_inner = 0; i_c_outer_inner < 2; ++i_c_outer_inner) {\n            for (int dj_inner = 0; dj_inner < 2; ++dj_inner) {\n              for (int b_c_inner = 0; b_c_inner < 2; ++b_c_inner) {\n                for (int i_c_inner = 0; i_c_inner < 2; ++i_c_inner) {\n                  DepthwiseConv2d_local[(((((b_c_outer_inner * 56) + (b_c_inner * 28)) + (c_c_outer_inner * 4)) + (i_c_outer_inner * 2)) + i_c_inner)] = (DepthwiseConv2d_local[(((((b_c_outer_inner * 56) + (b_c_inner * 28)) + (c_c_outer_inner * 4)) + (i_c_outer_inner * 2)) + i_c_inner)] + (PaddedInput_shared[((((((((b_c_outer_inner * 480) + (b_c_inner * 240)) + (((((int)threadIdx.x) % 42) / 7) * 40)) + (i_c_outer_inner * 20)) + (i_c_inner * 10)) + (dj_outer_inner * 2)) + dj_inner) + (((int)threadIdx.x) % 7))] * kernel_shared[((((((((int)blockIdx.x) >> 3) * 392) + ((((((((int)blockIdx.x) >> 2) * 49) + ((((int)threadIdx.x) / 42) * 7)) + c_c_outer_inner) % 98) * 4)) + (dj_outer_inner * 2)) + dj_inner) - ((((int)blockIdx.x) >> 2) * 196))]));\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int b_inner = 0; b_inner < 4; ++b_inner) {\n    for (int c_inner = 0; c_inner < 7; ++c_inner) {\n      for (int i_inner = 0; i_inner < 4; ++i_inner) {\n        DepthwiseConv2d[((((((((b_inner * 5861184) + ((((int)blockIdx.x) >> 2) * 32928)) + ((((int)threadIdx.x) / 42) * 4704)) + (c_inner * 672)) + ((((int)blockIdx.x) & 3) * 168)) + (((((int)threadIdx.x) % 42) / 7) * 28)) + (i_inner * 7)) + (((int)threadIdx.x) % 7))] = DepthwiseConv2d_local[(((b_inner * 28) + (c_inner * 4)) + i_inner)];\n      },\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 89, 98, 10), \"float32\"), kernel: T.Buffer((89, 98, 3, 4), \"float32\"), DepthwiseConv2d: T.Buffer((4, 8722, 96, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_c_outer_fused in T.parallel(8722):\n            DepthwiseConv2d_local = T.allocate([128], \"float32x7\", \"local\")\n            for i_outer in range(3):\n                DepthwiseConv2d_local_1 = T.Buffer((128,), \"float32x7\", data=DepthwiseConv2d_local, scope=\"local\")\n                for i_c_outer_inner_init, b_c_inner_init, i_c_inner_init in T.grid(16, 4, 2):\n                    DepthwiseConv2d_local_1[b_c_inner_init * 32 + i_c_outer_inner_init * 2 + i_c_inner_init] = T.Broadcast(T.float32(0), 7)\n                for dj_outer, i_c_outer_inner, di_inner, b_c_inner, i_c_inner in T.grid(4, 16, 3, 4, 2):\n                    cse_var_1: T.int32 = b_c_inner * 32 + i_c_outer_inner * 2 + i_c_inner\n                    data_1 = T.Buffer((348880,), data=data.data)\n                    kernel_1 = T.Buffer((104664,), data=kernel.data)\n                    DepthwiseConv2d_local_1[cse_var_1] = DepthwiseConv2d_local_1[cse_var_1] + data_1[b_c_inner * 87220 + b_outer_c_outer_fused // 98 * 980 + i_outer * 320 + i_c_outer_inner * 20 + i_c_inner * 10 + di_inner * 10 + dj_outer:b_c_inner * 87220 + b_outer_c_outer_fused // 98 * 980 + i_outer * 320 + i_c_outer_inner * 20 + i_c_inner * 10 + di_inner * 10 + dj_outer + 7] * T.Broadcast(kernel_1[b_outer_c_outer_fused * 12 + di_inner * 4 + dj_outer], 7)\n                for b_inner, i_inner in T.grid(4, 32):\n                    DepthwiseConv2d_1 = T.Buffer((23444736,), data=DepthwiseConv2d.data)\n                    DepthwiseConv2d_1[b_inner * 5861184 + b_outer_c_outer_fused * 672 + i_outer * 224 + i_inner * 7:b_inner * 5861184 + b_outer_c_outer_fused * 672 + i_outer * 224 + i_inner * 7 + 7] = DepthwiseConv2d_local_1[b_inner * 32 + i_inner]",
        "data": "4_89_98_10",
        "kernel": "89_98_3_4"
    },
    {
        "op_name": "depthwise_conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t DepthwiseConv2d_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* DepthwiseConv2d = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* DepthwiseConv2d_1 = (((DLTensor*)DepthwiseConv2d)[0].data);\n  void* default_function_DepthwiseConv2d_shape = (((DLTensor*)DepthwiseConv2d)[0].shape);\n  void* default_function_DepthwiseConv2d_strides = (((DLTensor*)DepthwiseConv2d)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_DepthwiseConv2d_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused < 5394; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused) {\n    for (int32_t c_outer_inner_init = 0; c_outer_inner_init < 5; ++c_outer_inner_init) {\n      for (int32_t b_inner_init = 0; b_inner_init < 2; ++b_inner_init) {\n        *(float7*)(((float*)DepthwiseConv2d_1) + ((((b_inner_init * 188790) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused / 29) * 1015)) + (c_outer_inner_init * 203)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 29) * 7))) = ((float7)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n      },\n    },\n    for (int32_t di_outer = 0; di_outer < 3; ++di_outer) {\n      for (int32_t c_outer_inner = 0; c_outer_inner < 5; ++c_outer_inner) {\n        for (int32_t dj_inner = 0; dj_inner < 4; ++dj_inner) {\n          for (int32_t b_inner = 0; b_inner < 2; ++b_inner) {\n            int32_t cse_var_3 = (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused / 29);\n            int32_t cse_var_2 = (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 29);\n            int32_t cse_var_1 = ((((b_inner * 188790) + (cse_var_3 * 1015)) + (c_outer_inner * 203)) + (cse_var_2 * 7));\n            int32_t7 v_ = int32_t7((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6));\n            int32_t7 v__1 = int32_t7((((((((b_inner * 9300) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused / 899) * 1550)) + ((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 899) / 29) * 5) + c_outer_inner) / 31) * 310)) + (di_outer * 10)) + (cse_var_2 * 10)) + dj_inner))+(1*0), (((((((b_inner * 9300) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused / 899) * 1550)) + ((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 899) / 29) * 5) + c_outer_inner) / 31) * 310)) + (di_outer * 10)) + (cse_var_2 * 10)) + dj_inner))+(1*1), (((((((b_inner * 9300) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused / 899) * 1550)) + ((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 899) / 29) * 5) + c_outer_inner) / 31) * 310)) + (di_outer * 10)) + (cse_var_2 * 10)) + dj_inner))+(1*2), (((((((b_inner * 9300) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused / 899) * 1550)) + ((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 899) / 29) * 5) + c_outer_inner) / 31) * 310)) + (di_outer * 10)) + (cse_var_2 * 10)) + dj_inner))+(1*3), (((((((b_inner * 9300) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused / 899) * 1550)) + ((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 899) / 29) * 5) + c_outer_inner) / 31) * 310)) + (di_outer * 10)) + (cse_var_2 * 10)) + dj_inner))+(1*4), (((((((b_inner * 9300) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused / 899) * 1550)) + ((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 899) / 29) * 5) + c_outer_inner) / 31) * 310)) + (di_outer * 10)) + (cse_var_2 * 10)) + dj_inner))+(1*5), (((((((b_inner * 9300) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused / 899) * 1550)) + ((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 899) / 29) * 5) + c_outer_inner) / 31) * 310)) + (di_outer * 10)) + (cse_var_2 * 10)) + dj_inner))+(1*6));\n            *(float7*)(((float*)DepthwiseConv2d_1) + cse_var_1) = ((float7(((float*)DepthwiseConv2d_1)[v_.s0],((float*)DepthwiseConv2d_1)[v_.s1],((float*)DepthwiseConv2d_1)[v_.s2],((float*)DepthwiseConv2d_1)[v_.s3],((float*)DepthwiseConv2d_1)[v_.s4],((float*)DepthwiseConv2d_1)[v_.s5],((float*)DepthwiseConv2d_1)[v_.s6])) + ((float7(((float*)data_1)[v__1.s0],((float*)data_1)[v__1.s1],((float*)data_1)[v__1.s2],((float*)data_1)[v__1.s3],((float*)data_1)[v__1.s4],((float*)data_1)[v__1.s5],((float*)data_1)[v__1.s6])) * ((float7)(((float*)kernel_1)[((((cse_var_3 * 60) + (c_outer_inner * 12)) + (di_outer * 4)) + dj_inner)], ((float*)kernel_1)[((((cse_var_3 * 60) + (c_outer_inner * 12)) + (di_outer * 4)) + dj_inner)], ((float*)kernel_1)[((((cse_var_3 * 60) + (c_outer_inner * 12)) + (di_outer * 4)) + dj_inner)], ((float*)kernel_1)[((((cse_var_3 * 60) + (c_outer_inner * 12)) + (di_outer * 4)) + dj_inner)], ((float*)kernel_1)[((((cse_var_3 * 60) + (c_outer_inner * 12)) + (di_outer * 4)) + dj_inner)], ((float*)kernel_1)[((((cse_var_3 * 60) + (c_outer_inner * 12)) + (di_outer * 4)) + dj_inner)], ((float*)kernel_1)[((((cse_var_3 * 60) + (c_outer_inner * 12)) + (di_outer * 4)) + dj_inner)]))));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel) {\n  float DepthwiseConv2d_local[124];\n  __shared__ float PaddedInput_shared[600];\n  __shared__ float kernel_shared[3720];\n  for (int b_c_inner_init = 0; b_c_inner_init < 2; ++b_c_inner_init) {\n    for (int c_c_inner_init = 0; c_c_inner_init < 62; ++c_c_inner_init) {\n      DepthwiseConv2d_local[((b_c_inner_init * 62) + c_c_inner_init)] = 0.000000e+00f;\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 18; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 7) + (((int)threadIdx.x) / 5)) < 120) {\n      PaddedInput_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 35) + ((int)threadIdx.x))] = data[((((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 7) + (((int)threadIdx.x) / 5)) / 60) * 9300) + ((((int)blockIdx.x) / 29) * 3100)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 7) + (((int)threadIdx.x) / 5)) % 60) / 6) * 310)) + ((((int)blockIdx.x) % 29) * 10)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 5) + ((int)threadIdx.x)) % 30))];\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 < 27; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 7) + (((int)threadIdx.x) / 5)) < 186) {\n      *(float4*)(kernel_shared + ((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 140) + (((int)threadIdx.x) * 4))) = *(float4*)(kernel + ((((((int)blockIdx.x) / 29) * 3720) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 140)) + (((int)threadIdx.x) * 4)));\n    },\n  },\n  __syncthreads();\n  for (int dj_outer_inner = 0; dj_outer_inner < 4; ++dj_outer_inner) {\n    for (int di_inner = 0; di_inner < 3; ++di_inner) {\n      for (int b_c_inner = 0; b_c_inner < 2; ++b_c_inner) {\n        for (int c_c_inner = 0; c_c_inner < 62; ++c_c_inner) {\n          DepthwiseConv2d_local[((b_c_inner * 62) + c_c_inner)] = (DepthwiseConv2d_local[((b_c_inner * 62) + c_c_inner)] + (PaddedInput_shared[((((((b_c_inner * 300) + ((((int)threadIdx.x) / 7) * 60)) + ((c_c_inner / 31) * 30)) + (di_inner * 10)) + dj_outer_inner) + (((int)threadIdx.x) % 7))] * kernel_shared[(((((((int)threadIdx.x) / 7) * 744) + (c_c_inner * 12)) + (di_inner * 4)) + dj_outer_inner)]));\n        },\n      },\n    },\n  },\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    for (int c_inner = 0; c_inner < 62; ++c_inner) {\n      DepthwiseConv2d[((((((b_inner * 188790) + ((((int)blockIdx.x) / 29) * 62930)) + ((((int)threadIdx.x) / 7) * 12586)) + (c_inner * 203)) + ((((int)blockIdx.x) % 29) * 7)) + (((int)threadIdx.x) % 7))] = DepthwiseConv2d_local[((b_inner * 62) + c_inner)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 30, 31, 10), \"float32\"), kernel: T.Buffer((30, 31, 3, 4), \"float32\"), DepthwiseConv2d: T.Buffer((2, 930, 29, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused in T.parallel(5394):\n            DepthwiseConv2d_1 = T.Buffer((377580,), data=DepthwiseConv2d.data)\n            for c_outer_inner_init, b_inner_init in T.grid(5, 2):\n                DepthwiseConv2d_1[b_inner_init * 188790 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused // 29 * 1015 + c_outer_inner_init * 203 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 29 * 7:b_inner_init * 188790 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused // 29 * 1015 + c_outer_inner_init * 203 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 29 * 7 + 7] = T.Broadcast(T.float32(0), 7)\n            for di_outer, c_outer_inner, dj_inner, b_inner in T.grid(3, 5, 4, 2):\n                cse_var_3: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused // 29\n                cse_var_2: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 29\n                cse_var_1: T.int32 = b_inner * 188790 + cse_var_3 * 1015 + c_outer_inner * 203 + cse_var_2 * 7\n                data_1 = T.Buffer((18600,), data=data.data)\n                kernel_1 = T.Buffer((11160,), data=kernel.data)\n                DepthwiseConv2d_1[cse_var_1:cse_var_1 + 7] = DepthwiseConv2d_1[cse_var_1:cse_var_1 + 7] + data_1[b_inner * 9300 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused // 899 * 1550 + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 899 // 29 * 5 + c_outer_inner) // 31 * 310 + di_outer * 10 + cse_var_2 * 10 + dj_inner:b_inner * 9300 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused // 899 * 1550 + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused % 899 // 29 * 5 + c_outer_inner) // 31 * 310 + di_outer * 10 + cse_var_2 * 10 + dj_inner + 7] * T.Broadcast(kernel_1[cse_var_3 * 60 + c_outer_inner * 12 + di_outer * 4 + dj_inner], 7)",
        "data": "2_30_31_10",
        "kernel": "30_31_3_4"
    },
    {
        "op_name": "depthwise_conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t DepthwiseConv2d_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* DepthwiseConv2d = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* DepthwiseConv2d_1 = (((DLTensor*)DepthwiseConv2d)[0].data);\n  void* default_function_DepthwiseConv2d_shape = (((DLTensor*)DepthwiseConv2d)[0].shape);\n  void* default_function_DepthwiseConv2d_strides = (((DLTensor*)DepthwiseConv2d)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_DepthwiseConv2d_strides == NULL)) {\n  },\n  for (int32_t b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused = 0; b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused < 399; ++b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused) {\n    void* DepthwiseConv2d_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)6500, 2, 32);\n    if (DepthwiseConv2d_local == NULL) {\n      return -1;\n    },\n    for (int32_t c_c_outer_inner_init = 0; c_c_outer_inner_init < 25; ++c_c_outer_inner_init) {\n      for (int32_t i_c_inner_init = 0; i_c_inner_init < 13; ++i_c_inner_init) {\n        ((float5*)DepthwiseConv2d_local)[((c_c_outer_inner_init * 13) + i_c_inner_init)] = ((float5)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n      },\n    },\n    for (int32_t di_outer = 0; di_outer < 3; ++di_outer) {\n      for (int32_t dj_outer = 0; dj_outer < 2; ++dj_outer) {\n        for (int32_t c_c_outer_inner = 0; c_c_outer_inner < 25; ++c_c_outer_inner) {\n          for (int32_t dj_inner = 0; dj_inner < 2; ++dj_inner) {\n            for (int32_t i_c_inner = 0; i_c_inner < 13; ++i_c_inner) {\n              int32_t cse_var_4 = (b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 3);\n              int32_t cse_var_3 = (b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused / 21);\n              int32_t cse_var_2 = (dj_outer * 2);\n              int32_t cse_var_1 = ((c_c_outer_inner * 13) + i_c_inner);\n              int32_t5 v_ = int32_t5(((((((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 21) / 3) * 11400) + (cse_var_3 * 600)) + ((((cse_var_4 * 5) + (c_c_outer_inner / 5)) / 3) * 120)) + (i_c_inner * 8)) + (di_outer * 8)) + cse_var_2) + dj_inner))+(1*0), ((((((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 21) / 3) * 11400) + (cse_var_3 * 600)) + ((((cse_var_4 * 5) + (c_c_outer_inner / 5)) / 3) * 120)) + (i_c_inner * 8)) + (di_outer * 8)) + cse_var_2) + dj_inner))+(1*1), ((((((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 21) / 3) * 11400) + (cse_var_3 * 600)) + ((((cse_var_4 * 5) + (c_c_outer_inner / 5)) / 3) * 120)) + (i_c_inner * 8)) + (di_outer * 8)) + cse_var_2) + dj_inner))+(1*2), ((((((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 21) / 3) * 11400) + (cse_var_3 * 600)) + ((((cse_var_4 * 5) + (c_c_outer_inner / 5)) / 3) * 120)) + (i_c_inner * 8)) + (di_outer * 8)) + cse_var_2) + dj_inner))+(1*3), ((((((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 21) / 3) * 11400) + (cse_var_3 * 600)) + ((((cse_var_4 * 5) + (c_c_outer_inner / 5)) / 3) * 120)) + (i_c_inner * 8)) + (di_outer * 8)) + cse_var_2) + dj_inner))+(1*4));\n              ((float5*)DepthwiseConv2d_local)[cse_var_1] = (((float5*)DepthwiseConv2d_local)[cse_var_1] + ((float5(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4])) * ((float5)(((float*)kernel_1)[((((((cse_var_3 * 900) + (cse_var_4 * 300)) + (c_c_outer_inner * 12)) + (di_outer * 4)) + cse_var_2) + dj_inner)], ((float*)kernel_1)[((((((cse_var_3 * 900) + (cse_var_4 * 300)) + (c_c_outer_inner * 12)) + (di_outer * 4)) + cse_var_2) + dj_inner)], ((float*)kernel_1)[((((((cse_var_3 * 900) + (cse_var_4 * 300)) + (c_c_outer_inner * 12)) + (di_outer * 4)) + cse_var_2) + dj_inner)], ((float*)kernel_1)[((((((cse_var_3 * 900) + (cse_var_4 * 300)) + (c_c_outer_inner * 12)) + (di_outer * 4)) + cse_var_2) + dj_inner)], ((float*)kernel_1)[((((((cse_var_3 * 900) + (cse_var_4 * 300)) + (c_c_outer_inner * 12)) + (di_outer * 4)) + cse_var_2) + dj_inner)]))));\n            },\n          },\n        },\n      },\n    },\n    for (int32_t c_inner = 0; c_inner < 25; ++c_inner) {\n      for (int32_t i_inner = 0; i_inner < 13; ++i_inner) {\n        *(float5*)(((float*)DepthwiseConv2d_1) + (((((((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 21) / 3) * 92625) + ((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused / 21) * 4875)) + ((b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 3) * 1625)) + (c_inner * 65)) + (i_inner * 5))) = ((float5*)DepthwiseConv2d_local)[((c_inner * 13) + i_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, DepthwiseConv2d_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(175) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(175) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel) {\n  float DepthwiseConv2d_local[13];\n  __shared__ float PaddedInput_shared[525];\n  __shared__ float kernel_shared[15];\n  for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 13; ++i_c_outer_inner_init) {\n    DepthwiseConv2d_local[i_c_outer_inner_init] = 0.000000e+00f;\n  },\n  for (int dj_outer_outer = 0; dj_outer_outer < 4; ++dj_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 3; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n      PaddedInput_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 175) + ((int)threadIdx.x))] = data[((((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 7) + (((int)threadIdx.x) / 25)) / 3) * 11400) + ((((int)blockIdx.x) / 3) * 120)) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 5) + (((int)threadIdx.x) / 5)) % 15) * 8)) + dj_outer_outer) + (((int)threadIdx.x) % 5))];\n    },\n    if (((int)threadIdx.x) < 15) {\n      kernel_shared[((int)threadIdx.x)] = kernel[(((((int)blockIdx.x) * 60) + (((int)threadIdx.x) * 4)) + dj_outer_outer)];\n    },\n    __syncthreads();\n    for (int di_outer_inner = 0; di_outer_inner < 3; ++di_outer_inner) {\n      for (int i_c_outer_inner = 0; i_c_outer_inner < 13; ++i_c_outer_inner) {\n        DepthwiseConv2d_local[i_c_outer_inner] = (DepthwiseConv2d_local[i_c_outer_inner] + (PaddedInput_shared[(((((((int)threadIdx.x) / 25) * 75) + (i_c_outer_inner * 5)) + (di_outer_inner * 5)) + (((int)threadIdx.x) % 5))] * kernel_shared[((((((int)threadIdx.x) % 25) / 5) * 3) + di_outer_inner)]));\n      },\n    },\n  },\n  for (int i_inner = 0; i_inner < 13; ++i_inner) {\n    DepthwiseConv2d[((((((((int)threadIdx.x) / 25) * 92625) + (((int)blockIdx.x) * 325)) + (((((int)threadIdx.x) % 25) / 5) * 65)) + (i_inner * 5)) + (((int)threadIdx.x) % 5))] = DepthwiseConv2d_local[i_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 95, 15, 8), \"float32\"), kernel: T.Buffer((95, 15, 3, 4), \"float32\"), DepthwiseConv2d: T.Buffer((7, 1425, 13, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused in T.parallel(399):\n            DepthwiseConv2d_local = T.allocate([325], \"float32x5\", \"local\")\n            DepthwiseConv2d_local_1 = T.Buffer((325,), \"float32x5\", data=DepthwiseConv2d_local, scope=\"local\")\n            for c_c_outer_inner_init, i_c_inner_init in T.grid(25, 13):\n                DepthwiseConv2d_local_1[c_c_outer_inner_init * 13 + i_c_inner_init] = T.Broadcast(T.float32(0), 5)\n            for di_outer, dj_outer, c_c_outer_inner, dj_inner, i_c_inner in T.grid(3, 2, 25, 2, 13):\n                cse_var_4: T.int32 = b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 3\n                cse_var_3: T.int32 = b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused // 21\n                cse_var_2: T.int32 = dj_outer * 2\n                cse_var_1: T.int32 = c_c_outer_inner * 13 + i_c_inner\n                data_1 = T.Buffer((79800,), data=data.data)\n                kernel_1 = T.Buffer((17100,), data=kernel.data)\n                DepthwiseConv2d_local_1[cse_var_1] = DepthwiseConv2d_local_1[cse_var_1] + data_1[b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 21 // 3 * 11400 + cse_var_3 * 600 + (cse_var_4 * 5 + c_c_outer_inner // 5) // 3 * 120 + i_c_inner * 8 + di_outer * 8 + cse_var_2 + dj_inner:b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 21 // 3 * 11400 + cse_var_3 * 600 + (cse_var_4 * 5 + c_c_outer_inner // 5) // 3 * 120 + i_c_inner * 8 + di_outer * 8 + cse_var_2 + dj_inner + 5] * T.Broadcast(kernel_1[cse_var_3 * 900 + cse_var_4 * 300 + c_c_outer_inner * 12 + di_outer * 4 + cse_var_2 + dj_inner], 5)\n            for c_inner, i_inner in T.grid(25, 13):\n                DepthwiseConv2d_1 = T.Buffer((648375,), data=DepthwiseConv2d.data)\n                DepthwiseConv2d_1[b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 21 // 3 * 92625 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused // 21 * 4875 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 3 * 1625 + c_inner * 65 + i_inner * 5:b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 21 // 3 * 92625 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused // 21 * 4875 + b_outer_outer_c_outer_outer_fused_i_outer_outer_fused_j_outer_outer_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 3 * 1625 + c_inner * 65 + i_inner * 5 + 5] = DepthwiseConv2d_local_1[c_inner * 13 + i_inner]",
        "data": "7_95_15_8",
        "kernel": "95_15_3_4"
    },
    {
        "op_name": "depthwise_conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t DepthwiseConv2d_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* DepthwiseConv2d = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* DepthwiseConv2d_1 = (((DLTensor*)DepthwiseConv2d)[0].data);\n  void* default_function_DepthwiseConv2d_shape = (((DLTensor*)DepthwiseConv2d)[0].shape);\n  void* default_function_DepthwiseConv2d_strides = (((DLTensor*)DepthwiseConv2d)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_DepthwiseConv2d_strides == NULL)) {\n  },\n  for (int32_t b_outer_c_outer_fused_i_outer_fused_j_outer_fused = 0; b_outer_c_outer_fused_i_outer_fused_j_outer_fused < 92; ++b_outer_c_outer_fused_i_outer_fused_j_outer_fused) {\n    void* DepthwiseConv2d_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)7480, 2, 32);\n    if (DepthwiseConv2d_local == NULL) {\n      return -1;\n    },\n    for (int32_t c_c_outer_inner_init = 0; c_c_outer_inner_init < 2; ++c_c_outer_inner_init) {\n      for (int32_t i_c_outer_inner_init = 0; i_c_outer_inner_init < 85; ++i_c_outer_inner_init) {\n        for (int32_t c_c_inner_init = 0; c_c_inner_init < 11; ++c_c_inner_init) {\n          ((float*)DepthwiseConv2d_local)[(((c_c_outer_inner_init * 935) + (c_c_inner_init * 85)) + i_c_outer_inner_init)] = 0.000000e+00f;\n        },\n      },\n    },\n    for (int32_t di_outer = 0; di_outer < 4; ++di_outer) {\n      for (int32_t c_c_outer_inner = 0; c_c_outer_inner < 2; ++c_c_outer_inner) {\n        for (int32_t i_c_outer_inner = 0; i_c_outer_inner < 85; ++i_c_outer_inner) {\n          for (int32_t dj_inner = 0; dj_inner < 3; ++dj_inner) {\n            for (int32_t c_c_inner = 0; c_c_inner < 11; ++c_c_inner) {\n              int32_t cse_var_2 = (di_outer * 3);\n              int32_t cse_var_1 = (((c_c_outer_inner * 935) + (c_c_inner * 85)) + i_c_outer_inner);\n              ((float*)DepthwiseConv2d_local)[cse_var_1] = (((float*)DepthwiseConv2d_local)[cse_var_1] + (((float*)data_1)[(((((b_outer_c_outer_fused_i_outer_fused_j_outer_fused >> 2) * 264) + (i_c_outer_inner * 3)) + cse_var_2) + dj_inner)] * ((float*)kernel_1)[(((((b_outer_c_outer_fused_i_outer_fused_j_outer_fused * 264) + (c_c_outer_inner * 132)) + (c_c_inner * 12)) + cse_var_2) + dj_inner)]));\n            },\n          },\n        },\n      },\n    },\n    for (int32_t c_inner = 0; c_inner < 22; ++c_inner) {\n      for (int32_t i_inner = 0; i_inner < 85; ++i_inner) {\n        int32_t cse_var_3 = (c_inner * 85);\n        ((float*)DepthwiseConv2d_1)[(((b_outer_c_outer_fused_i_outer_fused_j_outer_fused * 1870) + cse_var_3) + i_inner)] = ((float*)DepthwiseConv2d_local)[(cse_var_3 + i_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, DepthwiseConv2d_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(115) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(115) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel) {\n  float DepthwiseConv2d_local[4];\n  __shared__ float PaddedInput_shared[48];\n  __shared__ float kernel_shared[2112];\n  DepthwiseConv2d_local[0] = 0.000000e+00f;\n  DepthwiseConv2d_local[1] = 0.000000e+00f;\n  DepthwiseConv2d_local[2] = 0.000000e+00f;\n  DepthwiseConv2d_local[3] = 0.000000e+00f;\n  if (((int)threadIdx.x) < 48) {\n    PaddedInput_shared[((int)threadIdx.x)] = data[(((((((int)threadIdx.x) / 24) * 264) + ((((int)blockIdx.x) / 17) * 264)) + ((((int)blockIdx.x) % 17) * 15)) + (((int)threadIdx.x) % 24))];\n  },\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 7; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 115) + ((int)threadIdx.x)) < 704) {\n      *(float3*)(kernel_shared + ((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 345) + (((int)threadIdx.x) * 3))) = *(float3*)(kernel + ((((((int)blockIdx.x) / 17) * 1056) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 345)) + (((int)threadIdx.x) * 3)));\n    },\n  },\n  __syncthreads();\n  for (int di_inner = 0; di_inner < 4; ++di_inner) {\n    for (int dj_inner = 0; dj_inner < 3; ++dj_inner) {\n      DepthwiseConv2d_local[0] = (DepthwiseConv2d_local[0] + (PaddedInput_shared[(((((((((((int)blockIdx.x) / 17) * 23) + (((int)threadIdx.x) / 20)) / 22) * 24) + (di_inner * 3)) + ((((int)threadIdx.x) % 5) * 3)) + dj_inner) - ((((int)blockIdx.x) / 17) * 24))] * kernel_shared[(((((((int)blockIdx.x) / 17) * 48) + ((((int)threadIdx.x) / 5) * 12)) + (di_inner * 3)) + dj_inner)]));\n      DepthwiseConv2d_local[1] = (DepthwiseConv2d_local[1] + (PaddedInput_shared[(((((((((((int)blockIdx.x) / 17) * 23) + ((((int)threadIdx.x) + 115) / 20)) / 22) * 24) + (di_inner * 3)) + ((((int)threadIdx.x) % 5) * 3)) + dj_inner) - ((((int)blockIdx.x) / 17) * 24))] * kernel_shared[((((((((int)blockIdx.x) / 17) * 48) + ((((int)threadIdx.x) / 5) * 12)) + (di_inner * 3)) + dj_inner) + 276)]));\n      DepthwiseConv2d_local[2] = (DepthwiseConv2d_local[2] + (PaddedInput_shared[(((((((((((int)blockIdx.x) / 17) * 23) + ((((int)threadIdx.x) + 230) / 20)) / 22) * 24) + (di_inner * 3)) + ((((int)threadIdx.x) % 5) * 3)) + dj_inner) - ((((int)blockIdx.x) / 17) * 24))] * kernel_shared[((((((((int)blockIdx.x) / 17) * 48) + ((((int)threadIdx.x) / 5) * 12)) + (di_inner * 3)) + dj_inner) + 552)]));\n      DepthwiseConv2d_local[3] = (DepthwiseConv2d_local[3] + (PaddedInput_shared[(((((((((((int)blockIdx.x) / 17) * 23) + ((((int)threadIdx.x) + 345) / 20)) / 22) * 24) + (di_inner * 3)) + ((((int)threadIdx.x) % 5) * 3)) + dj_inner) - ((((int)blockIdx.x) / 17) * 24))] * kernel_shared[((((((((int)blockIdx.x) / 17) * 48) + ((((int)threadIdx.x) / 5) * 12)) + (di_inner * 3)) + dj_inner) + 828)]));\n    },\n  },\n  DepthwiseConv2d[(((((((int)blockIdx.x) / 17) * 7820) + ((((int)threadIdx.x) / 5) * 85)) + ((((int)blockIdx.x) % 17) * 5)) + (((int)threadIdx.x) % 5))] = DepthwiseConv2d_local[0];\n  DepthwiseConv2d[((((((((int)blockIdx.x) / 17) * 7820) + ((((int)threadIdx.x) / 5) * 85)) + ((((int)blockIdx.x) % 17) * 5)) + (((int)threadIdx.x) % 5)) + 1955)] = DepthwiseConv2d_local[1];\n  DepthwiseConv2d[((((((((int)blockIdx.x) / 17) * 7820) + ((((int)threadIdx.x) / 5) * 85)) + ((((int)blockIdx.x) % 17) * 5)) + (((int)threadIdx.x) % 5)) + 3910)] = DepthwiseConv2d_local[2];\n  DepthwiseConv2d[((((((((int)blockIdx.x) / 17) * 7820) + ((((int)threadIdx.x) / 5) * 85)) + ((((int)blockIdx.x) % 17) * 5)) + (((int)threadIdx.x) % 5)) + 5865)] = DepthwiseConv2d_local[3];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 23, 88, 3), \"float32\"), kernel: T.Buffer((23, 88, 4, 3), \"float32\"), DepthwiseConv2d: T.Buffer((1, 2024, 85, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_c_outer_fused_i_outer_fused_j_outer_fused in T.parallel(92):\n            DepthwiseConv2d_local = T.allocate([1870], \"float32\", \"local\")\n            DepthwiseConv2d_local_1 = T.Buffer((1870,), data=DepthwiseConv2d_local, scope=\"local\")\n            for c_c_outer_inner_init, i_c_outer_inner_init, c_c_inner_init in T.grid(2, 85, 11):\n                DepthwiseConv2d_local_1[c_c_outer_inner_init * 935 + c_c_inner_init * 85 + i_c_outer_inner_init] = T.float32(0)\n            for di_outer, c_c_outer_inner, i_c_outer_inner, dj_inner, c_c_inner in T.grid(4, 2, 85, 3, 11):\n                cse_var_2: T.int32 = di_outer * 3\n                cse_var_1: T.int32 = c_c_outer_inner * 935 + c_c_inner * 85 + i_c_outer_inner\n                data_1 = T.Buffer((6072,), data=data.data)\n                kernel_1 = T.Buffer((24288,), data=kernel.data)\n                DepthwiseConv2d_local_1[cse_var_1] = DepthwiseConv2d_local_1[cse_var_1] + data_1[b_outer_c_outer_fused_i_outer_fused_j_outer_fused // 4 * 264 + i_c_outer_inner * 3 + cse_var_2 + dj_inner] * kernel_1[b_outer_c_outer_fused_i_outer_fused_j_outer_fused * 264 + c_c_outer_inner * 132 + c_c_inner * 12 + cse_var_2 + dj_inner]\n            for c_inner, i_inner in T.grid(22, 85):\n                cse_var_3: T.int32 = c_inner * 85\n                DepthwiseConv2d_1 = T.Buffer((172040,), data=DepthwiseConv2d.data)\n                DepthwiseConv2d_1[b_outer_c_outer_fused_i_outer_fused_j_outer_fused * 1870 + cse_var_3 + i_inner] = DepthwiseConv2d_local_1[cse_var_3 + i_inner]",
        "data": "1_23_88_3",
        "kernel": "23_88_4_3"
    },
    {
        "op_name": "depthwise_conv2d_nchw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t DepthwiseConv2d_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* DepthwiseConv2d = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* DepthwiseConv2d_1 = (((DLTensor*)DepthwiseConv2d)[0].data);\n  void* default_function_DepthwiseConv2d_shape = (((DLTensor*)DepthwiseConv2d)[0].shape);\n  void* default_function_DepthwiseConv2d_strides = (((DLTensor*)DepthwiseConv2d)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_DepthwiseConv2d_strides == NULL)) {\n  },\n  for (int32_t b_outer_c_outer_fused_i_outer_fused_j_outer_fused = 0; b_outer_c_outer_fused_i_outer_fused_j_outer_fused < 722; ++b_outer_c_outer_fused_i_outer_fused_j_outer_fused) {\n    void* DepthwiseConv2d_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)14400, 2, 32);\n    if (DepthwiseConv2d_local == NULL) {\n      return -1;\n    },\n    for (int32_t c_c_outer_outer_inner = 0; c_c_outer_outer_inner < 2; ++c_c_outer_outer_inner) {\n      for (int32_t c_c_outer_inner_init = 0; c_c_outer_inner_init < 10; ++c_c_outer_inner_init) {\n        for (int32_t i_c_outer_inner_init = 0; i_c_outer_inner_init < 3; ++i_c_outer_inner_init) {\n          for (int32_t j_c_outer_inner_init = 0; j_c_outer_inner_init < 2; ++j_c_outer_inner_init) {\n            for (int32_t b_c_inner_init = 0; b_c_inner_init < 5; ++b_c_inner_init) {\n              for (int32_t c_c_inner_init = 0; c_c_inner_init < 6; ++c_c_inner_init) {\n                ((float*)DepthwiseConv2d_local)[((((((b_c_inner_init * 720) + (c_c_outer_outer_inner * 360)) + (c_c_outer_inner_init * 36)) + (c_c_inner_init * 6)) + (i_c_outer_inner_init * 2)) + j_c_outer_inner_init)] = 0.000000e+00f;\n              },\n            },\n          },\n        },\n      },\n      for (int32_t di_outer = 0; di_outer < 2; ++di_outer) {\n        for (int32_t c_c_outer_inner = 0; c_c_outer_inner < 10; ++c_c_outer_inner) {\n          for (int32_t i_c_outer_inner = 0; i_c_outer_inner < 3; ++i_c_outer_inner) {\n            for (int32_t j_c_outer_inner = 0; j_c_outer_inner < 2; ++j_c_outer_inner) {\n              for (int32_t di_inner = 0; di_inner < 2; ++di_inner) {\n                for (int32_t dj_inner = 0; dj_inner < 3; ++dj_inner) {\n                  for (int32_t b_c_inner = 0; b_c_inner < 5; ++b_c_inner) {\n                    for (int32_t c_c_inner = 0; c_c_inner < 6; ++c_c_inner) {\n                      int32_t cse_var_2 = ((b_outer_c_outer_fused_i_outer_fused_j_outer_fused % 361) / 19);\n                      int32_t cse_var_1 = ((((((b_c_inner * 720) + (c_c_outer_outer_inner * 360)) + (c_c_outer_inner * 36)) + (c_c_inner * 6)) + (i_c_outer_inner * 2)) + j_c_outer_inner);\n                      ((float*)DepthwiseConv2d_local)[cse_var_1] = (((float*)DepthwiseConv2d_local)[cse_var_1] + (((float*)data_1)[(((((((((((b_outer_c_outer_fused_i_outer_fused_j_outer_fused / 361) * 45600) + (b_c_inner * 9120)) + (cse_var_2 * 480)) + (c_c_outer_outer_inner * 240)) + ((b_outer_c_outer_fused_i_outer_fused_j_outer_fused % 19) * 12)) + (di_outer * 8)) + (i_c_outer_inner * 4)) + (di_inner * 4)) + j_c_outer_inner) + dj_inner)] * ((float*)kernel_1)[(((((((cse_var_2 * 1440) + (c_c_outer_outer_inner * 720)) + (c_c_outer_inner * 72)) + (c_c_inner * 12)) + (di_outer * 6)) + (di_inner * 3)) + dj_inner)]));\n                    },\n                  },\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t b_inner = 0; b_inner < 5; ++b_inner) {\n      for (int32_t c_inner = 0; c_inner < 120; ++c_inner) {\n        for (int32_t i_inner = 0; i_inner < 3; ++i_inner) {\n          int32_t cse_var_3 = (i_inner * 2);\n          int32_t2 v_ = int32_t2(((((b_inner * 720) + (c_inner * 6)) + cse_var_3))+(1*0), ((((b_inner * 720) + (c_inner * 6)) + cse_var_3))+(1*1));\n          *(float2*)(((float*)DepthwiseConv2d_1) + (((((((b_outer_c_outer_fused_i_outer_fused_j_outer_fused / 361) * 1299600) + (b_inner * 259920)) + (((b_outer_c_outer_fused_i_outer_fused_j_outer_fused % 361) / 19) * 13680)) + (c_inner * 114)) + ((b_outer_c_outer_fused_i_outer_fused_j_outer_fused % 19) * 6)) + cse_var_3)) = (float2(((float*)DepthwiseConv2d_local)[v_.s0],((float*)DepthwiseConv2d_local)[v_.s1]));\n        },\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, DepthwiseConv2d_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(190) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(190) default_function_kernel(float* __restrict__ DepthwiseConv2d, float* __restrict__ data, float* __restrict__ kernel) {\n  float DepthwiseConv2d_local[30];\n  __shared__ float PaddedInput_shared[800];\n  __shared__ float kernel_shared[7200];\n  for (int c_c_inner_init = 0; c_c_inner_init < 10; ++c_c_inner_init) {\n    DepthwiseConv2d_local[c_c_inner_init] = 0.000000e+00f;\n    DepthwiseConv2d_local[(c_c_inner_init + 10)] = 0.000000e+00f;\n    DepthwiseConv2d_local[(c_c_inner_init + 20)] = 0.000000e+00f;\n  },\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 5; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 19) + (((int)threadIdx.x) / 10)) < 80) {\n      PaddedInput_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 190) + ((int)threadIdx.x))] = data[((((((((((int)blockIdx.x) / 228) * 45600) + ((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 19) + (((int)threadIdx.x) / 10)) >> 4) * 9120)) + (((((((int)blockIdx.x) % 228) / 57) * 570) / 60) * 240)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 15) + (((int)threadIdx.x) >> 1)) % 80) >> 3) * 240)) + (((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 7) + (((int)threadIdx.x) >> 1)) & 7) >> 1) * 4)) + ((((int)blockIdx.x) % 57) * 4)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 2) + ((int)threadIdx.x)) & 3))];\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 < 38; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 19) + (((int)threadIdx.x) / 10)) < 720) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 190) + ((int)threadIdx.x))] = kernel[(((((((((int)blockIdx.x) % 228) / 57) * 570) / 60) * 720) + (ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer_1 * 190)) + ((int)threadIdx.x))];\n    },\n  },\n  __syncthreads();\n  for (int di_outer_inner = 0; di_outer_inner < 2; ++di_outer_inner) {\n    for (int di_inner = 0; di_inner < 2; ++di_inner) {\n      for (int dj_inner = 0; dj_inner < 3; ++dj_inner) {\n        for (int c_c_inner = 0; c_c_inner < 10; ++c_c_inner) {\n          DepthwiseConv2d_local[c_c_inner] = (DepthwiseConv2d_local[c_c_inner] + (PaddedInput_shared[((((((((((int)threadIdx.x) / 38) * 160) + (((((((((int)blockIdx.x) % 228) / 57) * 570) + (((((int)threadIdx.x) % 38) >> 1) * 10)) + c_c_inner) / 60) * 16)) + (di_outer_inner * 8)) + (di_inner * 4)) + dj_inner) + (((int)threadIdx.x) & 1)) - (((((((int)blockIdx.x) % 228) / 57) * 570) / 60) * 16))] * kernel_shared[(((((((((((int)blockIdx.x) % 228) / 57) * 6840) + (((((int)threadIdx.x) % 38) >> 1) * 120)) + (c_c_inner * 12)) + (di_outer_inner * 6)) + (di_inner * 3)) + dj_inner) - (((((((int)blockIdx.x) % 228) / 57) * 570) / 60) * 720))]));\n          DepthwiseConv2d_local[(c_c_inner + 10)] = (DepthwiseConv2d_local[(c_c_inner + 10)] + (PaddedInput_shared[((((((((((int)threadIdx.x) / 38) * 160) + ((((((((((int)blockIdx.x) % 228) / 57) * 570) + (((((int)threadIdx.x) % 38) >> 1) * 10)) + c_c_inner) + 190) / 60) * 16)) + (di_outer_inner * 8)) + (di_inner * 4)) + dj_inner) + (((int)threadIdx.x) & 1)) - (((((((int)blockIdx.x) % 228) / 57) * 570) / 60) * 16))] * kernel_shared[((((((((((((int)blockIdx.x) % 228) / 57) * 6840) + (((((int)threadIdx.x) % 38) >> 1) * 120)) + (c_c_inner * 12)) + (di_outer_inner * 6)) + (di_inner * 3)) + dj_inner) + 2280) - (((((((int)blockIdx.x) % 228) / 57) * 570) / 60) * 720))]));\n          DepthwiseConv2d_local[(c_c_inner + 20)] = (DepthwiseConv2d_local[(c_c_inner + 20)] + (PaddedInput_shared[((((((((((int)threadIdx.x) / 38) * 160) + ((((((((((int)blockIdx.x) % 228) / 57) * 570) + (((((int)threadIdx.x) % 38) >> 1) * 10)) + c_c_inner) + 380) / 60) * 16)) + (di_outer_inner * 8)) + (di_inner * 4)) + dj_inner) + (((int)threadIdx.x) & 1)) - (((((((int)blockIdx.x) % 228) / 57) * 570) / 60) * 16))] * kernel_shared[((((((((((((int)blockIdx.x) % 228) / 57) * 6840) + (((((int)threadIdx.x) % 38) >> 1) * 120)) + (c_c_inner * 12)) + (di_outer_inner * 6)) + (di_inner * 3)) + dj_inner) + 4560) - (((((((int)blockIdx.x) % 228) / 57) * 570) / 60) * 720))]));\n        },\n      },\n    },\n  },\n  for (int c_inner = 0; c_inner < 10; ++c_inner) {\n    DepthwiseConv2d[((((((((((int)blockIdx.x) / 228) * 1299600) + ((((int)threadIdx.x) / 38) * 259920)) + (((((int)blockIdx.x) % 228) / 57) * 64980)) + (((((int)threadIdx.x) % 38) >> 1) * 1140)) + (c_inner * 114)) + ((((int)blockIdx.x) % 57) * 2)) + (((int)threadIdx.x) & 1))] = DepthwiseConv2d_local[c_inner];\n    DepthwiseConv2d[(((((((((((int)blockIdx.x) / 228) * 1299600) + ((((int)threadIdx.x) / 38) * 259920)) + (((((int)blockIdx.x) % 228) / 57) * 64980)) + (((((int)threadIdx.x) % 38) >> 1) * 1140)) + (c_inner * 114)) + ((((int)blockIdx.x) % 57) * 2)) + (((int)threadIdx.x) & 1)) + 21660)] = DepthwiseConv2d_local[(c_inner + 10)];\n    DepthwiseConv2d[(((((((((((int)blockIdx.x) / 228) * 1299600) + ((((int)threadIdx.x) / 38) * 259920)) + (((((int)blockIdx.x) % 228) / 57) * 64980)) + (((((int)threadIdx.x) % 38) >> 1) * 1140)) + (c_inner * 114)) + ((((int)blockIdx.x) % 57) * 2)) + (((int)threadIdx.x) & 1)) + 43320)] = DepthwiseConv2d_local[(c_inner + 20)];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 38, 60, 4), \"float32\"), kernel: T.Buffer((38, 60, 4, 3), \"float32\"), DepthwiseConv2d: T.Buffer((10, 2280, 57, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for b_outer_c_outer_fused_i_outer_fused_j_outer_fused in T.parallel(722):\n            DepthwiseConv2d_local = T.allocate([3600], \"float32\", \"local\")\n            DepthwiseConv2d_local_1 = T.Buffer((3600,), data=DepthwiseConv2d_local, scope=\"local\")\n            for c_c_outer_outer_inner in range(2):\n                for c_c_outer_inner_init, i_c_outer_inner_init, j_c_outer_inner_init, b_c_inner_init, c_c_inner_init in T.grid(10, 3, 2, 5, 6):\n                    DepthwiseConv2d_local_1[b_c_inner_init * 720 + c_c_outer_outer_inner * 360 + c_c_outer_inner_init * 36 + c_c_inner_init * 6 + i_c_outer_inner_init * 2 + j_c_outer_inner_init] = T.float32(0)\n                for di_outer, c_c_outer_inner, i_c_outer_inner, j_c_outer_inner, di_inner, dj_inner, b_c_inner, c_c_inner in T.grid(2, 10, 3, 2, 2, 3, 5, 6):\n                    cse_var_2: T.int32 = b_outer_c_outer_fused_i_outer_fused_j_outer_fused % 361 // 19\n                    cse_var_1: T.int32 = b_c_inner * 720 + c_c_outer_outer_inner * 360 + c_c_outer_inner * 36 + c_c_inner * 6 + i_c_outer_inner * 2 + j_c_outer_inner\n                    data_1 = T.Buffer((91200,), data=data.data)\n                    kernel_1 = T.Buffer((27360,), data=kernel.data)\n                    DepthwiseConv2d_local_1[cse_var_1] = DepthwiseConv2d_local_1[cse_var_1] + data_1[b_outer_c_outer_fused_i_outer_fused_j_outer_fused // 361 * 45600 + b_c_inner * 9120 + cse_var_2 * 480 + c_c_outer_outer_inner * 240 + b_outer_c_outer_fused_i_outer_fused_j_outer_fused % 19 * 12 + di_outer * 8 + i_c_outer_inner * 4 + di_inner * 4 + j_c_outer_inner + dj_inner] * kernel_1[cse_var_2 * 1440 + c_c_outer_outer_inner * 720 + c_c_outer_inner * 72 + c_c_inner * 12 + di_outer * 6 + di_inner * 3 + dj_inner]\n            for b_inner, c_inner, i_inner in T.grid(5, 120, 3):\n                cse_var_3: T.int32 = i_inner * 2\n                DepthwiseConv2d_1 = T.Buffer((2599200,), data=DepthwiseConv2d.data)\n                DepthwiseConv2d_1[b_outer_c_outer_fused_i_outer_fused_j_outer_fused // 361 * 1299600 + b_inner * 259920 + b_outer_c_outer_fused_i_outer_fused_j_outer_fused % 361 // 19 * 13680 + c_inner * 114 + b_outer_c_outer_fused_i_outer_fused_j_outer_fused % 19 * 6 + cse_var_3:b_outer_c_outer_fused_i_outer_fused_j_outer_fused // 361 * 1299600 + b_inner * 259920 + b_outer_c_outer_fused_i_outer_fused_j_outer_fused % 361 // 19 * 13680 + c_inner * 114 + b_outer_c_outer_fused_i_outer_fused_j_outer_fused % 19 * 6 + cse_var_3 + 2] = DepthwiseConv2d_local_1[b_inner * 720 + c_inner * 6 + cse_var_3:b_inner * 720 + c_inner * 6 + cse_var_3 + 2]",
        "data": "10_38_60_4",
        "kernel": "38_60_4_3"
    },
    {
        "op_name": "group_conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)352, 2, 32);\n  if (conv1d_ncw_local == NULL) {\n    return -1;\n  },\n  for (int32_t nn_c_outer_outer_inner = 0; nn_c_outer_outer_inner < 2; ++nn_c_outer_outer_inner) {\n    for (int32_t nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 2; ++nn_c_outer_inner_init) {\n      for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 11; ++ff_c_outer_inner_init) {\n        for (int32_t yy_c_outer_inner_init = 0; yy_c_outer_inner_init < 2; ++yy_c_outer_inner_init) {\n          ((float*)conv1d_ncw_local)[((((nn_c_outer_outer_inner * 44) + (nn_c_outer_inner_init * 22)) + (ff_c_outer_inner_init * 2)) + yy_c_outer_inner_init)] = 0.000000e+00f;\n        },\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 71; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n        for (int32_t nn_c_outer_inner = 0; nn_c_outer_inner < 2; ++nn_c_outer_inner) {\n          for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 11; ++ff_c_outer_inner) {\n            for (int32_t yy_c_outer_inner = 0; yy_c_outer_inner < 2; ++yy_c_outer_inner) {\n              int32_t cse_var_1 = ((((nn_c_outer_outer_inner * 44) + (nn_c_outer_inner * 22)) + (ff_c_outer_inner * 2)) + yy_c_outer_inner);\n              ((float*)conv1d_ncw_local)[cse_var_1] = (((float*)conv1d_ncw_local)[cse_var_1] + (((float*)data_1)[(((((nn_c_outer_outer_inner * 568) + (nn_c_outer_inner * 284)) + (rc_outer * 4)) + yy_c_outer_inner) + ry_outer)] * ((float*)kernel_1)[(((ff_c_outer_inner * 213) + (rc_outer * 3)) + ry_outer)]));\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int32_t nn_inner = 0; nn_inner < 4; ++nn_inner) {\n    for (int32_t ff_inner = 0; ff_inner < 11; ++ff_inner) {\n      int32_t cse_var_2 = ((nn_inner * 22) + (ff_inner * 2));\n      int32_t2 v_ = int32_t2((cse_var_2)+(1*0), (cse_var_2)+(1*1));\n      *(float2*)(((float*)conv1d_ncw_1) + cse_var_2) = (float2(((float*)conv1d_ncw_local)[v_.s0],((float*)conv1d_ncw_local)[v_.s1]));\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(22) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(22) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[4];\n  __shared__ float pad_temp_shared[568];\n  __shared__ float kernel_shared[781];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  conv1d_ncw_local[1] = 0.000000e+00f;\n  conv1d_ncw_local[2] = 0.000000e+00f;\n  conv1d_ncw_local[3] = 0.000000e+00f;\n  for (int ry_outer_outer = 0; ry_outer_outer < 3; ++ry_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 26; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer * 11) + (((int)threadIdx.x) >> 1)) < 284) {\n        pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 22) + ((int)threadIdx.x))] = data[((((ax0_ax1_fused_ax2_fused_outer_outer * 44) + ((((int)threadIdx.x) >> 1) * 4)) + ry_outer_outer) + (((int)threadIdx.x) & 1))];\n      },\n    },\n    for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 36; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer_1 * 2) + (((int)threadIdx.x) / 11)) < 71) {\n        kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 22) + ((int)threadIdx.x))] = kernel[(((ax0_ax1_fused_ax2_fused_outer_outer_1 * 66) + (((int)threadIdx.x) * 3)) + ry_outer_outer)];\n      },\n    },\n    __syncthreads();\n    for (int rc_outer_inner = 0; rc_outer_inner < 71; ++rc_outer_inner) {\n      conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[((rc_outer_inner * 2) + (((int)threadIdx.x) & 1))] * kernel_shared[(((((int)threadIdx.x) >> 1) * 71) + rc_outer_inner)]));\n      conv1d_ncw_local[1] = (conv1d_ncw_local[1] + (pad_temp_shared[(((rc_outer_inner * 2) + (((int)threadIdx.x) & 1)) + 142)] * kernel_shared[(((((int)threadIdx.x) >> 1) * 71) + rc_outer_inner)]));\n      conv1d_ncw_local[2] = (conv1d_ncw_local[2] + (pad_temp_shared[(((rc_outer_inner * 2) + (((int)threadIdx.x) & 1)) + 284)] * kernel_shared[(((((int)threadIdx.x) >> 1) * 71) + rc_outer_inner)]));\n      conv1d_ncw_local[3] = (conv1d_ncw_local[3] + (pad_temp_shared[(((rc_outer_inner * 2) + (((int)threadIdx.x) & 1)) + 426)] * kernel_shared[(((((int)threadIdx.x) >> 1) * 71) + rc_outer_inner)]));\n    },\n  },\n  conv1d_ncw[((int)threadIdx.x)] = conv1d_ncw_local[0];\n  conv1d_ncw[(((int)threadIdx.x) + 22)] = conv1d_ncw_local[1];\n  conv1d_ncw[(((int)threadIdx.x) + 44)] = conv1d_ncw_local[2];\n  conv1d_ncw[(((int)threadIdx.x) + 66)] = conv1d_ncw_local[3];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 71, 4), \"float32\"), kernel: T.Buffer((11, 71, 3), \"float32\"), conv1d_ncw: T.Buffer((4, 11, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        conv1d_ncw_local = T.allocate([88], \"float32\", \"local\")\n        conv1d_ncw_local_1 = T.Buffer((88,), data=conv1d_ncw_local, scope=\"local\")\n        for nn_c_outer_outer_inner in range(2):\n            for nn_c_outer_inner_init, ff_c_outer_inner_init, yy_c_outer_inner_init in T.grid(2, 11, 2):\n                conv1d_ncw_local_1[nn_c_outer_outer_inner * 44 + nn_c_outer_inner_init * 22 + ff_c_outer_inner_init * 2 + yy_c_outer_inner_init] = T.float32(0)\n            for rc_outer, ry_outer, nn_c_outer_inner, ff_c_outer_inner, yy_c_outer_inner in T.grid(71, 3, 2, 11, 2):\n                cse_var_1: T.int32 = nn_c_outer_outer_inner * 44 + nn_c_outer_inner * 22 + ff_c_outer_inner * 2 + yy_c_outer_inner\n                data_1 = T.Buffer((1136,), data=data.data)\n                kernel_1 = T.Buffer((2343,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_c_outer_outer_inner * 568 + nn_c_outer_inner * 284 + rc_outer * 4 + yy_c_outer_inner + ry_outer] * kernel_1[ff_c_outer_inner * 213 + rc_outer * 3 + ry_outer]\n        for nn_inner, ff_inner in T.grid(4, 11):\n            cse_var_2: T.int32 = nn_inner * 22 + ff_inner * 2\n            conv1d_ncw_1 = T.Buffer((88,), data=conv1d_ncw.data)\n            conv1d_ncw_1[cse_var_2:cse_var_2 + 2] = conv1d_ncw_local_1[cse_var_2:cse_var_2 + 2]",
        "data": "4_71_4",
        "kernel": "11_71_3"
    },
    {
        "op_name": "group_conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_ff_outer_fused_yy_outer_fused = 0; nn_outer_ff_outer_fused_yy_outer_fused < 63; ++nn_outer_ff_outer_fused_yy_outer_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)48, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 2; ++nn_c_outer_inner_init) {\n      for (int32_t nn_c_inner_init = 0; nn_c_inner_init < 2; ++nn_c_inner_init) {\n        for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 3; ++ff_c_inner_init) {\n          ((float*)conv1d_ncw_local)[(((nn_c_outer_inner_init * 6) + (nn_c_inner_init * 3)) + ff_c_inner_init)] = 0.000000e+00f;\n        },\n      },\n    },\n    for (int32_t ry_outer = 0; ry_outer < 2; ++ry_outer) {\n      for (int32_t nn_c_outer_inner = 0; nn_c_outer_inner < 2; ++nn_c_outer_inner) {\n        for (int32_t rc_inner = 0; rc_inner < 34; ++rc_inner) {\n          for (int32_t ry_inner = 0; ry_inner < 2; ++ry_inner) {\n            for (int32_t nn_c_inner = 0; nn_c_inner < 2; ++nn_c_inner) {\n              for (int32_t ff_c_inner = 0; ff_c_inner < 3; ++ff_c_inner) {\n                int32_t cse_var_2 = (ry_outer * 2);\n                int32_t cse_var_1 = (((nn_c_outer_inner * 6) + (nn_c_inner * 3)) + ff_c_inner);\n                ((float*)conv1d_ncw_local)[cse_var_1] = (((float*)conv1d_ncw_local)[cse_var_1] + (((float*)data_1)[((((((nn_c_outer_inner * 408) + (nn_c_inner * 204)) + (rc_inner * 6)) + cse_var_2) + ry_inner) + (nn_outer_ff_outer_fused_yy_outer_fused % 3))] * ((float*)kernel_1)[((((((nn_outer_ff_outer_fused_yy_outer_fused / 3) * 408) + (ff_c_inner * 136)) + (rc_inner * 4)) + cse_var_2) + ry_inner)]));\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 4; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 3; ++ff_inner) {\n        ((float*)conv1d_ncw_1)[((((nn_inner * 189) + ((nn_outer_ff_outer_fused_yy_outer_fused / 3) * 9)) + (ff_inner * 3)) + (nn_outer_ff_outer_fused_yy_outer_fused % 3))] = ((float*)conv1d_ncw_local)[((nn_inner * 3) + ff_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(126) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(126) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[2];\n  __shared__ float pad_temp_shared[272];\n  __shared__ float kernel_shared[714];\n  for (int nn_c_inner_init = 0; nn_c_inner_init < 2; ++nn_c_inner_init) {\n    conv1d_ncw_local[nn_c_inner_init] = 0.000000e+00f;\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 2; ++rc_outer_outer) {\n    for (int ry_outer_outer = 0; ry_outer_outer < 2; ++ry_outer_outer) {\n      __syncthreads();\n      for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n        if (((ax0_ax1_fused_ax2_fused_outer_outer * 63) + (((int)threadIdx.x) >> 1)) < 68) {\n          *(float2*)(pad_temp_shared + ((ax0_ax1_fused_ax2_fused_outer_outer * 252) + (((int)threadIdx.x) * 2))) = *(float2*)(data + ((((((((ax0_ax1_fused_ax2_fused_outer_outer * 63) + (((int)threadIdx.x) >> 1)) / 17) * 204) + (rc_outer_outer * 102)) + ((((ax0_ax1_fused_ax2_fused_outer_outer * 12) + (((int)threadIdx.x) >> 1)) % 17) * 6)) + (ry_outer_outer * 2)) + ((((int)threadIdx.x) & 1) * 2)));\n        },\n      },\n      for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 6; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n        if (((ax0_ax1_fused_ax2_fused_outer_outer_1 * 3) + (((int)threadIdx.x) / 42)) < 17) {\n          kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 126) + ((int)threadIdx.x))] = kernel[((((((((int)blockIdx.x) * 2856) + ((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 63) + (((int)threadIdx.x) >> 1)) / 17) * 136)) + (rc_outer_outer * 68)) + ((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 12) + (((int)threadIdx.x) >> 1)) % 17) * 4)) + (ry_outer_outer * 2)) + (((int)threadIdx.x) & 1))];\n        },\n      },\n      __syncthreads();\n      for (int ry_outer_inner = 0; ry_outer_inner < 2; ++ry_outer_inner) {\n        for (int rc_inner = 0; rc_inner < 17; ++rc_inner) {\n          for (int nn_c_inner = 0; nn_c_inner < 2; ++nn_c_inner) {\n            conv1d_ncw_local[nn_c_inner] = (conv1d_ncw_local[nn_c_inner] + (pad_temp_shared[((((((((int)threadIdx.x) / 63) * 136) + (nn_c_inner * 68)) + (rc_inner * 4)) + ry_outer_inner) + (((int)threadIdx.x) % 3))] * kernel_shared[(((((((int)threadIdx.x) % 63) / 3) * 34) + (rc_inner * 2)) + ry_outer_inner)]));\n          },\n        },\n      },\n    },\n  },\n  for (int nn_inner = 0; nn_inner < 2; ++nn_inner) {\n    conv1d_ncw[(((((((int)threadIdx.x) / 63) * 378) + (nn_inner * 189)) + (((int)blockIdx.x) * 63)) + (((int)threadIdx.x) % 63))] = conv1d_ncw_local[nn_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 34, 6), \"float32\"), kernel: T.Buffer((63, 34, 4), \"float32\"), conv1d_ncw: T.Buffer((4, 63, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_ff_outer_fused_yy_outer_fused in T.parallel(63):\n            conv1d_ncw_local = T.allocate([12], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((12,), data=conv1d_ncw_local, scope=\"local\", align=32)\n            for nn_c_outer_inner_init, nn_c_inner_init, ff_c_inner_init in T.grid(2, 2, 3):\n                conv1d_ncw_local_1[nn_c_outer_inner_init * 6 + nn_c_inner_init * 3 + ff_c_inner_init] = T.float32(0)\n            for ry_outer, nn_c_outer_inner, rc_inner, ry_inner, nn_c_inner, ff_c_inner in T.grid(2, 2, 34, 2, 2, 3):\n                cse_var_2: T.int32 = ry_outer * 2\n                cse_var_1: T.int32 = nn_c_outer_inner * 6 + nn_c_inner * 3 + ff_c_inner\n                data_1 = T.Buffer((816,), data=data.data)\n                kernel_1 = T.Buffer((8568,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_c_outer_inner * 408 + nn_c_inner * 204 + rc_inner * 6 + cse_var_2 + ry_inner + nn_outer_ff_outer_fused_yy_outer_fused % 3] * kernel_1[nn_outer_ff_outer_fused_yy_outer_fused // 3 * 408 + ff_c_inner * 136 + rc_inner * 4 + cse_var_2 + ry_inner]\n            for nn_inner, ff_inner in T.grid(4, 3):\n                conv1d_ncw_1 = T.Buffer((756,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_inner * 189 + nn_outer_ff_outer_fused_yy_outer_fused // 3 * 9 + ff_inner * 3 + nn_outer_ff_outer_fused_yy_outer_fused % 3] = conv1d_ncw_local_1[nn_inner * 3 + ff_inner]",
        "data": "4_34_6",
        "kernel": "63_34_4"
    },
    {
        "op_name": "group_conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_ff_outer_fused_yy_outer_fused = 0; nn_outer_ff_outer_fused_yy_outer_fused < 11; ++nn_outer_ff_outer_fused_yy_outer_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)288, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t nn_c_outer_outer_inner = 0; nn_c_outer_outer_inner < 2; ++nn_c_outer_outer_inner) {\n      for (int32_t nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 3; ++nn_c_outer_inner_init) {\n        for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 3; ++ff_c_outer_inner_init) {\n          ((float4*)conv1d_ncw_local)[(((nn_c_outer_outer_inner * 9) + (nn_c_outer_inner_init * 3)) + ff_c_outer_inner_init)] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n        },\n      },\n      for (int32_t rc_outer = 0; rc_outer < 51; ++rc_outer) {\n        for (int32_t nn_c_outer_inner = 0; nn_c_outer_inner < 3; ++nn_c_outer_inner) {\n          for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 3; ++ff_c_outer_inner) {\n            for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n              int32_t cse_var_1 = (((nn_c_outer_outer_inner * 9) + (nn_c_outer_inner * 3)) + ff_c_outer_inner);\n              int32_t4 v_ = int32_t4((((((nn_c_outer_outer_inner * 918) + (nn_c_outer_inner * 306)) + (rc_outer * 6)) + ry_inner))+(1*0), (((((nn_c_outer_outer_inner * 918) + (nn_c_outer_inner * 306)) + (rc_outer * 6)) + ry_inner))+(1*1), (((((nn_c_outer_outer_inner * 918) + (nn_c_outer_inner * 306)) + (rc_outer * 6)) + ry_inner))+(1*2), (((((nn_c_outer_outer_inner * 918) + (nn_c_outer_inner * 306)) + (rc_outer * 6)) + ry_inner))+(1*3));\n              ((float4*)conv1d_ncw_local)[cse_var_1] = (((float4*)conv1d_ncw_local)[cse_var_1] + ((float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3])) * ((float4)(((float*)kernel_1)[((((nn_outer_ff_outer_fused_yy_outer_fused * 459) + (ff_c_outer_inner * 153)) + (rc_outer * 3)) + ry_inner)], ((float*)kernel_1)[((((nn_outer_ff_outer_fused_yy_outer_fused * 459) + (ff_c_outer_inner * 153)) + (rc_outer * 3)) + ry_inner)], ((float*)kernel_1)[((((nn_outer_ff_outer_fused_yy_outer_fused * 459) + (ff_c_outer_inner * 153)) + (rc_outer * 3)) + ry_inner)], ((float*)kernel_1)[((((nn_outer_ff_outer_fused_yy_outer_fused * 459) + (ff_c_outer_inner * 153)) + (rc_outer * 3)) + ry_inner)]))));\n            },\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 6; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 3; ++ff_inner) {\n        *(float4*)(((float*)conv1d_ncw_1) + (((nn_inner * 132) + (nn_outer_ff_outer_fused_yy_outer_fused * 12)) + (ff_inner * 4))) = ((float4*)conv1d_ncw_local)[((nn_inner * 3) + ff_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(33) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(33) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[2];\n  __shared__ float pad_temp_shared[102];\n  __shared__ float kernel_shared[1683];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  conv1d_ncw_local[1] = 0.000000e+00f;\n  for (int ry_outer_outer = 0; ry_outer_outer < 3; ++ry_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 4; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer * 11) + (((int)threadIdx.x) / 3)) < 34) {\n        pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 33) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) >> 2) * 612) + (ax0_ax1_fused_ax2_fused_outer_outer * 198)) + (((int)threadIdx.x) * 6)) + ry_outer_outer) + (((int)blockIdx.x) & 3))];\n      },\n    },\n    for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 51; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 33) + ((int)threadIdx.x))] = kernel[(((ax0_ax1_fused_ax2_fused_outer_outer_1 * 99) + (((int)threadIdx.x) * 3)) + ry_outer_outer)];\n    },\n    __syncthreads();\n    for (int rc_outer_inner = 0; rc_outer_inner < 3; ++rc_outer_inner) {\n      for (int rc_inner = 0; rc_inner < 17; ++rc_inner) {\n        conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[((rc_outer_inner * 17) + rc_inner)] * kernel_shared[(((((int)threadIdx.x) * 51) + (rc_outer_inner * 17)) + rc_inner)]));\n        conv1d_ncw_local[1] = (conv1d_ncw_local[1] + (pad_temp_shared[(((rc_outer_inner * 17) + rc_inner) + 51)] * kernel_shared[(((((int)threadIdx.x) * 51) + (rc_outer_inner * 17)) + rc_inner)]));\n      },\n    },\n  },\n  conv1d_ncw[((((((int)blockIdx.x) >> 2) * 264) + (((int)threadIdx.x) * 4)) + (((int)blockIdx.x) & 3))] = conv1d_ncw_local[0];\n  conv1d_ncw[(((((((int)blockIdx.x) >> 2) * 264) + (((int)threadIdx.x) * 4)) + (((int)blockIdx.x) & 3)) + 132)] = conv1d_ncw_local[1];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 51, 6), \"float32\"), kernel: T.Buffer((33, 51, 3), \"float32\"), conv1d_ncw: T.Buffer((6, 33, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_ff_outer_fused_yy_outer_fused in T.parallel(11):\n            conv1d_ncw_local = T.allocate([18], \"float32x4\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((18,), \"float32x4\", data=conv1d_ncw_local, scope=\"local\")\n            for nn_c_outer_outer_inner in range(2):\n                for nn_c_outer_inner_init, ff_c_outer_inner_init in T.grid(3, 3):\n                    conv1d_ncw_local_1[nn_c_outer_outer_inner * 9 + nn_c_outer_inner_init * 3 + ff_c_outer_inner_init] = T.Broadcast(T.float32(0), 4)\n                for rc_outer, nn_c_outer_inner, ff_c_outer_inner, ry_inner in T.grid(51, 3, 3, 3):\n                    cse_var_1: T.int32 = nn_c_outer_outer_inner * 9 + nn_c_outer_inner * 3 + ff_c_outer_inner\n                    data_1 = T.Buffer((1836,), data=data.data)\n                    kernel_1 = T.Buffer((5049,), data=kernel.data)\n                    conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_c_outer_outer_inner * 918 + nn_c_outer_inner * 306 + rc_outer * 6 + ry_inner:nn_c_outer_outer_inner * 918 + nn_c_outer_inner * 306 + rc_outer * 6 + ry_inner + 4] * T.Broadcast(kernel_1[nn_outer_ff_outer_fused_yy_outer_fused * 459 + ff_c_outer_inner * 153 + rc_outer * 3 + ry_inner], 4)\n            for nn_inner, ff_inner in T.grid(6, 3):\n                conv1d_ncw_1 = T.Buffer((792,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_inner * 132 + nn_outer_ff_outer_fused_yy_outer_fused * 12 + ff_inner * 4:nn_inner * 132 + nn_outer_ff_outer_fused_yy_outer_fused * 12 + ff_inner * 4 + 4] = conv1d_ncw_local_1[nn_inner * 3 + ff_inner]",
        "data": "6_51_6",
        "kernel": "33_51_3"
    },
    {
        "op_name": "group_conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 6; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)180, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 5; ++ff_c_outer_inner_init) {\n      for (int32_t yy_c_outer_inner_init = 0; yy_c_outer_inner_init < 3; ++yy_c_outer_inner_init) {\n        for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 3; ++ff_c_inner_init) {\n          ((float*)conv1d_ncw_local)[(((ff_c_outer_inner_init * 9) + (ff_c_inner_init * 3)) + yy_c_outer_inner_init)] = 0.000000e+00f;\n        },\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 38; ++rc_outer) {\n      for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 5; ++ff_c_outer_inner) {\n        for (int32_t yy_c_outer_inner = 0; yy_c_outer_inner < 3; ++yy_c_outer_inner) {\n          for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n            for (int32_t ff_c_inner = 0; ff_c_inner < 3; ++ff_c_inner) {\n              int32_t cse_var_1 = (((ff_c_outer_inner * 9) + (ff_c_inner * 3)) + yy_c_outer_inner);\n              ((float*)conv1d_ncw_local)[cse_var_1] = (((float*)conv1d_ncw_local)[cse_var_1] + (((float*)data_1)[((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused >> 1) * 304) + (rc_outer * 8)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 1) * 3)) + yy_c_outer_inner) + ry_inner)] * ((float*)kernel_1)[((((ff_c_outer_inner * 342) + (ff_c_inner * 114)) + (rc_outer * 3)) + ry_inner)]));\n            },\n          },\n        },\n      },\n    },\n    for (int32_t ff_inner = 0; ff_inner < 15; ++ff_inner) {\n      *(float3*)(((float*)conv1d_ncw_1) + ((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused >> 1) * 90) + (ff_inner * 6)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 1) * 3))) = *(float3*)(((float*)conv1d_ncw_local) + (ff_inner * 3));\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(90) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(90) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[3];\n  __shared__ float pad_temp_shared[684];\n  __shared__ float kernel_shared[570];\n  for (int nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 3; ++nn_c_outer_inner_init) {\n    conv1d_ncw_local[nn_c_outer_inner_init] = 0.000000e+00f;\n  },\n  for (int ry_outer_outer = 0; ry_outer_outer < 3; ++ry_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 8; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer * 5) + (((int)threadIdx.x) / 18)) < 38) {\n        pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 90) + ((int)threadIdx.x))] = data[((((ax0_ax1_fused_ax2_fused_outer_outer * 120) + ((((int)threadIdx.x) / 6) * 8)) + ry_outer_outer) + (((int)threadIdx.x) % 6))];\n      },\n    },\n    for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 7; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer_1 * 3) + (((int)threadIdx.x) / 30)) < 19) {\n        kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 90) + ((int)threadIdx.x))] = kernel[(((ax0_ax1_fused_ax2_fused_outer_outer_1 * 270) + (((int)threadIdx.x) * 3)) + ry_outer_outer)];\n      },\n    },\n    __syncthreads();\n    for (int nn_c_outer_inner = 0; nn_c_outer_inner < 3; ++nn_c_outer_inner) {\n      for (int rc_inner = 0; rc_inner < 38; ++rc_inner) {\n        conv1d_ncw_local[nn_c_outer_inner] = (conv1d_ncw_local[nn_c_outer_inner] + (pad_temp_shared[(((nn_c_outer_inner * 228) + (rc_inner * 6)) + (((int)threadIdx.x) % 6))] * kernel_shared[(((((int)threadIdx.x) / 6) * 38) + rc_inner)]));\n      },\n    },\n  },\n  for (int nn_inner = 0; nn_inner < 3; ++nn_inner) {\n    conv1d_ncw[((nn_inner * 90) + ((int)threadIdx.x))] = conv1d_ncw_local[nn_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 38, 8), \"float32\"), kernel: T.Buffer((15, 38, 3), \"float32\"), conv1d_ncw: T.Buffer((3, 15, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(6):\n            conv1d_ncw_local = T.allocate([45], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((45,), data=conv1d_ncw_local, scope=\"local\")\n            for ff_c_outer_inner_init, yy_c_outer_inner_init, ff_c_inner_init in T.grid(5, 3, 3):\n                conv1d_ncw_local_1[ff_c_outer_inner_init * 9 + ff_c_inner_init * 3 + yy_c_outer_inner_init] = T.float32(0)\n            for rc_outer, ff_c_outer_inner, yy_c_outer_inner, ry_inner, ff_c_inner in T.grid(38, 5, 3, 3, 3):\n                cse_var_1: T.int32 = ff_c_outer_inner * 9 + ff_c_inner * 3 + yy_c_outer_inner\n                data_1 = T.Buffer((912,), data=data.data)\n                kernel_1 = T.Buffer((1710,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 2 * 304 + rc_outer * 8 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 2 * 3 + yy_c_outer_inner + ry_inner] * kernel_1[ff_c_outer_inner * 342 + ff_c_inner * 114 + rc_outer * 3 + ry_inner]\n            for ff_inner in range(15):\n                conv1d_ncw_1 = T.Buffer((270,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 2 * 90 + ff_inner * 6 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 2 * 3:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 2 * 90 + ff_inner * 6 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 2 * 3 + 3] = conv1d_ncw_local_1[ff_inner * 3:ff_inner * 3 + 3]",
        "data": "3_38_8",
        "kernel": "15_38_3"
    },
    {
        "op_name": "group_conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused < 9; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused) {\n    for (int32_t ff_outer_inner_init = 0; ff_outer_inner_init < 7; ++ff_outer_inner_init) {\n      *(float5*)(((float*)conv1d_ncw_1) + ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 35) + (ff_outer_inner_init * 5))) = ((float5)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n    },\n    for (int32_t rc_outer = 0; rc_outer < 5; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n        for (int32_t ff_outer_inner = 0; ff_outer_inner < 7; ++ff_outer_inner) {\n          for (int32_t rc_inner = 0; rc_inner < 9; ++rc_inner) {\n            int32_t cse_var_1 = ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 35) + (ff_outer_inner * 5));\n            int32_t5 v_ = int32_t5((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4));\n            int32_t5 v__1 = int32_t5(((((rc_outer * 63) + (rc_inner * 7)) + ry_outer))+(1*0), ((((rc_outer * 63) + (rc_inner * 7)) + ry_outer))+(1*1), ((((rc_outer * 63) + (rc_inner * 7)) + ry_outer))+(1*2), ((((rc_outer * 63) + (rc_inner * 7)) + ry_outer))+(1*3), ((((rc_outer * 63) + (rc_inner * 7)) + ry_outer))+(1*4));\n            *(float5*)(((float*)conv1d_ncw_1) + cse_var_1) = ((float5(((float*)conv1d_ncw_1)[v_.s0],((float*)conv1d_ncw_1)[v_.s1],((float*)conv1d_ncw_1)[v_.s2],((float*)conv1d_ncw_1)[v_.s3],((float*)conv1d_ncw_1)[v_.s4])) + ((float5(((float*)data_1)[v__1.s0],((float*)data_1)[v__1.s1],((float*)data_1)[v__1.s2],((float*)data_1)[v__1.s3],((float*)data_1)[v__1.s4])) * ((float5)(((float*)kernel_1)[(((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 945) + (ff_outer_inner * 135)) + (rc_outer * 27)) + (rc_inner * 3)) + ry_outer)], ((float*)kernel_1)[(((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 945) + (ff_outer_inner * 135)) + (rc_outer * 27)) + (rc_inner * 3)) + ry_outer)], ((float*)kernel_1)[(((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 945) + (ff_outer_inner * 135)) + (rc_outer * 27)) + (rc_inner * 3)) + ry_outer)], ((float*)kernel_1)[(((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 945) + (ff_outer_inner * 135)) + (rc_outer * 27)) + (rc_inner * 3)) + ry_outer)], ((float*)kernel_1)[(((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 945) + (ff_outer_inner * 135)) + (rc_outer * 27)) + (rc_inner * 3)) + ry_outer)]))));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(105) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(105) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[3];\n  __shared__ float pad_temp_shared[315];\n  __shared__ float kernel_shared[8505];\n  for (int ff_c_inner_init = 0; ff_c_inner_init < 3; ++ff_c_inner_init) {\n    conv1d_ncw_local[ff_c_inner_init] = 0.000000e+00f;\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 3; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 105) + ((int)threadIdx.x))] = data[((ax0_ax1_fused_ax2_fused_outer_outer * 105) + ((int)threadIdx.x))];\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 81; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 105) + ((int)threadIdx.x))] = kernel[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 105) + ((int)threadIdx.x))];\n  },\n  __syncthreads();\n  for (int rc_outer_inner = 0; rc_outer_inner < 15; ++rc_outer_inner) {\n    for (int ry_outer_inner = 0; ry_outer_inner < 3; ++ry_outer_inner) {\n      for (int rc_inner = 0; rc_inner < 3; ++rc_inner) {\n        for (int ff_c_inner = 0; ff_c_inner < 3; ++ff_c_inner) {\n          conv1d_ncw_local[ff_c_inner] = (conv1d_ncw_local[ff_c_inner] + (pad_temp_shared[((((rc_outer_inner * 21) + (rc_inner * 7)) + ry_outer_inner) + (((int)threadIdx.x) % 5))] * kernel_shared[((((((((int)threadIdx.x) / 5) * 405) + (ff_c_inner * 135)) + (rc_outer_inner * 9)) + (rc_inner * 3)) + ry_outer_inner)]));\n        },\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 3; ++ff_inner) {\n    conv1d_ncw[((((((int)threadIdx.x) / 5) * 15) + (ff_inner * 5)) + (((int)threadIdx.x) % 5))] = conv1d_ncw_local[ff_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 45, 7), \"float32\"), kernel: T.Buffer((63, 45, 3), \"float32\"), conv1d_ncw: T.Buffer((1, 63, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused in T.parallel(9):\n            conv1d_ncw_1 = T.Buffer((315,), data=conv1d_ncw.data)\n            for ff_outer_inner_init in range(7):\n                conv1d_ncw_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 35 + ff_outer_inner_init * 5:nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 35 + ff_outer_inner_init * 5 + 5] = T.Broadcast(T.float32(0), 5)\n            for rc_outer, ry_outer, ff_outer_inner, rc_inner in T.grid(5, 3, 7, 9):\n                cse_var_1: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 35 + ff_outer_inner * 5\n                data_1 = T.Buffer((315,), data=data.data)\n                kernel_1 = T.Buffer((8505,), data=kernel.data)\n                conv1d_ncw_1[cse_var_1:cse_var_1 + 5] = conv1d_ncw_1[cse_var_1:cse_var_1 + 5] + data_1[rc_outer * 63 + rc_inner * 7 + ry_outer:rc_outer * 63 + rc_inner * 7 + ry_outer + 5] * T.Broadcast(kernel_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 945 + ff_outer_inner * 135 + rc_outer * 27 + rc_inner * 3 + ry_outer], 5)",
        "data": "1_45_7",
        "kernel": "63_45_3"
    },
    {
        "op_name": "group_conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused < 91; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused) {\n    for (int32_t ff_inner_init = 0; ff_inner_init < 5; ++ff_inner_init) {\n      *(float3*)(((float*)conv1d_ncw_1) + ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 15) + (ff_inner_init * 3))) = ((float3)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n    },\n    for (int32_t rc_outer = 0; rc_outer < 43; ++rc_outer) {\n      for (int32_t rc_inner = 0; rc_inner < 2; ++rc_inner) {\n        for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n          for (int32_t ff_inner = 0; ff_inner < 5; ++ff_inner) {\n            int32_t cse_var_1 = ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 15) + (ff_inner * 3));\n            int32_t3 v_ = int32_t3((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2));\n            int32_t3 v__1 = int32_t3(((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused / 13) * 430) + (rc_outer * 10)) + (rc_inner * 5)) + ry_inner))+(1*0), ((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused / 13) * 430) + (rc_outer * 10)) + (rc_inner * 5)) + ry_inner))+(1*1), ((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused / 13) * 430) + (rc_outer * 10)) + (rc_inner * 5)) + ry_inner))+(1*2));\n            *(float3*)(((float*)conv1d_ncw_1) + cse_var_1) = ((float3(((float*)conv1d_ncw_1)[v_.s0],((float*)conv1d_ncw_1)[v_.s1],((float*)conv1d_ncw_1)[v_.s2])) + ((float3(((float*)data_1)[v__1.s0],((float*)data_1)[v__1.s1],((float*)data_1)[v__1.s2])) * ((float3)(((float*)kernel_1)[((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 13) * 1290) + (ff_inner * 258)) + (rc_outer * 6)) + (rc_inner * 3)) + ry_inner)], ((float*)kernel_1)[((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 13) * 1290) + (ff_inner * 258)) + (rc_outer * 6)) + (rc_inner * 3)) + ry_inner)], ((float*)kernel_1)[((((((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 13) * 1290) + (ff_inner * 258)) + (rc_outer * 6)) + (rc_inner * 3)) + ry_inner)]))));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(13) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(13) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[3];\n  __shared__ float pad_temp_shared[430];\n  __shared__ float kernel_shared[3354];\n  for (int yy_c_inner_init = 0; yy_c_inner_init < 3; ++yy_c_inner_init) {\n    conv1d_ncw_local[yy_c_inner_init] = 0.000000e+00f;\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 34; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 13) + ((int)threadIdx.x)) < 430) {\n      pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 13) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) / 5) * 430) + (ax0_ax1_fused_ax2_fused_outer_outer * 13)) + ((int)threadIdx.x))];\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 258; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 13) + ((int)threadIdx.x))] = kernel[((((((int)blockIdx.x) % 5) * 3354) + (ax0_ax1_fused_ax2_fused_outer_outer_1 * 13)) + ((int)threadIdx.x))];\n  },\n  __syncthreads();\n  for (int rc_outer_inner = 0; rc_outer_inner < 43; ++rc_outer_inner) {\n    for (int rc_inner = 0; rc_inner < 2; ++rc_inner) {\n      for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n        for (int yy_c_inner = 0; yy_c_inner < 3; ++yy_c_inner) {\n          conv1d_ncw_local[yy_c_inner] = (conv1d_ncw_local[yy_c_inner] + (pad_temp_shared[((((rc_outer_inner * 10) + (rc_inner * 5)) + yy_c_inner) + ry_inner)] * kernel_shared[((((((int)threadIdx.x) * 258) + (rc_outer_inner * 6)) + (rc_inner * 3)) + ry_inner)]));\n        },\n      },\n    },\n  },\n  for (int yy_inner = 0; yy_inner < 3; ++yy_inner) {\n    conv1d_ncw[(((((int)blockIdx.x) * 39) + (((int)threadIdx.x) * 3)) + yy_inner)] = conv1d_ncw_local[yy_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 86, 5), \"float32\"), kernel: T.Buffer((65, 86, 3), \"float32\"), conv1d_ncw: T.Buffer((7, 65, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused in T.parallel(91):\n            conv1d_ncw_1 = T.Buffer((1365,), data=conv1d_ncw.data)\n            for ff_inner_init in range(5):\n                conv1d_ncw_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 15 + ff_inner_init * 3:nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 15 + ff_inner_init * 3 + 3] = T.Broadcast(T.float32(0), 3)\n            for rc_outer, rc_inner, ry_inner, ff_inner in T.grid(43, 2, 3, 5):\n                cse_var_1: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused * 15 + ff_inner * 3\n                data_1 = T.Buffer((3010,), data=data.data)\n                kernel_1 = T.Buffer((16770,), data=kernel.data)\n                conv1d_ncw_1[cse_var_1:cse_var_1 + 3] = conv1d_ncw_1[cse_var_1:cse_var_1 + 3] + data_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 13 * 430 + rc_outer * 10 + rc_inner * 5 + ry_inner:nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 13 * 430 + rc_outer * 10 + rc_inner * 5 + ry_inner + 3] * T.Broadcast(kernel_1[nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 13 * 1290 + ff_inner * 258 + rc_outer * 6 + rc_inner * 3 + ry_inner], 3)",
        "data": "7_86_5",
        "kernel": "65_86_3"
    },
    {
        "op_name": "group_conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 32; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)80, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 2; ++ff_c_outer_inner_init) {\n      for (int32_t nn_c_inner_init = 0; nn_c_inner_init < 2; ++nn_c_inner_init) {\n        for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 5; ++ff_c_inner_init) {\n          ((float*)conv1d_ncw_local)[(((nn_c_inner_init * 10) + (ff_c_outer_inner_init * 5)) + ff_c_inner_init)] = 0.000000e+00f;\n        },\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 42; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 2; ++ry_outer) {\n        for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 2; ++ff_c_outer_inner) {\n          for (int32_t rc_inner = 0; rc_inner < 2; ++rc_inner) {\n            for (int32_t ry_inner = 0; ry_inner < 2; ++ry_inner) {\n              for (int32_t nn_c_inner = 0; nn_c_inner < 2; ++nn_c_inner) {\n                for (int32_t ff_c_inner = 0; ff_c_inner < 5; ++ff_c_inner) {\n                  int32_t cse_var_2 = (ry_outer * 2);\n                  int32_t cse_var_1 = (((nn_c_inner * 10) + (ff_c_outer_inner * 5)) + ff_c_inner);\n                  ((float*)conv1d_ncw_local)[cse_var_1] = (((float*)conv1d_ncw_local)[cse_var_1] + (((float*)data_1)[((((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 7) >> 2) * 1176) + (nn_c_inner * 588)) + (rc_outer * 14)) + (rc_inner * 7)) + (((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 15) >> 3) * 2)) + cse_var_2) + ry_inner) + (nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 1))] * ((float*)kernel_1)[(((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused >> 4) * 6720) + (((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 3) >> 1) * 3360)) + (ff_c_outer_inner * 1680)) + (ff_c_inner * 336)) + (rc_outer * 8)) + (rc_inner * 4)) + cse_var_2) + ry_inner)]));\n                },\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 2; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 10; ++ff_inner) {\n        ((float*)conv1d_ncw_1)[(((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 7) >> 2) * 320) + (nn_inner * 160)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused >> 4) * 80)) + (((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 3) >> 1) * 40)) + (ff_inner * 4)) + (((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 15) >> 3) * 2)) + (nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 1))] = ((float*)conv1d_ncw_local)[((nn_inner * 10) + ff_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[8];\n  __shared__ float pad_temp_shared[1680];\n  __shared__ float kernel_shared[3360];\n  for (int ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 2; ++ff_c_outer_inner_init) {\n    for (int yy_c_inner_init = 0; yy_c_inner_init < 2; ++yy_c_inner_init) {\n      conv1d_ncw_local[((ff_c_outer_inner_init * 2) + yy_c_inner_init)] = 0.000000e+00f;\n      conv1d_ncw_local[(((ff_c_outer_inner_init * 2) + yy_c_inner_init) + 4)] = 0.000000e+00f;\n    },\n  },\n  for (int ry_outer_outer = 0; ry_outer_outer < 2; ++ry_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 42; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 40) + ((int)threadIdx.x))] = data[((((ax0_ax1_fused_ax2_fused_outer_outer * 56) + ((((int)threadIdx.x) / 5) * 7)) + (ry_outer_outer * 2)) + (((int)threadIdx.x) % 5))];\n    },\n    for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 84; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 40) + ((int)threadIdx.x))] = kernel[(((((((int)blockIdx.x) * 6720) + (ax0_ax1_fused_ax2_fused_outer_outer_1 * 80)) + ((((int)threadIdx.x) >> 1) * 4)) + (ry_outer_outer * 2)) + (((int)threadIdx.x) & 1))];\n    },\n    __syncthreads();\n    for (int rc_outer_inner = 0; rc_outer_inner < 42; ++rc_outer_inner) {\n      for (int ff_c_outer_inner = 0; ff_c_outer_inner < 2; ++ff_c_outer_inner) {\n        for (int rc_inner = 0; rc_inner < 2; ++rc_inner) {\n          for (int ry_inner = 0; ry_inner < 2; ++ry_inner) {\n            for (int yy_c_inner = 0; yy_c_inner < 2; ++yy_c_inner) {\n              conv1d_ncw_local[((ff_c_outer_inner * 2) + yy_c_inner)] = (conv1d_ncw_local[((ff_c_outer_inner * 2) + yy_c_inner)] + (pad_temp_shared[((((((((int)threadIdx.x) / 10) * 420) + (rc_outer_inner * 10)) + (rc_inner * 5)) + yy_c_inner) + ry_inner)] * kernel_shared[((((((((int)threadIdx.x) % 10) * 336) + (ff_c_outer_inner * 168)) + (rc_outer_inner * 4)) + (rc_inner * 2)) + ry_inner)]));\n              conv1d_ncw_local[(((ff_c_outer_inner * 2) + yy_c_inner) + 4)] = (conv1d_ncw_local[(((ff_c_outer_inner * 2) + yy_c_inner) + 4)] + (pad_temp_shared[(((((((((int)threadIdx.x) / 10) * 420) + (rc_outer_inner * 10)) + (rc_inner * 5)) + yy_c_inner) + ry_inner) + 2)] * kernel_shared[((((((((int)threadIdx.x) % 10) * 336) + (ff_c_outer_inner * 168)) + (rc_outer_inner * 4)) + (rc_inner * 2)) + ry_inner)]));\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 2; ++ff_inner) {\n    for (int yy_inner = 0; yy_inner < 2; ++yy_inner) {\n      conv1d_ncw[((((((((int)threadIdx.x) / 10) * 160) + (((int)blockIdx.x) * 80)) + ((((int)threadIdx.x) % 10) * 8)) + (ff_inner * 4)) + yy_inner)] = conv1d_ncw_local[((ff_inner * 2) + yy_inner)];\n      conv1d_ncw[(((((((((int)threadIdx.x) / 10) * 160) + (((int)blockIdx.x) * 80)) + ((((int)threadIdx.x) % 10) * 8)) + (ff_inner * 4)) + yy_inner) + 2)] = conv1d_ncw_local[(((ff_inner * 2) + yy_inner) + 4)];\n    },\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 84, 7), \"float32\"), kernel: T.Buffer((40, 84, 4), \"float32\"), conv1d_ncw: T.Buffer((4, 40, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(32):\n            conv1d_ncw_local = T.allocate([20], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((20,), data=conv1d_ncw_local, scope=\"local\")\n            for ff_c_outer_inner_init, nn_c_inner_init, ff_c_inner_init in T.grid(2, 2, 5):\n                conv1d_ncw_local_1[nn_c_inner_init * 10 + ff_c_outer_inner_init * 5 + ff_c_inner_init] = T.float32(0)\n            for rc_outer, ry_outer, ff_c_outer_inner, rc_inner, ry_inner, nn_c_inner, ff_c_inner in T.grid(42, 2, 2, 2, 2, 2, 5):\n                cse_var_2: T.int32 = ry_outer * 2\n                cse_var_1: T.int32 = nn_c_inner * 10 + ff_c_outer_inner * 5 + ff_c_inner\n                data_1 = T.Buffer((2352,), data=data.data)\n                kernel_1 = T.Buffer((13440,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 8 // 4 * 1176 + nn_c_inner * 588 + rc_outer * 14 + rc_inner * 7 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 16 // 8 * 2 + cse_var_2 + ry_inner + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 2] * kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 16 * 6720 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 4 // 2 * 3360 + ff_c_outer_inner * 1680 + ff_c_inner * 336 + rc_outer * 8 + rc_inner * 4 + cse_var_2 + ry_inner]\n            for nn_inner, ff_inner in T.grid(2, 10):\n                conv1d_ncw_1 = T.Buffer((640,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 8 // 4 * 320 + nn_inner * 160 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 16 * 80 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 4 // 2 * 40 + ff_inner * 4 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 16 // 8 * 2 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 2] = conv1d_ncw_local_1[nn_inner * 10 + ff_inner]",
        "data": "4_84_7",
        "kernel": "40_84_4"
    },
    {
        "op_name": "group_conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_ff_outer_fused_yy_outer_fused = 0; nn_outer_ff_outer_fused_yy_outer_fused < 73; ++nn_outer_ff_outer_fused_yy_outer_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)24, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t yy_c_outer_inner_init = 0; yy_c_outer_inner_init < 6; ++yy_c_outer_inner_init) {\n      ((float*)conv1d_ncw_local)[yy_c_outer_inner_init] = 0.000000e+00f;\n    },\n    for (int32_t rc_outer = 0; rc_outer < 83; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n        for (int32_t yy_c_outer_inner = 0; yy_c_outer_inner < 6; ++yy_c_outer_inner) {\n          ((float*)conv1d_ncw_local)[yy_c_outer_inner] = (((float*)conv1d_ncw_local)[yy_c_outer_inner] + (((float*)data_1)[(((rc_outer * 8) + yy_c_outer_inner) + ry_outer)] * ((float*)kernel_1)[(((nn_outer_ff_outer_fused_yy_outer_fused * 249) + (rc_outer * 3)) + ry_outer)]));\n        },\n      },\n    },\n    *(float6*)(((float*)conv1d_ncw_1) + (nn_outer_ff_outer_fused_yy_outer_fused * 6)) = *(float6*)(((float*)conv1d_ncw_local) + 0);\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[6];\n  __shared__ float pad_temp_shared[664];\n  __shared__ float kernel_shared[249];\n  for (int yy_c_outer_inner_init = 0; yy_c_outer_inner_init < 2; ++yy_c_outer_inner_init) {\n    conv1d_ncw_local[yy_c_outer_inner_init] = 0.000000e+00f;\n    conv1d_ncw_local[(yy_c_outer_inner_init + 2)] = 0.000000e+00f;\n    conv1d_ncw_local[(yy_c_outer_inner_init + 4)] = 0.000000e+00f;\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 664; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    pad_temp_shared[ax0_ax1_fused_ax2_fused_outer_outer] = data[ax0_ax1_fused_ax2_fused_outer_outer];\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 83; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    *(float3*)(kernel_shared + (ax0_ax1_fused_ax2_fused_outer_outer_1 * 3)) = *(float3*)(kernel + ((((int)blockIdx.x) * 249) + (ax0_ax1_fused_ax2_fused_outer_outer_1 * 3)));\n  },\n  __syncthreads();\n  for (int rc_outer_inner = 0; rc_outer_inner < 83; ++rc_outer_inner) {\n    for (int yy_c_outer_inner = 0; yy_c_outer_inner < 2; ++yy_c_outer_inner) {\n      for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n        conv1d_ncw_local[yy_c_outer_inner] = (conv1d_ncw_local[yy_c_outer_inner] + (pad_temp_shared[(((rc_outer_inner * 8) + yy_c_outer_inner) + ry_inner)] * kernel_shared[((rc_outer_inner * 3) + ry_inner)]));\n        conv1d_ncw_local[(yy_c_outer_inner + 2)] = (conv1d_ncw_local[(yy_c_outer_inner + 2)] + (pad_temp_shared[((((rc_outer_inner * 8) + yy_c_outer_inner) + ry_inner) + 2)] * kernel_shared[((rc_outer_inner * 3) + ry_inner)]));\n        conv1d_ncw_local[(yy_c_outer_inner + 4)] = (conv1d_ncw_local[(yy_c_outer_inner + 4)] + (pad_temp_shared[((((rc_outer_inner * 8) + yy_c_outer_inner) + ry_inner) + 4)] * kernel_shared[((rc_outer_inner * 3) + ry_inner)]));\n      },\n    },\n  },\n  for (int yy_inner = 0; yy_inner < 2; ++yy_inner) {\n    conv1d_ncw[((((int)blockIdx.x) * 6) + yy_inner)] = conv1d_ncw_local[yy_inner];\n    conv1d_ncw[(((((int)blockIdx.x) * 6) + yy_inner) + 2)] = conv1d_ncw_local[(yy_inner + 2)];\n    conv1d_ncw[(((((int)blockIdx.x) * 6) + yy_inner) + 4)] = conv1d_ncw_local[(yy_inner + 4)];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 83, 8), \"float32\"), kernel: T.Buffer((73, 83, 3), \"float32\"), conv1d_ncw: T.Buffer((1, 73, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_ff_outer_fused_yy_outer_fused in T.parallel(73):\n            conv1d_ncw_local = T.allocate([6], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((6,), data=conv1d_ncw_local, scope=\"local\", align=16)\n            for yy_c_outer_inner_init in range(6):\n                conv1d_ncw_local_1[yy_c_outer_inner_init] = T.float32(0)\n            for rc_outer, ry_outer, yy_c_outer_inner in T.grid(83, 3, 6):\n                data_1 = T.Buffer((664,), data=data.data)\n                kernel_1 = T.Buffer((18177,), data=kernel.data)\n                conv1d_ncw_local_1[yy_c_outer_inner] = conv1d_ncw_local_1[yy_c_outer_inner] + data_1[rc_outer * 8 + yy_c_outer_inner + ry_outer] * kernel_1[nn_outer_ff_outer_fused_yy_outer_fused * 249 + rc_outer * 3 + ry_outer]\n            conv1d_ncw_1 = T.Buffer((438,), data=conv1d_ncw.data)\n            conv1d_ncw_1[nn_outer_ff_outer_fused_yy_outer_fused * 6:nn_outer_ff_outer_fused_yy_outer_fused * 6 + 6] = conv1d_ncw_local_1[0:6]",
        "data": "1_83_8",
        "kernel": "73_83_3"
    },
    {
        "op_name": "group_conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_inner_init = 0; nn_outer_inner_init < 2; ++nn_outer_inner_init) {\n    for (int32_t ff_outer_inner_init = 0; ff_outer_inner_init < 9; ++ff_outer_inner_init) {\n      for (int32_t ff_inner_init = 0; ff_inner_init < 7; ++ff_inner_init) {\n        ((float*)conv1d_ncw_1)[(((nn_outer_inner_init * 63) + (ff_outer_inner_init * 7)) + ff_inner_init)] = 0.000000e+00f;\n      },\n    },\n  },\n  for (int32_t rc_outer = 0; rc_outer < 41; ++rc_outer) {\n    for (int32_t nn_outer_inner = 0; nn_outer_inner < 2; ++nn_outer_inner) {\n      for (int32_t ff_outer_inner = 0; ff_outer_inner < 9; ++ff_outer_inner) {\n        for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n          for (int32_t ff_inner = 0; ff_inner < 7; ++ff_inner) {\n            int32_t cse_var_2 = (rc_outer * 3);\n            int32_t cse_var_1 = (((nn_outer_inner * 63) + (ff_outer_inner * 7)) + ff_inner);\n            ((float*)conv1d_ncw_1)[cse_var_1] = (((float*)conv1d_ncw_1)[cse_var_1] + (((float*)data_1)[(((nn_outer_inner * 123) + cse_var_2) + ry_inner)] * ((float*)kernel_1)[((((ff_outer_inner * 861) + (ff_inner * 123)) + cse_var_2) + ry_inner)]));\n          },\n        },\n      },\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[2];\n  __shared__ float pad_temp_shared[246];\n  __shared__ float kernel_shared[7749];\n  for (int nn_c_outer_inner_init = 0; nn_c_outer_inner_init < 2; ++nn_c_outer_inner_init) {\n    conv1d_ncw_local[nn_c_outer_inner_init] = 0.000000e+00f;\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 4; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 21) + (((int)threadIdx.x) / 3)) < 82) {\n      pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 63) + ((int)threadIdx.x))] = data[((ax0_ax1_fused_ax2_fused_outer_outer * 63) + ((int)threadIdx.x))];\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 123; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 63) + ((int)threadIdx.x))] = kernel[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 63) + ((int)threadIdx.x))];\n  },\n  __syncthreads();\n  for (int nn_c_outer_inner = 0; nn_c_outer_inner < 2; ++nn_c_outer_inner) {\n    for (int rc_inner = 0; rc_inner < 41; ++rc_inner) {\n      for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n        conv1d_ncw_local[nn_c_outer_inner] = (conv1d_ncw_local[nn_c_outer_inner] + (pad_temp_shared[(((nn_c_outer_inner * 123) + (rc_inner * 3)) + ry_inner)] * kernel_shared[(((((int)threadIdx.x) * 123) + (rc_inner * 3)) + ry_inner)]));\n      },\n    },\n  },\n  for (int nn_inner = 0; nn_inner < 2; ++nn_inner) {\n    conv1d_ncw[((nn_inner * 63) + ((int)threadIdx.x))] = conv1d_ncw_local[nn_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 41, 3), \"float32\"), kernel: T.Buffer((63, 41, 3), \"float32\"), conv1d_ncw: T.Buffer((2, 63, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        conv1d_ncw_1 = T.Buffer((126,), data=conv1d_ncw.data)\n        for nn_outer_inner_init, ff_outer_inner_init, ff_inner_init in T.grid(2, 9, 7):\n            conv1d_ncw_1[nn_outer_inner_init * 63 + ff_outer_inner_init * 7 + ff_inner_init] = T.float32(0)\n        for rc_outer, nn_outer_inner, ff_outer_inner, ry_inner, ff_inner in T.grid(41, 2, 9, 3, 7):\n            cse_var_2: T.int32 = rc_outer * 3\n            cse_var_1: T.int32 = nn_outer_inner * 63 + ff_outer_inner * 7 + ff_inner\n            data_1 = T.Buffer((246,), data=data.data)\n            kernel_1 = T.Buffer((7749,), data=kernel.data)\n            conv1d_ncw_1[cse_var_1] = conv1d_ncw_1[cse_var_1] + data_1[nn_outer_inner * 123 + cse_var_2 + ry_inner] * kernel_1[ff_outer_inner * 861 + ff_inner * 123 + cse_var_2 + ry_inner]",
        "data": "2_41_3",
        "kernel": "63_41_3"
    },
    {
        "op_name": "group_conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_ff_outer_fused_yy_outer_fused = 0; nn_outer_ff_outer_fused_yy_outer_fused < 35; ++nn_outer_ff_outer_fused_yy_outer_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)36, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 3; ++ff_c_outer_inner_init) {\n      for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 3; ++ff_c_inner_init) {\n        ((float*)conv1d_ncw_local)[((ff_c_outer_inner_init * 3) + ff_c_inner_init)] = 0.000000e+00f;\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 76; ++rc_outer) {\n      for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 3; ++ff_c_outer_inner) {\n        for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n          for (int32_t ff_c_inner = 0; ff_c_inner < 3; ++ff_c_inner) {\n            int32_t cse_var_1 = ((ff_c_outer_inner * 3) + ff_c_inner);\n            ((float*)conv1d_ncw_local)[cse_var_1] = (((float*)conv1d_ncw_local)[cse_var_1] + (((float*)data_1)[(((rc_outer * 7) + ry_inner) + (nn_outer_ff_outer_fused_yy_outer_fused % 5))] * ((float*)kernel_1)[((((((nn_outer_ff_outer_fused_yy_outer_fused / 5) * 2052) + (ff_c_outer_inner * 684)) + (ff_c_inner * 228)) + (rc_outer * 3)) + ry_inner)]));\n          },\n        },\n      },\n    },\n    for (int32_t ff_inner = 0; ff_inner < 9; ++ff_inner) {\n      ((float*)conv1d_ncw_1)[((((nn_outer_ff_outer_fused_yy_outer_fused / 5) * 45) + (ff_inner * 5)) + (nn_outer_ff_outer_fused_yy_outer_fused % 5))] = ((float*)conv1d_ncw_local)[ff_inner];\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[1];\n  __shared__ float pad_temp_shared[532];\n  __shared__ float kernel_shared[2052];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 12; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 45) + ((int)threadIdx.x)) < 532) {\n      pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 45) + ((int)threadIdx.x))] = data[((ax0_ax1_fused_ax2_fused_outer_outer * 45) + ((int)threadIdx.x))];\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 3; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    for (int ax0_ax1_fused_ax2_fused_inner_s = 0; ax0_ax1_fused_ax2_fused_inner_s < 19; ++ax0_ax1_fused_ax2_fused_inner_s) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer_1 * 5) + (((int)threadIdx.x) / 9)) < 12) {\n        kernel_shared[(((ax0_ax1_fused_ax2_fused_outer_outer_1 * 855) + (((int)threadIdx.x) * 19)) + ax0_ax1_fused_ax2_fused_inner_s)] = kernel[((((((int)blockIdx.x) * 2052) + (ax0_ax1_fused_ax2_fused_outer_outer_1 * 855)) + (((int)threadIdx.x) * 19)) + ax0_ax1_fused_ax2_fused_inner_s)];\n      },\n    },\n  },\n  __syncthreads();\n  for (int rc_outer_inner = 0; rc_outer_inner < 4; ++rc_outer_inner) {\n    for (int rc_inner = 0; rc_inner < 19; ++rc_inner) {\n      for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n        conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[((((rc_outer_inner * 133) + (rc_inner * 7)) + ry_inner) + (((int)threadIdx.x) % 5))] * kernel_shared[(((((((int)threadIdx.x) / 5) * 228) + (rc_outer_inner * 57)) + (rc_inner * 3)) + ry_inner)]));\n      },\n    },\n  },\n  conv1d_ncw[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = conv1d_ncw_local[0];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 76, 7), \"float32\"), kernel: T.Buffer((63, 76, 3), \"float32\"), conv1d_ncw: T.Buffer((1, 63, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_ff_outer_fused_yy_outer_fused in T.parallel(35):\n            conv1d_ncw_local = T.allocate([9], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((9,), data=conv1d_ncw_local, scope=\"local\", align=32)\n            for ff_c_outer_inner_init, ff_c_inner_init in T.grid(3, 3):\n                conv1d_ncw_local_1[ff_c_outer_inner_init * 3 + ff_c_inner_init] = T.float32(0)\n            for rc_outer, ff_c_outer_inner, ry_inner, ff_c_inner in T.grid(76, 3, 3, 3):\n                cse_var_1: T.int32 = ff_c_outer_inner * 3 + ff_c_inner\n                data_1 = T.Buffer((532,), data=data.data)\n                kernel_1 = T.Buffer((14364,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[rc_outer * 7 + ry_inner + nn_outer_ff_outer_fused_yy_outer_fused % 5] * kernel_1[nn_outer_ff_outer_fused_yy_outer_fused // 5 * 2052 + ff_c_outer_inner * 684 + ff_c_inner * 228 + rc_outer * 3 + ry_inner]\n            for ff_inner in range(9):\n                conv1d_ncw_1 = T.Buffer((315,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_outer_ff_outer_fused_yy_outer_fused // 5 * 45 + ff_inner * 5 + nn_outer_ff_outer_fused_yy_outer_fused % 5] = conv1d_ncw_local_1[ff_inner]",
        "data": "1_76_7",
        "kernel": "63_76_3"
    },
    {
        "op_name": "group_conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 660; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)48, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 3; ++ff_c_outer_inner_init) {\n      ((float4*)conv1d_ncw_local)[ff_c_outer_inner_init] = ((float4)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n    },\n    for (int32_t rc_outer = 0; rc_outer < 73; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n        for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 3; ++ff_c_outer_inner) {\n          int32_t4 v_ = int32_t4(((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 66) * 730) + (rc_outer * 10)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 1) * 4)) + ry_outer))+(1*0), ((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 66) * 730) + (rc_outer * 10)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 1) * 4)) + ry_outer))+(1*1), ((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 66) * 730) + (rc_outer * 10)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 1) * 4)) + ry_outer))+(1*2), ((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 66) * 730) + (rc_outer * 10)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 1) * 4)) + ry_outer))+(1*3));\n          ((float4*)conv1d_ncw_local)[ff_c_outer_inner] = (((float4*)conv1d_ncw_local)[ff_c_outer_inner] + ((float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3])) * ((float4)(((float*)kernel_1)[((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 66) >> 1) * 657) + (ff_c_outer_inner * 219)) + (rc_outer * 3)) + ry_outer)], ((float*)kernel_1)[((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 66) >> 1) * 657) + (ff_c_outer_inner * 219)) + (rc_outer * 3)) + ry_outer)], ((float*)kernel_1)[((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 66) >> 1) * 657) + (ff_c_outer_inner * 219)) + (rc_outer * 3)) + ry_outer)], ((float*)kernel_1)[((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 66) >> 1) * 657) + (ff_c_outer_inner * 219)) + (rc_outer * 3)) + ry_outer)]))));\n        },\n      },\n    },\n    for (int32_t ff_inner = 0; ff_inner < 3; ++ff_inner) {\n      *(float4*)(((float*)conv1d_ncw_1) + ((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused >> 1) * 24) + (ff_inner * 8)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 1) * 4))) = ((float4*)conv1d_ncw_local)[ff_inner];\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[3];\n  __shared__ float pad_temp_shared[30];\n  __shared__ float kernel_shared[27];\n  for (int ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 3; ++ff_c_outer_inner_init) {\n    conv1d_ncw_local[ff_c_outer_inner_init] = 0.000000e+00f;\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 73; ++rc_outer_outer) {\n    __syncthreads();\n    if (((int)threadIdx.x) < 30) {\n      pad_temp_shared[((int)threadIdx.x)] = data[((((((((int)blockIdx.x) / 22) * 3650) + ((((int)threadIdx.x) / 6) * 730)) + (rc_outer_outer * 10)) + ((((int)blockIdx.x) & 1) * 4)) + (((int)threadIdx.x) % 6))];\n    },\n    if (((int)threadIdx.x) < 27) {\n      kernel_shared[((int)threadIdx.x)] = kernel[((((((((int)blockIdx.x) % 22) >> 1) * 1971) + ((((int)threadIdx.x) / 3) * 219)) + (rc_outer_outer * 3)) + (((int)threadIdx.x) % 3))];\n    },\n    __syncthreads();\n    for (int ff_c_outer_inner = 0; ff_c_outer_inner < 3; ++ff_c_outer_inner) {\n      for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n        conv1d_ncw_local[ff_c_outer_inner] = (conv1d_ncw_local[ff_c_outer_inner] + (pad_temp_shared[((((((int)threadIdx.x) / 12) * 6) + ry_inner) + (((int)threadIdx.x) & 3))] * kernel_shared[(((((((int)threadIdx.x) % 12) >> 2) * 9) + (ff_c_outer_inner * 3)) + ry_inner)]));\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 3; ++ff_inner) {\n    conv1d_ncw[((((((((((int)blockIdx.x) / 22) * 3960) + ((((int)threadIdx.x) / 12) * 792)) + (((((int)blockIdx.x) % 22) >> 1) * 72)) + (((((int)threadIdx.x) % 12) >> 2) * 24)) + (ff_inner * 8)) + ((((int)blockIdx.x) & 1) * 4)) + (((int)threadIdx.x) & 3))] = conv1d_ncw_local[ff_inner];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 73, 10), \"float32\"), kernel: T.Buffer((99, 73, 3), \"float32\"), conv1d_ncw: T.Buffer((10, 99, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(660):\n            conv1d_ncw_local = T.allocate([3], \"float32x4\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((3,), \"float32x4\", data=conv1d_ncw_local, scope=\"local\", align=32)\n            for ff_c_outer_inner_init in range(3):\n                conv1d_ncw_local_1[ff_c_outer_inner_init] = T.Broadcast(T.float32(0), 4)\n            for rc_outer, ry_outer, ff_c_outer_inner in T.grid(73, 3, 3):\n                data_1 = T.Buffer((7300,), data=data.data)\n                kernel_1 = T.Buffer((21681,), data=kernel.data)\n                conv1d_ncw_local_1[ff_c_outer_inner] = conv1d_ncw_local_1[ff_c_outer_inner] + data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 66 * 730 + rc_outer * 10 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 2 * 4 + ry_outer:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 66 * 730 + rc_outer * 10 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 2 * 4 + ry_outer + 4] * T.Broadcast(kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 66 // 2 * 657 + ff_c_outer_inner * 219 + rc_outer * 3 + ry_outer], 4)\n            for ff_inner in range(3):\n                conv1d_ncw_1 = T.Buffer((7920,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 2 * 24 + ff_inner * 8 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 2 * 4:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 2 * 24 + ff_inner * 8 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 2 * 4 + 4] = conv1d_ncw_local_1[ff_inner]",
        "data": "10_73_10",
        "kernel": "99_73_3"
    },
    {
        "op_name": "group_conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1344, 2, 32);\n  if (conv1d_ncw_local == NULL) {\n    return -1;\n  },\n  for (int32_t nn_c_outer_outer_inner = 0; nn_c_outer_outer_inner < 2; ++nn_c_outer_outer_inner) {\n    for (int32_t yy_c_outer_outer_inner = 0; yy_c_outer_outer_inner < 3; ++yy_c_outer_outer_inner) {\n      for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 8; ++ff_c_outer_inner_init) {\n        for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 7; ++ff_c_inner_init) {\n          ((float*)conv1d_ncw_local)[((((nn_c_outer_outer_inner * 168) + (ff_c_outer_inner_init * 21)) + (ff_c_inner_init * 3)) + yy_c_outer_outer_inner)] = 0.000000e+00f;\n        },\n      },\n      for (int32_t rc_outer = 0; rc_outer < 3; ++rc_outer) {\n        for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n          for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 8; ++ff_c_outer_inner) {\n            for (int32_t rc_inner = 0; rc_inner < 4; ++rc_inner) {\n              for (int32_t ff_c_inner = 0; ff_c_inner < 7; ++ff_c_inner) {\n                int32_t cse_var_1 = ((((nn_c_outer_outer_inner * 168) + (ff_c_outer_inner * 21)) + (ff_c_inner * 3)) + yy_c_outer_outer_inner);\n                ((float*)conv1d_ncw_local)[cse_var_1] = (((float*)conv1d_ncw_local)[cse_var_1] + (((float*)data_1)[(((((nn_c_outer_outer_inner * 60) + (rc_outer * 20)) + (rc_inner * 5)) + yy_c_outer_outer_inner) + ry_outer)] * ((float*)kernel_1)[(((((ff_c_outer_inner * 252) + (ff_c_inner * 36)) + (rc_outer * 12)) + (rc_inner * 3)) + ry_outer)]));\n              },\n            },\n          },\n        },\n      },\n    },\n  },\n  for (int32_t nn_inner = 0; nn_inner < 2; ++nn_inner) {\n    for (int32_t ff_inner = 0; ff_inner < 56; ++ff_inner) {\n      int32_t cse_var_2 = ((nn_inner * 168) + (ff_inner * 3));\n      int32_t3 v_ = int32_t3((cse_var_2)+(1*0), (cse_var_2)+(1*1), (cse_var_2)+(1*2));\n      *(float3*)(((float*)conv1d_ncw_1) + cse_var_2) = (float3(((float*)conv1d_ncw_local)[v_.s0],((float*)conv1d_ncw_local)[v_.s1],((float*)conv1d_ncw_local)[v_.s2]));\n    },\n  },\n  if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n    return -1;\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[1];\n  __shared__ float pad_temp_shared[20];\n  __shared__ float kernel_shared[42];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  for (int rc_outer_outer = 0; rc_outer_outer < 6; ++rc_outer_outer) {\n    __syncthreads();\n    if (((int)threadIdx.x) < 20) {\n      pad_temp_shared[((int)threadIdx.x)] = data[((((((int)threadIdx.x) / 10) * 60) + (rc_outer_outer * 10)) + (((int)threadIdx.x) % 10))];\n    },\n    kernel_shared[((int)threadIdx.x)] = kernel[((((((int)blockIdx.x) * 252) + ((((int)threadIdx.x) / 6) * 36)) + (rc_outer_outer * 6)) + (((int)threadIdx.x) % 6))];\n    __syncthreads();\n    for (int rc_outer_inner = 0; rc_outer_inner < 2; ++rc_outer_inner) {\n      for (int ry_outer_inner = 0; ry_outer_inner < 3; ++ry_outer_inner) {\n        conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[(((((((int)threadIdx.x) / 21) * 10) + (rc_outer_inner * 5)) + ry_outer_inner) + (((int)threadIdx.x) % 3))] * kernel_shared[(((((((int)threadIdx.x) % 21) / 3) * 6) + (rc_outer_inner * 3)) + ry_outer_inner)]));\n      },\n    },\n  },\n  conv1d_ncw[((((((int)threadIdx.x) / 21) * 168) + (((int)blockIdx.x) * 21)) + (((int)threadIdx.x) % 21))] = conv1d_ncw_local[0];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 12, 5), \"float32\"), kernel: T.Buffer((56, 12, 3), \"float32\"), conv1d_ncw: T.Buffer((2, 56, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        conv1d_ncw_local = T.allocate([336], \"float32\", \"local\")\n        conv1d_ncw_local_1 = T.Buffer((336,), data=conv1d_ncw_local, scope=\"local\")\n        for nn_c_outer_outer_inner, yy_c_outer_outer_inner in T.grid(2, 3):\n            for ff_c_outer_inner_init, ff_c_inner_init in T.grid(8, 7):\n                conv1d_ncw_local_1[nn_c_outer_outer_inner * 168 + ff_c_outer_inner_init * 21 + ff_c_inner_init * 3 + yy_c_outer_outer_inner] = T.float32(0)\n            for rc_outer, ry_outer, ff_c_outer_inner, rc_inner, ff_c_inner in T.grid(3, 3, 8, 4, 7):\n                cse_var_1: T.int32 = nn_c_outer_outer_inner * 168 + ff_c_outer_inner * 21 + ff_c_inner * 3 + yy_c_outer_outer_inner\n                data_1 = T.Buffer((120,), data=data.data)\n                kernel_1 = T.Buffer((2016,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_c_outer_outer_inner * 60 + rc_outer * 20 + rc_inner * 5 + yy_c_outer_outer_inner + ry_outer] * kernel_1[ff_c_outer_inner * 252 + ff_c_inner * 36 + rc_outer * 12 + rc_inner * 3 + ry_outer]\n        for nn_inner, ff_inner in T.grid(2, 56):\n            cse_var_2: T.int32 = nn_inner * 168 + ff_inner * 3\n            conv1d_ncw_1 = T.Buffer((336,), data=conv1d_ncw.data)\n            conv1d_ncw_1[cse_var_2:cse_var_2 + 3] = conv1d_ncw_local_1[cse_var_2:cse_var_2 + 3]",
        "data": "2_12_5",
        "kernel": "56_12_3"
    },
    {
        "op_name": "group_conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 156; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)48, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 4; ++ff_c_outer_inner_init) {\n      for (int32_t nn_c_inner_init = 0; nn_c_inner_init < 3; ++nn_c_inner_init) {\n        ((float*)conv1d_ncw_local)[((nn_c_inner_init * 4) + ff_c_outer_inner_init)] = 0.000000e+00f;\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 45; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n        for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 4; ++ff_c_outer_inner) {\n          for (int32_t nn_c_inner = 0; nn_c_inner < 3; ++nn_c_inner) {\n            int32_t cse_var_1 = ((nn_c_inner * 4) + ff_c_outer_inner);\n            ((float*)conv1d_ncw_local)[cse_var_1] = (((float*)conv1d_ncw_local)[cse_var_1] + (((float*)data_1)[((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 3) * 810) + (nn_c_inner * 270)) + (rc_outer * 6)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 12) / 3)) + ry_outer)] * ((float*)kernel_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 12) * 540) + (ff_c_outer_inner * 135)) + (rc_outer * 3)) + ry_outer)]));\n          },\n        },\n      },\n    },\n    for (int32_t nn_inner = 0; nn_inner < 3; ++nn_inner) {\n      for (int32_t ff_inner = 0; ff_inner < 4; ++ff_inner) {\n        ((float*)conv1d_ncw_1)[((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 3) * 624) + (nn_inner * 208)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 12) * 16)) + (ff_inner * 4)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 12) / 3))] = ((float*)conv1d_ncw_local)[((nn_inner * 4) + ff_inner)];\n      },\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(52) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(52) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[6];\n  __shared__ float pad_temp_shared[180];\n  __shared__ float kernel_shared[390];\n  for (int ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 2; ++ff_c_outer_inner_init) {\n    conv1d_ncw_local[ff_c_outer_inner_init] = 0.000000e+00f;\n    conv1d_ncw_local[(ff_c_outer_inner_init + 2)] = 0.000000e+00f;\n    conv1d_ncw_local[(ff_c_outer_inner_init + 4)] = 0.000000e+00f;\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 3; ++rc_outer_outer) {\n    for (int ry_outer_outer = 0; ry_outer_outer < 3; ++ry_outer_outer) {\n      __syncthreads();\n      for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 4; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n        if (((ax0_ax1_fused_ax2_fused_outer_outer * 13) + (((int)threadIdx.x) >> 2)) < 45) {\n          pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 52) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) >> 1) * 810) + ((((ax0_ax1_fused_ax2_fused_outer_outer * 13) + (((int)threadIdx.x) >> 2)) / 15) * 270)) + (rc_outer_outer * 90)) + ((((ax0_ax1_fused_ax2_fused_outer_outer * 13) + (((int)threadIdx.x) >> 2)) % 15) * 6)) + ry_outer_outer) + (((int)threadIdx.x) & 3))];\n        },\n      },\n      for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 8; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n        if (((ax0_ax1_fused_ax2_fused_outer_outer_1 * 2) + (((int)threadIdx.x) / 26)) < 15) {\n          kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 52) + ((int)threadIdx.x))] = kernel[((((((((int)blockIdx.x) & 1) * 3510) + ((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 52) + ((int)threadIdx.x)) / 15) * 135)) + (rc_outer_outer * 45)) + ((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 7) + ((int)threadIdx.x)) % 15) * 3)) + ry_outer_outer)];\n        },\n      },\n      __syncthreads();\n      for (int rc_outer_inner = 0; rc_outer_inner < 5; ++rc_outer_inner) {\n        for (int ff_c_outer_inner = 0; ff_c_outer_inner < 2; ++ff_c_outer_inner) {\n          for (int rc_inner = 0; rc_inner < 3; ++rc_inner) {\n            conv1d_ncw_local[ff_c_outer_inner] = (conv1d_ncw_local[ff_c_outer_inner] + (pad_temp_shared[(((rc_outer_inner * 12) + (rc_inner * 4)) + (((int)threadIdx.x) & 3))] * kernel_shared[(((((((int)threadIdx.x) >> 2) * 30) + (ff_c_outer_inner * 15)) + (rc_outer_inner * 3)) + rc_inner)]));\n            conv1d_ncw_local[(ff_c_outer_inner + 2)] = (conv1d_ncw_local[(ff_c_outer_inner + 2)] + (pad_temp_shared[((((rc_outer_inner * 12) + (rc_inner * 4)) + (((int)threadIdx.x) & 3)) + 60)] * kernel_shared[(((((((int)threadIdx.x) >> 2) * 30) + (ff_c_outer_inner * 15)) + (rc_outer_inner * 3)) + rc_inner)]));\n            conv1d_ncw_local[(ff_c_outer_inner + 4)] = (conv1d_ncw_local[(ff_c_outer_inner + 4)] + (pad_temp_shared[((((rc_outer_inner * 12) + (rc_inner * 4)) + (((int)threadIdx.x) & 3)) + 120)] * kernel_shared[(((((((int)threadIdx.x) >> 2) * 30) + (ff_c_outer_inner * 15)) + (rc_outer_inner * 3)) + rc_inner)]));\n          },\n        },\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 2; ++ff_inner) {\n    conv1d_ncw[((((((((int)blockIdx.x) >> 1) * 624) + ((((int)blockIdx.x) & 1) * 104)) + ((((int)threadIdx.x) >> 2) * 8)) + (ff_inner * 4)) + (((int)threadIdx.x) & 3))] = conv1d_ncw_local[ff_inner];\n    conv1d_ncw[(((((((((int)blockIdx.x) >> 1) * 624) + ((((int)blockIdx.x) & 1) * 104)) + ((((int)threadIdx.x) >> 2) * 8)) + (ff_inner * 4)) + (((int)threadIdx.x) & 3)) + 208)] = conv1d_ncw_local[(ff_inner + 2)];\n    conv1d_ncw[(((((((((int)blockIdx.x) >> 1) * 624) + ((((int)blockIdx.x) & 1) * 104)) + ((((int)threadIdx.x) >> 2) * 8)) + (ff_inner * 4)) + (((int)threadIdx.x) & 3)) + 416)] = conv1d_ncw_local[(ff_inner + 4)];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 45, 6), \"float32\"), kernel: T.Buffer((52, 45, 3), \"float32\"), conv1d_ncw: T.Buffer((9, 52, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(156):\n            conv1d_ncw_local = T.allocate([12], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((12,), data=conv1d_ncw_local, scope=\"local\", align=32)\n            for ff_c_outer_inner_init, nn_c_inner_init in T.grid(4, 3):\n                conv1d_ncw_local_1[nn_c_inner_init * 4 + ff_c_outer_inner_init] = T.float32(0)\n            for rc_outer, ry_outer, ff_c_outer_inner, nn_c_inner in T.grid(45, 3, 4, 3):\n                cse_var_1: T.int32 = nn_c_inner * 4 + ff_c_outer_inner\n                data_1 = T.Buffer((2430,), data=data.data)\n                kernel_1 = T.Buffer((7020,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 3 * 810 + nn_c_inner * 270 + rc_outer * 6 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 12 // 3 + ry_outer] * kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 12 * 540 + ff_c_outer_inner * 135 + rc_outer * 3 + ry_outer]\n            for nn_inner, ff_inner in T.grid(3, 4):\n                conv1d_ncw_1 = T.Buffer((1872,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 3 * 624 + nn_inner * 208 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 12 * 16 + ff_inner * 4 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 12 // 3] = conv1d_ncw_local_1[nn_inner * 4 + ff_inner]",
        "data": "9_45_6",
        "kernel": "52_45_3"
    },
    {
        "op_name": "group_conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 372; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)12, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    ((float3*)conv1d_ncw_local)[0] = ((float3)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n    for (int32_t rc_outer = 0; rc_outer < 2; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n        for (int32_t rc_inner = 0; rc_inner < 31; ++rc_inner) {\n          int32_t3 v_ = int32_t3(((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 6) >> 1) * 496) + (rc_outer * 248)) + (rc_inner * 8)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 1) * 3)) + ry_outer))+(1*0), ((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 6) >> 1) * 496) + (rc_outer * 248)) + (rc_inner * 8)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 1) * 3)) + ry_outer))+(1*1), ((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 6) >> 1) * 496) + (rc_outer * 248)) + (rc_inner * 8)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 1) * 3)) + ry_outer))+(1*2));\n          ((float3*)conv1d_ncw_local)[0] = (((float3*)conv1d_ncw_local)[0] + ((float3(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2])) * ((float3)(((float*)kernel_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 6) * 186) + (rc_outer * 93)) + (rc_inner * 3)) + ry_outer)], ((float*)kernel_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 6) * 186) + (rc_outer * 93)) + (rc_inner * 3)) + ry_outer)], ((float*)kernel_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 6) * 186) + (rc_outer * 93)) + (rc_inner * 3)) + ry_outer)]))));\n        },\n      },\n    },\n    *(float3*)(((float*)conv1d_ncw_1) + (((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 6) >> 1) * 372) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 6) * 6)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 1) * 3))) = ((float3*)conv1d_ncw_local)[0];\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(558) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(558) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[1];\n  __shared__ float pad_temp_shared[1488];\n  __shared__ float kernel_shared[5766];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 3; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 3) + (((int)threadIdx.x) / 186)) < 8) {\n      pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 558) + ((int)threadIdx.x))] = data[((ax0_ax1_fused_ax2_fused_outer_outer * 558) + ((int)threadIdx.x))];\n    },\n  },\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 11; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer_1 * 3) + (((int)threadIdx.x) / 186)) < 31) {\n      kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 558) + ((int)threadIdx.x))] = kernel[(((((int)blockIdx.x) * 5766) + (ax0_ax1_fused_ax2_fused_outer_outer_1 * 558)) + ((int)threadIdx.x))];\n    },\n  },\n  __syncthreads();\n  for (int rc_inner = 0; rc_inner < 62; ++rc_inner) {\n    for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n      conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[(((((((int)threadIdx.x) / 186) * 496) + (rc_inner * 8)) + ry_inner) + (((int)threadIdx.x) % 6))] * kernel_shared[(((((((int)threadIdx.x) % 186) / 6) * 186) + (rc_inner * 3)) + ry_inner)]));\n    },\n  },\n  conv1d_ncw[((((((int)threadIdx.x) / 186) * 372) + (((int)blockIdx.x) * 186)) + (((int)threadIdx.x) % 186))] = conv1d_ncw_local[0];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 62, 8), \"float32\"), kernel: T.Buffer((62, 62, 3), \"float32\"), conv1d_ncw: T.Buffer((3, 62, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(372):\n            conv1d_ncw_local = T.allocate([1], \"float32x3\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((1,), \"float32x3\", data=conv1d_ncw_local, scope=\"local\", align=8)\n            conv1d_ncw_local_1[0] = T.Broadcast(T.float32(0), 3)\n            for rc_outer, ry_outer, rc_inner in T.grid(2, 3, 31):\n                data_1 = T.Buffer((1488,), data=data.data)\n                kernel_1 = T.Buffer((11532,), data=kernel.data)\n                conv1d_ncw_local_1[0] = conv1d_ncw_local_1[0] + data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 6 // 2 * 496 + rc_outer * 248 + rc_inner * 8 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 2 * 3 + ry_outer:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 6 // 2 * 496 + rc_outer * 248 + rc_inner * 8 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 2 * 3 + ry_outer + 3] * T.Broadcast(kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 6 * 186 + rc_outer * 93 + rc_inner * 3 + ry_outer], 3)\n            conv1d_ncw_1 = T.Buffer((1116,), data=conv1d_ncw.data)\n            conv1d_ncw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 6 // 2 * 372 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 6 * 6 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 2 * 3:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 6 // 2 * 372 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 6 * 6 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 2 * 3 + 3] = conv1d_ncw_local_1[0]",
        "data": "3_62_8",
        "kernel": "62_62_3"
    },
    {
        "op_name": "group_conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 8; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)136, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t yy_c_outer_inner_init = 0; yy_c_outer_inner_init < 2; ++yy_c_outer_inner_init) {\n      for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 17; ++ff_c_inner_init) {\n        ((float*)conv1d_ncw_local)[((ff_c_inner_init * 2) + yy_c_outer_inner_init)] = 0.000000e+00f;\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 74; ++rc_outer) {\n      for (int32_t yy_c_outer_inner = 0; yy_c_outer_inner < 2; ++yy_c_outer_inner) {\n        for (int32_t ry_inner = 0; ry_inner < 3; ++ry_inner) {\n          for (int32_t ff_c_inner = 0; ff_c_inner < 17; ++ff_c_inner) {\n            int32_t cse_var_1 = ((ff_c_inner * 2) + yy_c_outer_inner);\n            ((float*)conv1d_ncw_local)[cse_var_1] = (((float*)conv1d_ncw_local)[cse_var_1] + (((float*)data_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused >> 2) * 296) + (rc_outer * 4)) + yy_c_outer_inner) + ry_inner)] * ((float*)kernel_1)[(((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused & 3) * 3774) + (ff_c_inner * 222)) + (rc_outer * 3)) + ry_inner)]));\n          },\n        },\n      },\n    },\n    for (int32_t ff_inner = 0; ff_inner < 17; ++ff_inner) {\n      int32_t cse_var_2 = (ff_inner * 2);\n      int32_t2 v_ = int32_t2((cse_var_2)+(1*0), (cse_var_2)+(1*1));\n      *(float2*)(((float*)conv1d_ncw_1) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 34) + cse_var_2)) = (float2(((float*)conv1d_ncw_local)[v_.s0],((float*)conv1d_ncw_local)[v_.s1]));\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(34) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(34) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[1];\n  __shared__ float pad_temp_shared[74];\n  __shared__ float kernel_shared[2516];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  for (int ry_outer_outer = 0; ry_outer_outer < 3; ++ry_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 3; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer * 17) + (((int)threadIdx.x) >> 1)) < 37) {\n        pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 34) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) >> 2) * 296) + (ax0_ax1_fused_ax2_fused_outer_outer * 136)) + (((int)threadIdx.x) * 4)) + ry_outer_outer) + (((int)blockIdx.x) & 1))];\n      },\n    },\n    for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 37; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n      int2 v_ = make_int2((((((((((int)blockIdx.x) & 3) >> 1) * 7548) + (ax0_ax1_fused_ax2_fused_outer_outer_1 * 204)) + (((int)threadIdx.x) * 6)) + ry_outer_outer))+(3*0), (((((((((int)blockIdx.x) & 3) >> 1) * 7548) + (ax0_ax1_fused_ax2_fused_outer_outer_1 * 204)) + (((int)threadIdx.x) * 6)) + ry_outer_outer))+(3*1));\n      *(float2*)(kernel_shared + ((ax0_ax1_fused_ax2_fused_outer_outer_1 * 68) + (((int)threadIdx.x) * 2))) = make_float2(kernel[v_.x],kernel[v_.y]);\n    },\n    __syncthreads();\n    for (int rc_outer_inner = 0; rc_outer_inner < 74; ++rc_outer_inner) {\n      conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[rc_outer_inner] * kernel_shared[((((int)threadIdx.x) * 74) + rc_outer_inner)]));\n    },\n  },\n  conv1d_ncw[((((((int)blockIdx.x) >> 1) * 68) + (((int)threadIdx.x) * 2)) + (((int)blockIdx.x) & 1))] = conv1d_ncw_local[0];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 74, 4), \"float32\"), kernel: T.Buffer((68, 74, 3), \"float32\"), conv1d_ncw: T.Buffer((2, 68, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(8):\n            conv1d_ncw_local = T.allocate([34], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((34,), data=conv1d_ncw_local, scope=\"local\")\n            for yy_c_outer_inner_init, ff_c_inner_init in T.grid(2, 17):\n                conv1d_ncw_local_1[ff_c_inner_init * 2 + yy_c_outer_inner_init] = T.float32(0)\n            for rc_outer, yy_c_outer_inner, ry_inner, ff_c_inner in T.grid(74, 2, 3, 17):\n                cse_var_1: T.int32 = ff_c_inner * 2 + yy_c_outer_inner\n                data_1 = T.Buffer((592,), data=data.data)\n                kernel_1 = T.Buffer((15096,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1] = conv1d_ncw_local_1[cse_var_1] + data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 4 * 296 + rc_outer * 4 + yy_c_outer_inner + ry_inner] * kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 4 * 3774 + ff_c_inner * 222 + rc_outer * 3 + ry_inner]\n            for ff_inner in range(17):\n                cse_var_2: T.int32 = ff_inner * 2\n                conv1d_ncw_1 = T.Buffer((272,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 34 + cse_var_2:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 34 + cse_var_2 + 2] = conv1d_ncw_local_1[cse_var_2:cse_var_2 + 2]",
        "data": "2_74_4",
        "kernel": "68_74_3"
    },
    {
        "op_name": "group_conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 92; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)8, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t ff_c_inner_init = 0; ff_c_inner_init < 2; ++ff_c_inner_init) {\n      ((float*)conv1d_ncw_local)[ff_c_inner_init] = 0.000000e+00f;\n    },\n    for (int32_t rc_outer = 0; rc_outer < 28; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 4; ++ry_outer) {\n        for (int32_t rc_inner = 0; rc_inner < 2; ++rc_inner) {\n          for (int32_t ff_c_inner = 0; ff_c_inner < 2; ++ff_c_inner) {\n            int32_t cse_var_2 = (rc_outer * 8);\n            int32_t cse_var_1 = (rc_inner * 4);\n            ((float*)conv1d_ncw_local)[ff_c_inner] = (((float*)conv1d_ncw_local)[ff_c_inner] + (((float*)data_1)[((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 46) / 23) * 224) + cse_var_2) + cse_var_1) + ry_outer)] * ((float*)kernel_1)[(((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 46) * 10304) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 23) * 448)) + (ff_c_inner * 224)) + cse_var_2) + cse_var_1) + ry_outer)]));\n          },\n        },\n      },\n    },\n    for (int32_t ff_inner = 0; ff_inner < 2; ++ff_inner) {\n      ((float*)conv1d_ncw_1)[((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 46) / 23) * 92) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 46) * 46)) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 23) * 2)) + ff_inner)] = ((float*)conv1d_ncw_local)[ff_inner];\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(92) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(92) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[1];\n  __shared__ float pad_temp_shared[4];\n  __shared__ float kernel_shared[92];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  for (int rc_outer_outer = 0; rc_outer_outer < 28; ++rc_outer_outer) {\n    for (int ry_outer_outer = 0; ry_outer_outer < 4; ++ry_outer_outer) {\n      __syncthreads();\n      if (((int)threadIdx.x) < 4) {\n        pad_temp_shared[((int)threadIdx.x)] = data[(((((((int)threadIdx.x) >> 1) * 224) + (rc_outer_outer * 8)) + ((((int)threadIdx.x) & 1) * 4)) + ry_outer_outer)];\n      },\n      kernel_shared[((int)threadIdx.x)] = kernel[(((((((int)blockIdx.x) * 10304) + ((((int)threadIdx.x) >> 1) * 224)) + (rc_outer_outer * 8)) + ((((int)threadIdx.x) & 1) * 4)) + ry_outer_outer)];\n      __syncthreads();\n      for (int rc_outer_inner = 0; rc_outer_inner < 2; ++rc_outer_inner) {\n        conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[(((((int)threadIdx.x) / 46) * 2) + rc_outer_inner)] * kernel_shared[(((((int)threadIdx.x) % 46) * 2) + rc_outer_inner)]));\n      },\n    },\n  },\n  conv1d_ncw[((((((int)threadIdx.x) / 46) * 92) + (((int)blockIdx.x) * 46)) + (((int)threadIdx.x) % 46))] = conv1d_ncw_local[0];\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 56, 4), \"float32\"), kernel: T.Buffer((92, 56, 4), \"float32\"), conv1d_ncw: T.Buffer((2, 92, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(92):\n            conv1d_ncw_local = T.allocate([2], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((2,), data=conv1d_ncw_local, scope=\"local\", align=8)\n            for ff_c_inner_init in range(2):\n                conv1d_ncw_local_1[ff_c_inner_init] = T.float32(0)\n            for rc_outer, ry_outer, rc_inner, ff_c_inner in T.grid(28, 4, 2, 2):\n                cse_var_2: T.int32 = rc_outer * 8\n                cse_var_1: T.int32 = rc_inner * 4\n                data_1 = T.Buffer((448,), data=data.data)\n                kernel_1 = T.Buffer((20608,), data=kernel.data)\n                conv1d_ncw_local_1[ff_c_inner] = conv1d_ncw_local_1[ff_c_inner] + data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 46 // 23 * 224 + cse_var_2 + cse_var_1 + ry_outer] * kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 46 * 10304 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 23 * 448 + ff_c_inner * 224 + cse_var_2 + cse_var_1 + ry_outer]\n            for ff_inner in range(2):\n                conv1d_ncw_1 = T.Buffer((184,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 46 // 23 * 92 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 46 * 46 + nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 23 * 2 + ff_inner] = conv1d_ncw_local_1[ff_inner]",
        "data": "2_56_4",
        "kernel": "92_56_4"
    },
    {
        "op_name": "group_conv1d_ncw",
        "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  },\n  if (!(default_function_kernel_strides == NULL)) {\n  },\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  },\n  for (int32_t nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused = 0; nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused < 126; ++nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused) {\n    void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)72, 2, 32);\n    if (conv1d_ncw_local == NULL) {\n      return -1;\n    },\n    for (int32_t ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 3; ++ff_c_outer_inner_init) {\n      for (int32_t yy_c_outer_inner_init = 0; yy_c_outer_inner_init < 2; ++yy_c_outer_inner_init) {\n        *(float3*)(((float*)conv1d_ncw_local) + ((ff_c_outer_inner_init * 6) + (yy_c_outer_inner_init * 3))) = ((float3)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f));\n      },\n    },\n    for (int32_t rc_outer = 0; rc_outer < 19; ++rc_outer) {\n      for (int32_t ry_outer = 0; ry_outer < 2; ++ry_outer) {\n        for (int32_t ff_c_outer_inner = 0; ff_c_outer_inner < 3; ++ff_c_outer_inner) {\n          for (int32_t yy_c_outer_inner = 0; yy_c_outer_inner < 2; ++yy_c_outer_inner) {\n            for (int32_t rc_inner = 0; rc_inner < 5; ++rc_inner) {\n              for (int32_t ry_inner = 0; ry_inner < 2; ++ry_inner) {\n                int32_t cse_var_3 = (yy_c_outer_inner * 3);\n                int32_t cse_var_2 = (ry_outer * 2);\n                int32_t cse_var_1 = ((ff_c_outer_inner * 6) + cse_var_3);\n                int32_t3 v_ = int32_t3((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2));\n                int32_t3 v__1 = int32_t3(((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 18) * 855) + (rc_outer * 45)) + (rc_inner * 9)) + cse_var_3) + cse_var_2) + ry_inner))+(1*0), ((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 18) * 855) + (rc_outer * 45)) + (rc_inner * 9)) + cse_var_3) + cse_var_2) + ry_inner))+(1*1), ((((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused / 18) * 855) + (rc_outer * 45)) + (rc_inner * 9)) + cse_var_3) + cse_var_2) + ry_inner))+(1*2));\n                *(float3*)(((float*)conv1d_ncw_local) + cse_var_1) = ((float3(((float*)conv1d_ncw_local)[v_.s0],((float*)conv1d_ncw_local)[v_.s1],((float*)conv1d_ncw_local)[v_.s2])) + ((float3(((float*)data_1)[v__1.s0],((float*)data_1)[v__1.s1],((float*)data_1)[v__1.s2])) * ((float3)(((float*)kernel_1)[(((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 18) * 1140) + (ff_c_outer_inner * 380)) + (rc_outer * 20)) + (rc_inner * 4)) + cse_var_2) + ry_inner)], ((float*)kernel_1)[(((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 18) * 1140) + (ff_c_outer_inner * 380)) + (rc_outer * 20)) + (rc_inner * 4)) + cse_var_2) + ry_inner)], ((float*)kernel_1)[(((((((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 18) * 1140) + (ff_c_outer_inner * 380)) + (rc_outer * 20)) + (rc_inner * 4)) + cse_var_2) + ry_inner)]))));\n              },\n            },\n          },\n        },\n      },\n    },\n    for (int32_t ff_inner = 0; ff_inner < 3; ++ff_inner) {\n      int32_t cse_var_4 = (ff_inner * 6);\n      int32_t6 v__2 = int32_t6((cse_var_4)+(1*0), (cse_var_4)+(1*1), (cse_var_4)+(1*2), (cse_var_4)+(1*3), (cse_var_4)+(1*4), (cse_var_4)+(1*5));\n      *(float6*)(((float*)conv1d_ncw_1) + ((nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 18) + cse_var_4)) = (float6(((float*)conv1d_ncw_local)[v__2.s0],((float*)conv1d_ncw_local)[v__2.s1],((float*)conv1d_ncw_local)[v__2.s2],((float*)conv1d_ncw_local)[v__2.s3],((float*)conv1d_ncw_local)[v__2.s4],((float*)conv1d_ncw_local)[v__2.s5]));\n    },\n    if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n      return -1;\n    },\n  },\n  return 0;\n},\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n},\n",
        "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[18];\n  __shared__ float pad_temp_shared[30];\n  __shared__ float kernel_shared[270];\n  for (int ff_c_outer_inner_init = 0; ff_c_outer_inner_init < 3; ++ff_c_outer_inner_init) {\n    for (int ff_c_inner_init = 0; ff_c_inner_init < 2; ++ff_c_inner_init) {\n      conv1d_ncw_local[((ff_c_outer_inner_init * 2) + ff_c_inner_init)] = 0.000000e+00f;\n      conv1d_ncw_local[(((ff_c_outer_inner_init * 2) + ff_c_inner_init) + 6)] = 0.000000e+00f;\n      conv1d_ncw_local[(((ff_c_outer_inner_init * 2) + ff_c_inner_init) + 12)] = 0.000000e+00f;\n    },\n  },\n  for (int rc_outer_outer = 0; rc_outer_outer < 19; ++rc_outer_outer) {\n    for (int ry_outer_outer = 0; ry_outer_outer < 4; ++ry_outer_outer) {\n      __syncthreads();\n      for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n        if (((ax0_ax1_fused_ax2_fused_outer_outer * 3) + (((int)threadIdx.x) / 6)) < 5) {\n          pad_temp_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 18) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) * 855) + (rc_outer_outer * 45)) + (ax0_ax1_fused_ax2_fused_outer_outer * 27)) + ((((int)threadIdx.x) / 6) * 9)) + ry_outer_outer) + (((int)threadIdx.x) % 6))];\n        },\n      },\n      for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 15; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n        kernel_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 18) + ((int)threadIdx.x))] = kernel[(((((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 18) + ((int)threadIdx.x)) / 5) * 380) + (rc_outer_outer * 20)) + ((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 3) + ((int)threadIdx.x)) % 5) * 4)) + ry_outer_outer)];\n      },\n      __syncthreads();\n      for (int ff_c_outer_inner = 0; ff_c_outer_inner < 3; ++ff_c_outer_inner) {\n        for (int rc_inner = 0; rc_inner < 5; ++rc_inner) {\n          for (int ff_c_inner = 0; ff_c_inner < 2; ++ff_c_inner) {\n            conv1d_ncw_local[((ff_c_outer_inner * 2) + ff_c_inner)] = (conv1d_ncw_local[((ff_c_outer_inner * 2) + ff_c_inner)] + (pad_temp_shared[((rc_inner * 6) + (((int)threadIdx.x) & 1))] * kernel_shared[(((((((int)threadIdx.x) >> 1) * 30) + (ff_c_outer_inner * 10)) + (ff_c_inner * 5)) + rc_inner)]));\n            conv1d_ncw_local[(((ff_c_outer_inner * 2) + ff_c_inner) + 6)] = (conv1d_ncw_local[(((ff_c_outer_inner * 2) + ff_c_inner) + 6)] + (pad_temp_shared[(((rc_inner * 6) + (((int)threadIdx.x) & 1)) + 2)] * kernel_shared[(((((((int)threadIdx.x) >> 1) * 30) + (ff_c_outer_inner * 10)) + (ff_c_inner * 5)) + rc_inner)]));\n            conv1d_ncw_local[(((ff_c_outer_inner * 2) + ff_c_inner) + 12)] = (conv1d_ncw_local[(((ff_c_outer_inner * 2) + ff_c_inner) + 12)] + (pad_temp_shared[(((rc_inner * 6) + (((int)threadIdx.x) & 1)) + 4)] * kernel_shared[(((((((int)threadIdx.x) >> 1) * 30) + (ff_c_outer_inner * 10)) + (ff_c_inner * 5)) + rc_inner)]));\n          },\n        },\n      },\n    },\n  },\n  for (int ff_inner = 0; ff_inner < 6; ++ff_inner) {\n    conv1d_ncw[((((((int)blockIdx.x) * 324) + ((((int)threadIdx.x) >> 1) * 36)) + (ff_inner * 6)) + (((int)threadIdx.x) & 1))] = conv1d_ncw_local[ff_inner];\n    conv1d_ncw[(((((((int)blockIdx.x) * 324) + ((((int)threadIdx.x) >> 1) * 36)) + (ff_inner * 6)) + (((int)threadIdx.x) & 1)) + 2)] = conv1d_ncw_local[(ff_inner + 6)];\n    conv1d_ncw[(((((((int)blockIdx.x) * 324) + ((((int)threadIdx.x) >> 1) * 36)) + (ff_inner * 6)) + (((int)threadIdx.x) & 1)) + 4)] = conv1d_ncw_local[(ff_inner + 12)];\n  },\n},\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 95, 9), \"float32\"), kernel: T.Buffer((54, 95, 4), \"float32\"), conv1d_ncw: T.Buffer((7, 54, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)},)\n        for nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused in T.parallel(126):\n            conv1d_ncw_local = T.allocate([18], \"float32\", \"local\")\n            conv1d_ncw_local_1 = T.Buffer((18,), data=conv1d_ncw_local, scope=\"local\")\n            for ff_c_outer_inner_init, yy_c_outer_inner_init in T.grid(3, 2):\n                conv1d_ncw_local_1[ff_c_outer_inner_init * 6 + yy_c_outer_inner_init * 3:ff_c_outer_inner_init * 6 + yy_c_outer_inner_init * 3 + 3] = T.Broadcast(T.float32(0), 3)\n            for rc_outer, ry_outer, ff_c_outer_inner, yy_c_outer_inner, rc_inner, ry_inner in T.grid(19, 2, 3, 2, 5, 2):\n                cse_var_3: T.int32 = yy_c_outer_inner * 3\n                cse_var_2: T.int32 = ry_outer * 2\n                cse_var_1: T.int32 = ff_c_outer_inner * 6 + cse_var_3\n                data_1 = T.Buffer((5985,), data=data.data)\n                kernel_1 = T.Buffer((20520,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1:cse_var_1 + 3] = conv1d_ncw_local_1[cse_var_1:cse_var_1 + 3] + data_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 18 * 855 + rc_outer * 45 + rc_inner * 9 + cse_var_3 + cse_var_2 + ry_inner:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused // 18 * 855 + rc_outer * 45 + rc_inner * 9 + cse_var_3 + cse_var_2 + ry_inner + 3] * T.Broadcast(kernel_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused % 18 * 1140 + ff_c_outer_inner * 380 + rc_outer * 20 + rc_inner * 4 + cse_var_2 + ry_inner], 3)\n            for ff_inner in range(3):\n                cse_var_4: T.int32 = ff_inner * 6\n                conv1d_ncw_1 = T.Buffer((2268,), data=conv1d_ncw.data)\n                conv1d_ncw_1[nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 18 + cse_var_4:nn_outer_outer_ff_outer_outer_fused_yy_outer_outer_fused_nn_outer_inner_fused_ff_outer_inner_fused_yy_outer_inner_fused * 18 + cse_var_4 + 6] = conv1d_ncw_local_1[cse_var_4:cse_var_4 + 6]",
        "data": "7_95_9",
        "kernel": "54_95_4"
    }
]